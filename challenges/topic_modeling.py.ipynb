{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Initialization"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "import unicodedata\n",
    "from gensim import corpora\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import *\n",
    "from gensim.models.wrappers import LdaMallet\n",
    "from gensim.models import CoherenceModel\n",
    "import pyLDAvis.gensim\n",
    "import json\n",
    "import html\n",
    "import warnings\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from openpyxl import load_workbook\n",
    "warnings.filterwarnings('ignore')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "def convert(o):\n",
    "    if isinstance(o, np.int64): return int(o)\n",
    "    raise TypeError\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # remove non ascii\n",
    "    text = unicodedata.normalize('NFKD', text).encode(\n",
    "        'ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    text = text.lower()\n",
    "    # unescaping\n",
    "    text = html.unescape(text)\n",
    "    text = re.sub(r\"```[^\\S\\r\\n]*[a-z]*\\n.*?\\n```\", '', text, 0, re.DOTALL) # removing code segment\n",
    "    text = re.sub(r'<code>(.|\\n)*?</code>','', text) # removing <code>...</code>\n",
    "    text = re.sub(r'<a.*?</a>', '', text)  # removing whole anchor tags\n",
    "    text = re.sub(r'(<.*?>)', '', text)  # removing html markup\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # removing punctuation\n",
    "    text = re.sub('_', ' ', text)  # updating hyphen with space\n",
    "    text = re.sub(r'[\\d]', '', text)  # removing digits\n",
    "    # remove stopwords\n",
    "    tokenized = []\n",
    "    for word in text.split():\n",
    "        if word in stop_words_set:\n",
    "            continue\n",
    "        tokenized.append(word)\n",
    "    for i in range(len(tokenized)):\n",
    "        word = tokenized[i]\n",
    "        word = WordNetLemmatizer().lemmatize(word, pos='v')\n",
    "        tokenized[i] = stemmer.stem(word)\n",
    "        # tokenized[i] = word\n",
    "    return tokenized\n",
    "\n",
    "def create_dir(parent_dir, dir_name):\n",
    "    temp = os.path.join(parent_dir,dir_name)\n",
    "    try:\n",
    "        os.makedirs(temp)\n",
    "    except OSError as error:\n",
    "        print(error)\n",
    "        # pass\n",
    "    return temp"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "pyLDAvis.enable_notebook()\n",
    "os.environ.update({'MALLET_HOME':'/home/ajoy.das/bin/Mallet'})\n",
    "mallet_path = '/home/ajoy.das/bin/Mallet/bin/mallet'\n",
    "TOPIC_DIR = '/tmp/issues'\n",
    "RES_DIR = create_dir(TOPIC_DIR, 'Topic Modeling Results')\n",
    "CURR_DIR = os.getcwd()\n",
    "coherence_scores = []"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# stop words set\n",
    "STOP_WORDS_FILES = [CURR_DIR + '/mallet_stop_words.txt', CURR_DIR + '/custom_stop_words.txt']\n",
    "stop_words_set = set()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "for word in stop_words:\n",
    "    if('\\'' in word):\n",
    "        stop_words_set.add(word.strip().replace('\\'', ''))\n",
    "    stop_words_set.add(word)\n",
    "for swfile in STOP_WORDS_FILES:\n",
    "    try:\n",
    "        with open(swfile, 'r') as f:\n",
    "            words = f.readlines()\n",
    "            for word in words:\n",
    "                if('\\'' in word):\n",
    "                    stop_words_set.add(word.strip().replace('\\'', ''))\n",
    "                stop_words_set.add(word.strip())\n",
    "    except:\n",
    "        print('STOP_WORDS_FILES not found.')\n",
    "        # pass"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preprocessing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ajoy.das/projects/bias_study_updated/replica_package_generate/replication_package/challenges\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "                               repo_url   type          id  \\\n0  https://github.com/Trusted-AI/AIF360  issue  1106204177   \n1  https://github.com/Trusted-AI/AIF360  issue  1098675380   \n2  https://github.com/Trusted-AI/AIF360  issue  1098612148   \n3  https://github.com/Trusted-AI/AIF360  issue  1097184512   \n4  https://github.com/Trusted-AI/AIF360  issue  1092070135   \n\n                                                text           created_at  \\\n0  Debiasing: # of layers and predicted probabili...  2022-01-17 19:41:27   \n1  Query regarding debiased model saving in Adver...  2022-01-11 05:40:35   \n2  Query regarding COMPAS dataset Hi, \\r\\nI am wo...  2022-01-11 03:32:35   \n3                            Pytorch Why no pytorch?  2022-01-09 12:35:09   \n4  ValueError: at least one array or dtype is req...  2022-01-02 18:03:59   \n\n                                               url  \\\n0  https://github.com/Trusted-AI/AIF360/issues/287   \n1  https://github.com/Trusted-AI/AIF360/issues/286   \n2  https://github.com/Trusted-AI/AIF360/issues/285   \n3  https://github.com/Trusted-AI/AIF360/issues/284   \n4  https://github.com/Trusted-AI/AIF360/issues/283   \n\n                                                 raw  \\\n0  Debiasing: # of layers and predicted probabili...   \n1  Query regarding debiased model saving in Adver...   \n2  Query regarding COMPAS dataset Hi, \\r\\nI am wo...   \n3                            Pytorch Why no pytorch?   \n4  ValueError: at least one array or dtype is req...   \n\n                                        preprocessed  \n0  [debias, layer, predict, probabl, question, ad...  \n1  [queri, debias, model, save, adversari, debias...  \n2  [queri, compa, dataset, work, project, relat, ...  \n3                                 [pytorch, pytorch]  \n4  [valueerror, array, dtype, requir, face, value...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>repo_url</th>\n      <th>type</th>\n      <th>id</th>\n      <th>text</th>\n      <th>created_at</th>\n      <th>url</th>\n      <th>raw</th>\n      <th>preprocessed</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>https://github.com/Trusted-AI/AIF360</td>\n      <td>issue</td>\n      <td>1106204177</td>\n      <td>Debiasing: # of layers and predicted probabili...</td>\n      <td>2022-01-17 19:41:27</td>\n      <td>https://github.com/Trusted-AI/AIF360/issues/287</td>\n      <td>Debiasing: # of layers and predicted probabili...</td>\n      <td>[debias, layer, predict, probabl, question, ad...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>https://github.com/Trusted-AI/AIF360</td>\n      <td>issue</td>\n      <td>1098675380</td>\n      <td>Query regarding debiased model saving in Adver...</td>\n      <td>2022-01-11 05:40:35</td>\n      <td>https://github.com/Trusted-AI/AIF360/issues/286</td>\n      <td>Query regarding debiased model saving in Adver...</td>\n      <td>[queri, debias, model, save, adversari, debias...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>https://github.com/Trusted-AI/AIF360</td>\n      <td>issue</td>\n      <td>1098612148</td>\n      <td>Query regarding COMPAS dataset Hi, \\r\\nI am wo...</td>\n      <td>2022-01-11 03:32:35</td>\n      <td>https://github.com/Trusted-AI/AIF360/issues/285</td>\n      <td>Query regarding COMPAS dataset Hi, \\r\\nI am wo...</td>\n      <td>[queri, compa, dataset, work, project, relat, ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>https://github.com/Trusted-AI/AIF360</td>\n      <td>issue</td>\n      <td>1097184512</td>\n      <td>Pytorch Why no pytorch?</td>\n      <td>2022-01-09 12:35:09</td>\n      <td>https://github.com/Trusted-AI/AIF360/issues/284</td>\n      <td>Pytorch Why no pytorch?</td>\n      <td>[pytorch, pytorch]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>https://github.com/Trusted-AI/AIF360</td>\n      <td>issue</td>\n      <td>1092070135</td>\n      <td>ValueError: at least one array or dtype is req...</td>\n      <td>2022-01-02 18:03:59</td>\n      <td>https://github.com/Trusted-AI/AIF360/issues/283</td>\n      <td>ValueError: at least one array or dtype is req...</td>\n      <td>[valueerror, array, dtype, requir, face, value...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_issue_dis = f'issue_discussions_metadata_2540_discussions.csv'\n",
    "df_issue_dis = pd.read_csv(file_issue_dis)\n",
    "df = df_issue_dis\n",
    "df['raw'] = df['text']\n",
    "df['preprocessed'] = \"\"\n",
    "for i in range(len(df)):\n",
    "    df['preprocessed'].iloc[i] = preprocess_text(df.raw.iloc[i])\n",
    "# df.to_csv('preprocesseedData.csv')\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Finding out optimum topic number"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data = df['preprocessed']\n",
    "dictionary = gensim.corpora.Dictionary(data)\n",
    "corpus = [dictionary.doc2bow(doc) for doc in data]\n",
    "coherence_scores = []\n",
    "for num_topics in tqdm(range(5,51)):\n",
    "    ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=num_topics, id2word=dictionary, alpha=50/num_topics)\n",
    "    coherenceModel = CoherenceModel(model=ldamallet, texts=data, dictionary=dictionary, coherence='c_v')\n",
    "    score = coherenceModel.get_coherence()\n",
    "    coherence_scores.append([num_topics,score])\n",
    "# save scores as csv\n",
    "ch_df = pd.DataFrame(coherence_scores,columns=['Num Topic','Score'])\n",
    "ch_df.to_csv(f'{RES_DIR}/TopicModeling_Coherence_Scores-code-5-50-filtered-issues.csv')\n",
    "# plot\n",
    "plt.xlabel('Number of Topics')\n",
    "plt.ylabel('Coherence Score')\n",
    "x = []\n",
    "y = []\n",
    "for score in coherence_scores:\n",
    "    x.append(score[0])\n",
    "    y.append(score[1])\n",
    "plt.plot(x,y,c='r')\n",
    "plt.gca().set_aspect('auto')\n",
    "plt.grid()\n",
    "plt.savefig(f'{RES_DIR}/scores-code-5-50-filtered-issues.png', dpi=500)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "# !ls '/tmp/issues/Topic Modeling Results/10 Topics'\n",
    "# !cat '/tmp/issues/Topic Modeling Results/TopicModeling_Coherence_Scores-code-5-50-filtered-issues.csv'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Saving found topics"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data = df['preprocessed']\n",
    "dictionary = gensim.corpora.Dictionary(data)\n",
    "corpus = [dictionary.doc2bow(doc) for doc in data]\n",
    "# create folder for topic number\n",
    "NUM_TOPIS = [10, 20, 25, 33]\n",
    "for num_topics in NUM_TOPIS:\n",
    "    topic_dir = create_dir(RES_DIR, f'{num_topics} Topics')\n",
    "    if os.path.isfile(os.path.join(topic_dir, 'ldamallet.pkl')):\n",
    "        ldamallet = pickle.load(\n",
    "            open(os.path.join(topic_dir, 'ldamallet.pkl'), \"rb\"))\n",
    "    else:\n",
    "        ldamallet = gensim.models.wrappers.LdaMallet(\n",
    "            mallet_path, corpus=corpus, num_topics=num_topics, id2word=dictionary, alpha=50 / num_topics)\n",
    "        # save the model as pickle\n",
    "        pickle.dump(ldamallet, open(os.path.join(\n",
    "            topic_dir, 'ldamallet.pkl'), \"wb\"))\n",
    "    topics = [[(word, word_prob) for word, word_prob in ldamallet.show_topic(\n",
    "        n, topn=30)] for n in range(ldamallet.num_topics)]\n",
    "    # term-topic matrix\n",
    "    topics_df = pd.DataFrame([[f'{word} {round(word_prob, 4)}' for word, word_prob in topic] for topic in topics],\n",
    "                             columns=[\n",
    "                                 f'Term {i}' for i in range(1, 31)],\n",
    "                             index=[f'Topic {n}' for n in range(ldamallet.num_topics)]).T\n",
    "    topics_df.to_csv(os.path.join(topic_dir, 'term x topic.csv'))\n",
    "    # topic words\n",
    "    topic_words_dir = create_dir(topic_dir, 'TopicWords')\n",
    "    for n in range(num_topics):\n",
    "        topic_words_df = pd.DataFrame(\n",
    "            [[word_prob, word] for word, word_prob in topics[n]], columns=['Prob', 'Word'])\n",
    "        topic_words_df.to_csv(os.path.join(topic_words_dir, f'{n}.csv'))\n",
    "    # post to dominant topic\n",
    "    corpus_topic_df = pd.DataFrame()\n",
    "    corpus_topic_df['repo_url'] = df['repo_url']\n",
    "    corpus_topic_df['id'] = df['id']\n",
    "    corpus_topic_df['type'] = df['type']\n",
    "    corpus_topic_df['url'] = df['url']\n",
    "    corpus_topic_df['text'] = df['text']\n",
    "    # for i in range(len(corpus_topic_df)):\n",
    "    #     corpus_topic_df.link.iloc[i] = make_link(df.id.iloc[i],df.qa.iloc[i])\n",
    "    topic_model_results = ldamallet[corpus]\n",
    "    corpus_topics = [sorted(doc, key=lambda x: -x[1])[0]\n",
    "                     for doc in topic_model_results]\n",
    "    corpus_topic_df['Dominant Topic'] = [item[0] for item in corpus_topics]\n",
    "    corpus_topic_df['Correlation'] = [item[1] for item in corpus_topics]\n",
    "    corpus_topic_df.to_csv(os.path.join(topic_dir, 'postToTopic.csv'))\n",
    "    topic_to_post_dir = create_dir(topic_dir, 'TopicToPost')\n",
    "\n",
    "    for i in range(num_topics):\n",
    "        temp = create_dir(topic_to_post_dir, str(i))\n",
    "        temp_q_df = corpus_topic_df.loc[corpus_topic_df['Dominant Topic'] == i]\n",
    "        temp_q_df.drop(columns=['Dominant Topic']).to_csv(\n",
    "            os.path.join(temp, 'Comments.csv'), index=False)\n",
    "\n",
    "        topic_comments_xlsx_path = os.path.join(topic_to_post_dir, 'Comments.xlsx')\n",
    "        try:\n",
    "            # book = load_workbook(topic_comments_xlsx_path)\n",
    "            writer = pd.ExcelWriter(topic_comments_xlsx_path, engine='openpyxl', mode='a')\n",
    "            # writer.book = book\n",
    "            # writer.sheets = dict((ws.title, ws) for ws in book.worksheets)\n",
    "        except:\n",
    "            writer = pd.ExcelWriter(topic_comments_xlsx_path, engine='openpyxl')\n",
    "            pass\n",
    "        temp_q_df.drop(columns=['Dominant Topic']).to_excel(\n",
    "            writer, sheet_name='topic_' + str(i), index=False)\n",
    "        writer.save()\n",
    "    # post count under any topic\n",
    "    topic_post_cnt_df = corpus_topic_df.groupby('Dominant Topic').agg(\n",
    "        Document_Count=('Dominant Topic', np.size),\n",
    "        Percentage=('Dominant Topic', np.size)).reset_index()\n",
    "    topic_post_cnt_df['Percentage'] = topic_post_cnt_df['Percentage'].apply(\n",
    "        lambda x: round((x * 100) / len(corpus), 2))\n",
    "    topic_post_cnt_df.to_csv(os.path.join(topic_dir, 'postPerTopic.csv'))\n",
    "    # pyLDAvis\n",
    "    vis = pyLDAvis.gensim.prepare(\n",
    "        gensim.models.wrappers.ldamallet.malletmodel2ldamodel(ldamallet), corpus, dictionary)\n",
    "    pyLDAvis.save_html(vis, os.path.join(topic_dir, f'pyLDAvis-{num_topics}.html'))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
