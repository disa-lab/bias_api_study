"repo_url","id","type","url","text","Dominant Topic","Correlation","topic_label","topic_type"
"https://github.com/Trusted-AI/AIF360","1106204177","issue","https://github.com/Trusted-AI/AIF360/issues/287","Debiasing: # of layers and predicted probabilities Two questions regarding adversarial debiasing in hopes to leverage this package for it:

(1) Is there any easy way to customize the neural network settings to for example, add additional layers to make the prediction more powerful, or to use an optimizer like Adam? 
(2) I am unable to extract predicted probabilities from the model. I see a function called ""predict_proba"" but have had no luck with it. When I run it, I get ""'AdversarialDebiasing' object has no attribute 'predict_proba'"". However, the ""predict' function for class labels works just fine. 

Any help on these? Thanks!","23","0.4952451678115397","Bias mitigation methodology","Design"
"https://github.com/Trusted-AI/AIF360","1098675380","issue","https://github.com/Trusted-AI/AIF360/issues/286","Query regarding debiased model saving in Adversarial Debiasing Hello, is there a way to save and reuse the debiased model created in Adversarial Debiasing? 
This model:
<img width=""479"" alt=""model"" src=""https://user-images.githubusercontent.com/40518134/148887330-6659de20-5f9c-4899-bb65-44c736f75a01.png"">
I have tried pickle , joblib and keras method, none of them seem to work.
<img width=""542"" alt=""tried"" src=""https://user-images.githubusercontent.com/40518134/148887449-ef58577a-4eec-41dd-b5fa-be2877ac59d3.png"">

","22","0.679888951628082","Bias mitigation methodology","Design"
"https://github.com/Trusted-AI/AIF360","1098612148","issue","https://github.com/Trusted-AI/AIF360/issues/285","Query regarding COMPAS dataset Hi, 
I am working on a project related to Fairness Testing. I am using the COMPAS dataset. As per my understanding, the original COMPAS dataset has 52 attributes. However, on referring to your GitHub repository (), a significant number of attributes (41) from the COMPAS dataset while retaining the remaining (11). Could you please explain the reasoning behind this decision? Thank you!","0","0.6114118598718189","Dataset usage","Requirement Analysis"
"https://github.com/Trusted-AI/AIF360","1097184512","issue","https://github.com/Trusted-AI/AIF360/issues/284","Pytorch Why no pytorch?","28","0.5820271682340651","Bias mitigation methodology","Design"
"https://github.com/Trusted-AI/AIF360","1092070135","issue","https://github.com/Trusted-AI/AIF360/issues/283","ValueError: at least one array or dtype is required Hi,
    I am facing ValueError while fitting GerryFairClassifier on the German dataset. See this code:  

`dataset = GermanDataset()`
`inp = GerryFairClassifier()`
`inp.fit(dataset)`

On executing the above code, I get the error 'ValueError: at least one array or dtype is required'. Am I doing something wrong?
PS: The above code runs fine for Compas, Adult and Bank datasets.
","22","0.595016701272908","Bias mitigation methodology","Design"
"https://github.com/Trusted-AI/AIF360","1090782500","issue","https://github.com/Trusted-AI/AIF360/issues/282","Optimized Preprocessing for Bank marketing Dataset I am trying to preprocess Bank Marketing dataset using the Optimized preprocessing technique. However, I am unable to find the distortion function for this dataset.  The distortion function is only available for 3 other datasets like get_distortion_adult, get_distortion_german amd get_distortion_compas. Please guide me on how to go about this issue?","0","0.6757157118474865","Dataset usage","Requirement Analysis"
"https://github.com/Trusted-AI/AIF360","1090690225","issue","https://github.com/Trusted-AI/AIF360/issues/281","Instance weights for Inprocessing algorithm Hi,
    I am trying to execute ReWeighing operation followed by Prejudice Remover. The first intervention modifies instance weights. However, I am not able to incorporate the modified weights into the Prejudice Remover because its fit() function does allow for sample_weight parameter.  FYI, this parameter is a part of sklearn's fit() function. Please guide me on how can I go about this issue   ","23","0.5655594405594407","Bias mitigation methodology","Design"
"https://github.com/Trusted-AI/AIF360","1085075820","issue","https://github.com/Trusted-AI/AIF360/issues/280","A bug in exponentiated_gradient_reduction.py `drop_prot_attr` doesn't work for the reductions method because of the bug in the `predict_proba` function. 

The line 148: `X = X.drop(self.prot_attr)`

should be updated to: `X = X.drop(self.prot_attr, axis=1)`","26","0.6786598378147674","Bias mitigation methodology","Design"
"https://github.com/Trusted-AI/AIF360","1071272298","issue","https://github.com/Trusted-AI/AIF360/issues/279","Broken link in docs- datasets.StructuredDataset 
On the doc page for aif360.datasets.StructuredDataset, there is a broken link in the explanatory text for instance_weights in the variables bullets. 

Link to page in docs: https://aif360.readthedocs.io/en/latest/modules/generated/aif360.datasets.StructuredDataset.html?highlight=instance%20weights

Broken link: https://aif360.readthedocs.io/en/latest/modules/generated/aif360.datasets.StructuredDataset.html?highlight=instance%20weights","14","0.6256638550452985","Documentation","Development"
"https://github.com/Trusted-AI/AIF360","1042393726","issue","https://github.com/Trusted-AI/AIF360/issues/276","AdversaryLossWeight_incongruence https://github.com/Trusted-AI/AIF360/blob/a5dac73786265415138ed6e2f8c926d9ee968ee5/aif360/algorithms/inprocessing/adversarial_debiasing.py#L68 
Hi, I'm a PhD student from Politecnico di Bari, Italy;
I was reproducing this algorithm following the paper and the AIF360 implementation but not all is equal to the paper. At page 6 the authors says: 
""Without tweaking, this algorithm ran into issues with local minima, and the resulting models were often closer to
demographic parity than equality of odds. We implemented
a technique that helped: by increasing the hyperparameter α
in Eqn. 1 over time, the predictor had a much easier time
learning to deceive the adversary and therefore more strictly
enforce equality of odds. We set α =
√
t (where t is the step
counter), and to avoid divergence we set the predictor’s step
size to η ∝ 1/t""
I think the term self.adversary_loss_weight should be update in each iteration as equal to sqrt(epoch) (for epoch in range(self.num_epochs)).","28","0.2568989318215325","Bias mitigation methodology","Design"
"https://github.com/Trusted-AI/AIF360","1039108093","issue","https://github.com/Trusted-AI/AIF360/issues/275","List of 70 Metrics Hello,

This is a simple question - Can someone point me to the master list of 70+ fairness metrics ? I read on the AI 360 site that ""There are more than 70 metrics in the GitHub repository already. Add new metrics to the repository and use the Slack channel to let the community know about them."" (https://aif360.mybluemix.net/).

Apologies if I missed the link. Thanks in advance.","25","0.2874105704760834","Research","Requirement Analysis"
"https://github.com/Trusted-AI/AIF360","1017167971","issue","https://github.com/Trusted-AI/AIF360/issues/274","R example applications of adversarial_debiasing Are there examples start to finish of training a model to predict an outcome and minimize prediction of a protected class, using adversarial debiasing? I have not been able to locate one, and it would be extremely helpful in figuring out how to apply the package's functions.","22","0.3610590440487348","Bias mitigation methodology","Design"
"https://github.com/Trusted-AI/AIF360","1002046958","issue","https://github.com/Trusted-AI/AIF360/issues/273","Theil Index using binary prediction According to Theil index definition, the formula would be undertemined when y_score is 0 and y_target is 1.

Since b_i = y_score - y_target + 1
would give b_i = 0
And then log(0) not defined.

Shouldn't the package have a class which does not use BinaryLabelDataset for predictions, in order to calculate a Theil Indez with a continous score instead of a binary score?

I assume it is using the trick a*log(a) is equals to log(a**a), but not sure Theil Index is supposed to be calculated that way.

I did not expected this implementation, looks like a trick to be able to have the same interface for all metrics, instead of the proper calculation.

Sorry for my lack of knowledge in the subject if that is supposed to be the proper calculation.

https://github.com/Trusted-AI/AIF360/blob/746e763191ef46ba3ab5c601b96ce3f6dcb772fd/aif360/metrics/classification_metric.py#L694
","3","0.4327190363430454","Bias detection metrics validation","Validation"
"https://github.com/Trusted-AI/AIF360","997482319","issue","https://github.com/Trusted-AI/AIF360/issues/272","Memory leaks from splitting the standarddataset in evolutionary algorithm Line 114 of the attached .py file leads to a memory leak and crash after several thousands of iterations
![133501333-f45a4790-29c9-45cc-a53d-b08804b5d855](https://user-images.githubusercontent.com/39309644/133503555-2da1ba00-2185-4bcb-93fc-cd8b5951057b.jpeg)
 when called from the attached notebook. The error is resolved when setting
moving it out of the looped `eval` method.

This problem persists when calling

del dataset_orig_train
del dataset_orig_test 
gc.collect()
The attached files can be found [here](https://github.com/Trusted-AI/AIF360/files/7172981/files.1.tar.gz)
","27","0.6000783876890072","Troubleshooting","Maintenance"
"https://github.com/Trusted-AI/AIF360","997473548","issue","https://github.com/Trusted-AI/AIF360/issues/271","Memory leaks from deepcopy in evolutionary algorithm Line 70 of the attached .py file leads to a memory leak and crash after several thousands of iterations 
![55749810-9c7d-4524-ae4d-f42cc5bdf214](https://user-images.githubusercontent.com/39309644/133501333-f45a4790-29c9-45cc-a53d-b08804b5d855.jpeg) when called from the attached notebook. The error is resolved when setting 
`predicted_dataset = self.dataset_orig_test.copy(deepcopy=True)` to
`predicted_dataset = self.dataset_orig_test.copy(deepcopy=False)`.

This problem persists when calling  
```
del predicted_dataset 
gc.collect()
```
The attached files can be found [here](https://github.com/Trusted-AI/AIF360/files/7172939/files.tar.gz) ","27","0.5776141262866927","Troubleshooting","Maintenance"
"https://github.com/Trusted-AI/AIF360","995704620","issue","https://github.com/Trusted-AI/AIF360/issues/270","adversarial debiasing - how to set statistical parity or equality of odds Hi there! 

In the paper, the authors mentioned the strategy to reach demographic parity or equality of odds by changing slightly the architecture of the network. How can this option be set in the implementation of this repository? Does the network optimize the demographic parity or the equality of odds by default? 

In the documentation, it is not mentioned: 
[https://aif360.readthedocs.io/en/latest/modules/generated/aif360.sklearn.inprocessing.AdversarialDebiasing.html#aif360.sklearn.inprocessing.AdversarialDebiasing](https://aif360.readthedocs.io/en/latest/modules/generated/aif360.sklearn.inprocessing.AdversarialDebiasing.html#aif360.sklearn.inprocessing.AdversarialDebiasing)

Thank you in advance for you answer! ","3","0.5394042817251009","Bias detection metrics validation","Validation"
"https://github.com/Trusted-AI/AIF360","983765870","issue","https://github.com/Trusted-AI/AIF360/issues/267","ExponentiatedGradientReduction does not work for drop_prot_attr=True, needs one-line fix In the implementation for ExponentiatedGradientReduction the dropping of the protected attribute fails during prediction.

This is due to [this line](https://github.com/Trusted-AI/AIF360/blob/fcda24e8760cfb66ba5a1b24c2cec33e8101a730/aif360/sklearn/inprocessing/exponentiated_gradient_reduction.py#L147), which does not try to drop the protected attribute from the columns correctly. 

It should rather be (cf. [this line above](https://github.com/Trusted-AI/AIF360/blob/fcda24e8760cfb66ba5a1b24c2cec33e8101a730/aif360/sklearn/inprocessing/exponentiated_gradient_reduction.py#L127) from similar method)
```python
X = X.drop(self.prot_attr, axis=1)
```

In the current implementation, initialising ExponentiatedGradientReduction with `drop_prot_attr=True` allows fitting the inprocessing model, but throws an exception at prediction time like this:

```
---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
<ipython-input-194-e05fe27a609c> in <module>
     12         eta_mul=2.0,
     13         drop_prot_attr=True) # Bug in AIF360 causes True to crash algorithm...
---> 14     train_inprocessed(clf, train_discriminated_protected, test_discriminated, test_ground_truth,
     15                       label=f""## Exponentiated Gradient Reduction, eps={eps}"")
     16     print()

<ipython-input-182-0b8ee41d60dc> in train_inprocessed(inprocessing, train, test, construct_test, label)
      6 
      7 
----> 8     pred = inprocessing.predict(test)
      9 
     10     eval_fairness(pred)

~/.miniconda3/lib/python3.8/site-packages/aif360/algorithms/transformer.py in wrapper(self, *args, **kwargs)
     25     @wraps(func)
     26     def wrapper(self, *args, **kwargs):
---> 27         new_dataset = func(self, *args, **kwargs)
     28         if isinstance(new_dataset, Dataset):
     29             new_dataset.metadata = new_dataset.metadata.copy()

~/.miniconda3/lib/python3.8/site-packages/aif360/algorithms/inprocessing/exponentiated_gradient_reduction.py in predict(self, dataset)
    111         try:
    112             # Probability of favorable label
--> 113             scores = self.model.predict_proba(X_df)[:, fav]
    114             dataset_new.scores = scores.reshape(-1, 1)
    115         except (AttributeError, NotImplementedError):

~/.miniconda3/lib/python3.8/site-packages/aif360/sklearn/inprocessing/exponentiated_gradient_reduction.py in predict_proba(self, X)
    145         """"""
    146         if self.drop_prot_attr:
--> 147             X = X.drop(self.prot_attr)
    148 
    149         return self.model._pmf_predict(X)

~/.local/lib/python3.8/site-packages/pandas/core/frame.py in drop(self, labels, axis, index, columns, level, inplace, errors)
   4303                 weight  1.0     0.8
   4304         """"""
-> 4305         return super().drop(
   4306             labels=labels,
   4307             axis=axis,

~/.local/lib/python3.8/site-packages/pandas/core/generic.py in drop(self, labels, axis, index, columns, level, inplace, errors)
   4150         for axis, labels in axes.items():
   4151             if labels is not None:
-> 4152                 obj = obj._drop_axis(labels, axis, level=level, errors=errors)
   4153 
   4154         if inplace:

~/.local/lib/python3.8/site-packages/pandas/core/generic.py in _drop_axis(self, labels, axis, level, errors)
   4185                 new_axis = axis.drop(labels, level=level, errors=errors)
   4186             else:
-> 4187                 new_axis = axis.drop(labels, errors=errors)
   4188             result = self.reindex(**{axis_name: new_axis})
   4189 

~/.local/lib/python3.8/site-packages/pandas/core/indexes/base.py in drop(self, labels, errors)
   5589         if mask.any():
   5590             if errors != ""ignore"":
-> 5591                 raise KeyError(f""{labels[mask]} not found in axis"")
   5592             indexer = indexer[~mask]
   5593         return self.delete(indexer)

KeyError: ""['protected_attr'] not found in axis""
```

Kind regards","26","0.5918883668991629","Bias mitigation methodology","Design"
"https://github.com/Trusted-AI/AIF360","980916029","issue","https://github.com/Trusted-AI/AIF360/issues/266","where is AIF360 web address? I see from paper ""AIF360 includes not only the main toolkit code, but also an interactive Web experience. ""
but I don't find the web address for AIF360 web application?
","13","0.5164621998532033","Artifact generation and benchmarking","Deployment"
"https://github.com/Trusted-AI/AIF360","977869215","issue","https://github.com/Trusted-AI/AIF360/issues/265","Privileged class in BankDataset In AIF360, age>=25 is set as the privileged class in BankDataset. However, I find that this class is not really privileged.

In fact, 12.35% of samples with age>=25 were marked as ""yes"", while 23.03% of samples with age<25 were marked as ""yes"". Therefore, I think age<25 should be the privileged class.","26","0.6786598378147674","Bias mitigation methodology","Design"
"https://github.com/Trusted-AI/AIF360","977288441","issue","https://github.com/Trusted-AI/AIF360/issues/263","Docker setup for AIF360 Users may want to try AIF360 inside a docker container to launch on Kubernetes/Openshift environments.","21","0.6794258373205742","Installation and shell commands","Deployment"
"https://github.com/Trusted-AI/AIF360","970106634","issue","https://github.com/Trusted-AI/AIF360/issues/261","Update R package dependencies This issue is to track R package dependencies and update to make sure it has recent updates from python.","21","0.3706539074960128","Installation and shell commands","Deployment"
"https://github.com/Trusted-AI/AIF360","959281925","issue","https://github.com/Trusted-AI/AIF360/issues/258","Inprocessing Reduction techniques issue Both the ExponentiatedGradientReduction and GridSearchReduction inprocessing techniques fail with the error: module 'fairlearn.reductions' has no attribute 'TruePositiveRateDifference'. I dug into fairlearn.reductions myself and they no longer have 'TruePositiveRateDifference' in that module which is breaking the utility of these inprocessing methods. 
Let me know if I have misunderstood anything and just need a correction on my end. Thanks.


`grid_red = GridSearchReduction(LogisticRegression(solver='lbfgs'), constraints='DemographicParity', prot_attr='sex)
grid_red.fit(train_bin)`
`preds = grid_red.predict(test_bin)` 
AND
`exp_red = ExponentiatedGradientReduction(estimator=LogisticRegression(solver='lbfgs'), constraints='DemographicParity')
exp_red.fit(train_bin)`
`exp_red_pred = exp_red.predict(test_bin)`","28","0.4748115896413646","Bias mitigation methodology","Design"
"https://github.com/Trusted-AI/AIF360","954125401","issue","https://github.com/Trusted-AI/AIF360/issues/256","Add Regression Metric to the Toolkit This issue is to track regression metric addition to the toolkit. 

- [ ] Mean Absolute Error","6","0.2355661881977672","API expansion","Development"
"https://github.com/Trusted-AI/AIF360","945647657","issue","https://github.com/Trusted-AI/AIF360/issues/253","Learning Fair Representations Issue The Learning Fair Representations class (LFR) parameters are 
`LFR(unprivileged_groups, privileged_groups, k=5, Ax=0.01, Ay=1.0, Az=50.0, print_interval=250, verbose=0, seed=None)`. The only required parameters are unprivileged_groups and privileged_groups, so I have used `LFR = preprocessing.LFR(unprivileged_groups, privileged_groups)` to initialize the LFR.

However, when I run the LFR fit_transform and then retrain and test the model (sklearn's Random Forest) on the updated dataset, It now only predicts positive outcomes for every single datapoint. I have played around with the initialization of LFR, but no changes to that have made a difference.

Do you have any suggestions on how to fix the LFR so that it doesn't predict only positive outcomes?","27","0.3082154555736717","Troubleshooting","Maintenance"
"https://github.com/Trusted-AI/AIF360","945634481","issue","https://github.com/Trusted-AI/AIF360/issues/252","Optimized Preprocessing Issue I've been having trouble initializing the Optimized Preprocessing class. According to the AIF360 documentation, the initialization command follows the template `OptimPreproc(optimizer, optim_options, unprivileged_groups=None, privileged_groups=None, verbose=False, seed=None)`.
I have tried using `OP = preprocessing.OptimPreproc(optimizer=SGD, optim_options={""learning_rate"":0.1}, unprivileged_groups=unprivileged_groups, privileged_groups=privileged_groups)`, with SGD imported from tensorflow.keras.optimizers, but when I call the fit_transform method of OptimPreproc, I get ""TypeError: Unexpected keyword argument passed to optimizer: df"". The error comes up regardless of what I put for optim_options (even ""None""). I have also confirmed that I have passed in the correct parameter when calling fit_transform, as it works for every other preprocessing algorithm. This leads me to believe that there is an issue with the initialization.

Does anyone have any suggestions on how to initialize the OptimPreproc class and use its fit_transform method? Example code would be much appreciated :)","27","0.3913561017614507","Troubleshooting","Maintenance"
"https://github.com/Trusted-AI/AIF360","937968687","issue","https://github.com/Trusted-AI/AIF360/issues/251","Install AIF360 latest from github source Can we provide some pointers on README.md on how we can install AIF360 from the github source itself? Some ideas here:
https://stackoverflow.com/questions/15268953/how-to-install-python-package-from-github","21","0.6331676136363636","Installation and shell commands","Deployment"
"https://github.com/Trusted-AI/AIF360","920767694","issue","https://github.com/Trusted-AI/AIF360/issues/250","Tensorflow dependency on BinaryLabelDatasetMetric  Is it possible to change the metrics not to be dependent on tensorflow due to the auditor module as for simpler tasks it would be great not to install tensorflow.

```python
      8 from aif360.datasets import BinaryLabelDataset
      9 
---> 10 from aif360.metrics import BinaryLabelDatasetMetric, ClassificationMetric
     11 
     12 

~/miniconda/envs/AIF/lib/python3.8/site-packages/aif360/metrics/__init__.py in <module>
      1 from aif360.metrics.metric import Metric
      2 from aif360.metrics.dataset_metric import DatasetMetric
----> 3 from aif360.metrics.binary_label_dataset_metric import BinaryLabelDatasetMetric
      4 from aif360.metrics.classification_metric import ClassificationMetric
      5 from aif360.metrics.sample_distortion_metric import SampleDistortionMetric

~/miniconda/envs/AIF/lib/python3.8/site-packages/aif360/metrics/binary_label_dataset_metric.py in <module>
      1 import numpy as np
      2 from sklearn.neighbors import NearestNeighbors
----> 3 from aif360.algorithms.inprocessing.gerryfair.auditor import Auditor
      4 from aif360.datasets import BinaryLabelDataset
      5 from aif360.datasets.multiclass_label_dataset import MulticlassLabelDataset

~/miniconda/envs/AIF/lib/python3.8/site-packages/aif360/algorithms/inprocessing/__init__.py in <module>
      4 from aif360.algorithms.inprocessing.meta_fair_classifier import MetaFairClassifier
      5 from aif360.algorithms.inprocessing.gerryfair_classifier import GerryFairClassifier
----> 6 from aif360.algorithms.inprocessing.exponentiated_gradient_reduction import ExponentiatedGradientReduction
      7 from aif360.algorithms.inprocessing.grid_search_reduction import GridSearchReduction

~/miniconda/envs/AIF/lib/python3.8/site-packages/aif360/algorithms/inprocessing/exponentiated_gradient_reduction.py in <module>
     10 
     11 from aif360.algorithms import Transformer
---> 12 from aif360.sklearn.inprocessing import ExponentiatedGradientReduction as skExpGradRed
     13 
     14 

~/miniconda/envs/AIF/lib/python3.8/site-packages/aif360/sklearn/inprocessing/__init__.py in <module>
      2 In-processing algorithms train a fair classifier (data in, predictions out).
      3 """"""
----> 4 from aif360.sklearn.inprocessing.adversarial_debiasing import AdversarialDebiasing
      5 from aif360.sklearn.inprocessing.exponentiated_gradient_reduction import ExponentiatedGradientReduction
      6 from aif360.sklearn.inprocessing.grid_search_reduction import GridSearchReduction

~/miniconda/envs/AIF/lib/python3.8/site-packages/aif360/sklearn/inprocessing/adversarial_debiasing.py in <module>
      5 from sklearn.utils import check_random_state
      6 from sklearn.utils.validation import check_is_fitted
----> 7 import tensorflow.compat.v1 as tf
      8 
      9 from aif360.sklearn.utils import check_inputs, check_groups

ModuleNotFoundError: No module named 'tensorflow.compat' 
```","27","0.5060562779395671","Troubleshooting","Maintenance"
"https://github.com/Trusted-AI/AIF360","913659846","issue","https://github.com/Trusted-AI/AIF360/issues/248","The definition of TPR here is actually recall TPR = TP/P and, in this project, P means ""actual positives"", rather than ""predicted positives"". Therefore, TPR in this project means ""recall"" in general ML convention. Can you confirm this is a correct understanding? If so, what is the reason or rational behind your choices against the convention?","30","0.4460426265123423","Troubleshooting","Maintenance"
"https://github.com/Trusted-AI/AIF360","904095775","issue","https://github.com/Trusted-AI/AIF360/issues/245","Issue with hyphenated module name kamfadm-2012ecmlpkdd I'm facing issue when testing a conda recipe due to the hyphen/dash naming convention used in the module named `kamfadm-2012ecmlpkdd`
- Python Prefers Underscores
Python packages and modules can not use hyphens, only underscores. This section of PEP-8 gives us guidance:
- Package and Module Names: Modules should have short, all-lowercase names. Underscores can be used in the module name if it improves readability. Python packages should also have short, all-lowercase names, although the use of underscores is discouraged.","30","0.4975181886176652","Troubleshooting","Maintenance"
"https://github.com/Trusted-AI/AIF360","891324675","issue","https://github.com/Trusted-AI/AIF360/issues/244","low accuracy for gerryfair classifier The gerryfair classifier has extremely low accuracy on both the adult income and compas datasets, with each individual classifier almost always outputting a 1 / almost always outputting a 0. Is this behaviour to be expected?","23","0.3705295197171119","Bias mitigation methodology","Design"
"https://github.com/Trusted-AI/AIF360","869486247","issue","https://github.com/Trusted-AI/AIF360/issues/241","Disparate Impact Remover doesn't seem to be working I'm trying to run the disparate impact remover on a dataset - however after varying the repair value between 0 -> 1.0 I see no change in disparate impact metric.

Example:
My dataset (df_encoded) looks like the below with around 600 samples.

![image](https://user-images.githubusercontent.com/25558903/116347391-bef96f00-a7b1-11eb-99fe-5903c3911679.png)

```
dataset_train = StandardDataset(df_encoded, label_name='outcome',
                    favorable_classes=[0],
                    protected_attribute_names=['sensitive'],
                    privileged_classes=[[1]])

di_remover = DisparateImpactRemover(repair_level=1.0)
train_repd = di_remover.fit_transform(dataset_train)

privileged_groups = [{'sensitive': 1}]
unprivileged_groups = [{'sensitive': 0}]
    
metric_train_repd = BinaryLabelDatasetMetric(train_repd, 
                unprivileged_groups=unprivileged_groups,
                privileged_groups=privileged_groups)

explainer = MetricTextExplainer(metric_train_repd)
explainer.disparate_impact()
```

In my dataset i get a disparate impact of 0.71. When I change ""repair_level"" to 0 or 1.0 I see no change in disparate impact.

","27","0.4105480133627123","Troubleshooting","Maintenance"
"https://github.com/Trusted-AI/AIF360","857895182","issue","https://github.com/Trusted-AI/AIF360/issues/237","Names aren't in alphabetical order I know this isn't a major issue but the contributing.md file in the root directory of the project doesnt have the contributors' names in alphabetical order. ","14","0.5985031917235307","Documentation","Development"
"https://github.com/Trusted-AI/AIF360","854891100","issue","https://github.com/Trusted-AI/AIF360/issues/236","Bias in multi-class protected attribute I am working on understanding Bias in a multi-class protected attribute, where one particular class is a privileged class (Dataset Contains more favorable outcomes for this class, as compared to other subclasses) and another particular class is an unprivileged class (Dataset contains the least favorable outcome for this class, as compared to other subclasses). How do I handle other subclasses of this protected attribute?
 For example, if I have 4 subclasses in my protected attribute ""Region"" in a binary label prediction problem, where ""North"" is privileged and ""West"" is unprivileged, how should I handle the other 2 subclasses (""South"" & ""East"")?","26","0.7638811616779796","Bias mitigation methodology","Design"
"https://github.com/Trusted-AI/AIF360","851969205","issue","https://github.com/Trusted-AI/AIF360/issues/235","Can AIF360 detect and mitigate bias on a dataset without the protected attributes? If my dataset does not have protected attributes, can AIF360 detect and mitigate bias in this case? I am trying to detect and mitigate bias using pre processing algorithms before training the model.

","22","0.6726598494212833","Bias mitigation methodology","Design"
"https://github.com/Trusted-AI/AIF360","848250243","issue","https://github.com/Trusted-AI/AIF360/issues/234","compas dataset cannot be loaded I am trying to load the compas dataset. However, neither original nor the preprocessed version of the dataset can be loaded. Whenever I try to run the command `dataset_prep = load_preproc_data_compas()` or `dataset_orig = CompasDataset()`, I get the following error:

""File ""C:\anaconda3\envs\aif360\lib\site-packages\pandas\io\parsers.py"", line 1705, in ix
    raise ValueError(f""Index {col} invalid"")

ValueError: Index id invalid""

How can this issue be solved? Is there a problem with the package itself?","0","0.3156426578928545","Dataset usage","Requirement Analysis"
"https://github.com/Trusted-AI/AIF360","844637729","issue","https://github.com/Trusted-AI/AIF360/issues/233","How I can use AIF360 in Computer Vision Hello,

I am looking to use AIF360 with an image dataset for object detection.
Has anyone succeeded to use it in this case? or even more simple cases such as image classification?

Thanks,
F.M.","13","0.4131776480400331","Artifact generation and benchmarking","Deployment"
"https://github.com/Trusted-AI/AIF360","837634198","issue","https://github.com/Trusted-AI/AIF360/issues/232","Regression Fairness metrics? What would be the fairness metrics for the RegressionDataset","6","0.7601417399804503","API expansion","Development"
"https://github.com/Trusted-AI/AIF360","827993625","issue","https://github.com/Trusted-AI/AIF360/issues/231","fetch_adult() and fetch_german() does not work in sklearn package I have been trying to use the sklearn extension of AIF360 and import some datasets. For that, I have followed one of the example Jupyter notebooks called [demo_exponentiated_gradient_reduction_sklearn.ipynb](https://github.com/Trusted-AI/AIF360/blob/master/examples/sklearn/demo_exponentiated_gradient_reduction_sklearn.ipynb). I have tried the following code which is literally the first two blocks of that Jupyter notebook:

`%matplotlib inline`
`import matplotlib.pyplot as plt`
`import numpy as np`
`import pandas as pd`

`from sklearn.linear_model import LogisticRegression`
`from sklearn.metrics import accuracy_score`
`from sklearn.model_selection import GridSearchCV, train_test_split`
`from sklearn.preprocessing import OneHotEncoder`
 
`from aif360.sklearn.inprocessing import ExponentiatedGradientReduction`

`from aif360.sklearn.datasets import fetch_adult, fetch_german`
`from aif360.sklearn.metrics import disparate_impact_ratio, average_odds_error, generalized_fpr`
`from aif360.sklearn.metrics import generalized_fnr, difference`

`X, y, sample_weight = fetch_adult()`

And I get the following error:

`AttributeError: 'NoneType' object has no attribute 'items'`

I could not find the reason for this error. Can anyone help with it?","27","0.4626858343643828","Troubleshooting","Maintenance"
"https://github.com/Trusted-AI/AIF360","807098800","issue","https://github.com/Trusted-AI/AIF360/issues/228","Report: memory issue When I repeat running 'for i in range(100000):
a =BinaryLabelDatasetMetric()',
 the memory will continuously increase.  How to fixed it.","7","0.4574960127591707","Opinion","Requirement Analysis"
"https://github.com/Trusted-AI/AIF360","804597368","issue","https://github.com/Trusted-AI/AIF360/issues/227","Error in metric_json_explainer.consistency() Hello,

While calculating consistency using MetricJSONExplainer, I'm seeing the error `ValueError: too many values to unpack (expected 2)` probably due to error in line 147 in the source code. Moving the text to line 146 should resolve it.

https://github.com/Trusted-AI/AIF360/blob/b67877a6d719523d5329987da67cdbe7c4165fba/aif360/explainers/metric_json_explainer.py#L141-L150

Thanks!","26","0.2805009717123731","Bias mitigation methodology","Design"
"https://github.com/Trusted-AI/AIF360","802520846","issue","https://github.com/Trusted-AI/AIF360/issues/226","Modify predict_proba in exponentiated_gradient_reduction.py  Per discussion in #215, predict_proba in  exponentiated_gradient_reduction.py needs to be modified.

See https://github.com/Trusted-AI/AIF360/pull/215#discussion_r523334486 for  details.","23","0.4468167917709201","Bias mitigation methodology","Design"
"https://github.com/Trusted-AI/AIF360","802519072","issue","https://github.com/Trusted-AI/AIF360/issues/225","assert statement fails in exponentiated gradient reduction (sklearn compatible) notebook The assert  statement

```assert egr_aoe_race<lr_aoe_race```

used in  the notebook example demo_exponentiated_gradient_reduction_sklearn.ipynb seems to  fail  when run in travis while it passes locally. Needs further investigation.

For now, we commented this to merge PR #215, but we can incorporate this  test back again after fixing this issue.

","32","0.5521124002545651","Dependency and Release","Deployment"
"https://github.com/Trusted-AI/AIF360","799076983","issue","https://github.com/Trusted-AI/AIF360/issues/224","Train on None Protected features Is there a way of training models without the sensitive attributes?","27","0.3446565297099103","Troubleshooting","Maintenance"
"https://github.com/Trusted-AI/AIF360","788541734","issue","https://github.com/Trusted-AI/AIF360/issues/222","MEPS dataset issue - no .csv files For version 0.3.0 when reading MEPS dataset no .csv files found under the path specified below.

Code:
(dataset_orig_panel19_train,
 dataset_orig_panel19_val,
 dataset_orig_panel19_test) = MEPSDataset19().split([0.5, 0.8], shuffle=True)

Result:
IOError: [Errno 2] No such file or directory: '/usr/local/lib/python3.6/dist-packages/aif360/datasets/../data/raw/meps/h181.csv'
To use this class, please follow the instructions in:

	/usr/local/lib/python3.6/dist-packages/aif360/data/raw/meps/README.md


 to download and convert the 2015 data and place the final h181.csv file, as-is, in the folder:

	/usr/local/lib/python3.6/dist-packages/aif360/data/raw/meps

An exception has occurred, use %tb to see the full traceback.

I checked in the filesystem - files indeed missing. When I used version 0.2.3 files are there. When I upgraded it to version 0.3.0 - the files are gone. I did a check using google Colab.","21","0.3800567248843111","Installation and shell commands","Deployment"
"https://github.com/Trusted-AI/AIF360","784710951","issue","https://github.com/Trusted-AI/AIF360/issues/221","aif360.metrics.DatasetMetric metric_fun options I am interested in using the ""difference"" and ""ratio"" methods from the [DatasetMetric](https://aif360.readthedocs.io/en/latest/modules/generated/aif360.metrics.DatasetMetric.html) class. I need to include ""metric_fun"" as a parameter for both of those methods; however, I cannot find a list of possible metric_fun options. I looked in the class' [code](https://github.com/Trusted-AI/AIF360/blob/master/aif360/metrics/dataset_metric.py#L73-L77) as well but still couldn't find a description of the parameter nor options for it. Where can I find this? Thanks so much!","15","0.7290362052832362","Metrics operation","Validation"
"https://github.com/Trusted-AI/AIF360","753020648","issue","https://github.com/Trusted-AI/AIF360/issues/216","Error creating Standard Dataset when privileged classes are strings Installing AIF360 [python] through Conda, pip and locally fails to pull the latest version of AIF360. 

https://github.com/Trusted-AI/AIF360/pull/115

Running the following I see.
```
StandardDataset(df = pd.DataFrame({""a"" : [""m"", ""f""], ""b"": [0,1]}),
                label_name = ""b"",
                favorable_classes=[0],
                protected_attribute_names= [""a""],
                privileged_classes = [[""m""]]
                )
```

Has the following error. I don't seem to be able to grab the latest version that uses .to_numpy() 
```
---------------------------------------------------------------------------
NotImplementedError                       Traceback (most recent call last)
<ipython-input-23-91cce4b5ff1b> in <module>
      3                 favorable_classes=[0],
      4                 protected_attribute_names= [""a""],
----> 5                 privileged_classes = [[""m""]]
      6                 )

~/miniconda3/envs/gt_vis/lib/python3.7/site-packages/aif360/datasets/standard_dataset.py in __init__(self, df, label_name, favorable_classes, protected_attribute_names, privileged_classes, instance_weights_name, scores_name, categorical_features, features_to_keep, features_to_drop, na_values, custom_preprocessing, metadata)
    119             else:
    120                 # find all instances which match any of the attribute values
--> 121                 priv = np.logical_or.reduce(np.equal.outer(vals, df[attr]))
    122                 df.loc[priv, attr] = privileged_values[0]
    123                 df.loc[~priv, attr] = unprivileged_values[0]

~/miniconda3/envs/gt_vis/lib/python3.7/site-packages/pandas/core/series.py in __array_ufunc__(self, ufunc, method, *inputs, **kwargs)
    746             return None
    747         else:
--> 748             return construct_return(result)
    749 
    750     def __array__(self, dtype=None) -> np.ndarray:

~/miniconda3/envs/gt_vis/lib/python3.7/site-packages/pandas/core/series.py in construct_return(result)
    735                 if method == ""outer"":
    736                     # GH#27198
--> 737                     raise NotImplementedError
    738                 return result
    739             return self._constructor(result, index=index, name=name, copy=False)

NotImplementedError: 
```

","26","0.7363458156094309","Bias mitigation methodology","Design"
"https://github.com/Trusted-AI/AIF360","740073968","issue","https://github.com/Trusted-AI/AIF360/issues/213","[Pre-processing algorithm] - Python LFR: No module named 'Numba.decorators': LFR will be unavailable On running the example notebook: [demo_lfr.ipynb](https://github.com/Trusted-AI/AIF360/blob/master/examples/demo_lfr.ipynb), I get the following error:

```
WARNING:root:No module named 'numba.decorators': LFR will be unavailable. To install, run:
pip install 'aif360[LFR]'
<class 'aif360.datasets.adult_dataset.AdultDataset'>
(34189, 18)
1.0 0.0
['sex', 'race']
[array([1.]), array([1.])] [array([0.]), array([0.])]
['race', 'sex', 'Age (decade)=10', 'Age (decade)=20', 'Age (decade)=30', 'Age (decade)=40', 'Age (decade)=50', 'Age (decade)=60', 'Age (decade)=>=70', 'Education Years=6', 'Education Years=7', 'Education Years=8', 'Education Years=9', 'Education Years=10', 'Education Years=11', 'Education Years=12', 'Education Years=<6', 'Education Years=>12']
<IPython.core.display.Markdown object>
Difference in mean outcomes between unprivileged and privileged groups = -0.195439
<IPython.core.display.Markdown object>
Difference in mean outcomes between unprivileged and privileged groups = -0.192336
Traceback (most recent call last):
  File ""/Users/nikunjlad/github/Bais-Fairness-Library/src/learn_fair_representation.py"", line 77, in <module>
    main()
  File ""/Users/nikunjlad/github/Bais-Fairness-Library/src/learn_fair_representation.py"", line 73, in main
    TR = TR.fit(dataset_orig_train, maxiter=5000, maxfun=5000)
  File ""/Users/nikunjlad/.virtualenvs/bias/lib/python3.6/site-packages/aif360/algorithms/transformer.py"", line 27, in wrapper
    new_dataset = func(self, *args, **kwargs)
  File ""/Users/nikunjlad/.virtualenvs/bias/lib/python3.6/site-packages/aif360/algorithms/preprocessing/lfr.py"", line 105, in fit
    self.learned_model = optim.fmin_l_bfgs_b(lfr_helpers.LFR_optim_obj, x0=model_inits, epsilon=1e-5,
NameError: name 'lfr_helpers' is not defined
```","22","0.5618594798881449","Bias mitigation methodology","Design"
"https://github.com/Trusted-AI/AIF360","729885480","issue","https://github.com/Trusted-AI/AIF360/issues/211","[AIF360-R] Post-processing algorithms - [ ] EqOddPostprocessing

- [ ] CalibratedEqOddProcessing","28","0.6039464411557435","Bias mitigation methodology","Design"
"https://github.com/Trusted-AI/AIF360","729883782","issue","https://github.com/Trusted-AI/AIF360/issues/210","[AIF360-R]  In-processing algorithms - [ ] ARTClassifier

- [ ] GerryFairClassifier

- [ ] MetaFairClassifier","22","0.8044965786901274","Bias mitigation methodology","Design"
"https://github.com/Trusted-AI/AIF360","729882004","issue","https://github.com/Trusted-AI/AIF360/issues/209","[AIF360-R]  Pre-processing Algorithms - [ ] LFR

- [ ] OptimPreproc","27","0.4504580690627202","Troubleshooting","Maintenance"
"https://github.com/Trusted-AI/AIF360","714033498","issue","https://github.com/Trusted-AI/AIF360/issues/207","How to calculate accuracy metric in German credit scoring example? I'm trying to reconstruct the steps in the [German credit scoring tutorial](https://aif360.mybluemix.net/resources#tutorials) in R.
I would like to calculate the drop in accuracy after bias mitigation, but I don't know how.
I get that you need to create a classification metric and provide the ground truth and predicted labels, but I'm not sure how. 
How do I derive the input dataset and the classified dataset from the AIF compatible dataset that is created when I execute `german_dataset <- german_dataset()`?
Could you please point that out to me?
This is my code example:
https://gist.github.com/FrieseWoudloper/711d2ea8ec655cd79c8b488d1d241604","22","0.4172575335527983","Bias mitigation methodology","Design"
"https://github.com/Trusted-AI/AIF360","707046926","issue","https://github.com/Trusted-AI/AIF360/issues/205","Error running install_aif360() on Windows 8.1 After I install en load `aif360` from Github on my Windows machine, I get an error:      
`Error: Installing Python packages into a virtualenv is not supported on Windows`","21","0.7974391805377721","Installation and shell commands","Deployment"
"https://github.com/Trusted-AI/AIF360","701503142","issue","https://github.com/Trusted-AI/AIF360/issues/204","Error in demo_disparate_impact_remover.ipynb? Hi, I just read these lines from the disparate impact remover demo example:  
```python3
test, train = ad.split([16281])
train.features = scaler.fit_transform(train.features)
test.features = scaler.fit_transform(test.features)

index = train.feature_names.index(protected)
```
Shouldn't the test data be scaled by the same scaler fit on training data?  
Like this:  
```python3
train.features = scaler.fit_transform(train.features)
test.features = scaler.transform(test.features)
```
Or it's deliberately done so?","27","0.7505507416654427","Troubleshooting","Maintenance"
"https://github.com/Trusted-AI/AIF360","701285886","issue","https://github.com/Trusted-AI/AIF360/issues/203","TensorFlow 2? Will there be a plan to upgrade code for Adversarial Debiasing to fit TensorFlow 2 API?  
Or at least call the `tf`-related functions in `tf.compat.v1`?","22","0.3499156211020617","Bias mitigation methodology","Design"
"https://github.com/Trusted-AI/AIF360","690874284","issue","https://github.com/Trusted-AI/AIF360/issues/200","Issues in installing aif360 library on Rstudio - windows10 Hello! I have many trouble in setting the environment properly for using this library in Rstudio. I've tried to follow the few instructions I found in some blogs, but it is still impossible for me to run the simple example you provide for R. It does not recognize the datasets in the package or I have some trouble with the load_aif360_lib() function.
Anyone who did it that can help me?
Thank you!","21","0.4566519213986942","Installation and shell commands","Deployment"
"https://github.com/Trusted-AI/AIF360","680742066","issue","https://github.com/Trusted-AI/AIF360/issues/199","No module named 'common_utils' Hello,

I work on mac os and try to run the [LFR demo](https://github.com/Trusted-AI/AIF360/blob/master/examples/demo_lfr.ipynb). I created a conda environment as in the README and install using pip the entire package( `pip install 'aif360[all]'
`).

When I run the first cell in the notebook I get the error `No module named 'common_utils'`. This script exists in the examples directory ([here](https://github.com/Trusted-AI/AIF360/blob/master/examples/common_utils.py)) but is not imported correctly.","21","0.6250471520181067","Installation and shell commands","Deployment"
"https://github.com/Trusted-AI/AIF360","676567304","issue","https://github.com/Trusted-AI/AIF360/issues/198","R - Next Batch Updates **Batch 1 - Housekeeping**

- [ ] Update repo location in CRAN

- [ ] Add about R in the [documentation](https://aif360.readthedocs.io/en/latest/index.html)

- [ ] Code check 

**Advocacy**

- [x] Talk submission 

- [x] Workshop submission

**Coding** 

- [ ] Add vignettes","18","0.4045528939145961","Troubleshooting","Maintenance"
"https://github.com/Trusted-AI/AIF360","675512521","issue","https://github.com/Trusted-AI/AIF360/issues/197","Protected Attribute: Age The German and Bank dataset both have the protected attribute ""age"", and mention that they determine privileged groups in a similar fashion.

However, the German dataset use age>25 for privileged groups, the Bank dataset uses age>=25.
The files can be found here:
https://github.com/Trusted-AI/AIF360/blob/master/aif360/datasets/german_dataset.py
https://github.com/Trusted-AI/AIF360/blob/master/aif360/datasets/bank_dataset.py
","22","0.6095814149438779","Bias mitigation methodology","Design"
"https://github.com/Trusted-AI/AIF360","667182739","issue","https://github.com/Trusted-AI/AIF360/issues/194","aif360-r use cases, vignette Hi! Python package seems to have a big user community lots of examples, use cases, and applications of the package. It would be cool to have some vignettes and use-cases showing capabilities and how to use `aif360` in R.  As for now there is not a lot of examples to learn from.

","25","0.4609894367633264","Research","Requirement Analysis"
"https://github.com/Trusted-AI/AIF360","638829280","issue","https://github.com/Trusted-AI/AIF360/issues/185","ModuleNotFoundError: No module named 'numba.decorators' There is `from numba.decorators import jit` in `aif360/algorithms/preprocessing/lfr_helpers/helpers.py` which fails with 
`ModuleNotFoundError: No module named 'numba.decorators'`. Seems like numba released a new version on June 11th, which may have caused this. Pinning the version in setup.py may solve it, at least for now.","27","0.5991679921693381","Troubleshooting","Maintenance"
"https://github.com/Trusted-AI/AIF360","635996460","issue","https://github.com/Trusted-AI/AIF360/issues/182","Unused variables https://github.com/IBM/AIF360/blob/7763c1c5a36647afbee310249fee82e1211b0f40/aif360/algorithms/inprocessing/gerryfair/heatmap.py#L90

Variables declared but not used, or reassigned before they are used, can be the result of programmer negligence, which often means that there is a bug in the program.","22","0.3587937486242569","Bias mitigation methodology","Design"
"https://github.com/Trusted-AI/AIF360","635991283","issue","https://github.com/Trusted-AI/AIF360/issues/181","Dynamic Code Evaluation https://github.com/IBM/AIF360/blob/7763c1c5a36647afbee310249fee82e1211b0f40/mlops/kubeflow/bias_detector_pytorch/src/fairness_check.py#L78-L79

Python allows users to execute instructions dynamically, and when this capability is exploited by malicious users, dynamic code parsing attacks occur.","21","0.5005147486683674","Installation and shell commands","Deployment"
"https://github.com/Trusted-AI/AIF360","632001902","issue","https://github.com/Trusted-AI/AIF360/issues/180","LightGBM Hi, 

I have identified ways to use LightGBM with many of your tools. Let me know if it is something you would like to incorporate into your package. Maybe we can house a tutorial on the repo AIF360 that can show people how to use your tools with LightGBM as I had to develop a few work-around strategies. 

https://github.com/firmai/ml-fairness-framework

Best,
Derek","13","0.7197440240918503","Artifact generation and benchmarking","Deployment"
"https://github.com/Trusted-AI/AIF360","629448425","issue","https://github.com/Trusted-AI/AIF360/issues/177","make_scorer on sklearn metrics.py wrong attribute The `make_scorer` from `aif360/sklearn/metrics/metrics.py` file has the attribute `ratio` which I think it should be `is_ratio` instead since if the attribute is `ratio` it will always be True.

","30","0.4827769886363634","Troubleshooting","Maintenance"
"https://github.com/Trusted-AI/AIF360","614634454","issue","https://github.com/Trusted-AI/AIF360/issues/175","Documentation of  Scikitlearn modules re input data prep The current example file for scikitlearn **demo_new_features** is very helpful and shows the use of standard adult uci dataset - prepped by the fetch_adult module. Could you  also show prep from scratch on a user generated  dataset? e.g. starting with any  dataset/dataframe? 
I used the standardardize_dataset function  on my dataframe **data** but not sure this is best practice ? I think a generic dataframe preparation  example would be helpful in terms of broadening out the use of these excellent modules. 

`import pandas as pd
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.preprocessing import OneHotEncoder
......
* read dataset into a dataframe  data*

X,y = standardize_dataset(**data**,
                              prot_attr=[protectedAttribute],
                              target=[target_variable],
                              
                              #usecols=usecols, dropcols=dropcols,
                              #numeric_only=numeric_only, dropna=dropna
                                         )
Then proceed as per the current demo notebook 
X.index = pd.MultiIndex.from_arrays(X.index.codes, names=X.index.names)
y.index = pd.MultiIndex.from_arrays(y.index.codes, names=y.index.names)
etc.","27","0.4123948867291206","Troubleshooting","Maintenance"
"https://github.com/Trusted-AI/AIF360","612898326","issue","https://github.com/Trusted-AI/AIF360/issues/173","LFR seems to make the data more unfair In demo_lfr.ipynb, the transformed dataset seems to be making it more unfair rather than more fair? The difference in mean outcomes starts out as -.196576, but then after the transformation into a supposedly more fair representation, this difference gets larger at all thresholds. Perhaps there’s a bug in the LFR code?","27","0.3869652106529631","Troubleshooting","Maintenance"
"https://github.com/Trusted-AI/AIF360","610736984","issue","https://github.com/Trusted-AI/AIF360/issues/172","PrejudiceRemover returns same result when trying with different eta Hi, I've been exploring your library for one of my projects. When I was experimenting on using Prejudice Remover algorithm on Adult dataset, it will return same results no matter what the eta is. However, it works fine when I used different datasets such as COMPAS and German Credit Score -- the results differ with different etas. Can you help me to understand why is it happening? Thanks a lot!

For Adult Dataset, I used your `load_preproc_data_adult`.
I tested on `etas = [1,15,50,100]` iteratively, and I tested on `sensitive_attr='race'` and `sensitive_attr='sex'`. Both are giving me same results no matter what the eta is.
","0","0.4990912799970329","Dataset usage","Requirement Analysis"
"https://github.com/Trusted-AI/AIF360","608752117","issue","https://github.com/Trusted-AI/AIF360/issues/171","Adversarial debiasing doesn't have Tensorflow reuse persistence I'm trying to use the adversarial debiasing method but always getting this issue:

```
Variable debiased_classifier/classifier_mode/W1 already exists, disallowed. Did you mean to set reuse-True...
```

I suggest using something like 

```
with tf.variable_scope(""classifier_model"") as scope:
            W1 = tf.get_variable('W1', [features_dim, self.classifier_num_hidden_units],
                                  initializer=tf.contrib.layers.xavier_initializer(seed=self.seed1))
            b1 = tf.Variable(tf.zeros(shape=[self.classifier_num_hidden_units]), name='b1')
            scope.reuse_variables()
            h1 = tf.nn.relu(tf.matmul(features, W1) + b1)
            h1 = tf.nn.dropout(h1, keep_prob=keep_prob, seed=self.seed2)
            W2 = tf.get_variable('W2', [self.classifier_num_hidden_units, 1],
                                 initializer=tf.contrib.layers.xavier_initializer(seed=self.seed3))
            b2 = tf.Variable(tf.zeros(shape=[1]), name='b2')
            pred_logit = tf.matmul(h1, W2) + b2
            pred_label = tf.sigmoid(pred_logit)
```

within the code but I may have missed something. Would love some eyes on this issue.","22","0.9427815021990694","Bias mitigation methodology","Design"
"https://github.com/Trusted-AI/AIF360","605988246","issue","https://github.com/Trusted-AI/AIF360/issues/169","Missing dependencies from setup.py If one does a simple pip install aif360, the DIRemover intervention and the AdversarialDebiasing model won't work straight out of the box. This is due to their missing dependencies, namely, BlackBoxAuditing and Tensorflow.

I noticed that the requirements.txt has these dependencies and it is used in Travis tests but not in setup.py. Tensorflow is a particularly tricky installation because by default tf2 is installed through pip and one is forced to find the correct tensorflow version for AIF360 manually.

Is this intentional for some reason?","21","0.3521679019061218","Installation and shell commands","Deployment"
"https://github.com/Trusted-AI/AIF360","598143450","issue","https://github.com/Trusted-AI/AIF360/issues/167","load_preproc_data_adult() raises not implemented error with pandas 1.0.0 https://pandas.pydata.org/docs/whatsnew/v1.0.0.html :
The ‘outer’ method on Numpy ufuncs, e.g. np.subtract.outer operating on Series objects is no longer supported,","26","0.4464520986260116","Bias mitigation methodology","Design"
"https://github.com/Trusted-AI/AIF360","589783974","issue","https://github.com/Trusted-AI/AIF360/issues/164","inverse_transform() not implemented for disparate impact remover I am running the disparate impact demo. The output is encoded data and columns, and there isn't any inverse_transform function implemented in the code, as is the often the case, or maybe I am missing something ?

I would need to have the old 15-16 columns for post processing analysis, and try to avoid doing the recovery manually!","30","0.3368375383793886","Troubleshooting","Maintenance"
"https://github.com/Trusted-AI/AIF360","589705871","issue","https://github.com/Trusted-AI/AIF360/issues/163","For some reason struggling with GerryFairClassifier ImportError: cannot import name 'GerryFairClassifier'

Reproducible https://colab.research.google.com/drive/1G4w7egBd-t0PJ8L7mddlHIzXMxK1T-2A","29","0.5303030303030303","Troubleshooting","Maintenance"
"https://github.com/Trusted-AI/AIF360","587386160","issue","https://github.com/Trusted-AI/AIF360/issues/162","NotImplementedError in standard dataset Related to #109. Previously it warned `Returning an ndarray, but in the future this will raise a 'NotImplementedError'.` but now it returns the following error:

```
NotImplementedError                       Traceback (most recent call last)
<ipython-input-3-3996a519ec26> in <module>()
      1 from aif360.datasets import AdultDataset
----> 2 data = AdultDataset()

/Users/staceyro/anaconda/envs/aif360/lib/python3.7/site-packages/aif360/datasets/adult_dataset.py in __init__(self, label_name, favorable_classes, protected_attribute_names, privileged_classes, instance_weights_name, categorical_features, features_to_keep, features_to_drop, na_values, custom_preprocessing, metadata)
    110             features_to_keep=features_to_keep,
    111             features_to_drop=features_to_drop, na_values=na_values,
--> 112             custom_preprocessing=custom_preprocessing, metadata=metadata)

/Users/staceyro/anaconda/envs/aif360/lib/python3.7/site-packages/aif360/datasets/standard_dataset.py in __init__(self, df, label_name, favorable_classes, protected_attribute_names, privileged_classes, instance_weights_name, scores_name, categorical_features, features_to_keep, features_to_drop, na_values, custom_preprocessing, metadata)
    119             else:
    120                 # find all instances which match any of the attribute values
--> 121                 priv = np.logical_or.reduce(np.equal.outer(vals, df[attr]))
    122                 df.loc[priv, attr] = privileged_values[0]
    123                 df.loc[~priv, attr] = unprivileged_values[0]

/Users/staceyro/anaconda/envs/aif360/lib/python3.7/site-packages/pandas/core/series.py in __array_ufunc__(self, ufunc, method, *inputs, **kwargs)
    703             return None
    704         else:
--> 705             return construct_return(result)
    706 
    707     def __array__(self, dtype=None) -> np.ndarray:

/Users/staceyro/anaconda/envs/aif360/lib/python3.7/site-packages/pandas/core/series.py in construct_return(result)
    692                 if method == ""outer"":
    693                     # GH#27198
--> 694                     raise NotImplementedError
    695                 return result
    696             return self._constructor(result, index=index, name=name, copy=False)

NotImplementedError: 
```

Python: 3.7.2
Pandas: 1.0.3
AIF360: 0.2.3 (built from source)

To replicate:

```
import aif360
from aif360.datasets import AdultDataset
data = AdultDataset()
```","26","0.7840403598443413","Bias mitigation methodology","Design"
"https://github.com/Trusted-AI/AIF360","578983825","issue","https://github.com/Trusted-AI/AIF360/issues/160","GerryFairClassifier Pre and Postprocessing Equivalent Is there pre and post-processing techniques available with respect to rich subgroup fairness. And is there any studies comparing model-agnostic methods with model-specific methods? I just can't imagine none existing, I have looked everywhere. Thanks. ","22","0.338033212479298","Bias mitigation methodology","Design"
"https://github.com/Trusted-AI/AIF360","576840322","issue","https://github.com/Trusted-AI/AIF360/issues/156","Slack channel link broken The invitation link to the slack channel is currently broken.","11","0.5855383809274293","Troubleshooting","Maintenance"
"https://github.com/Trusted-AI/AIF360","569953253","issue","https://github.com/Trusted-AI/AIF360/issues/154","Port pre-processing algorithms to sklearn-compatible API - [ ] DisparateImpactRemover
- [ ] LearnedFairRepresentation
- [ ] OptimizedPreprocessing
- [X] Reweighing","30","0.2896690245393414","Troubleshooting","Maintenance"
"https://github.com/Trusted-AI/AIF360","569419639","issue","https://github.com/Trusted-AI/AIF360/issues/152","Port in-processing algorithms to sklearn-compatible API - [X] AdversarialDebiasing
- [ ] ARTClassifier
- [ ] MetaFairClassifier
- [ ] PrejudiceRemover
- [ ] GerryFairClassifier","22","0.5925837320574163","Bias mitigation methodology","Design"
"https://github.com/Trusted-AI/AIF360","569419543","issue","https://github.com/Trusted-AI/AIF360/issues/151","Port post-processing algorithms to sklearn-compatible API - [X] CalibratedEqualizedOdds
- [ ] EqualizedOdds
- [ ] RejectOptionClassifier","30","0.4485620536575951","Troubleshooting","Maintenance"
"https://github.com/Trusted-AI/AIF360","569415260","issue","https://github.com/Trusted-AI/AIF360/issues/150","Port micellaneous items to sklearn-compatible API - [ ] MEPS dataset
- [ ] differential fairness metrics
- [ ] rich subgroup fairness","30","0.5849307268931484","Troubleshooting","Maintenance"
"https://github.com/Trusted-AI/AIF360","569400348","issue","https://github.com/Trusted-AI/AIF360/issues/149","Pre-processing for adult, german and compas dataset I am trying to use AIF360 tool in one of my projects. I have facing problem in understanding the purpose of pre-processing, say, german credit dataset as described in the file AIF360/aif360/algorithms/preprocessing/optim_preproc_helpers/data_preproc_functions.py. 
Why is the custom processing described here needed in several algorithms like optimum pre-processing, meta classifier, reject classification. On the other hand, several algorithms do not require this for eg. adversarial debiasing, disparate impact remover, reweighing. Can you please help me understand the purpose? 

Thank you.","22","0.3355490841563265","Bias mitigation methodology","Design"
"https://github.com/Trusted-AI/AIF360","560736714","issue","https://github.com/Trusted-AI/AIF360/issues/144","Raw Probabilities for adversarial debiasing Is there a way to retrieve the raw predicted probabilities (not labels, but probabilities that someone falls within a class)?","23","0.3980160347873351","Bias mitigation methodology","Design"
"https://github.com/Trusted-AI/AIF360","542715308","issue","https://github.com/Trusted-AI/AIF360/issues/136","Subset of Dataset I want to train in-processing algorithms like Prejudice Remover, ART classifier, etc. using a specific subset of dataset like Adult Income, etc. Is there a way to create a subset based on row numbers like df.loc[[2,3,4,5]]?
If so, I can then train and predict using the existing api which take 'dataset' type as input for fit & predict function respectively. 
Not sure if this functionality is supported out of the box or if there is some workaround?  ","0","0.4290372075182202","Dataset usage","Requirement Analysis"
"https://github.com/Trusted-AI/AIF360","534670697","issue","https://github.com/Trusted-AI/AIF360/issues/132","failed to pip install cvxpy ![error](https://user-images.githubusercontent.com/58676064/70405424-1e999200-1a78-11ea-8c31-37d0fde4d1c0.png)
When I setup the environment, I meet the above error","21","0.7229062963933277","Installation and shell commands","Deployment"
"https://github.com/Trusted-AI/AIF360","532853525","issue","https://github.com/Trusted-AI/AIF360/issues/131","AIF360 common_utils import issue Hi,

I have installed aif360 with all dependencies.
But, unfortunately running into this error.
<img width=""1124"" alt=""Screen Shot 2019-12-04 at 1 15 35 PM"" src=""https://user-images.githubusercontent.com/15235441/70169241-7317c880-1698-11ea-8c9f-840910ac75da.png"">


Thank you.","21","0.8285739236747878","Installation and shell commands","Deployment"
"https://github.com/Trusted-AI/AIF360","532027019","issue","https://github.com/Trusted-AI/AIF360/issues/130","AIF360 calculations are off Hi all,

I am checking the values that AIF360 spits out. I have noticed that these values seem off. I have calculated them on my own using the lines below, where protected and unprotected are lists of zero and one.

sum(protected)/len(protected) = 0.7643312101910829
sum(unprotected)/len(unprotected) = 0.1386481802426343

Disparate impact by AIF360: 3.6549252892407136
Disparate impact by me: 5.512738853503185

Statistical parity difference by AIF360: 0.6256830299484484
Statistical parity difference by me: 0.6256830299484486

What especially strikes me is the difference between the disparate impact ""error"" and the statistical parity difference ""error"" as they are using practically the same numbers. Does anyone encoutered this issue before?","3","0.6181863511706446","Bias detection metrics validation","Validation"
"https://github.com/Trusted-AI/AIF360","527602757","issue","https://github.com/Trusted-AI/AIF360/issues/128","The german credit dataset's privileged class should be Age > 25, not Age >= 25 https://github.com/IBM/AIF360/blob/c718f1d6cd11f1a7536a1317ea280abd961a7c79/aif360/datasets/german_dataset.py#L31

So I looked at the original paper on this dataset: F. Kamiran and T. Calders, “Classifying without discriminating,” They mentioned that 190 account holders were classified as young. This corresponds to classifying people <= 25 as Young. If set as < 25, it would remove about 50 instances from the Young group. This might be why the results on this dataset was unstable. Changing this should also improve the results on the website.","22","0.4958388407499877","Bias mitigation methodology","Design"
"https://github.com/Trusted-AI/AIF360","523623685","issue","https://github.com/Trusted-AI/AIF360/issues/126","Documentation Typo for average_odds_difference() Hi AIF360 team,

Thank you for the excellent repo & superb documentation! 

I believe there is a typo for both:
https://aif360.readthedocs.io/en/latest/modules/metrics.html#aif360.metrics.ClassificationMetric.average_odds_difference

https://aif360.readthedocs.io/en/latest/modules/metrics.html#aif360.metrics.ClassificationMetric.average_abs_odds_difference

The implementation correctly returns 

1/2[(FPRD=unprivileged−FPRD=privileged)+(TPRD=**unprivileged** - TPRD=**privileged**))]

vs the stated

1/2[(FPRD=unprivileged−FPRD=privileged)+(TPRD=**privileged** - TPRD=**unprivileged**))]","3","0.6211638167004797","Bias detection metrics validation","Validation"
"https://github.com/Trusted-AI/AIF360","512984398","issue","https://github.com/Trusted-AI/AIF360/issues/119","Missing dependencies: ModuleNotFoundError: No module named 'numba' `from aif360.datasets import GermanDataset`
`from aif360.metrics import BinaryLabelDatasetMetric`
`from aif360.algorithms.preprocessing import Reweighing`

> ---------------------------------------------------------------------------
> ModuleNotFoundError                       Traceback (most recent call last)
> <ipython-input-11-8c39b9f7a866> in <module>
>       1 from aif360.datasets import GermanDataset
>       2 from aif360.metrics import BinaryLabelDatasetMetric
> ----> 3 from aif360.algorithms.preprocessing import Reweighing
> 
> ~/.local/lib/python3.6/site-packages/aif360/algorithms/preprocessing/__init__.py in <module>
>       1 from aif360.algorithms.preprocessing.disparate_impact_remover import DisparateImpactRemover
> ----> 2 from aif360.algorithms.preprocessing.lfr import LFR
>       3 from aif360.algorithms.preprocessing.optim_preproc import OptimPreproc
>       4 from aif360.algorithms.preprocessing.reweighing import Reweighing
> 
> ~/.local/lib/python3.6/site-packages/aif360/algorithms/preprocessing/lfr.py in <module>
>       3 
>       4 from aif360.algorithms import Transformer
> ----> 5 from aif360.algorithms.preprocessing.lfr_helpers import helpers as lfr_helpers
>       6 
>       7 
> 
> ~/.local/lib/python3.6/site-packages/aif360/algorithms/preprocessing/lfr_helpers/helpers.py in <module>
>       1 # Based on code from https://github.com/zjelveh/learning-fair-representations
> ----> 2 from numba.decorators import jit
>       3 import numpy as np
>       4 
>       5 @jit
> 
> ModuleNotFoundError: No module named 'numba'","27","0.9484954162373516","Troubleshooting","Maintenance"
"https://github.com/Trusted-AI/AIF360","501394659","issue","https://github.com/Trusted-AI/AIF360/issues/112","unused requirements Why are there a bunch of packages in this repo that are not used anywhere in the codebase?  Specifically:
Orange3
scs
networkx

Furthermore, there are packages only used in the notebooks and not in the library:
all the ipython stuff
lime
tqdm
matplotlib

","21","0.7083568605307737","Installation and shell commands","Deployment"
"https://github.com/Trusted-AI/AIF360","495351076","issue","https://github.com/Trusted-AI/AIF360/issues/109","fix deprecation warning in standard dataset WARNING:root:Missing Data: 3620 rows removed from AdultDataset.
.../python3.7/site-packages/aif360/datasets/standard_dataset.py:121: FutureWarning: outer method for ufunc <ufunc 'equal'> is not implemented on pandas objects. Returning an ndarray, but in the future this will raise a 'NotImplementedError'. Consider explicitly converting the Series to an array with '.array' first.
  priv = np.logical_or.reduce(np.equal.outer(vals, df[attr]))","26","0.7705079730396187","Bias mitigation methodology","Design"
"https://github.com/Trusted-AI/AIF360","495274069","issue","https://github.com/Trusted-AI/AIF360/issues/108","Statistical_parity/disparate impact results without prediction? Dear IBM,

How is it possible to get results for the metrics in the title (among other) without providing any prediction? ","3","0.578883000407664","Bias detection metrics validation","Validation"
"https://github.com/Trusted-AI/AIF360","491854358","issue","https://github.com/Trusted-AI/AIF360/issues/105","Docstrings incorrect for generalized FN, FP & TN The comments for generalized FN, FP, TN in `aif360.metrics.classification_metric`, the definitions all incorrectly include `the weighted sum of predicted scores where true labels are 'favorable'` and need to be updated to reflect the appropriate labels e.g.
```def num_generalized_false_negatives(self, privileged=None):
        """"""Return the generalized number of false negatives, :math:\`GFN\`, the
        weighted sum of predicted scores where true labels are 'favorable',
        optionally conditioned on protected attributes.``` ","30","0.2780534802130482","Troubleshooting","Maintenance"
"https://github.com/Trusted-AI/AIF360","491853218","issue","https://github.com/Trusted-AI/AIF360/issues/104","Method for Reject Option Classification fit_predict is missing arguments The `fit_predict` function for Reject Option Classification is defined as: ```    def fit_predict(self, dataset):
        """"""fit and predict methods sequentially.""""""
        return self.fit().predict(dataset)``` 
only expecting one dataset but the `fit` function requires two (dataset_true, dataset_pred)","23","0.5300186161960907","Bias mitigation methodology","Design"
"https://github.com/Trusted-AI/AIF360","487259814","issue","https://github.com/Trusted-AI/AIF360/issues/101","Remove gender recognition example Automated Gender Recognition has [been thoroughly shown](https://nbviewer.jupyter.org/github/IBM/AIF360/blob/master/examples/tutorial_gender_classification.ipynb) to be fundamentally discriminatory against trans people. Given that, it's somewhat bizzarre to see a tutorial in [how to make it ""fairer""](https://nbviewer.jupyter.org/github/IBM/AIF360/blob/master/examples/tutorial_gender_classification.ipynb) in a project aiming to reduce discriminatory outcomes from ML. I would really suggest (and appreciate it!) if the example was removed - it has no place here.","22","0.3639905967217338","Bias mitigation methodology","Design"
"https://github.com/Trusted-AI/AIF360","487149045","issue","https://github.com/Trusted-AI/AIF360/issues/97","Reconsider use of ""bias"" in README There are some issues with the way ""bias"" is being used in the README that are both inconsistent with what is actually possible with the tool & the current state of social science.

> ""The AI Fairness 360 toolkit is an open-source library to help detect and remove bias in machine learning models. The AI Fairness 360 Python package includes a comprehensive set of metrics for datasets and models to test for biases, explanations for these metrics, and algorithms to mitigate bias in datasets and models.""

While it's clear that we intend Bias in this context to meant ""prejudice"" or a disproportionate skew towards something, we must consider the larger impacts of this perspective. First off, it is impossible to not be prejudiced. In the social sciences, we call this ""subjectivity"". While some disciplines treat ""objectivity"" as an ideal, this concept is not transferable to human behavior, which is what we are modeling in our software applications. All humans have a distinct perspective, therefore all humans are prejudiced in some way.

The data & software applications that AIF360 seeks to ""de-bias"" were made by people based on their assumptions about how the world works & what they think matters. A software application is one group of people's collective opinion about what their stakeholders need. The structure of the software itself is based on these people's experience & is therefore prejudiced. For example, efforts like [Gendermag](https://gendermag.org/) & [A11y](https://a11yproject.com/) are helping people working in open source projects address assumptions about how their stakeholders are using their software tools at a very fundamental level.

When training data is both curated & labeled, assumptions are made by the curators & it's clear from the description of the project that this is what AIF360 can help to address. However we must also acknowledge that AIF360 is also prejudiced by the assumptions of its creators & maintainers, & therefore can neither remove bias nor de-bias.

What is ""fair"" & ""correct"" is highly situational. What is ""fair"" in one situation may not be ""fair"" in another. In some social situations, such as in Law Enforcement, the lack of fairness is indicative of larger social issues & ""de-biasing"" could potentially further harm disadvantaged stakeholders who have been excluded from having their own voice. We should assume such deviations on ""fairness"" to be the norm, not an aberration. This assumption of a universal ideal norm (or bias if you will) exists within AIF360 itself.

I suggest we reconsider that bias & prejudice are tied to something inherent in the human condition & are therefore unavoidable. Instead of a definition that dictates our own biased ""Truth"" through the removal of any non-conforming perspectives, I propose we re-define bias as simply ""limited perspective"" & re-evaluate our language & explanations from that starting point.

This issue is quite significant because our framing of ""bias"" presents potential harms to IBM's own credibility & intention. IBM has historically had moments where we were very narrow & short-sighted in our contribution to software projects. The global scale of our Thought-Leadership & influence meant our mistakes significantly impacted people's lives. That said, for all the harms we've done, we achieve great things too! [IBM has the advantage of having a much longer-term history than other tech companies to draw upon.](https://www.youtube.com/watch?v=OxpuU6baGqY) Given the scale of our potential impact, we have a corporate social responsibility to thoughtfully consider who is at stake when we bring new ideas into the world. 

### ""To visualize the future of IBM, you must know something of the past"" -Thomas J. Watson, Sr.

- IBM worked with the NYPD post-9/11 to develop surveillance software that lets police search by skin color [https://theintercept.com/2018/09/06/nypd-surveillance-camera-skin-tone-search/](https://theintercept.com/2018/09/06/nypd-surveillance-camera-skin-tone-search/)

“Now, thanks to confidential corporate documents and interviews with many of the technologists involved in developing the software, The Intercept and the Investigative Fund have learned that IBM began developing this object identification technology using secret access to NYPD camera footage. With access to images of thousands of unknowing New Yorkers offered up by NYPD officials, as early as 2012, IBM was creating new search features that allow other police departments to search camera footage for images of people by hair color, facial hair, and skin tone.”

Documents - [https://www.documentcloud.org/documents/4452844-IBM-SVS-Analytics-4-0-Plan-Update-for-NYPD-6.html](https://www.documentcloud.org/documents/4452844-IBM-SVS-Analytics-4-0-Plan-Update-for-NYPD-6.html)

- That time when IBM helped Indiana kick people off [Welfare](https://www.thenation.com/article/want-cut-welfare-theres-app/)

“In November 2006, Indiana Governor Mitch Daniels announced a 10-year, $1.16 billion contract with a consortium of tech companies, led by IBM and Affiliated Computer Services (ACS), to modernize and privatize eligibility procedures for the state’s Medicaid, food-stamp, and cash-assistance programs.”

“The design of electronic-governance systems affects our material well-being, the health of our democracy, and equity in our communities. But somehow, when we talk about data-driven government, we conveniently omit the often terrible impacts that these systems have on the poor and working-class people”

- IBM's punchcard system enabled the Nazis to efficiently round up Jews during WWII. [“There was an IBM customer site in every concentration camp.” ](http://www.opednews.com/articles/American-Corporate-Complic-by-Edwin-Black-090311-541.html)

### Further reading on the ""bias"" in software itself:

- Forsythe, Diana E., and David J. Hess. Studying Those Who Study Us: An Anthropologist in the World of Artificial Intelligence. Stanford University Press, 2002
- Eubanks, Virginia. Automating Inequality: How High-Tech Tools Profile, Police, and Punish the Poor. Picador, 2019.

","8","0.3362605708892347","Understanding Fairness Definition","Requirement Analysis"
"https://github.com/Trusted-AI/AIF360","471101020","issue","https://github.com/Trusted-AI/AIF360/issues/95","Include fairness metrics for ranking, not just binary classifiers. As suggested by @krvarshney I want to request inclusion of metrics for ranking algorithms e.g. as described here: https://arxiv.org/pdf/1706.06368.pdf.","6","0.2321259469696969","API expansion","Development"
"https://github.com/Trusted-AI/AIF360","461859497","issue","https://github.com/Trusted-AI/AIF360/issues/94","Upgrade tensorflow>=1.12.1 Upgrade tensorflow to version 1.12.1 or later for security fixes
Details
CVE-2019-9635 More information
moderate severity
Vulnerable versions: >= 1.0.0, < 1.12.1
Patched version: 1.12.1
NULL pointer dereference in Google TensorFlow before 1.12.2 could cause a denial of service via an invalid GIF file.

CVE-2018-7575 More information
critical severity
Vulnerable versions: >= 1.0.0, < 1.7.1
Patched version: 1.7.1
Google TensorFlow 1.7.x and earlier is affected by a Buffer Overflow vulnerability. The type of exploitation is context-dependent.

CVE-2018-7577 More information
high severity
Vulnerable versions: >= 1.1.0, < 1.7.1
Patched version: 1.7.1
Memcpy parameter overlap in Google Snappy library 1.1.4, as used in Google TensorFlow before 1.7.1, could result in a crash or read from other parts of process memory.

CVE-2018-10055 More information
high severity
Vulnerable versions: >= 1.1.0, < 1.7.1
Patched version: 1.7.1
Invalid memory access and/or a heap buffer overflow in the TensorFlow XLA compiler in Google TensorFlow before 1.7.1 could cause a crash or read from other parts of process memory via a crafted configuration file.

CVE-2018-7576 More information
moderate severity
Vulnerable versions: >= 1.0.0, < 1.6.0
Patched version: 1.6.0
Google TensorFlow 1.6.x and earlier is affected by: Null Pointer Dereference. The type of exploitation is: context-dependent.

","21","0.7586200894644315","Installation and shell commands","Deployment"
"https://github.com/Trusted-AI/AIF360","461510880","issue","https://github.com/Trusted-AI/AIF360/issues/92","Unable to replicate demo_lfr.ipynb notebook Whilst trying to replicate [demo_lrf](https://github.com/IBM/AIF360/blob/master/examples/demo_lfr.ipynb) notebook, I am getting following error.
I also tried changes mentioned in [issue 83](https://github.com/IBM/AIF360/issues/83). 

    /AIF360/aif360/algorithms/preprocessing/lfr_helpers/helpers.py:62: NumbaWarning: 
    Compilation is falling back to object mode WITH looplifting enabled because Function ""LFR_optim_obj"" failed type inference due to: Unknown attribute 'iters' of type recursive(type(CPUDispatcher(<function LFR_optim_obj at 0x7f5fea38a2f0>)))

    File ""AIF360/aif360/algorithms/preprocessing/lfr_helpers/helpers.py"", line 66:
    def LFR_optim_obj(params, data_sensitive, data_nonsensitive, y_sensitive,
        <source elided>

        LFR_optim_obj.iters += 1
        ^

    [1] During: typing of get attribute at /AIF360/aif360/algorithms/preprocessing/lfr_helpers/helpers.py (66)

    File ""AIF360/aif360/algorithms/preprocessing/lfr_helpers/helpers.py"", line 66:
    def LFR_optim_obj(params, data_sensitive, data_nonsensitive, y_sensitive,
        <source elided>

        LFR_optim_obj.iters += 1
        ^

    @jit
    /AIF360/aif360/algorithms/preprocessing/lfr_helpers/helpers.py:62: NumbaWarning: 
    Compilation is falling back to object mode WITHOUT looplifting enabled because Function ""LFR_optim_obj"" failed type inference due to: cannot determine Numba type of <class 'numba.dispatcher.LiftedLoop'>

    File ""AIF360/aif360/algorithms/preprocessing/lfr_helpers/helpers.py"", line 85:
    def LFR_optim_obj(params, data_sensitive, data_nonsensitive, y_sensitive,
        <source elided>
        L_z = 0.0
        for j in range(k):
        ^

    @jit","27","0.5838353686114879","Troubleshooting","Maintenance"
"https://github.com/Trusted-AI/AIF360","452126352","issue","https://github.com/Trusted-AI/AIF360/issues/88","Include additional classification metrics Include the following metrics:
1. Equalized odds difference: `max(|FPR_unpriv - FPR_priv|, |TPR_unpriv - TPR_priv|)`
2. Generalized equalized odds difference: `max(|GFPR_unpriv - GFPR_priv|, |GTPR_unpriv - GTPR_priv|)`
3. Generalized selection rate: mean score possibly conditioned by the group `E[\hat{S}]`","3","0.5579715327945065","Bias detection metrics validation","Validation"
"https://github.com/Trusted-AI/AIF360","450935021","issue","https://github.com/Trusted-AI/AIF360/issues/87","base rate computation in Equalized odds post processing Base rates for the privileged and unprivileged groups are not correctly estimated in the code.

https://github.com/IBM/AIF360/blob/0cf736f0310075a244dc63a045dbc7636dda2f2c/aif360/algorithms/postprocessing/eq_odds_postprocessing.py#L93","3","0.5305758071715521","Bias detection metrics validation","Validation"
"https://github.com/Trusted-AI/AIF360","442363935","issue","https://github.com/Trusted-AI/AIF360/issues/83","Incorrect index in LFR I'm pretty sure it should be

```python
dists = np.zeros((N, k))
```

as dists matches the distance between each data sample and prototype.

https://github.com/IBM/AIF360/blob/master/aif360/algorithms/preprocessing/lfr_helpers/helpers.py#L10-L17

","4","0.2315262718932442","Installation and shell commands","Deployment"
"https://github.com/Trusted-AI/AIF360","441236712","issue","https://github.com/Trusted-AI/AIF360/issues/80","Possibly incorrect indentation of for loop The for loop in line 125 should be outside the else part. 
Otherwise for loop runs for the initializations done in else part but it should work with both, if and else initializations.

https://github.com/IBM/AIF360/blob/0cf736f0310075a244dc63a045dbc7636dda2f2c/aif360/algorithms/postprocessing/reject_option_classification.py#L116-L132","22","0.5140950792326937","Bias mitigation methodology","Design"
"https://github.com/Trusted-AI/AIF360","422835299","issue","https://github.com/Trusted-AI/AIF360/issues/75","(enhancement) pre/post/in processing classes within aif360.algorithms could take in key word arguments during instantiation For example, refer to module aif360.preprocessing.optim_preproc.py

In order to create a OptimPreproc object, a user may have to pass in a bunch of arguments - that will be a mix of key word and positional arguments. In the below constructor, optiom_options is a dictionary but the rest is just positional.

```
class OptimPreproc(Transformer):
       def  __init__(self, optimizer, optim_options, unprivileged_groups=None,
               privileged_groups=None, verbose=False, seed=None):
 ```

It maybe better to simply pass in keyword arguments instead.","27","0.3490621391933073","Troubleshooting","Maintenance"
"https://github.com/Trusted-AI/AIF360","419514089","issue","https://github.com/Trusted-AI/AIF360/issues/74","Memory issues while opening StandardDataset ```
import pandas as pd
import sys
import numpy as np
np.random.seed(0)
from aif360.datasets import StructuredDataset as SD
from aif360.datasets import BinaryLabelDataset as BLD
from aif360.metrics import ClassificationMetric as CM
from aif360.metrics import BinaryLabelDatasetMetric
from aif360.algorithms.preprocessing import Reweighing
from sklearn.ensemble import RandomForestClassifier as RF
from sklearn.datasets import make_classification as mc 
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

data, label = mc(n_samples=10000,n_features=30)
bias_feature = label.copy()
np.random.shuffle(bias_feature)
agg_data = np.hstack([data,  bias_feature.reshape(-1,1), label.reshape(-1,1),])
pd_data = pd.DataFrame(agg_data, columns=list(range(1,31)) + [""gender"", ""labels""])
dataset = BLD(favorable_label=0, unfavorable_label=1,df=pd_data,
              label_names=[""labels""], protected_attribute_names=[""gender""], 
              privileged_protected_attributes=[2])
```
running ```BLD(favorable_label=0, unfavorable_label=1,df=pd_data, label_names=[""labels""], protected_attribute_names=[""gender""], privileged_protected_attributes=[2])```
in a python jupyter notebook 3 times runs in a memoryerror","27","0.9179525127671044","Troubleshooting","Maintenance"
"https://github.com/Trusted-AI/AIF360","419512924","issue","https://github.com/Trusted-AI/AIF360/issues/73","The classification metric of accuracy, precision, and recall differs from scikit-learn ```
import pandas as pd
import sys
import numpy as np
np.random.seed(0)
from aif360.datasets import StructuredDataset as SD
from aif360.datasets import BinaryLabelDataset as BLD
from aif360.metrics import ClassificationMetric as CM
from aif360.metrics import BinaryLabelDatasetMetric
from aif360.algorithms.preprocessing import Reweighing
from sklearn.ensemble import RandomForestClassifier as RF
from sklearn.datasets import make_classification as mc 
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

data, label = mc(n_samples=10000,n_features=30)
bias_feature = label.copy()
np.random.shuffle(bias_feature)
agg_data = np.hstack([data,  bias_feature.reshape(-1,1), label.reshape(-1,1),])
pd_data = pd.DataFrame(agg_data, columns=list(range(1,31)) + [""gender"", ""labels""])
dataset = BLD(favorable_label=0, unfavorable_label=1,df=pd_data,
              label_names=[""labels""], protected_attribute_names=[""gender""], 
              privileged_protected_attributes=[2])
dataset_orig_train, dataset_orig_test = dataset.split([0.7], shuffle=True)
dataset_orig_test_pred = dataset_orig_test.copy(deepcopy=True)
privileged_groups = [{'gender': 0}]
unprivileged_groups = [{'gender': 1}]
metric_orig_train = BinaryLabelDatasetMetric(dataset_orig_train, 
                                             unprivileged_groups=unprivileged_groups,
                                             privileged_groups=privileged_groups)
clf = RF()
clf.fit(dataset_orig_train.features,dataset_orig_train.labels)

predictions = clf.predict(dataset_orig_test.features)
proba_predictions = clf.predict_proba(dataset_orig_test.features)

dataset_orig_test_pred.scores = proba_predictions[:,0].reshape(-1,1)
dataset_orig_test_pred.labels = predictions.reshape(-1, 1)

cm_pred_valid = CM(dataset_orig_test, dataset_orig_test_pred, unprivileged_groups=unprivileged_groups,
                             privileged_groups=privileged_groups)

cm = [""precision"",""recall"", ""accuracy""]


metrics = {}
for c in cm:
    metric = eval(""cm_pred_valid."" + c + ""()"")
    metrics[c] =  metric


metrics[""recall""], metrics[""accuracy""], metrics[""precision""]


print(""Scikit-learn metrics"")
for key,value in {""recall"": recall_score,""accuracy"": accuracy_score, ""precision"": precision_score}.items():
    metric = value(dataset_orig_test.labels,predictions)
    print(""{} score is: {}"".format(key,metric))

print(""AIF360 metrics"")
for key in [""recall"",""accuracy"", ""precision""]:
    print(""{} score is: {}"".format(key,metrics[key]))
``` 

produces the following:

i.e. for scikit-learn
```
recall score is: 0.8780649436713055
accuracy score is: 0.8856666666666667
precision score is: 0.8928571428571429
```
and for AIF360

```
recall score is: 0.8933601609657947
accuracy score is: 0.8856666666666667
precision score is: 0.8786279683377308
```","27","0.8274572173802406","Troubleshooting","Maintenance"
"https://github.com/Trusted-AI/AIF360","403912000","issue","https://github.com/Trusted-AI/AIF360/issues/71","Unable to replicate MetaFairClassifier notebook The predicted labels and scores in MetaFairClassifier need to be transposed to be `(n, 1)` instead of `(1, n)`:
https://github.com/IBM/AIF360/blob/master/aif360/algorithms/inprocessing/meta_fair_classifier.py#L87-L88

I tried to fix this and the associated notebook but I was unable to replicate the results even after undo-ing the changes.","0","0.4293713532843968","Dataset usage","Requirement Analysis"
"https://github.com/Trusted-AI/AIF360","386456140","issue","https://github.com/Trusted-AI/AIF360/issues/58","make estimators and scorers sklearn compatible The `inprocessing` algorithms are basically like an `Estimator`. Ideally, it should be possible to replace a classifier in a scikit-learn pipeline, with one from aif360. An example pipeline (taken from [this](https://github.com/adrinjalali/aif360_tutorial/blob/master/building_fair_AI_models.ipynb) example) looks like:

```
model = make_pipeline(StandardScaler(),
                      LogisticRegression(solver='liblinear'))
X_train = meps_orig_train.features
y_train = meps_orig_train.labels.ravel()

model_lr = model.fit(X_train, y_train,
                     **{""logisticregression__sample_weight"":meps_orig_train.instance_weights})
```

But when we move to use a model such as `PrejudiceRemover`, we have to break the pipeline and have the model separate, since it doesn't follow the API requirements to be fit for a pipeline.

```
model = PrejudiceRemover(sensitive_attr=sens_attr, eta = 25.0)
scale = StandardScaler().fit(tr_dataset.features)

tr_dataset.features = scale.transform(tr_dataset.features)
model.fit(tr_dataset)
```

Once it can be fit in a pipeline, then we can use all the other mechanisms already available in sklearn, such as `GridSearchCV` to find best hyperparameters for the problem at hand.

That also brings are to the scorers. AIF36's scorers also don't fit into the scoring mechanism of sklearn. Once they do, we could use functions such as [`make_scorer`](https://scikit-learn.org/stable/modules/model_evaluation.html#defining-your-scoring-strategy-from-metric-functions) to create a scoring function and feed it into the sklearn's `GridSearchCV` for instance.

This may not be a trivial task, and some useful things such as having multiple scoring functions recorded and reported by the grid search are still in discussion and not yet available in sklearn. Until then, we can provide an easy way for the users to combine antibias scoring functions with performance scoring ones and use them to choose their best pipeline.

Another point regarding the API conventions is that if the `preprocessing` modules also fit in the `sklearn.Pipeline` as a transformer, then we can put their selection also in a hyperparameter search and do the search much easier than having to manually run them one by one and going through them to find the best solution.

Right now transformers which would change the number of samples or change the output are not supported in sklearn (AFAIK), but that's also in discussion and this usecase may be a good push for it.","30","0.3106371853290506","Troubleshooting","Maintenance"
"https://github.com/Trusted-AI/AIF360","383799460","issue","https://github.com/Trusted-AI/AIF360/issues/57","Use sphinx-gallery and `.py` examples instead of ipynb Using `sphinx` and `sphinx-gallery` we can generate docs and ipynb notebooks automatically from python example files.

This would greatly improve traceability of changes in the examples, and `make` can use them as a part of tests.","14","0.6575802336511345","Documentation","Development"
"https://github.com/Trusted-AI/AIF360","381857436","issue","https://github.com/Trusted-AI/AIF360/issues/56","make flake8 on Travis PR diff aware, and change line length Right now, the `.travis.cli` checks for some `flake8` errors with giving out warnings, with this line:

```
# exit-zero treats all errors as warnings.  The GitHub editor is 127 chars wide
flake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics
```

I know github's editor's max line is 127, but that's far too long for easily reviewing PRs on different systems and screens. I think it may make more sense to change that to 80 (what PEP8 recommends, with convincing arguments), and since we don't want to have to fix what already is in the repo, we can try and enforce that on new code.

`scikit-learn` for instance, uses a nice script which only fails only if there are new `flake8` warnings introduced, which can be found here: https://github.com/scikit-learn/scikit-learn/blob/master/build_tools/travis/flake8_diff.sh
","21","0.2409762651141961","Installation and shell commands","Deployment"
"https://github.com/Trusted-AI/AIF360","381094195","issue","https://github.com/Trusted-AI/AIF360/issues/53","Use OpenML and `sklearn.datasets.fetch_openml` for datasets. Right now the datasets are expected to be manually downloaded by the user and put in the right place.

They can be uploaded to openml (https://www.openml.org/) and retrieved by [`sklearn.datasets.fetch_openml()`](https://scikit-learn.org/dev/modules/generated/sklearn.datasets.fetch_openml.html#sklearn.datasets.fetch_openml) to avoid having to have the user manually download the datasets.","0","0.5831380818494224","Dataset usage","Requirement Analysis"
"https://github.com/Trusted-AI/AIF360","380543856","issue","https://github.com/Trusted-AI/AIF360/issues/51","Executing the Credit Risk notebook does not generate a de-biased dataset Executing the [Credit Risk Notebook](https://github.com/IBM/AIF360/blob/master/examples/tutorial_credit_scoring.ipynb) does not generate a de-biased dataset.  The results below are from a brand now GIT pull from the AIF360 repo.  As shown at the end, the new ""debiased"" model now over twice as biased as the original model:

![image](https://user-images.githubusercontent.com/814855/48462282-6b833a00-e78c-11e8-9218-3d351838a46e.png)
![image](https://user-images.githubusercontent.com/814855/48462288-72aa4800-e78c-11e8-92f5-703a566cded4.png)
![image](https://user-images.githubusercontent.com/814855/48462303-78079280-e78c-11e8-9e59-7ff9e04233c9.png)
","22","0.3163187023946519","Bias mitigation methodology","Design"
"https://github.com/Trusted-AI/AIF360","379841701","issue","https://github.com/Trusted-AI/AIF360/issues/48","Another typo in the credit scoring notebook The Step 4, the notebooks says `...was getting 19.4% more positive outcomes...` but should instead say `12.1%`.  The screenshot below shows the section in more detail.

![image](https://user-images.githubusercontent.com/814855/48359016-e1828680-e650-11e8-81a7-5e0f329bcc9d.png)
","0","0.4012146517365726","Dataset usage","Requirement Analysis"
"https://github.com/Trusted-AI/AIF360","379458301","issue","https://github.com/Trusted-AI/AIF360/issues/47","credit score tutorial I would like to run this tutorial in Watson Studio. How can I access data files?
How can I change path of data files?","22","0.40623726049735","Bias mitigation methodology","Design"
"https://github.com/Trusted-AI/AIF360","379038103","issue","https://github.com/Trusted-AI/AIF360/issues/45","Typo in the German Credit Scoring tutorial In step 5 of the Credit Scoring notebook, there is a typo in the section below.  The decimal place is shifted 1 to the left in this text above the cell: `the difference in mean outcomes is now 0.18250.`  The value should instead by `0.018250` as shown below the cell.

![image](https://user-images.githubusercontent.com/814855/48246452-59785480-e3a4-11e8-9b71-a2a2fd35f848.png)
","30","0.3279834426741643","Troubleshooting","Maintenance"
"https://github.com/Trusted-AI/AIF360","376918830","issue","https://github.com/Trusted-AI/AIF360/issues/44","Support for MacOS What OS was AIF360 built on?  I'm attempting to run on a Macbook Pro with the latest Mac OS however I am getting errors when attempting to install the module 'cvxpy'.  During the 'pip install cvxpy', I get numerous compilation errors:

```
  warning: include path for stdlibc++ headers not found; pass '-std=libc++' on the command line to use the libc++ standard library instead [-Wstdlibcxx-not-found]
  In file included from cvxpy/cvxcore/src/cvxcore.cpp:15:
  cvxpy/cvxcore/src/cvxcore.hpp:18:10: fatal error: 'vector' file not found
  #include <vector>
           ^~~~~~~~
  1 warning and 1 error generated.
  error: command 'gcc' failed with exit status 1
  
  ----------------------------------------
  Failed building wheel for cvxpy
```","21","0.4539579018493223","Installation and shell commands","Deployment"
"https://github.com/Trusted-AI/AIF360","376555308","issue","https://github.com/Trusted-AI/AIF360/issues/43","features_to_drop in Class  MEPSDataset19 not working I tried to run the MEPSDataset19 example with features_to_drop. It ran with some features but not with some eg. 'PHQ242'

Error it gave: KeyError: ""['PHQ242'] not in index""

Not sure if I did something wrong but found a workaround by not keeping it in features_to_keep so that the question to drop does not arise.","26","0.3545310588065933","Bias mitigation methodology","Design"
"https://github.com/Trusted-AI/AIF360","376554025","issue","https://github.com/Trusted-AI/AIF360/issues/42","File: tutorial_medical_expenditure.ipynb Prejudice Remover not working on Python 2.7 Prejudice Remover does not work on python 2.7 because of subprocess.run which is not supported.","22","0.5849307268931487","Bias mitigation methodology","Design"
"https://github.com/Trusted-AI/AIF360","369237432","issue","https://github.com/Trusted-AI/AIF360/issues/35","Python 3.6 failing The latest PR failed for python 3.6 (but passed for 2.7).  Here's the details: https://travis-ci.com/IBM/AIF360/jobs/151221128 ","32","0.6912060082088901","Dependency and Release","Deployment"
"https://github.com/Trusted-AI/AIF360","368717814","issue","https://github.com/Trusted-AI/AIF360/issues/34","Error getting UCI Adult dataset The Travis build is failing due to the ftp to grab the UCI adult dataset not working.  Details here:  https://travis-ci.com/IBM/AIF360/jobs/150380518   I tried the failing wget command from a local cgywin shell on my laptop and it worked for me, so the problem doesn't seem to be on the UCI side.","0","0.4497150033813157","Dataset usage","Requirement Analysis"
"https://github.com/Trusted-AI/AIF360","360372987","issue","https://github.com/Trusted-AI/AIF360/issues/16","Undefined name: CaldersVerwerTwoNaiveBayes in test_cv2nb.py This failure only happens on Python 3 and is probably caused by the scoping difference in Python 2 and Python 3.

[flake8](http://flake8.pycqa.org) testing of https://github.com/IBM/AIF360 on Python 3.6

$ __flake8 . --count --select=E901,E999,F821,F822,F823 --show-source --statistics__
```
./aif360/algorithms/inprocessing/kamfadm-2012ecmlpkdd/fadm/nb/tests/test_cv2nb.py:18:26: F821 undefined name 'CaldersVerwerTwoNaiveBayes'
        self.assertEqual(CaldersVerwerTwoNaiveBayes.N_CLASSES, 2)
                         ^
./aif360/algorithms/inprocessing/kamfadm-2012ecmlpkdd/fadm/nb/tests/test_cv2nb.py:19:26: F821 undefined name 'CaldersVerwerTwoNaiveBayes'
        self.assertEqual(CaldersVerwerTwoNaiveBayes.N_S_VALUES, 2)
                         ^
./aif360/algorithms/inprocessing/kamfadm-2012ecmlpkdd/fadm/nb/tests/test_cv2nb.py:20:13: F821 undefined name 'CaldersVerwerTwoNaiveBayes'
        m = CaldersVerwerTwoNaiveBayes(5, [2, 2, 2, 2, 3], 1.0, 0.8)
            ^
3    F821 undefined name 'CaldersVerwerTwoNaiveBayes'
3
```","7","0.7114299313252195","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","1090403625","issue","https://github.com/Trusted-AI/AIF360/issues/1004","Error when using MetricFrame with sample_params Hi, I am using `MetricFrame` to generate evaluations with a set of built-in metrics with extra parameters. The parameters are passed to `MetricFrame` using `sample_params`. I found that for built-in metrics with `sample_weight` and `pos_label` as extra parameters, `sample_weight` is always working well, but when `pos_label` was passed with a not-None value, it produced IndexError from fairlearn/metrics/_function_container.py. I've tried all *_rate metrics imported from fairlearn.metrics, like `false_positive_rate`, and also similar metrics imported from sklearn.metrics, like `recall_score`. All behaved similarly. 

Here is the minimal code to reproduce, and I am using fairlearn 0.7.0

```
import fairlearn.metrics as flm
from fairlearn.metrics import MetricFrame
y_true = [0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1]
y_pred = [0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1]
gp = ['d', 'a', 'c', 'b', 'b', 'c', 'c', 'c',
                         'b', 'd', 'c', 'a', 'b', 'd', 'c', 'c']
sample_weight = [0.1]*len(y_true) 
pos_label = 1
print(flm.false_positive_rate(y_true, y_pred, sample_weight, pos_label))
gm = MetricFrame(metrics=flm.false_positive_rate,
                             y_true=y_true,
                             y_pred=y_pred,
                             sensitive_features=gp,
                sample_params={'sample_weight': sample_weight, 'pos_label': pos_label})
print(gm.overall)
```

The above code will print 0.666666 for the direct computation by calling the metric function, and throw error when using in MetricFrame. The last part of the error message is: 
```
/home/cdsw/.local/lib/python3.6/site-packages/fairlearn/metrics/_function_container.py in generate_sample_params_for_mask(self, mask)
     82         curr_sample_params = dict()
     83         for name, value in self.sample_params_.items():
---> 84             curr_sample_params[name] = value[mask]
     85         return curr_sample_params
     86 

IndexError: too many indices for array: array is 0-dimensional, but 1 were indexed
```
and the screenshot is:
![image](https://user-images.githubusercontent.com/13775926/147650055-e6cc344a-e191-4dc1-ad05-747ce451a022.png)
![image](https://user-images.githubusercontent.com/13775926/147650371-7072ccd8-c7da-4baf-bfa8-aa7b328c9259.png)

When setting pos_label to None, both computation will work and print out the same result. 

Thanks in advance for any suggestion and help. 

","12","0.5590797704844992","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","1073748904","issue","https://github.com/Trusted-AI/AIF360/issues/1002","[Website Redesign] Incorporate new illustrations into current homepage ## New illustrations are complete for incremental integration

Related to closed issue:
https://github.com/fairlearn/fairlearn/issue

Please find new illustrations below:
[EPS.zip](https://github.com/fairlearn/fairlearn/files/7671495/EPS.zip)
[PNGs.zip](https://github.com/fairlearn/fairlearn/files/7671497/PNGs.zip)
[RESOURCE ICONS.zip](https://github.com/fairlearn/fairlearn/files/7671498/RESOURCE.ICONS.zip)
[SVGs.zip](https://github.com/fairlearn/fairlearn/files/7671499/SVGs.zip)

Redesign incorporation:
Redesign pending frontend developer volunteer:
[ms_Fairlearn_AllPages_V3b.pdf](https://github.com/fairlearn/fairlearn/files/7671503/ms_Fairlearn_AllPages_V3b.pdf)
New pages not seen in PDF do not have custom illustrations.

## **Top of page**: 

Narrow width of text block (currently too long for accessibility), and place illustration on right:

<img width=""1380"" alt=""Screen Shot 2021-12-07 at 12 46 11 PM"" src=""https://user-images.githubusercontent.com/6819397/145103697-630292bb-0812-422a-b58c-b665b7a9eb6e.png"">
 Ilustration:

![ms_Fairlearn_Homepage_header](https://user-images.githubusercontent.com/6819397/145103685-19bb3ba9-1399-4c4f-b3bb-b736836d22a5.png)

## Resource icons: 

Icons in attachments above are labeled accordingly:
<img width=""652"" alt=""Screen Shot 2021-12-07 at 12 49 02 PM"" src=""https://user-images.githubusercontent.com/6819397/145104352-962c5244-4b77-4eec-a5fd-349e9b08323a.png"">
","14","0.4711691147280668","Documentation","Development"
"https://github.com/fairlearn/fairlearn","1067461138","issue","https://github.com/Trusted-AI/AIF360/issues/999","Error Bars/Uncertainty Quantification for MetricFrame ## Is your feature request related to a problem? Please describe.
<!-- A clear and concise description of what the problem is. Ex. I'm always
frustrated when ... -->

There is currently no built-in way of assessing the volatility or uncertainty associated with metrics generated via `MetricFrame`. This has important implications for the interpretation of any observed values or differences between groups. For example, if a metric changes significantly under small changes (e.g. through resampling), the difference might be explained fully by something like [sampling error](https://en.wikipedia.org/wiki/Sampling_error). 

For example, here is a plot from the FairLearn documentation: 

[<img src=""https://fairlearn.org/v0.7.0/_images/sphx_glr_plot_quickstart_002.png"" width=""600"" />](https://fairlearn.org/v0.7.0/_images/sphx_glr_plot_quickstart_002.png)

The differences between the two groups in `accuracy` and `selection_rate` appear to be larger than the differences in `precision` and `recall`. However, it could just be that metrics like `selection_rate` are inherently higher variance metrics than `precision`, which might contextualize why the magnitude of the differences is larger.

## Describe the solution you'd like
<!-- A clear and concise description of what you want to happen. -->

I'd like to propose a [bootstrap](https://en.wikipedia.org/wiki/Bootstrapping_(statistics)) based solution built directly into `MetricFrame`. Bootstrap is a conceptually simple procedure that involves building an empirical distribution of a given metric/dataset by repeatedly:

1. Sampling up to the full size of the dataset _with replacement_
2. Re-calculating the metric on this new sample

When repeated hundreds or thousands of times, we can study the empirical distribution and produce useful summaries like [confidence intervals](https://en.wikipedia.org/wiki/Bootstrapping_(statistics)#Deriving_confidence_intervals_from_the_bootstrap_distribution) for the metric.  

There are several reasons why bootstrap might be an appropriate solution here:

- It is widely adopted and studied, making it a statistically defensible choice
- Bootstrap is agnostic to characteristics of the dataset and metric, making it largely assumption-free and widely applicable
- It is straightforward to explain and implement as a higher level layer on top of the core MetricFrame subroutines

However, it does come with a few downsides. Primarily, bootstrap is very computationally intensive. By definition, it is 100x - 1000x+ as intensive as running the core MetricFrame routine, which makes performance a key concern. This might be mitigated by:

- Having the bootstrap procedure be opt-in, and defaulted to ""off"". This has the added benefit of being completely backwards compatible.
- Improving the performance of the current MetricFrame (which seems to be being addressed by PRs like https://github.com/fairlearn/fairlearn/pull/985). 
- Allowing for parallel processing at the bootstrap layer, as bootstrap is an embarassingly parallel operation. This would achieve an approximately linear speedup with the number of available processes on a user machine. 

Like most methods, bootstrap also does not work if the user supplied dataset is not representative of the population. In addition, bootstrap will struggle with very low sample size or incredibly homogenous groups (e.g. if every member of group A had an identical label). These should also be addressed with appropriate warnings. 

Finally, it's important to point out that error bars derived from bootstrap cannot directly be used to make claims about [statistical significance](https://en.wikipedia.org/wiki/Statistical_significance), especially amongst multiple groups simultaneously. In other words, simply looking at overlaps between error bars does not constitute a formal hypothesis test. 

## Describe alternatives you've considered, if relevant
<!-- A clear and concise description of any alternative solutions or features
you've considered. -->

There are some alternative resampling methods that might provide tighter estimates or be slightly more computationally efficient, like the [jackknife](https://en.wikipedia.org/wiki/Jackknife_resampling), but these are not as well known in the literature. 

There is also a complementary line of work possible in structuring formal hypothesis tests, but it would supplement this contribution (and not be in place of it). 

## Additional context
<!-- Add any other context or screenshots about the feature request here. -->

I'd like to work on adding support for bootstrap and confidence intervals in FairLearn. I'm opening this issue to solicit community and maintainer feedback on the general idea, proposed approach, and finer details around the implementation. In addition to general comments, some thoughts on the following topics would be appreciated. 


#### Performance / Parallel Processing

As mentioned earlier, one way to mitigate performance concerns would be to introduce parallel processing capabilities into FairLearn specifically for the bootstrap layer. Because `scikit-learn` is already a FairLearn dependency, we could use the `joblib` library to achieve this with relatively little additional code without taking on any additional dependencies. I'd just like to make sure the concept of multiprocessing and an explicit (rather than implicit) joblib dependency is OK first. 

#### API

There'd be a few proposed changes to the MetricFrame API here, both in the input parameters to the constructor and the output data formats. Here's an example of what the new constructor may look like (additional parameters annoted with a # NEW comment): 

```python3
def __init__(self,
             *,
             metrics: Union[Callable, Dict[str, Callable]],
             y_true,
             y_pred,
             sensitive_features,
             control_features: Optional = None,
             sample_params: Optional[Union[Dict[str, Any], Dict[str, Dict[str, Any]]]] = None,
             n_boot: Optional[int] = None,  # NEW
             quantiles: Tuple[float, ...] = (2.5, 97.5), # NEW
             n_jobs: int = -1 # NEW
):
```

`n_boot` is the number of bootstrap iterations to run. This can be defaulted to None, which means skipping the bootstrap process altogether and ignoring the following parameters. In practice, it would be recommended to set this to ~1000.

`quantiles` are the percentiles from the empirical bootstrap distribution to preserve and report back to the user. This is defaulted to (2.5, 97.5) to represent a 95% confidence interval, but can easily be changed to an 80% interval by setting it to (10, 90). With this type of parameter specification, users can also sample more points than just the endpoints of the confidence interval -- for example, if you are interested in every quartile, the API would be flexible enough to allow for e.g `quantiles = (2.5, 25, 50, 75, 97.5)`, or to allow for asymmetric confidence intervals. We could easily drop this though and replace it with a single floating point number representing the width of the CI. 

`n_jobs` is the popular joblib/scikit-learn parameter representing how many cores a user is willing to use on their machine for parallelization. `n_jobs=1` would use a single processor, `n_jobs=-1` uses every available processor, `n_jobs=-2` is all but one processor, etc. More information here: https://scikit-learn.org/stable/glossary.html#term-n_jobs

In terms of output structure, there are two choices that come to mind: modifying the internals of each `.by_group` dataframe to hold something like a tuple instead of a scalar in each cell, or returning a separate frame for each requested quantile. I tend to prefer the latter approach, which may look something like this: 

![image](https://user-images.githubusercontent.com/5302119/144090553-e81d345a-bbf8-447d-9927-4f5a9d92888d.png)

because of its readability and ease of working with -- any operation or workflow established on the ""main"" frame would also work for the quantiles. It would be great to hear thoughts on this, and if it seems like a decent idea, how it might be exposed off the main MetricFrame object (for example, maybe `mf.by_group_intervals` or `mf.by_group_quantiles`)? 

Finally, in terms of plotting, the Pandas Dataframe plotting API has support for error bars: https://pandas.pydata.org/pandas-docs/stable/user_guide/visualization.html#plotting-with-error-bars , and it might be straightforward to extend the plotting documentation to accommodate this.

Would appreciate any and all thoughts!
-Harsha 
","7","0.335066079134792","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","1051869959","issue","https://github.com/Trusted-AI/AIF360/issues/994","DOC typo make_derived_metric API ref There is a small typo in the API ref entry of `make_derived_metric`.

In the sentence under Returns: *Function with the same signature as the metric but with additional sensitive_feature= and method= arguments, to enable the required computation*, `sensitive_feature` should say `sensitive_features`
","2","0.3399184149184151","Performance measurement","Validation"
"https://github.com/fairlearn/fairlearn","1051865348","issue","https://github.com/Trusted-AI/AIF360/issues/993","DOC make_derived_metric API ref should contain options for transform #### Describe the issue linked to the documentation

The [API ref entry for `make_derived_metric`](https://github.com/fairlearn/fairlearn/blob/v0.7.0/fairlearn/metrics/_make_derived_metric.py#L96-L155) currently does not contain the list of strings that can be used as argument for the `transform` parameter.

#### Suggest a potential alternative/fix

Add the list of options to the docstring, i.e., something like ""one of `['difference', 'group_min', 'group_max', 'ratio']`""
","3","0.4456134582716862","Bias detection metrics validation","Validation"
"https://github.com/fairlearn/fairlearn","1051215844","issue","https://github.com/Trusted-AI/AIF360/issues/992","DOC Translating Fairlearn documentation to Spanish <!--
If your issue is a usage question, please submit it in one of these other
channels instead:
- StackOverflow with the `fairlearn` tag:
  https://stackoverflow.com/questions/tagged/fairlearn
- Discord: https://discord.gg/R22yCfgsRn
The issue tracker is used only to report bugs and feature requests. For
questions, please use either of the above platforms. Most question issues are
closed without an answer on this issue tracker. Thanks for your understanding.
-->

#### Is your feature request related to a problem? Please describe.
<!-- A clear and concise description of what the problem is. Ex. I'm always
frustrated when ... -->

To improve accessibility of Fairlearn, I would like to help translate the documentation to Spanish. I chatted with the Python in Spanish community about how they translate Python documentation, and here are their recommendations

1. Since Fairlearn documentation is in `.rst` format, I can use `sphinx` to extract text and generate `.po` files
2. With the `.po` files I can organize translations by file
3. Whenever I am ready to show documentation, I can use `sphinx-intl `to generate it in the new language, and display it as they original documents are
4. All translations would be worked on here https://github.com/python/python-docs-es 
5. The translated documentation can be hosted separately (like Scikit-learn https://qu4nt.github.io/sklearn-doc-es/index.html)
6.  Then a link can be added to the Fairlearn website

#### Describe the solution you'd like
<!-- A clear and concise description of what you want to happen. -->

Help assessing what documents to start with helps, help automating file transformation helps too. I can start translating and seek support recruiting translators. 

#### Describe alternatives you've considered, if relevant
<!-- A clear and concise description of any alternative solutions or features
you've considered. -->

#### Additional context
<!-- Add any other context or screenshots about the feature request here. -->
","11","0.6057586367308966","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","1049589457","issue","https://github.com/Trusted-AI/AIF360/issues/991","DOC add user guide ThresholdOptimizer #### Describe the issue linked to the documentation

There is currently no user guide for `fairlearn.postprocessing.ThresholdOptimizer`.

#### Suggest a potential alternative/fix

Add a user guide for `fairlearn.postprocessing.ThresholdOptimizer`.","14","0.6844436009362944","Documentation","Development"
"https://github.com/fairlearn/fairlearn","1049588291","issue","https://github.com/Trusted-AI/AIF360/issues/990","DOC add user guide CorrelationRemover #### Describe the issue linked to the documentation

There is currently no user guide for `fairlearn.preprocessing.CorrelationRemover `.

#### Suggest a potential alternative/fix

Add a user guide for `fairlearn.preprocessing.CorrelationRemover `.","14","0.692098437401152","Documentation","Development"
"https://github.com/fairlearn/fairlearn","1049587506","issue","https://github.com/Trusted-AI/AIF360/issues/989","DOC add user guide GridSearch #### Describe the issue linked to the documentation

There is currently no user guide for `fairlearn.reductions.GridSearch`.

#### Suggest a potential alternative/fix

Add a user guide for `fairlearn.reductions.GridSearch`.
","14","0.7916113114442973","Documentation","Development"
"https://github.com/fairlearn/fairlearn","1049586215","issue","https://github.com/Trusted-AI/AIF360/issues/988","DOC Add user guide for ExponentiatedGradient  #### Describe the issue linked to the documentation

There is currently no user guide for `ExponentiatedGradient`.

#### Suggest a potential alternative/fix

Add the user guide for `ExponentiatedGradient`.
","14","0.6767887644714371","Documentation","Development"
"https://github.com/fairlearn/fairlearn","1049577695","issue","https://github.com/Trusted-AI/AIF360/issues/987","DOC standardize where what information is put (user guide, API ref, etc.) #### Describe the issue linked to the documentation

In the current API reference we vary quite a lot between very minimal descriptions (e.g., in [postprocessing](https://fairlearn.org/v0.7.0/api_reference/fairlearn.postprocessing.html#fairlearn.postprocessing.ThresholdOptimizer)) and quite detailed descriptions (e.g., [reductions](https://fairlearn.org/v0.7.0/api_reference/fairlearn.reductions.html) and [`MetricFrame`](https://fairlearn.org/v0.7.0/api_reference/fairlearn.metrics.html#fairlearn.metrics.MetricFrame)). It also varies a bit where what type of information can be found, e.g., sometimes the ""Notes"" are quite heavily used (e.g., [preprocessing](https://fairlearn.org/v0.7.0/api_reference/fairlearn.preprocessing.html)) and references to papers can be found in various places in the docstring.

As we are in the process of adding several techniques, I think it makes sense to somewhat standardize what we want our documentation to look like.

#### Suggest a potential alternative/fix
- [ ] Decide where we want each type of information to go (and perhaps create a template?).
- [ ] Add section to contributor guide that explains where what information should go.
- [ ] Update existing documentation accordingly.

In my view I think it would make sense to follow scikit-learn here, where the description of a technique in the API ref is usually very minimal and accompanied with a link to the relevant User Guide item. Some relevant links:
* https://scikit-learn.org/stable/developers/contributing.html#guidelines-for-writing-documentation
* ""New features have some maintenance overhead. We expect PR authors to take part in the maintenance for the code they submit, at least initially. New features need to be illustrated with narrative documentation in the user guide, with small code snippets. If relevant, please also add references in the literature, with PDF links when possible.""  [from PR checklist](https://scikit-learn.org/stable/developers/contributing.html#pull-request-checklist)","14","0.4962731795567617","Documentation","Development"
"https://github.com/fairlearn/fairlearn","1037709316","issue","https://github.com/Trusted-AI/AIF360/issues/986","DOC: Revise website navbar menu structure to reflect website redesign #### Describe the issue linked to the documentation
The current website navbar menu structure does not allow Fairlearn users to easily find existing material or easily allow contributors to contribute new material.

#### Suggest a potential alternative/fix

Update the website menu hierarchy to reflect the website redesign from @nessamilan. The revised menu structure can be found [here](https://www.figma.com/file/wt76z0M87RBDDEoyAEpD2g/%5BOPEN-SOURCE%5D-Fairlearn-Redesign?node-id=2565%3A12134). See #978 for a related issue.

This includes menus for: Learn, API Docs, Contribute, and About Us.

Note that some of the suggested pages may not yet have content that exists for them. We can open issues for these pages to specify content to add. And, some of the menu items that were proposed in the re-design may no longer be relevant. Let's have some discussion in the comments about those.


### Learn Menu
- Learn
  - Overview
  - User Guide
  - Code Examples
  - Sociotechnical Examples
  - Case Studies
  - Resources
 

<img width=""772"" alt=""Fairlearn_navbar_Learn"" src=""https://user-images.githubusercontent.com/5882008/139094691-a7e5d23d-7be3-4426-8c44-608d726eaa78.png"">




### API Docs Menu
- API Docs
  - Overview
  - fairlearn.metrics package
  - fairlearn.postprocessing package
  - fairlearn.reductions package
  - fairlearn.widget package
  - fairlearn.datasets package

<img width=""752"" alt=""Fairlearn_navbar_API"" src=""https://user-images.githubusercontent.com/5882008/139095053-e887e13e-ac02-4531-8ec1-63bc05b41495.png"">

### Contribute Menu
- Contribute
  - Overview
  - How to talk about fairness
  - Fairlearn Repository
  - Development Process
  - Contribute Code
  - Contributing Examples
  - API and Design Proposals

<img width=""749"" alt=""Fairlearn_navbar_Contribute"" src=""https://user-images.githubusercontent.com/5882008/139095268-5a807e14-0934-4849-be02-3d7c8c219bc3.png"">

### About Us Menu

- About Us
  - Overview
  - Governance
  - Code of Conduct
  - Roadmap

<img width=""847"" alt=""Fairlearn_navbar_About"" src=""https://user-images.githubusercontent.com/5882008/139095457-e5e4c970-3bee-45eb-895a-7fd2a638b091.png"">
","14","0.7266902878221615","Documentation","Development"
"https://github.com/fairlearn/fairlearn","1029889726","issue","https://github.com/Trusted-AI/AIF360/issues/984","ENH: Incorporating folktables datasets from Ding et al. <!--
If your issue is a usage question, please submit it in one of these other
channels instead:
- StackOverflow with the `fairlearn` tag:
  https://stackoverflow.com/questions/tagged/fairlearn
- Discord: https://discord.gg/R22yCfgsRn
The issue tracker is used only to report bugs and feature requests. For
questions, please use either of the above platforms. Most question issues are
closed without an answer on this issue tracker. Thanks for your understanding.
-->

#### Is your feature request related to a problem? Please describe.
<!-- A clear and concise description of what the problem is. Ex. I'm always
frustrated when ... -->
Fairlearn currently exposes and makes use of the UCI Adult dataset in a few places, e.g. [in this example](https://github.com/fairlearn/fairlearn/blob/1247b081b5cbdf9373595b7e4627c142ae30ae44/examples/plot_grid_search_census.py). (Some recent work)[https://arxiv.org/abs/2108.04884] I've been apart of has highlighted a few limitations with UCI Adult and introduced a [few new datasets](https://github.com/zykls/folktables) for fair ML intended to serve as a replacement.

I discussed this briefly with @romanlutz, and I'd love to find a way to incorporate these datasets into fairlearn, perhaps as a dedicated example or as a replacement to UCI Adult in the current example. I'm happy to do most or all of the implementation work needed to make something like that happen, but I first wanted to discuss with the maintainers or others involved with the project what the best way of incorporating these datasets would be. 

Looking forward to discussing.
","11","0.4825454130486657","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","1025602327","issue","https://github.com/Trusted-AI/AIF360/issues/978","DOC Make navigation bar consistent across website This builds on #825. The smaller textual inconsistencies between the headers of the custom landing page and sphinx-generated pages were resolved through #976, but the layout is still off. This is because the landing page is custom html while the remaining pages all use the pydata-sphinx-theme's default header.

Some perhaps useful information:
The top navbar of our theme seems to be configurable as described in https://pydata-sphinx-theme.readthedocs.io/en/latest/user_guide/sections.html#the-navbar-items
The default templates are at https://github.com/pydata/pydata-sphinx-theme/tree/master/pydata_sphinx_theme/_templates

It is not entirely clear to me whether changing the sphinx header or the custom HTML of the landing page to be consistent with the other is the best way forward. Exploring that is part of this issue and already a meaningful contribution.

There is another planned change to adjust the header to be consistent with the latest design, but this requires bigger changes. E.g., in order to change the ""user guide"" label to ""learn"" we'd need to merge the existing user guide with example notebooks in a reasonable way. All of these changes are not part of this issue, which only targets the layout adjustments necessary to get the headers to be consistent.","14","0.8327117619552702","Documentation","Development"
"https://github.com/fairlearn/fairlearn","1025592997","issue","https://github.com/Trusted-AI/AIF360/issues/977","DOC Restructure the assessment section of the user guide This first came up in #943 through [a comment](https://github.com/fairlearn/fairlearn/issues/943#issuecomment-914997930_) by @hildeweerts.

The existing headers and structure may not be ideal for helping users discover the section they're actually looking for and are somewhat confusing (as #943 shows). As we're working on the user guide anyway we should consider better ways to structure this section.

The original comment by @hildeweerts:
> All information in `MetricFrame` relate to group comparisons, which is why it is under 'Metrics with Grouping'. I can see why the headers might be a bit confusing though, as `MetricFrame.by_group` by itself is not strictly a metric on its own but a table of disaggregated ""ungrouped"" metrics, so some renaming/restructuring of this section is probably not a bad idea. On a related note, I'd also be in favor of retiring the Fairlearn Dashboard as a separate top-level header. This is all a bit out of the scope for this PR, though (we always try to keep PR's as small as possible to avoid blocking small (uncontroversial) changes by other changes that require more discussion). I'd suggest to open a new issue to re-evaluate the structure of the Assessment section of the user guide.

FYI @SeanMcCarren since he's the author of #943 (Thank you!) and may be able to point out additional parts that are confusing in the existing user guide.","15","0.2818866325499526","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","1016262151","issue","https://github.com/Trusted-AI/AIF360/issues/972","fairlearn.metrics._group_metric_set._create_group_metric_set fails for binary classification. Describe the bug
I am trying to upload the fairness dashboard to Azure using the below code.

```
from fairlearn.metrics._group_metric_set import _create_group_metric_set

dash_dict = _create_group_metric_set(y_true=Y_test,
                                    predictions=ys_pred,
                                    sensitive_features=sf,
                                    prediction_type='binary_classification')
```

Y_test is an array containing values [1,0,1,1,0,0,0,1,0,0...................................]
where len(np.unique(y_true)) == 2. And its binary classification problem.

Expected Results
We would expect to get the dashboard dict returned for this classification model instead of getting an exception.

Actual Results
from fairlearn.metrics._group_metric_set import _create_group_metric_set
      4 
----> 5 dash_dict = _create_group_metric_set(y_true=Y_test,
      6                                     predictions=ys_pred,
      7                                     sensitive_features=sf,

/databricks/python/lib/python3.8/site-packages/fairlearn/metrics/_group_metric_set.py in _create_group_metric_set(y_true, predictions, sensitive_features, prediction_type)
    177             metric_dict = dict()
    178             for metric_key, metric_func in function_dict.items():
--> 179                 gmr = MetricFrame(metrics=metric_func,
    180                                   y_true=result[_Y_TRUE],
    181                                   y_pred=prediction,

/databricks/python/lib/python3.8/site-packages/fairlearn/metrics/_metric_frame.py in compatible_metric_frame_init(self, metric, *args, **kwargs)
     79         # Call the new constructor with positional arguments passed as keyword arguments
     80         # and with the `metric` keyword argument renamed to `metrics`.
---> 81         new_metric_frame_init(self,
     82                               **metric_arg_dict,
     83                               **positional_dict,

/databricks/python/lib/python3.8/site-packages/fairlearn/metrics/_metric_frame.py in __init__(self, metrics, y_true, y_pred, sensitive_features, control_features, sample_params)
    226 
    227         self._overall = self._compute_overall(func_dict, y_t, y_p, cf_list)
--> 228         self._by_group = self._compute_by_group(func_dict, y_t, y_p, sf_list, cf_list)
    229 
    230     def _compute_overall(self, func_dict, y_true, y_pred, cf_list):

/databricks/python/lib/python3.8/site-packages/fairlearn/metrics/_metric_frame.py in _compute_by_group(self, func_dict, y_true, y_pred, sf_list, cf_list)
    244             rows = copy.deepcopy(cf_list) + rows
    245 
--> 246         return self._compute_dataframe_from_rows(func_dict, y_true, y_pred, rows)
    247 
    248     def _compute_dataframe_from_rows(self, func_dict, y_true, y_pred, rows):

/databricks/python/lib/python3.8/site-packages/fairlearn/metrics/_metric_frame.py in _compute_dataframe_from_rows(self, func_dict, y_true, y_pred, rows)
    269                 # Only call the metric function if the mask is non-empty
    270                 if sum(mask) > 0:
--> 271                     curr_metric = func_dict[func_name].evaluate(y_true, y_pred, mask)
    272                     result[func_name][row_curr] = curr_metric
    273         return result

/databricks/python/lib/python3.8/site-packages/fairlearn/metrics/_function_container.py in evaluate(self, y_true, y_pred, mask)
    101         params = self.generate_sample_params_for_mask(mask)
    102 
--> 103         return self.func_(y_true[mask], y_pred[mask], **params)
    104 
    105     def evaluate_all(self,

/databricks/python/lib/python3.8/site-packages/sklearn/metrics/_ranking.py in roc_auc_score(y_true, y_score, average, sample_weight, max_fpr, multi_class, labels)
    564         labels = np.unique(y_true)
    565         y_true = label_binarize(y_true, classes=labels)[:, 0]
--> 566         return _average_binary_score(
    567             partial(_binary_roc_auc_score, max_fpr=max_fpr),
    568             y_true,

/databricks/python/lib/python3.8/site-packages/sklearn/metrics/_base.py in _average_binary_score(binary_metric, y_true, y_score, average, sample_weight)
     73 
     74     if y_type == ""binary"":
---> 75         return binary_metric(y_true, y_score, sample_weight=sample_weight)
     76 
     77     check_consistent_length(y_true, y_score, sample_weight)

/databricks/python/lib/python3.8/site-packages/sklearn/metrics/_ranking.py in _binary_roc_auc_score(y_true, y_score, sample_weight, max_fpr)
    335     """"""Binary roc auc score.""""""
    336     if len(np.unique(y_true)) != 2:
--> 337         raise ValueError(
    338             ""Only one class present in y_true. ROC AUC score ""
    339             ""is not defined in that case.""

ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

Versions:
I am running this on Databricks running python 3.8 with fairlearn version 0.7.0 from pypi.

Thanks!","12","0.751422400657206","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","1015682811","issue","https://github.com/Trusted-AI/AIF360/issues/971","FEAT: add conditional demographic disparity metric from Wachter et al.  Full paper: https://arxiv.org/abs/2005.05906 ""Why Fairness Cannot Be Automated: Bridging the Gap Between EU Non-Discrimination Law and AI"" by Sandra Wachter, Brent Mittelstadt, Chris Russell

The [implementation details from AWS](https://docs.aws.amazon.com/sagemaker/latest/dg/clarify-data-bias-metric-cddl.html) may be helpful.

This issue is merely about implementing the CDD metric as part of `fairlearn.metrics`. It may prove somewhat challenging for the following reasons:

- If I read our code correctly we're basically taking `metrics, y_true, y_pred, sensitive_features` in our `MetricFrame`, and then we split the `y_true, y_pred` based off of `sensitive_features` and pass those splits individually to the metric functions. That's how we calculate the disaggregated metrics. For the aggregated case we don't do the splits. For CDD, I am a bit puzzled as to how that's supposed to work since it's somewhat more complex and needs the number of positive and negative samples for both the sensitive feature group in question as well as overall. If we only pass the split data then we don't have the overall data to calculate CDD. We could use `functools.partial` to define a function where we pass in the positives and negatives for the overall dataset (or rather the model's predictions on the dataset since we'd be ignoring `y_true`) to work around this problem. I suppose a helper function that does this based on `y_pred` would be nice.
- The aggregate of CDD for the whole dataset doesn't make a lot of sense since it would just be 0. That's fine, though.
- The name CDD can cause confusion as we have a metric called demographic parity.

Assuming people agree with this rough outline, this item requires:
- implementation of CDD in `fairlearn.metrics`
- proper API documentation in the numpydoc format, ideally with a small example and attribution to the paper mentioned above. Importantly, we should explain what high (1) and low (-1) values mean and what the ideal value is (0) 
- In the user guide we have a [table](https://fairlearn.org/v0.7.0/user_guide/assessment.html?highlight=partial#scalar-results-from-metricframe) where this could be added. I believe `group_max` would be the most meaningful aggregation (?) 


I'm looking for comments from other @fairlearn/fairlearn-maintainers  and members of the community. If people agree with this framing I'll tag it as ""help wanted""","15","0.2639351425399748","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","1013769695","issue","https://github.com/Trusted-AI/AIF360/issues/970","DOC: styleguide for referring to groups of people I've noticed we that we have a few occurrences of terms like ""blacks"", ""whites"", ""females"", ""males"", and perhaps others in our documentation. This made me aware that I don't know the best way to refer to various groups of people. Is it ""blacks"", ""Blacks"", ""Black people"", ""black people"", or something completely different? Same applies to other groups.

I vaguely remember somebody telling me that we should use ""Black people"" instead of ""Blacks"" (and the same for other groups of people). I searched for a reference and only found this admittedly somewhat old reference from 2016 https://splinternews.com/why-we-dont-call-people-blacks-and-whites-1793864403

Similarly, for ""female"" I stumbled upon https://jezebel.com/the-problem-with-calling-women-females-1683808274 which mentions similar points. Using ""<adjective>s"" rather than ""<adjective> people"" is dehumanizing. I tend to agree, but I'm certainly no expert and would prefer to defer to people who belong to the respective groups. From my point of view, ""white people"" is definitely preferable to ""whites"". 

""Males"" sounds kind of weird to me for some of the same reasons mentioned in https://medium.com/fearless-she-wrote/woman-vs-female-67fd4c36fe59 about ""female"".

I'd love for people to chime in if they have thoughts, or even just to drop in a link if there's a good article that explains your point of view. Then we can hopefully collect the best ways to refer to the various groups of people and document it in a styleguide in our contributor guide. Ideally, if someone already knows of such a guide we can also just refer to it.

Bonus: find and adjust the various places in our documentation where we use terms that do not conform to the styleguide.","25","0.3953231336918871","Research","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","1011978955","issue","https://github.com/Trusted-AI/AIF360/issues/966","Remove python 3.6 support In #963 I discovered that scikit-learn isn't supporting py3.6 anymore. This issue is there to discuss dropping this in Fairlearn, too. Additionally, as @adrinjalali mentions in that issue:

> re: py36 support, that one's long gone, and even py37 support is already removed from the main branches of numpy and scipy AFAIK, and we'll be removing it too for the next release.

So should we also make these changes? I'm definitely in favor. (If you're also in favor and have nothing to add, simply upvote. If you're against or you have something to add please comment!)

Indeed, numpy dropped 3.7 already https://github.com/numpy/numpy/blob/main/setup.py
The full deprecation schedule is at https://numpy.org/neps/nep-0029-deprecation_policy.html and shows that 3.7 support is dropped in December 2021. I guess that means that they won't fix issues anymore, but dropping it from newer versions happens even before that (?)","32","0.8260329783699663","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","1011953154","issue","https://github.com/Trusted-AI/AIF360/issues/965","Include a CI step against scikit-learn's nightly builds I suggest we include a test which would be optional on PR merges, or even not at all on PRs, and rather a cron job, which tests against scikit-learn nightly builds, to be able to detect situations such as the ones we're having in https://github.com/fairlearn/fairlearn/issues/964 and https://github.com/fairlearn/fairlearn/issues/963

The nightly builds can be conveniently installed as:

    pip install --pre --extra-index https://pypi.anaconda.org/scipy-wheels-nightly/simple scikit-learn

@romanlutz @riedgar-ms WDYT?","32","0.7090860136094386","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","1011909526","issue","https://github.com/Trusted-AI/AIF360/issues/964","TST: CorrelationRemover is failing estimator checks #### Describe the bug

`CorrelationRemover` is failing in our unit tests to check for estimator API compatibility: 
```
TypeError: fit() got an unexpected keyword argument 'sample_weight'
```

In a recent scikit-learn commit the following change occurred: https://github.com/scikit-learn/scikit-learn/commit/cbfbd863804da1005eda2fe8ee34ebe6380f9b3a# 
The important part there is https://github.com/scikit-learn/scikit-learn/commit/cbfbd863804da1005eda2fe8ee34ebe6380f9b3a#diff-49f814863816a0e8d199ba647472450ecc59a4375171db23cb1ca36063d6dc25R876-R888 that is, it doesn't check whether `sample_weight` is an argument on `fit` anymore. This seems to be the root cause.

This is failing tests so we are blocked from merging PRs until it's fixed.

#### Steps/Code to Reproduce
`python -m pytest test/unit/preprocessing`

#### Versions
```
System:
    python: 3.8.10 (default, Jun  2 2021, 10:49:15)  [GCC 9.4.0]
executable: /usr/bin/python
   machine: Linux-5.11.0-34-generic-x86_64-with-glibc2.29

Python dependencies:
    Cython: None
matplotlib: 3.4.3
     numpy: 1.21.2
    pandas: 1.3.3
       pip: 20.0.2
     scipy: 1.7.1
setuptools: 45.2.0
   sklearn: 1.0
    tempeh: None

```


@adrinjalali do you have a good intuition on what the fix here should be? Since that technique doesn't yet support `sample_weight` (and we have no plans of adding that anytime soon) it seems wrong to add the argument just to satisfy this check. Wdyt?
FYI @koaning who added `CorrelationRemover`","29","0.3583956168299437","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","1011894769","issue","https://github.com/Trusted-AI/AIF360/issues/963","BUG: sklearn.confusion_matrix does not work with None anymore #### Describe the bug
The following tests fail in the marked lines: 
- https://github.com/fairlearn/fairlearn/blob/94229647dda802a59bcbfe7f6d64295d64e96efa/test/unit/metrics/test_extra_metrics.py#L420
- https://github.com/fairlearn/fairlearn/blob/94229647dda802a59bcbfe7f6d64295d64e96efa/test/unit/metrics/test_extra_metrics.py#L388
- https://github.com/fairlearn/fairlearn/blob/94229647dda802a59bcbfe7f6d64295d64e96efa/test/unit/metrics/test_extra_metrics.py#L379

This build shows the errors in full detail https://dev.azure.com/fairlearn/Fairlearn/_build/results?buildId=508&view=logs&j=011e1ec8-6569-5e69-4f06-baf193d1351e&t=e41ce7ed-8506-5fe7-0eaa-68d3583c0fff&l=670

As far as I can tell from looking at the error output it seems like sklearn.confusion_matrix is struggling to compare the returned `None` from `unique_labels = [None, pos_label]` with the present values.
```
TypeError: '<' not supported between instances of 'int' and 'NoneType'
``` 
This behavior probably changed in the latest scikit-learn release. We should fix this asap since it's possible people hit this issue after upgrading to latest versions (or with fresh installation), and our tests are failing which prevents us from merging PRs.

#### Steps/Code to Reproduce
Just run `python -m pytest test/unit/metrics` to reproduce

#### Versions
```
System:
    python: 3.8.10 (default, Jun  2 2021, 10:49:15)  [GCC 9.4.0]
executable: /usr/bin/python
   machine: Linux-5.11.0-34-generic-x86_64-with-glibc2.29

Python dependencies:
    Cython: None
matplotlib: 3.4.3
     numpy: 1.21.2
    pandas: 1.3.3
       pip: 20.0.2
     scipy: 1.7.1
setuptools: 45.2.0
   sklearn: 1.0
    tempeh: None
```


FYI @riedgar-ms who wrote a bunch of these tests and @adrinjalali who may know what's wrong with `confusion_matrix` (or rather our way of using it).","29","0.4383235249330431","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","1003019934","issue","https://github.com/Trusted-AI/AIF360/issues/959","FEAT: Fairness of exposure ## Fairness of exposure
Would like to extend Fairlearn with tools to deal with fairness in rankings (https://github.com/fairlearn/fairlearn/issues/945). I think that an intuitive and good metric for fairness in rankings is exposure. From the paper: [Fairness of exposure in rankings](https://dl.acm.org/doi/10.1145/3219819.3220088) by Ashudeep Singh and Thorsten Joachims.

Where exposure is defined as:
<img src=""https://render.githubusercontent.com/render/math?math=Exposure(d_i | P) = \sum_{j=1}^N P_{i,j} v_j"">
,with document d_i, probabilistic ranking P and logarithmic discount v_j to deal with position bias (higher rankings get exponential more attention)
<img src=""https://render.githubusercontent.com/render/math?math=v_j = \frac{1}{log_2(1 %2B j)}"">

With exposure, all kinds of fairness metrics can be constructed. Such as:

### 1. Allocation harm
Where you equalize the average exposure of documents.
Denoting average exposure in group k with:

<img src=""https://render.githubusercontent.com/render/math?math=Exposure(G_k | P) = \frac{1}{|G_k|} \sum_{d_i \in G_k} Exposure(d_i | P)"">
And denoting the demographic parity constraint with:

<img src=""https://render.githubusercontent.com/render/math?math=Exposure(G_0 | P) = Exposure(G_1 | P)"">


### 2. Quality-of-service harm
<img src=""https://user-images.githubusercontent.com/1357585/134221120-e9fee764-90dd-42c1-aaac-2d16346ce290.JPG"" width=300 align=right>
Where you try to keep the relevance of the items proportional to the exposure. Like in the example on the right, small differences in relevance between candidates can lead to huge differences in exposure. 

<img src=""https://render.githubusercontent.com/render/math?math=\frac{Exposure(G_0 | P)}{U(G_0 | q)} = \frac{Exposure(G_1 | P)}{U(G_1 | q)} "">
,where U(G|q), is the average utility of a group. And utility is the relevance score, on which the documents are ranked. 
<img src=""https://render.githubusercontent.com/render/math?math=U(G_k | q) = \frac{1}{|G_k|} \sum_{d_i \in G_k} u_i "">


### Problem
The problem with this metric is that you need a given probabilistic ranking P, which you can create when you have multiple rankings with the same documents. To work around this, and try to make the metric work for a single ranking, I thought about defining exposure as the sum of logarithmic discounts for ranking tau. Which would define demographic parity as:

<img src=""https://render.githubusercontent.com/render/math?math=Exposure(G_k | \tau) = \frac{1}{|G_k|} \sum_{d_i \in G_k} \frac{1}{log(1 %2B \tau_i)}"">

### Conclusion
Think that with the adjustment, this is an effective way to deal quantify fairness in rankings. Please let me know what you think


","3","0.8921986522754278","Bias detection metrics validation","Validation"
"https://github.com/fairlearn/fairlearn","1000690579","issue","https://github.com/Trusted-AI/AIF360/issues/958","ENH Add Reject Object Classification and Discrimination-Aware Ensemble based on Kamiran, Karim, Zhang The paper [Decision Theory for Discrimination-Aware Classification](https://ieeexplore.ieee.org/document/6413831) by Kamiran, Karim and Zhang presents two solutions for discrimination-aware classification that neither require data modification nor classifier tweaking; Reject Option based Classification and Discrimination-Aware Ensemble (DAE) . The goal of this task is to implement them. 

Completing this item requires:

- code for the technique in ?? (@fairlearn/fairlearn-maintainers I'm not entirely sure where this should go?)
- unit tests in `test.unit.??`
- descriptive API reference (directly in the docstring)
- ideally a short user guide in docs.user_guide.mitigation.rst

A fully fledged example notebook is not required.

PS. I think I covered all the stuff I'm required to put in here, if not please let me know :). ","28","0.5602954372886035","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","999339532","issue","https://github.com/Trusted-AI/AIF360/issues/957","ENH: Use flake8-pytest-importorskip #### Is your feature request related to a problem? Please describe.

In the `test_othermlpackages` directory, we use `pytest.importorskip()` to make things better behaved when various packages aren't present. However, this means that subsequent imports have to have a `#noqa` suppression since they are technically placed after the first executable line.

#### Describe the solution you'd like

Consider using:
https://pypi.org/project/flake8-pytest-importorskip/
The package does note that it works in a 'slightly hacky' way, so sticking to the `#noqa` suppressions may be preferable.","21","0.2640914262074672","Installation and shell commands","Deployment"
"https://github.com/fairlearn/fairlearn","995998032","issue","https://github.com/Trusted-AI/AIF360/issues/956","FEAT: add fairness-aware Principal Component Analysis (preprocessing) PCA, one of the most common dimensionality reduction techniques might produce data representations that are inherently prone to unfairness. For example, an embedding of the data created by PCA might represent a certain race group much better than another.
 
In recent years a couple of alternatives have been proposed to ensure that disparity between sensitive groups is mitigated.
My inspiration/literature research is based on the following three papers:

[Fair PCA](https://arxiv.org/abs/1811.00103), [Efficient Fair PCA](https://arxiv.org/abs/1911.04931), [Multi Objective approach](https://arxiv.org/abs/2006.06137).

The first paper: Fair PCA aims to minimize the maximum additional reconstruction error for each population above the optimal n to d projection for that population alone. (the optimal projection is obtained by running PCA only on the sensitive group you are considering). See the image below for more details.

![image](https://user-images.githubusercontent.com/38749496/133259524-218023b0-1d2f-4815-ab62-8a6356a77d06.png)
 
The algorithm used to solve this problem relaxes the problem to a semidefinite optimization problem and solves the SDP, next they solve an LP designed to reduce the rank of said problem. This results in a relatively slow algorithm with as output a projection that has optimal fairness using at most d + k -1 features,, where d is the desired number of output features and k is the number of sensitive groups. (this algorithm is later changed to run slightly faster and require fewer additional features for the projection, see [updated algorithm](https://www.researchgate.net/publication/331429524_Fair_Dimensionality_Reduction_and_Iterative_Rounding_for_SDPs)).

The second paper [Efficient Fair PCA](https://arxiv.org/abs/1911.04931) uses a different loss function, namely that of pairwise disparity error. It is much alike the previous loss function however computes the difference in additional reconstruction error between each group. All of these errors are added as additional minimization objectives and the optimization then becomes to minimize the general reconstruction error as well as all additional objectives. They use an adaptive optimization technique based on gradient descent to solve this. It is guaranteed to converge to a Pareto stationary point and runs much faster than the previous discussed algorithm. Furthermore, it does not require any additional features and seems to work better for mitigating fairness by the experiments in their paper.

Finally, [Multi Objective approach](https://arxiv.org/abs/2006.06137) takes a different approach. Instead of creating a different projection matrix they aim to make the ""fairest"" selection of principal components. They measure fairness by taking the difference of reconstruction errors between both sensitive groups. They try to optimize the fairness measure and the total reconstruction error simultaneously by an evolutionary algorithm called [SPEA2](https://www.semanticscholar.org/paper/SPEA2%3A-Improving-the-strength-pareto-evolutionary-Zitzler-Laumanns/b13724cb54ae4171916f3f969d304b9e9752a57f). 

This method is nice for already deployed algorithms since it does not require to run a different PCA version. It only requires to reorder the principal components. 

I still have my doubts on the fairness measures accompanied by the algorithms and would like some discussion about that.
However, I do believe that implementing both  [Efficient Fair PCA](https://arxiv.org/abs/1911.04931), [Multi Objective approach](https://arxiv.org/abs/2006.06137) would be useful. Note that both focus on solving a multi objective optimization approach so I would need to check whether one of the two optimization algorithms is faster and whether they can be applied to both problems.

I would take this upon me so if someone can assign me to this, that would be great!

Let me know what you think and feel free to ask any questions!

Tasks:

- [ ] Think of a reasonable fairness measure
- [ ] Check whether optimization methods are applicable to both and check which one is faster
- [ ] implement efficient Fair PCA
- [ ] implement post processing fair PCA
","6","0.8082456613613883","API expansion","Development"
"https://github.com/fairlearn/fairlearn","995761039","issue","https://github.com/Trusted-AI/AIF360/issues/954","DOC Contributing documentation: API reference #### Describe the issue linked to the documentation

For new contributors it can be unclear that the API reference is generated by Sphinx.

#### Suggest a potential alternative/fix

Make the following changes to the page [contributing documentation](https://fairlearn.org/v0.7.0/contributor_guide/contributing_documentation.html):
* Add a few more sentences on how Sphinx works  to the introduction paragraph, including some specifics of our own website:
    * Most pages on the website are generated based on ReST files that can be found in the docs folder of the repository
    * The home page is static and can be found in subfolder of docs
    * The API reference is generated based upon the page structure defined in the docs folder, but the content is based on Numpy docstrings in the fairlearn folder in the root directory.
* Put current information about building the website to a new section called 'Building the website locally', which is split up in 'Single version' and 'For all versions'","14","0.7803681853720945","Documentation","Development"
"https://github.com/fairlearn/fairlearn","995712962","issue","https://github.com/Trusted-AI/AIF360/issues/953","DOC improve contributor guide: add Roman's sprint instructions #### Describe the issue linked to the documentation

The process of adding your first PR can be quite confusing for prospective contributors that are new to open source development.

#### Suggest a potential alternative/fix

Add @romanlutz's [sprint instructions](https://github.com/romanlutz/fairlearn-pycon-sprint) to the contributor guide. Perhaps a title such as ""Create Your first Pull Request"" (or something similar that makes it easy to identify for new people) would work nice here.","14","0.4454888138248395","Documentation","Development"
"https://github.com/fairlearn/fairlearn","995708762","issue","https://github.com/Trusted-AI/AIF360/issues/952","DOC improve contributor guide: style  #### Describe the issue linked to the documentation

Based upon feedback from new contributors I think the [code style section in our contributor guide](https://fairlearn.org/v0.7.0/contributor_guide/contributing_code.html#code-style) can be improved.
* It is hard to locate (especially if you are contributing to the documentation)
* It is very minimal: it only refers to PEP8 but does not specify
    * Max line length for documentation
    * PR/commit messages 

#### Suggest a potential alternative/fix
I think there are a few things we can do to make it easier to locate style instructions
1. Add a separate 'Code Style' section to the Contributor Guide in which we mention the most important PEP8 style guide items as well as our preferred PR/commit message style
2. Move the subsection 'Code Style' up in the section 'Contributing Code' and refer to (1))
3. Add subsection 'Style' to 'Contributing Documentation' and possibly refer to (1)","14","0.8021462379631141","Documentation","Development"
"https://github.com/fairlearn/fairlearn","995674080","issue","https://github.com/Trusted-AI/AIF360/issues/951","DOC Add versionadded and versionchanged to API reference (fairlearn.metrics) Related to #855 ","14","0.7883101477407524","Documentation","Development"
"https://github.com/fairlearn/fairlearn","995673302","issue","https://github.com/Trusted-AI/AIF360/issues/950","DOC Add versionadded and versionchanged to API reference (fairlearn.postprocessing) Related to #855 ","14","0.8144074193896255","Documentation","Development"
"https://github.com/fairlearn/fairlearn","995672816","issue","https://github.com/Trusted-AI/AIF360/issues/949","DOC Add versionadded and versionchanged to API reference (fairlearn.preprocessing) Related to #855 ","14","0.8144074193896255","Documentation","Development"
"https://github.com/fairlearn/fairlearn","994675654","issue","https://github.com/Trusted-AI/AIF360/issues/948","DOC Add versionadded and versionchanged to API reference (fairlearn.reductions) Add versionadded and versionchanged to API reference of fairlearn.reductions. Sub-issue of #855. 
","14","0.8748471259682022","Documentation","Development"
"https://github.com/fairlearn/fairlearn","989971705","issue","https://github.com/Trusted-AI/AIF360/issues/945","FEAT: Fairness metrics for ranking #### Is your feature request related to a problem? Please describe.
Hi everyone!
I have been researching ways to quantify fairness in rankings and think that this would be a great addition to the Fairlearn package. 

Examples of rankings that have fairness risks are:
- SAT datasets
If a university can only hire k students. They might want to rank students based on their SAT scores. In this dataset, women scored on average 25 points less on this test compared to men.
- German credit dataset
A bank can only give out k loans and want to rank loan applicants by credit score. But the credit score system is known to have a bias against women and younger applicants.
- Compas recidivism prediction
When you want to rank the top-k criminals least likely to recidivate to consider a reduced sentence. There might be discrimination by race, age, or gender.
- Other applications include fields like job hiring or dating apps

#### Describe the solution you'd like
<!-- A clear and concise description of what you want to happen. -->
Some promising fairness metrics are:
_Unfortunately, the image turned out blurry, it gets better when you click on the image._ 

![Ranking_metrics](https://user-images.githubusercontent.com/1357585/132348441-ddc1beed-75a5-42db-b295-97be80f662cc.PNG)

The link to the papers:
[rND, rKL, rRD](https://dl.acm.org/doi/10.1145/3085504.3085526)
[Exposure](https://dl.acm.org/doi/10.1145/3219819.3220088)

Please let me know if measuring fairness in rankings is a direction that would be beneficial to Fairlearn, and which metrics you like/dislike.

#### Tasklist in order
- [ ] Adding exposure-based metrics https://github.com/fairlearn/fairlearn/issues/959 
- [ ] Thinking about whether we should implement rND, rKL, rRD
- [ ] Create an example notebook on how to work with the metrics
- [ ] Think about real life usecases / find datasets.
- [ ] Implement post-processing methods
- [ ] Create example notebook on mitigating fairness in ranking
- [ ] Think about an equalized odds metric for rankings?

#### Additional context
<!-- Add any other context or screenshots about the feature request here. -->
I would like to implement these metrics as part of my internship. 
These metrics could be used in combination with synthetic rank dataset creation and post-processing algorithms (like [example](https://dl.acm.org/doi/10.1145/3132847.3132938))
","8","0.2894069654396766","Understanding Fairness Definition","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","989890040","issue","https://github.com/Trusted-AI/AIF360/issues/944","DOC add example code for the metrics package The API section of the website does not contain code examples.  Adding code examples would help new users to get acquainted with Fairlearn.  

This is a sub issue for #894. 

","14","0.6606500843788978","Documentation","Development"
"https://github.com/fairlearn/fairlearn","986788954","issue","https://github.com/Trusted-AI/AIF360/issues/940","Bug when using custom Lagrange multiplier grids #### Describe the bug

When passing a custom set of Lagrangian multipliers to the `GridSearch` reduction, via the `grid` kwarg, the loss computation for model selection throws a `KeyError`.

The multipliers at `self.gammas_`, which is a pandas.DataFrame, are being inserted using the name of the column as index.
See [line 162](https://github.com/fairlearn/fairlearn/blob/main/fairlearn/reductions/_grid_search/grid_search.py#L162) for the values of `i` and [line 194](https://github.com/fairlearn/fairlearn/blob/main/fairlearn/reductions/_grid_search/grid_search.py#L194) for how the values are inserted.

Afterwards, in [line 201](https://github.com/fairlearn/fairlearn/blob/main/fairlearn/reductions/_grid_search/grid_search.py#L201), these values are accessed assuming the indices are ordered integer values.
This works when using the default GridGenerator because the column names are indeed ordered integer indices, but not when providing a custom DF.


#### Steps/Code to Reproduce

Running a GridSearch reduction with a custom grid represented by a DF whose column names are not ordered integer indices.

#### Versions
This happens on all versions as far as I can tell, including the most recent on the `main` branch.
","17","0.3587398034698804","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","969955236","issue","https://github.com/Trusted-AI/AIF360/issues/937","Matplotlib deprecation messages when running Example Notebook code When I run the MetricFrame Visualization code sample (plot_quickstart.ipynb), I get the following error messages:

`C:\Users\xxxxxxx\Anaconda3\envs\fairlearn_env\lib\site-packages\pandas\plotting\_matplotlib\tools.py:331: MatplotlibDeprecationWarning: 
The is_first_col function was deprecated in Matplotlib 3.4 and will be removed two minor releases later. Use ax.get_subplotspec().is_first_col() instead.
  if ax.is_first_col():
C:\Users\xxxxxxx\Anaconda3\envs\fairlearn_env\lib\site-packages\pandas\plotting\_matplotlib\tools.py:331: MatplotlibDeprecationWarning: 
The is_first_col function was deprecated in Matplotlib 3.4 and will be removed two minor releases later. Use ax.get_subplotspec().is_first_col() instead.
  if ax.is_first_col():
C:\Users\xxxxxxx\Anaconda3\envs\fairlearn_env\lib\site-packages\pandas\plotting\_matplotlib\core.py:1547: MatplotlibDeprecationWarning: normalize=None does not normalize if the sum is less than 1 but this behavior is deprecated since 3.3 until two minor releases later. After the deprecation period the default value will be normalize=True. To prevent normalization pass normalize=False 
  results = ax.pie(y, labels=blabels, **kwds)`

This behavior persisted after I did a 'conda update matplotlib' to ensure I was using the latest version.","26","0.5567559883431659","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","969336278","issue","https://github.com/Trusted-AI/AIF360/issues/936","DOC Use BibTeX for citations #### Describe the issue linked to the documentation

When we wish to cite papers in our documentation, we are currently writing out the citations manually. Not only is this tedious, but it is also highly error prone (since the links in the text are numbers along the lines of `[4]_`). This is particularly acute if merge conflicts occur in the 'References' section, since then the whole page must be cross checked, to ensure that all the links to the references point at the right paper.

#### Suggest a potential alternative/fix

Somewhat over 30 years ago, BibTex was invented to help with precisely this issue. Furthermore, online paper databases often provide BibTeX snippets for inclusion in `.bib` files. There is also a [BibTeX package for Sphinx](https://sphinxcontrib-bibtex.readthedocs.io/en/latest/quickstart.html) which looks to be under active development. A possible solution suggests itself.
","14","0.4264333765352096","Documentation","Development"
"https://github.com/fairlearn/fairlearn","966258492","issue","https://github.com/Trusted-AI/AIF360/issues/935","Why no difference/ratio for some metrics? In this table https://fairlearn.org/main/user_guide/assessment.html#scalar-results-from-metricframe I can see that difference/ratio can be computed with pre-built functions for some metrics (e.g. accuracy_score), but not for others.

Is there any reason why, for example, there's not a pre-built function to compute a ratio for mean_squared_error() ?

Isn't the MSE ratio between different groups an useful indicator?","12","0.6726598494212835","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","964555996","issue","https://github.com/Trusted-AI/AIF360/issues/933","Keyword Arguments Change Affecting Fairness Dashboard <!--
Before submitting a bug, please make sure the issue hasn't been already
addressed by searching through the past issues.

If your issue is a usage question, please submit it in one of these other
channels instead:
- StackOverflow with the `fairlearn` tag:
  https://stackoverflow.com/questions/tagged/fairlearn
- Discord: https://discord.gg/R22yCfgsRn
The issue tracker is used only to report bugs and feature requests. For
questions, please use either of the above platforms. Most question issues are
closed without an answer on this issue tracker. Thanks for your understanding.
-->
With the new change that arguments must be keywords, a warning is thrown when attempting to pass in multiple predictions into y_pred as shown in the sample code below. This seems to be an issue specifically in the dashboard.

#### Describe the bug
<!--
A clear and concise description of what the bug is.
-->

Key work argument requirement leading to bug in the Fairness dashboard, but not in the API 

#### Steps/Code to Reproduce
```python
FairnessDashboard(sensitive_features=sensitive_cols,
                y_true=golden_labels,
                y_pred=predictions)
```

#### Expected Results
No error is thrown.

#### Actual Results
You have provided 'metrics', 'y_true', 'y_pred' as positional arguments. Please pass them as keyword arguments. From version 0.10.0 passing them as positional arguments will result in an error.

#### Screenshots
<!-- If applicable, add screenshots to help explain your problem. -->
![Model Comparison Error](https://user-images.githubusercontent.com/69976597/128803024-5dd9e0f8-e8bd-47d0-90e7-5232773629af.PNG)

#### Versions
<!--
Please provide the following information:
- OS: [e.g. Windows]
- Browser (if you're reporting a bug in jupyter): [e.g. Edge, Firefox, Chrome, Safari]
- Python version: [e.g. 3.7.4]
- Fairlearn version: [e.g. 0.4.5 or installed from main branch in editable mode]
- version of Python packages: please run the following snippet and paste the output:
  ```python
  import fairlearn
  fairlearn.show_versions()
  ```
-->

<!-- Thanks for contributing! -->
","29","0.3829781199926261","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","962797412","issue","https://github.com/Trusted-AI/AIF360/issues/932","DOC new fonts are a bit large #### Describe the issue linked to the documentation

The new fonts (#928) are a bit larger than the previous ones, which causes the landing page to look a bit off, in my opinion.

Old font:
<img width=""1266"" alt=""oldfont"" src=""https://user-images.githubusercontent.com/24417440/128526682-30e4bb86-8d67-474c-8317-088bc841eeb9.png"">

Current font:
<img width=""1263"" alt=""currentfont"" src=""https://user-images.githubusercontent.com/24417440/128526720-5bba60e4-0501-450a-ae12-54a7afe10c61.png"">



#### Suggest a potential alternative/fix

Use smaller font sizes.

Tagging @nessamilan","14","0.8173808570284344","Documentation","Development"
"https://github.com/fairlearn/fairlearn","962793190","issue","https://github.com/Trusted-AI/AIF360/issues/931","DOC update meta-image and favicon with new logo As we've updated our logo (see #858) we also need to update the following files to the new font:
* https://github.com/fairlearn/fairlearn/blob/main/docs/static_landing_page/images/fairlearn-meta-image.jpg (this image is shown when you link to fairlearn on e.g., social media)
* https://github.com/fairlearn/fairlearn/blob/main/docs/static_landing_page/images/fairlearn-favicon.ico (the favicon shown in e.g., browser tabs)

Tagging @nessamilan","14","0.5850600460456724","Documentation","Development"
"https://github.com/fairlearn/fairlearn","960180137","issue","https://github.com/Trusted-AI/AIF360/issues/927","CircleCI credits and issues Seems like CircleCI isn't giving open source projects unlimited credits anymore.

As @romanlutz  said on discord: 

> Looks like it's not triggering for new pull requests. I checked their pages and can't really find anything definitive, but it seems like we have 2,500 weekly credits of which we've used 2,342. That sounds like we still have some... That said, this is for the week starting Aug 1, so it definitely looks like our usage is high for this tier. They're trying to sell us a $60/month plan.

We should investigate this issue and figure out how we can proceed. I'll check what we have on the sklearn side.

@amueller do you happen to know anything related to this?

cc @fairlearn/fairlearn-maintainers ","32","0.2900207284572105","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","958580400","issue","https://github.com/Trusted-AI/AIF360/issues/924","DOC Fix CITATION.cff formatting It looks like I formatted it the wrong way when adding the file in #921 and hence it doesn't render properly

![image](https://user-images.githubusercontent.com/10245648/127935204-8543b8a3-b334-4724-9420-9397be4a0a3b.png)


https://github.com/fairlearn/fairlearn/blob/main/CITATION.cff","30","0.3322462291072156","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","956277065","issue","https://github.com/Trusted-AI/AIF360/issues/923","Support minibatch training #### Is your feature request related to a problem? Please describe.
It is hard to train with large data/high-dimension language embedding with fairlearn because it requires passing in the whole x matrix, which results in out-of-memory. 

#### Describe the solution you'd like
Support minibatch training. (I understand that for ExponentiatedGradient it requires the same data every run with different weights, but this should be fine if the minibatches are kept consistent every time fit is called.)

#### Describe alternatives you've considered, if relevant
<!-- A clear and concise description of any alternative solutions or features
you've considered. -->
Currently I need to find low-dimension sentence embeddings, and implement my own ""fit"" function to take minibatches, and ensure it is consistent every time fit is called so that the termination condition of ExponentiatedGradient makes sense.

#### Additional context
<!-- Add any other context or screenshots about the feature request here. -->
","11","0.4099537516394009","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","956272986","issue","https://github.com/Trusted-AI/AIF360/issues/922","""Phase 1 of the simplex method failed to find a feasible solution"" error in linprog of ExponentiatedGradient #### Describe the bug

I got the error ""ValueError: Phase 1 of the simplex method failed to find a feasible solution. The pseudo-objective function evaluates to 1.2e+02 which exceeds the required tolerance of 1e-09 for a solution to be considered 'close enough' to zero to be a basic solution. Consider increasing the tolerance to be greater than 1.2e+02. If this tolerance is unacceptably large the problem may be infeasible.""

#### Steps/Code to Reproduce

This does depend on data, and is not consistent. If needed, I can provide data.

```
import torch
import torch.nn as nn

import random
def get_minibatch_iter(df_full, minibatch_size, seed=0): # this is circular, will never stop, shuffle after per epoch
    i = 0
    fulllen = df_full.shape[0]
    batch_ct = 0
    while True:
        while i >= fulllen:
            # shuffle
            idxs = np.arange(df_full.shape[0])
            random.seed(seed)
            random.shuffle(idxs)
            seed += 1
            #print(idxs)
            df_full = df_full.iloc[idxs]
            i -= fulllen
        if i < fulllen and i+minibatch_size >= fulllen:
            circular_idx = i+minibatch_size - fulllen
            tail = df_full.iloc[i:fulllen]
           
            # shuffle
            idxs = np.arange(df_full.shape[0])
            random.seed(seed)
            random.shuffle(idxs)
            seed += 1
            #print(idxs)
            df_full = df_full.iloc[idxs]
            start = df_full.iloc[:circular_idx]
            chunk = pd.concat([tail,start])
    
        else:
            chunk = df_full.iloc[i:i+minibatch_size]

        yield chunk
        i += minibatch_size

def get_balanced_minibatch_iter(df_full, minibatch_size, seed=0):
    df_ones = df_full[df_full['label']==1]
    df_zeros = df_full[df_full['label']==0]
    half_batch_size = int(minibatch_size/2)
    one_iter = get_minibatch_iter(df_ones, half_batch_size, seed=seed+1)
    zero_iter = get_minibatch_iter(df_zeros, half_batch_size, seed=seed+2)

    while True:
        one_halfbatch = one_iter.__next__()
        zero_halfbatch = zero_iter.__next__()
        idxs = np.arange(minibatch_size)
        random.seed(seed)
        random.shuffle(idxs)
        seed += 1
        combined = pd.concat([one_halfbatch, zero_halfbatch],axis=0).iloc[idxs]
        yield combined

def train_embedding(model, loss, opt, x, y, train_weight, batch_size, num_batches, evaldf, coreset_weight=None):
    # there we do need to remake df since fairlearn API only takes x,y,sens separately
    #print(""train once"")
    embed_series = pd.Series(list(x), dtype=np.dtype(""object""))
    traindf = pd.DataFrame({""embedding"":embed_series, ""label"":y})

    # when we pass in a weighted coreset, train_weight is a vector based on sensitive&y label. We retain that for each sample, but coreset
    # weight is only used for sampling the minibatch, within per minibatch, per sample uses the normal train weight, so there shouldn't be a problem.
    # trainweight is only used when aggregating the loss
    # coresetweight is only used for (nonuniform sampling for) generating (unweighted) minibatches
    if train_weight is not None:
        traindf['trainweight'] = train_weight # trainweight should be passed by the reduction algorithm, this is not the coreset importance weight
        # we never manually pass this weight
    if coreset_weight is not None:
        traindf.insert(0,'coresetweight',coreset_weight)
        # coreset importance weight is handled here, as a column of traindf called ""weight"", it is just used for sampling minibatches, each resulting batch doesn't have importance weights.
        batch_iter = get_weightbalanced_minibatch_iter(traindf, batch_size)
    else:
        batch_iter = get_balanced_minibatch_iter(traindf, batch_size)

    #train_auc_list = []
    val_auc_list = []
    val_loss_list = []

    for i in range(num_batches):
        df_batch = batch_iter.__next__()
        opt.zero_grad()
        pred = model.forward(df_batch)
        #print(pred)
        output_unreduced = loss(pred, torch.LongTensor(np.array(df_batch['label'])))
        if 'trainweight' in traindf.columns:# has weight
            ws = torch.tensor(np.array(df_batch['trainweight']))
            output = torch.mean(ws * output_unreduced)
        else:
            output = torch.mean(output_unreduced)
        output.backward()
        opt.step()

        if (i+1) % 20000 == 0:
            print(""-------------batch number ""+str(i)+""------------"")
            #full_auc_train = eval_on_df(model, vectorizer, df_batch, batch_size, 1)
            full_auc_val, loss_val = eval_on_df(model, loss, evaldf, batch_size, 500)
            print("" val auc: ""+str(full_auc_val)+"" loss: ""+str(loss_val))

            #train_auc_list.append(full_auc_train)    
            val_auc_list.append(full_auc_val)
            val_loss_list.append(loss_val)
            model.train()

    return model, val_auc_list, val_loss_list

class TwoLayerModelCoreset(torch.nn.Module):
    def __init__(self, input_dim, coreset_weight=None, seed=31):
        super(TwoLayerModelCoreset, self).__init__()
        torch.manual_seed(seed)
        self.linear1 = nn.Linear(input_dim, 10)
        self.linear2 = nn.Linear(10, 2)
        self.coreset_weight = coreset_weight

    def forward(self, df):
        cur_array = np.concatenate(list(df['embedding'])).reshape(df.shape[0],-1)
        #print(cur_array.shape)
        mid = self.linear1(torch.FloatTensor(cur_array))
        out = self.linear2(mid)
        return out

    def forward_embedding(self, embedding):
        mid = self.linear1(torch.FloatTensor(embedding))
        out = self.linear2(mid)
        return out

    def predict_proba(self, df, vectorizer):
        self.eval()
        batch_size = 128
        all_pred = []

        for i in range(int(df.shape[0]/batch_size)+1):
            start_idx = batch_size*i
            end_idx = np.min([batch_size*(i+1), df.shape[0]])
            df_batch = df.iloc[start_idx:end_idx]
            if start_idx == end_idx:
                break
            with torch.no_grad():
                pred = self.forward(df_batch)
                pred_softmax = nn.Softmax(dim=1)(pred)
                all_pred.append(pred_softmax.detach().numpy()[:,1])
        allpred = np.concatenate(all_pred,axis=0)
        return allpred

    def predict(self, x):
        self.eval()
        batch_size = 128
        all_pred = []

        for i in range(int(x.shape[0]/batch_size)+1):
            start_idx = batch_size*i
            end_idx = np.min([batch_size*(i+1), x.shape[0]])
            x_batch = x[start_idx:end_idx]
            if start_idx == end_idx:
                break
            with torch.no_grad():
                pred = self.forward_embedding(x_batch)
                pred_softmax = nn.Softmax(dim=1)(pred)
                all_pred.append(pred_softmax.detach().numpy()[:,1])
        allpred = np.concatenate(all_pred,axis=0)
        return allpred

    def predict_label(self, x):
        self.eval()
        batch_size = 128
        all_pred = []

        for i in range(int(x.shape[0]/batch_size)+1):
            start_idx = batch_size*i
            end_idx = np.min([batch_size*(i+1), x.shape[0]])
            x_batch = x[start_idx:end_idx]
            if start_idx == end_idx:
                break
            with torch.no_grad():
                pred = self.forward_embedding(x_batch)
                pred_softmax = nn.Softmax(dim=1)(pred)
                all_pred.append(pred_softmax.detach().numpy()[:,1])
        allpred = np.concatenate(all_pred,axis=0)
        out = np.array([np.random.binomial(1,p=p_i) for p_i in allpred])
        return out

    def fit(self, x, y, sample_weight=None):
        loss_v = nn.CrossEntropyLoss()
        opt_v = torch.optim.Adam(self.parameters(), lr=1e-3)
        _ = train_embedding(self, loss_v, opt_v, x, y, sample_weight, 128, 5000, val_df) # accept flattened coreset (no coreset weight)

x_core = np.concatenate(list(flattened_core['embedding']),axis=0).reshape(-1,512) # 512-dim embedding
m_core = TwoLayerModelCoreset(512)
m_core_eq = ExponentiatedGradient(m_core, constraints=EqualizedOdds())
m_core_eq.fit(x_core,flattened_core['label'], sensitive_features=flattened_core['sens'])
```

#### Expected Results
No error is thrown and fit returns properly (I am training on different subsets of data, most of time it returns properly, occasionally throws a warning related to the error above, errored out once).

#### Actual Results
<!-- Please paste or specifically describe the actual output or traceback. -->
stack trace:
/anaconda/envs/azureml_py36/lib/python3.6/site-packages/fairlearn/reductions/_exponentiated_gradient/exponentiated_gradient.py in fit(self, X, y, **kwargs)
    133                 # saddle point optimization over the convex hull of
    134                 # classifiers returned so far
--> 135                 Q_LP, self.lambda_vecs_LP_[t], result_LP = lagrangian.solve_linprog(self.nu)
    136                 gap_LP = result_LP.gap()
    137 

/anaconda/envs/azureml_py36/lib/python3.6/site-packages/fairlearn/reductions/_exponentiated_gradient/_lagrangian.py in solve_linprog(self, nu)
    141                                   b_ub=dual_b_ub,
    142                                   bounds=dual_bounds,
--> 143                                   method='simplex')
    144         lambda_vec = pd.Series(result_dual.x[:-1], self.constraints.index)
    145         self.last_linprog_n_hs = n_hs

/anaconda/envs/azureml_py36/lib/python3.6/site-packages/scipy/optimize/_linprog.py in linprog(c, A_ub, b_ub, A_eq, b_eq, bounds, method, callback, options, x0)
    561                                                        complete, status,
    562                                                        message, tol,
--> 563                                                        iteration, disp)
    564 
    565     sol = {

/anaconda/envs/azureml_py36/lib/python3.6/site-packages/scipy/optimize/_linprog_util.py in _postprocess(x, postsolve_args, complete, status, message, tol, iteration, disp)
   1538     status, message = _check_result(
   1539         x, fun, status, slack, con,
-> 1540         bounds, tol, message
   1541     )
   1542 

/anaconda/envs/azureml_py36/lib/python3.6/site-packages/scipy/optimize/_linprog_util.py in _check_result(x, fun, status, slack, con, bounds, tol, message)
   1451         # nearly basic feasible solution. Postsolving can make the solution
   1452         # basic, however, this solution is NOT optimal
-> 1453         raise ValueError(message)
   1454 
   1455     return status, message

#### Screenshots
<!-- If applicable, add screenshots to help explain your problem. -->

#### Versions
<!--
Please provide the following information:
- OS: Linux-5.4.0-1049-azure-x86_64-with-debian-buster-sid
- Browser (if you're reporting a bug in jupyter): Edge
- Python version: 3.6.9
- Fairlearn version: 0.6.2

System:
    python: 3.6.9 |Anaconda, Inc.| (default, Jul 30 2019, 19:07:31)  [GCC 7.3.0]
executable: /anaconda/envs/azureml_py36/bin/python
   machine: Linux-5.4.0-1049-azure-x86_64-with-debian-buster-sid

Python dependencies:
    Cython: 0.29.23
matplotlib: 3.2.1
     numpy: 1.19.5
    pandas: 1.1.5
       pip: 20.1.1
     scipy: 1.5.2
setuptools: 50.3.0
   sklearn: 0.22.2.post1
    tempeh: None
-->

<!-- Thanks for contributing! -->
","10","0.8206131696795601","Model development","Development"
"https://github.com/fairlearn/fairlearn","951946646","issue","https://github.com/Trusted-AI/AIF360/issues/915","MNT Bump requirements for compatibility with python 3.9 Some of our min requirements don't exist in Python 3.9, e.g., scikit-learn seems to have adopted python 3.9 in version 0.24, whereas our nightly ""requirements-fixed"" build runs with our minimum requirement of 0.22.1 and fails accordingly.

Should we bump our min requirements to

```
numpy>=1.20.0
pandas>=1.2.0
scikit-learn>=0.24.0
scipy>=1.6.0
```
from currently
```
numpy>=1.17.2
pandas>=0.25.1
scikit-learn>=0.22.1
scipy>=1.4.1
```
(and probably several others in requirements-dev.txt as well as `matplotlib>=3.4.0` for requirements-customplots.txt)?

I'm generally in favor of moving forward rather than staying on old versions, but I know there are people who are forced into old versions of some packages that might not be able to use our latest version as a consequence.

@fairlearn/fairlearn-maintainers Wdyt?","32","0.7579500891265598","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","948671251","issue","https://github.com/Trusted-AI/AIF360/issues/911","Enable Reweighing to handle categorical output instead of only 0/1  From: https://github.com/fairlearn/fairlearn/pull/906#discussion_r673118210

Some of our algorithms require the output to be 0/1. However, they only need the output to be categorical, and in case of binary classification they need to know which one is the positive label, i.e. `pos_lable`. This issue is to fix those instances.","23","0.4617584293640631","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","948436757","issue","https://github.com/Trusted-AI/AIF360/issues/910","TST xgboost tests are failing The error occurs in our `othermlpackages` tests, and the most recent logs can be found at https://dev.azure.com/fairlearn/Fairlearn/_build?definitionId=5&_a=summary

At first glance it seems like a `FutureWarning`. This has been failing since July 5th (without any change compared to July 4th) which is roughly when pandas released a new version. Technically that was 3 days earlier, but I confirmed that the pipeline was still using the pandas version 1.2.5 until July 4th.

Since this test doesn't do anything special I'm wondering whether it's an xgboost internal issue. @riedgar-ms any thoughts?

```

        if level is not None:
>           warnings.warn(
                ""Using the level keyword in DataFrame and Series aggregations is ""
                ""deprecated and will be removed in a future version. Use groupby ""
                ""instead. df.sum(level=1) should use df.groupby(level=1).sum()."",
                FutureWarning,
                stacklevel=4,
            )
E           FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().

/usr/share/miniconda/envs/test-conda-env/lib/python3.8/site-packages/pandas/core/generic.py:10404: FutureWarning
----------------------------- Captured stdout call -----------------------------
[08:02:14] WARNING: /tmp/build/80754af9/xgboost-split_1619724447847/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
=============================== warnings summary ===============================
test_othermlpackages/test_xgboost.py::test_expgrad_classification
test_othermlpackages/test_xgboost.py::test_gridsearch_classification
test_othermlpackages/test_xgboost.py::test_thresholdoptimizer_classification
test_othermlpackages/test_xgboost.py::test_thresholdoptimizer_classification
  /usr/share/miniconda/envs/test-conda-env/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
    warnings.warn(label_encoder_deprecation_msg, UserWarning)

-- Docs: https://docs.pytest.org/en/stable/warnings.html
------------ generated xml file: /home/vsts/work/1/s/TEST-TEST.xml -------------
=========================== short test summary info ============================
FAILED test_othermlpackages/test_xgboost.py::test_expgrad_classification - Fu...
FAILED test_othermlpackages/test_xgboost.py::test_gridsearch_classification
=================== 2 failed, 1 passed, 4 warnings in 36.09s ===================
```","26","0.4708941398251373","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","944203268","issue","https://github.com/Trusted-AI/AIF360/issues/900","DOC update fairlearn repository overview #### Describe the issue linked to the documentation

The [Fairlearn repository overview](https://fairlearn.org/main/contributor_guide/fairlearn_repository_overview.html) on our website is outdated. In particular, Visualization source code is no longer in the visualization directory.

#### Suggest a potential alternative/fix

* remove ""Visualization source code in the visualization directory""
* add description of scripts directory
* add description of notebooks directory
* add description of test_othermlpackages directory ","14","0.5406623274443886","Documentation","Development"
"https://github.com/fairlearn/fairlearn","944171875","issue","https://github.com/Trusted-AI/AIF360/issues/899","DOC sphinx fairlearn logo link is broken #### Describe the issue linked to the documentation

Currently, the Fairlearn logo in the docs generated by sphinx points to *""../_static/_static/images/fairlearn_full_color.png""* which doesn't exist. 

~~It seems that '_static' is currently prepended in `html_logo = ""_static/images/fairlearn_full_color.png""` in conf.py even though we also define the `html_static_path = ['_static']`. I expect that this used to be a bug in sphinx and that it is now fixed there, breaking our link.~~ 

#### Suggest a potential alternative/fix

~~In [conf.py](https://github.com/fairlearn/fairlearn/blob/main/docs/conf.py) replace line 144:~~

~~`html_logo = ""_static/images/fairlearn_full_color.png""`~~

~~with:~~

~~`html_logo = ""/images/fairlearn_full_color.png""`~~

UPDATE: As shown in #901 this apparently wasn't the right solution. When we change the url like this the image cannot be found at all while building the docs (`WARNING: logo file '/images/fairlearn_full_color.png' does not exist`). 

UPDATE 2: It turns out we are not the only one with this issue: see [this issue](https://github.com/sphinx-doc/sphinx/issues/9438). So we need to make sure to update to sphinx 4.1.1 and then all will be fine.","14","0.9595446252991208","Documentation","Development"
"https://github.com/fairlearn/fairlearn","943979593","issue","https://github.com/Trusted-AI/AIF360/issues/898","Bug in mitigation using Fairlearn on Azure AutoML models <!--
Before submitting a bug, please make sure the issue hasn't been already
addressed by searching through the past issues.

If your issue is a usage question, please submit it in one of these other
channels instead:
- StackOverflow with the `fairlearn` tag:
  https://stackoverflow.com/questions/tagged/fairlearn
- Discord: https://discord.gg/R22yCfgsRn
The issue tracker is used only to report bugs and feature requests. For
questions, please use either of the above platforms. Most question issues are
closed without an answer on this issue tracker. Thanks for your understanding.
-->

#### Describe the bug

`GridSearch` is being too restrictive when validating its inputs, rejecting data which it should be able to handle

#### Steps/Code to Reproduce

```python
from sklearn.model_selection import train_test_split
from fairlearn.reductions import GridSearch
from fairlearn.reductions import DemographicParity, ErrorRate
from fairlearn.metrics import MetricFrame, selection_rate, count
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn import metrics as skm
import pandas as pd


from sklearn.datasets import fetch_openml
data = fetch_openml(data_id=1590, as_frame=True)
X_raw = data.data
Y = (data.target == '>50K') * 1


A = X_raw[""sex""]
X = X_raw.drop(labels=['sex'], axis=1)

le = LabelEncoder()
Y = le.fit_transform(Y)

X_train, X_test, Y_train, Y_test, A_train, A_test = train_test_split(X,
                                                                     Y,
                                                                     A,
                                                                     test_size=0.2,
                                                                     random_state=0,
                                                                     stratify=Y)

# Work around indexing bug
X_train = X_train.reset_index(drop=True)
A_train = A_train.reset_index(drop=True)
X_test = X_test.reset_index(drop=True)
A_test = A_test.reset_index(drop=True)



# The following shows that LGBMClassifier can handle this dataset
# without the scaling and one-hot encoding used in other examples
from lightgbm import LGBMClassifier

unmitigated_predictor = LGBMClassifier()
unmitigated_predictor.fit(X_train, Y_train)

metric_frame = MetricFrame(metrics={""accuracy"": skm.accuracy_score,
                                    ""selection_rate"": selection_rate,
                                    ""count"": count},
                           sensitive_features=A_test,
                           y_true=Y_test,
                           y_pred=unmitigated_predictor.predict(X_test))
print(metric_frame.overall)
print(metric_frame.by_group)

# However a GridSearch fails:
sweep = GridSearch(LGBMClassifier(),
                   constraints=DemographicParity(),
                   grid_size=71)

sweep.fit(X_train, Y_train,
          sensitive_features=A_train)

```

#### Expected Results

That the `sweep.fit()` call works

#### Actual Results

```
c:\users\riedgar\appdata\local\continuum\miniconda3\envs\nb-automl-fairlearn\lib\site-packages\sklearn\utils\validation.py in check_array(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)
    709             try:
--> 710                 array = array.astype(np.float64)
    711             except ValueError as e:

ValueError: could not convert string to float: 'Private'

The above exception was the direct cause of the following exception:

ValueError                                Traceback (most recent call last)
<ipython-input-9-cb592bed5a23> in <module>
      1 sweep.fit(X_train, Y_train,
----> 2           sensitive_features=A_train)
      3 
      4 predictors = sweep.predictors_

c:\users\riedgar\appdata\local\continuum\miniconda3\envs\nb-automl-fairlearn\lib\site-packages\fairlearn\reductions\_grid_search\grid_search.py in fit(self, X, y, **kwargs)
    135         # Prep the parity constraints and objective
    136         logger.debug(""Preparing constraints and objective"")
--> 137         self.constraints.load_data(X, y, **kwargs)
    138         objective = self.constraints.default_objective()
    139         objective.load_data(X, y, **kwargs)

c:\users\riedgar\appdata\local\continuum\miniconda3\envs\nb-automl-fairlearn\lib\site-packages\fairlearn\reductions\_moments\utility_parity.py in load_data(self, X, y, sensitive_features, control_features)
    309                                          enforce_binary_labels=True,
    310                                          sensitive_features=sensitive_features,
--> 311                                          control_features=control_features)
    312 
    313         base_event = pd.Series(data=_ALL, index=y_train.index)

c:\users\riedgar\appdata\local\continuum\miniconda3\envs\nb-automl-fairlearn\lib\site-packages\fairlearn\utils\_input_validation.py in _validate_and_reformat_input(X, y, expect_y, enforce_binary_labels, **kwargs)
     70             y = y.to_numpy().reshape(-1)
     71 
---> 72         X, y = check_X_y(X, y)
     73         y = check_array(y, ensure_2d=False, dtype='numeric')
     74         if enforce_binary_labels and not set(np.unique(y)).issubset(set([0, 1])):
```

Obviously our call to `check_X_y()` is too restrictive.","11","0.315174587733493","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","943977302","issue","https://github.com/Trusted-AI/AIF360/issues/897","Mitigation with Fairlearn for Regression <!--
If your issue is a usage question, please submit it in one of these other
channels instead:
- StackOverflow with the `fairlearn` tag:
  https://stackoverflow.com/questions/tagged/fairlearn
- Discord: https://discord.gg/R22yCfgsRn
The issue tracker is used only to report bugs and feature requests. For
questions, please use either of the above platforms. Most question issues are
closed without an answer on this issue tracker. Thanks for your understanding.
-->

#### Is your feature request related to a problem? Please describe.
I have trained a regression model for predicting wait time in hospitals and would love to showcase it as ademo for MST customers that how fairlearn can help to mitigate for this regression model while there is no regression support for mitigation in Fairlearn now!

#### Describe the solution you'd like
Desigin a similar module that we have for classification, now this time for regression to mitigate it using GridSearch or something similar.

#### Describe alternatives you've considered, if relevant
There is no alternative

#### Additional context
<!-- Add any other context or screenshots about the feature request here. -->
","11","0.6421481328087251","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","941905294","issue","https://github.com/Trusted-AI/AIF360/issues/894","Add code examples for API Reference section #### Describe the issue linked to the documentation

<!--
Tell us about the confusion introduced in the documentation.
-->

This issue is related to adding usage examples in API reference guide. When some one is stumbling into API reference of Metrics Frame, a code example showing usage will greatly help the user.

For more context check the issue #730, https://github.com/fairlearn/fairlearn/issues/730#issuecomment-878110294","14","0.6979963910148715","Documentation","Development"
"https://github.com/fairlearn/fairlearn","940941080","issue","https://github.com/Trusted-AI/AIF360/issues/892","DOC typo in control features documentation #### Describe the issue linked to the documentation

![image](https://user-images.githubusercontent.com/10245648/125114348-bae5a980-e09e-11eb-983b-47b366148eba.png)

should be

![image](https://user-images.githubusercontent.com/10245648/125114400-c89b2f00-e09e-11eb-9264-13653286df06.png)

","14","0.5250398724082936","Documentation","Development"
"https://github.com/fairlearn/fairlearn","940234602","issue","https://github.com/Trusted-AI/AIF360/issues/890","Generalization issue of equalized odds (exponentiatedgradient and thresholdoptimizer) Hello,

I ran into the problem of generalization in enforcing equalized odds - if I equalize on training set, on test set I get quite unfair (significant after bootstrapping) performance. The test data is not out of distribution. I tried both thresholdOptimizer and exponentiatedGradient, and ran into generalization issues in both. I am wondering if this is a common issue.","3","0.3651047924175679","Bias detection metrics validation","Validation"
"https://github.com/fairlearn/fairlearn","939679372","issue","https://github.com/Trusted-AI/AIF360/issues/888","DOC Add funding page to highlight institutions that support Fairlearn From #433: add a funding page to highlight which institutions support(ed) the development of Fairlearn, similar to [scikit-learn](https://scikit-learn.org/stable/about.html#funding) or [pandas](https://pandas.pydata.org/about/sponsors.html).

Two open questions:
* *Where should the page live?* I think a section under *About Us* would make the most sense. Additionally, we could add some logos and/or link to section in the footer (like scikit-learn) or on our home page (like pandas).
* *What should be on it?* IMO the minimal version should have a header with a tittle like *Funding* and a simple list of organizations and how they are involved (e.g., *Organization X funds person Y since Z*). Can be with logos but a first version could be a simple list.

Thoughts? @fairlearn/fairlearn-maintainers ","20","0.4474796928897298","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","939381107","issue","https://github.com/Trusted-AI/AIF360/issues/886","DOC replace Gitter with Discord on the webpage #### Describe the issue linked to the documentation

Our website points people to Gitter even though our Discord is much more active and not mentioned anywhere. We should replace occurrences of Gitter with Discord.

- nav bar at the top 
![image](https://user-images.githubusercontent.com/10245648/124847369-90d1a180-df4f-11eb-970f-1ca165260250.png) find code at `docs/conf.py`, specifically [these lines](https://github.com/fairlearn/fairlearn/blob/378f1e693cff01c69de3a9d9504b336ae1e0064e/docs/conf.py#L134)
- [landing page](https://fairlearn.org/) 
![image](https://user-images.githubusercontent.com/10245648/124847419-ac3cac80-df4f-11eb-913e-4788493eecc6.png) find code at `docs/static_landing_page/index.html`
- https://fairlearn.org/main/contributor_guide/index.html#real-time-communication, find code at `docs/contributor_guide/index.rst`

Note that older versions will still say Gitter so we can't delete our Gitter or abandon it completely, but we might as well point people in the right direction without adding another hop.
","14","0.6277835342022501","Documentation","Development"
"https://github.com/fairlearn/fairlearn","938681414","issue","https://github.com/Trusted-AI/AIF360/issues/884","Convert the README.md to an .rst file Our README.md is a markdown file, but we can have more flexibility if it's converted to an RST file, which would also be more consistent with the rest of the documentation.

The converted file can be checked for errors using `twine check`.","14","0.4927443448570208","Documentation","Development"
"https://github.com/fairlearn/fairlearn","932884054","issue","https://github.com/Trusted-AI/AIF360/issues/880","[Website Redesign] Incorporate new illustrations into website - WIP When complete, Illustrations will be added to:

- Homepage
- Get Started
- Learn Landing
- Community Landing","14","0.8841256602724488","Documentation","Development"
"https://github.com/fairlearn/fairlearn","928949375","issue","https://github.com/Trusted-AI/AIF360/issues/874","DOC: multi-version behaves weird on example notebooks #### Describe the issue linked to the documentation

When you go to the [example notebooks](https://fairlearn.org/main/auto_examples/index.html) page, it always shows whatever version you had previously selected and ""Documentation page not present"" for the other versions. This makes it impossible to switch versions once you're on the Example Notebooks page.

Examples:

<img width=""1408"" alt=""image"" src=""https://user-images.githubusercontent.com/24417440/123223339-1cb7e800-d4d1-11eb-83b7-4ec15a57f9a3.png"">
<img width=""1388"" alt=""image"" src=""https://user-images.githubusercontent.com/24417440/123223381-23def600-d4d1-11eb-8c18-70f041cc5d45.png"">


#### Suggest a potential alternative/fix
The preferred behavior would be that one is able to click on the other versions on the Example Notebooks page.

I'm not really sure why this happens but I'm assuming that there is some issue with the way `sphinx-gallery` and `sphinx-multiversion` work together.","14","0.5711789427117894","Documentation","Development"
"https://github.com/fairlearn/fairlearn","927031162","issue","https://github.com/Trusted-AI/AIF360/issues/872","DOC: remove duplicates index example notebooks #### Describe the issue linked to the documentation

The lefthand side index of our [Example Notebooks](https://fairlearn.org/main/auto_examples) contains duplicate items. I think this is because some are [in the `toctree` of README.rst](https://github.com/fairlearn/fairlearn/edit/main/examples/README.rst) as well as in the automatically generated examples.

#### Suggest a potential alternative/fix

Remove the items from README.rst and only show automatically generated examples.

@fairlearn/fairlearn-maintainers 

*[if you want to work on to this issue, please let us know in the comments.]*","14","0.7463639187777119","Documentation","Development"
"https://github.com/fairlearn/fairlearn","921474854","issue","https://github.com/Trusted-AI/AIF360/issues/866","Combining Metrics with Errors As we start thinking about computing errors on metrics, we need to consider the computation of quantities based on metrics whch have errors applied. The most obvious places for this are the `difference()` and `ratio()` methods of `MetricFrame`.

If we have nice, normally distributed errors, then we can simply add errors in quadrature (absolute or relative errors as appropriate). If the errors are specified as asymmetric confidence bounds, we could simply compute the worst case (e.g. ""largest possible upper bound - smallest possible lower bound"") to give an idea of the range.

Actually, this can feed into the max and min as well. Which should be treated as larger: 10+/-1 or 10.01+/-0.01 ?","7","0.4113378446904125","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","915243335","issue","https://github.com/Trusted-AI/AIF360/issues/858","[Website Redesign] Replace Logo New logotype for word ""Fairlearn"" in progress
","14","0.827456055214062","Documentation","Development"
"https://github.com/fairlearn/fairlearn","913829535","issue","https://github.com/Trusted-AI/AIF360/issues/855","DOC Add versionadded and versionchanged to API reference Add `.. versionadded` and `.. versionchanged` to all methods (and classes if possible)

```
    .. versionadded:: 0.7
            In previous versions only the ``predict`` method was used
            implicitly.
    .. versionchanged:: 0.7
            From version 0.7, 'predict' is deprecated and the default will
            change to 'auto' from v0.10.
```

_Related to the post by @romanlutz in https://github.com/fairlearn/fairlearn/pull/854#r646574960_


Tracking this by module:
- [x] fairlearn.datasets #929 
- [ ] fairlearn.metrics #951 
- [ ] fairlearn.reductions #948 
- [ ] fairlearn.postprocessing #950 
- [ ] fairlearn.preprocessing #949 ","14","0.5582515231329979","Documentation","Development"
"https://github.com/fairlearn/fairlearn","913501505","issue","https://github.com/Trusted-AI/AIF360/issues/853","RFC Mention the author(s) and the PR and/issue in changelog. Our changelog right now doesn't credit the author nor does it mention the PR or the issue corresponding to an entry. Having those makes it easier to track the change, example: https://scikit-learn.org/stable/whats_new/v0.24.html

Should we introduce using `:pr:`, `:issue:` and `:user:` roles?

cc @fairlearn/fairlearn-maintainers ","20","0.5049631044686669","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","913497452","issue","https://github.com/Trusted-AI/AIF360/issues/852","DOC fix warnings in documentation build There are warnings during doc build which need to be taken care of:

<details>

```
Output from /home/adrin/Projects/fairlearn/fairlearn/examples/plot_new_metrics.py                                                                                                       
/home/adrin/Projects/fairlearn/fairlearn/examples/plot_new_metrics.py:91: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  X_raw['race'] = X_raw['race'].map(race_transform).fillna('Other').astype('category')
['Black' 'Other' 'White']
Output from /home/adrin/Projects/fairlearn/fairlearn/examples/plot_new_metrics.py
/home/adrin/Projects/fairlearn/fairlearn/examples/plot_new_metrics.py:125: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  A['Credit Score'] = col_credit
/home/adrin/Projects/fairlearn/fairlearn/examples/plot_new_metrics.py:126: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  A['Loan Size'] = col_loan_size
Output from /home/adrin/Projects/fairlearn/fairlearn/examples/plot_new_metrics.py
/home/adrin/miniforge3/envs/fairlearn/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1495: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(
/home/adrin/miniforge3/envs/fairlearn/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1495: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(
Output from /home/adrin/Projects/fairlearn/fairlearn/examples/plot_new_metrics.py
/home/adrin/miniforge3/envs/fairlearn/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1495: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(
Found 36 subgroups. Evaluation may be slow
/home/adrin/miniforge3/envs/fairlearn/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1495: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(
/home/adrin/miniforge3/envs/fairlearn/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1495: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(
/home/adrin/miniforge3/envs/fairlearn/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1495: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(
/home/adrin/miniforge3/envs/fairlearn/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1495: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(
/home/adrin/miniforge3/envs/fairlearn/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1495: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(
/home/adrin/miniforge3/envs/fairlearn/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1495: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(
/home/adrin/miniforge3/envs/fairlearn/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1495: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(
/home/adrin/miniforge3/envs/fairlearn/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1495: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(
/home/adrin/miniforge3/envs/fairlearn/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1495: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(
/home/adrin/Projects/fairlearn/fairlearn/docs/../fairlearn/datasets/__init__.py:docstring of fairlearn.datasets.fetch_adult:: WARNING: py:class reference target not found: boolean
/home/adrin/Projects/fairlearn/fairlearn/docs/../fairlearn/datasets/__init__.py:docstring of fairlearn.datasets.fetch_adult:: WARNING: py:class reference target not found: default=True
/home/adrin/Projects/fairlearn/fairlearn/docs/../fairlearn/datasets/__init__.py:docstring of fairlearn.datasets.fetch_adult:: WARNING: py:class reference target not found: optional
/home/adrin/Projects/fairlearn/fairlearn/docs/../fairlearn/datasets/__init__.py:docstring of fairlearn.datasets.fetch_adult:: WARNING: py:class reference target not found: default: None
/home/adrin/Projects/fairlearn/fairlearn/docs/../fairlearn/datasets/__init__.py:docstring of fairlearn.datasets.fetch_adult:: WARNING: py:class reference target not found: boolean
/home/adrin/Projects/fairlearn/fairlearn/docs/../fairlearn/datasets/__init__.py:docstring of fairlearn.datasets.fetch_adult:: WARNING: py:class reference target not found: default=False
/home/adrin/Projects/fairlearn/fairlearn/docs/../fairlearn/datasets/__init__.py:docstring of fairlearn.datasets.fetch_adult:: WARNING: py:class reference target not found: boolean
/home/adrin/Projects/fairlearn/fairlearn/docs/../fairlearn/datasets/__init__.py:docstring of fairlearn.datasets.fetch_adult:: WARNING: py:class reference target not found: default=False.
/home/adrin/Projects/fairlearn/fairlearn/docs/../fairlearn/datasets/__init__.py:docstring of fairlearn.datasets.fetch_adult:33: WARNING: py:class reference target not found: sklearn.utils.Bunch
/home/adrin/Projects/fairlearn/fairlearn/docs/../fairlearn/datasets/__init__.py:docstring of fairlearn.datasets.fetch_bank_marketing:: WARNING: py:class reference target not found: boolean
/home/adrin/Projects/fairlearn/fairlearn/docs/../fairlearn/datasets/__init__.py:docstring of fairlearn.datasets.fetch_bank_marketing:: WARNING: py:class reference target not found: default=True
/home/adrin/Projects/fairlearn/fairlearn/docs/../fairlearn/datasets/__init__.py:docstring of fairlearn.datasets.fetch_bank_marketing:: WARNING: py:class reference target not found: optional
/home/adrin/Projects/fairlearn/fairlearn/docs/../fairlearn/datasets/__init__.py:docstring of fairlearn.datasets.fetch_bank_marketing:: WARNING: py:class reference target not found: default: None
/home/adrin/Projects/fairlearn/fairlearn/docs/../fairlearn/datasets/__init__.py:docstring of fairlearn.datasets.fetch_bank_marketing:: WARNING: py:class reference target not found: boolean
/home/adrin/Projects/fairlearn/fairlearn/docs/../fairlearn/datasets/__init__.py:docstring of fairlearn.datasets.fetch_bank_marketing:: WARNING: py:class reference target not found: default=False
/home/adrin/Projects/fairlearn/fairlearn/docs/../fairlearn/datasets/__init__.py:docstring of fairlearn.datasets.fetch_bank_marketing:: WARNING: py:class reference target not found: boolean
/home/adrin/Projects/fairlearn/fairlearn/docs/../fairlearn/datasets/__init__.py:docstring of fairlearn.datasets.fetch_bank_marketing:: WARNING: py:class reference target not found: default=False.
/home/adrin/Projects/fairlearn/fairlearn/docs/../fairlearn/datasets/__init__.py:docstring of fairlearn.datasets.fetch_bank_marketing:39: WARNING: py:class reference target not found: sklearn.utils.Bunch
/home/adrin/Projects/fairlearn/fairlearn/docs/../fairlearn/datasets/__init__.py:docstring of fairlearn.datasets.fetch_boston:: WARNING: py:class reference target not found: boolean
/home/adrin/Projects/fairlearn/fairlearn/docs/../fairlearn/datasets/__init__.py:docstring of fairlearn.datasets.fetch_boston:: WARNING: py:class reference target not found: default=True
/home/adrin/Projects/fairlearn/fairlearn/docs/../fairlearn/datasets/__init__.py:docstring of fairlearn.datasets.fetch_boston:: WARNING: py:class reference target not found: optional
/home/adrin/Projects/fairlearn/fairlearn/docs/../fairlearn/datasets/__init__.py:docstring of fairlearn.datasets.fetch_boston:: WARNING: py:class reference target not found: default: None
/home/adrin/Projects/fairlearn/fairlearn/docs/../fairlearn/datasets/__init__.py:docstring of fairlearn.datasets.fetch_boston:: WARNING: py:class reference target not found: boolean
/home/adrin/Projects/fairlearn/fairlearn/docs/../fairlearn/datasets/__init__.py:docstring of fairlearn.datasets.fetch_boston:: WARNING: py:class reference target not found: default=False
/home/adrin/Projects/fairlearn/fairlearn/docs/../fairlearn/datasets/__init__.py:docstring of fairlearn.datasets.fetch_boston:: WARNING: py:class reference target not found: boolean
/home/adrin/Projects/fairlearn/fairlearn/docs/../fairlearn/datasets/__init__.py:docstring of fairlearn.datasets.fetch_boston:: WARNING: py:class reference target not found: default=False.
/home/adrin/Projects/fairlearn/fairlearn/docs/../fairlearn/datasets/__init__.py:docstring of fairlearn.datasets.fetch_boston:: WARNING: py:class reference target not found: boolean
/home/adrin/Projects/fairlearn/fairlearn/docs/../fairlearn/datasets/__init__.py:docstring of fairlearn.datasets.fetch_boston:: WARNING: py:class reference target not found: default=True.
/home/adrin/Projects/fairlearn/fairlearn/docs/../fairlearn/datasets/__init__.py:docstring of fairlearn.datasets.fetch_boston:64: WARNING: py:class reference target not found: sklearn.utils.Bunch
/home/adrin/Projects/fairlearn/fairlearn/docs/../fairlearn/metrics/_metric_frame.py:docstring of fairlearn.metrics.MetricFrame:: WARNING: py:class reference target not found: callable
/home/adrin/Projects/fairlearn/fairlearn/docs/../fairlearn/metrics/_metric_frame.py:docstring of fairlearn.metrics.MetricFrame:: WARNING: py:class reference target not found: dict of 1d arrays
/home/adrin/Projects/fairlearn/fairlearn/docs/../fairlearn/metrics/_metric_frame.py:docstring of fairlearn.metrics.MetricFrame:: WARNING: py:class reference target not found: dict of 1d arrays
/home/adrin/Projects/fairlearn/fairlearn/docs/../fairlearn/metrics/_metric_frame.py:docstring of fairlearn.metrics.MetricFrame.difference:: WARNING: py:class reference target not found: pandas.core.series.Series
/home/adrin/Projects/fairlearn/fairlearn/docs/../fairlearn/metrics/_metric_frame.py:docstring of fairlearn.metrics.MetricFrame.difference:: WARNING: py:class reference target not found: pandas.core.frame.DataFrame
/home/adrin/Projects/fairlearn/fairlearn/docs/../fairlearn/metrics/_metric_frame.py:docstring of fairlearn.metrics.MetricFrame.group_max:: WARNING: py:class reference target not found: pandas.core.series.Series
/home/adrin/Projects/fairlearn/fairlearn/docs/../fairlearn/metrics/_metric_frame.py:docstring of fairlearn.metrics.MetricFrame.group_max:: WARNING: py:class reference target not found: pandas.core.frame.DataFrame
/home/adrin/Projects/fairlearn/fairlearn/docs/../fairlearn/metrics/_metric_frame.py:docstring of fairlearn.metrics.MetricFrame.group_min:: WARNING: py:class reference target not found: pandas.core.series.Series
/home/adrin/Projects/fairlearn/fairlearn/docs/../fairlearn/metrics/_metric_frame.py:docstring of fairlearn.metrics.MetricFrame.group_min:: WARNING: py:class reference target not found: pandas.core.frame.DataFrame
/home/adrin/Projects/fairlearn/fairlearn/docs/../fairlearn/metrics/_metric_frame.py:docstring of fairlearn.metrics.MetricFrame.group_min:: WARNING: py:class reference target not found: typing.Any pandas.Series
/home/adrin/Projects/fairlearn/fairlearn/docs/../fairlearn/metrics/_metric_frame.py:docstring of fairlearn.metrics.MetricFrame.ratio:: WARNING: py:class reference target not found: pandas.core.series.Series
/home/adrin/Projects/fairlearn/fairlearn/docs/../fairlearn/metrics/_metric_frame.py:docstring of fairlearn.metrics.MetricFrame.ratio:: WARNING: py:class reference target not found: pandas.core.frame.DataFrame
/home/adrin/Projects/fairlearn/fairlearn/docs/../fairlearn/metrics/__init__.py:docstring of fairlearn.metrics.count:: WARNING: py:class reference target not found: array_like
/home/adrin/Projects/fairlearn/fairlearn/docs/../fairlearn/metrics/__init__.py:docstring of fairlearn.metrics.count:: WARNING: py:class reference target not found: array_like
/home/adrin/Projects/fairlearn/fairlearn/docs/../fairlearn/metrics/__init__.py:docstring of fairlearn.metrics.demographic_parity_difference:: WARNING: py:class reference target not found: array-like
/home/adrin/Projects/fairlearn/fairlearn/docs/../fairlearn/metrics/__init__.py:docstring of fairlearn.metrics.demographic_parity_difference:: WARNING: py:class reference target not found: array-like
/home/adrin/Projects/fairlearn/fairlearn/docs/../fairlearn/metrics/__init__.py:docstring of fairlearn.metrics.demographic_parity_difference:: WARNING: py:class reference target not found: array-like
/home/adrin/Projects/fairlearn/fairlearn/docs/../fairlearn/metrics/__init__.py:docstring of fairlearn.metrics.demographic_parity_ratio:: WARNING: py:class reference target not found: array-like
/home/adrin/Projects/fairlearn/fairlearn/docs/../fairlearn/metrics/__init__.py:docstring of fairlearn.metrics.demographic_parity_ratio:: WARNING: py:class reference target not found: array-like
/home/adrin/Projects/fairlearn/fairlearn/docs/../fairlearn/metrics/__init__.py:docstring of fairlearn.metrics.demographic_parity_ratio:: WARNING: py:class reference target not found: array-like
/home/adrin/Projects/fairlearn/fairlearn/docs/../fairlearn/metrics/__init__.py:docstring of fairlearn.metrics.equalized_odds_difference:: WARNING: py:class reference target not found: array-like
/home/adrin/Projects/fairlearn/fairlearn/docs/../fairlearn/metrics/__init__.py:docstring of fairlearn.metrics.equalized_odds_difference:: WARNING: py:class reference target not found: array-like
/home/adrin/Projects/fairlearn/fairlearn/docs/../fairlearn/metrics/__init__.py:docstring of fairlearn.metrics.equalized_odds_difference:: WARNING: py:class reference target not found: array-like
/home/adrin/Projects/fairlearn/fairlearn/docs/../fairlearn/metrics/__init__.py:docstring of fairlearn.metrics.equalized_odds_ratio:: WARNING: py:class reference target not found: array-like
/home/adrin/Projects/fairlearn/fairlearn/docs/../fairlearn/metrics/__init__.py:docstring of fairlearn.metrics.equalized_odds_ratio:: WARNING: py:class reference target not found: array-like
/home/adrin/Projects/fairlearn/fairlearn/docs/../fairlearn/metrics/__init__.py:docstring of fairlearn.metrics.equalized_odds_ratio:: WARNING: py:class reference target not found: array-like
/home/adrin/Projects/fairlearn/fairlearn/docs/../fairlearn/metrics/__init__.py:docstring of fairlearn.metrics.false_negative_rate:: WARNING: py:class reference target not found: array-like
/home/adrin/Projects/fairlearn/fairlearn/docs/../fairlearn/metrics/__init__.py:docstring of fairlearn.metrics.false_negative_rate:: WARNING: py:class reference target not found: array-like
/home/adrin/Projects/fairlearn/fairlearn/docs/../fairlearn/metrics/__init__.py:docstring of fairlearn.metrics.false_negative_rate:: WARNING: py:class reference target not found: array-like
/home/adrin/Projects/fairlearn/fairlearn/docs/../fairlearn/metrics/__init__.py:docstring of fairlearn.metrics.false_negative_rate:: WARNING: py:class reference target not found: optional
/home/adrin/Projects/fairlearn/fairlearn/docs/../fairlearn/metrics/__init__.py:docstring of fairlearn.metrics.false_negative_rate:: WARNING: py:class reference target not found: scalar
/home/adrin/Projects/fairlearn/fairlearn/docs/../fairlearn/metrics/__init__.py:docstring of fairlearn.metrics.false_negative_rate:: WARNING: py:class reference target not found: optional
/home/adrin/Projects/fairlearn/fairlearn/docs/../fairlearn/metrics/__init__.py:docstring of fairlearn.metrics.false_positive_rate:: WARNING: py:class reference target not found: array-like
/home/adrin/Projects/fairlearn/fairlearn/docs/../fairlearn/metrics/__init__.py:docstring of fairlearn.metrics.false_positive_rate:: WARNING: py:class reference target not found: array-like
/home/adrin/Projects/fairlearn/fairlearn/docs/../fairlearn/metrics/__init__.py:docstring of fairlearn.metrics.false_positive_rate:: WARNING: py:class reference target not found: array-like
/home/adrin/Projects/fairlearn/fairlearn/docs/../fairlearn/metrics/__init__.py:docstring of fairlearn.metrics.false_positive_rate:: WARNING: py:class reference target not found: optional
/home/adrin/Projects/fairlearn/fairlearn/docs/../fairlearn/metrics/__init__.py:docstring of fairlearn.metrics.false_positive_rate:: WARNING: py:class reference target not found: scalar
/home/adrin/Projects/fairlearn/fairlearn/docs/../fairlearn/metrics/__init__.py:docstring of fairlearn.metrics.false_positive_rate:: WARNING: py:class reference target not found: optional
/home/adrin/Projects/fairlearn/fairlearn/docs/../fairlearn/metrics/__init__.py:docstring of fairlearn.metrics.make_derived_metric:: WARNING: py:class reference target not found: callable
/home/adrin/Projects/fairlearn/fairlearn/docs/../fairlearn/metrics/__init__.py:docstring of fairlearn.metrics.make_derived_metric:: WARNING: py:class reference target not found: callable
/home/adrin/Projects/fairlearn/fairlearn/docs/../fairlearn/metrics/__init__.py:docstring of fairlearn.metrics.mean_prediction:: WARNING: py:class reference target not found: array_like
/home/adrin/Projects/fairlearn/fairlearn/docs/../fairlearn/metrics/__init__.py:docstring of fairlearn.metrics.mean_prediction:: WARNING: py:class reference target not found: array_like
/home/adrin/Projects/fairlearn/fairlearn/docs/../fairlearn/metrics/__init__.py:docstring of fairlearn.metrics.mean_prediction:: WARNING: py:class reference target not found: array_like
/home/adrin/Projects/fairlearn/fairlearn/docs/../fairlearn/metrics/__init__.py:docstring of fairlearn.metrics.selection_rate:: WARNING: py:class reference target not found: array_like
/home/adrin/Projects/fairlearn/fairlearn/docs/../fairlearn/metrics/__init__.py:docstring of fairlearn.metrics.selection_rate:: WARNING: py:class reference target not found: array_like
/home/adrin/Projects/fairlearn/fairlearn/docs/../fairlearn/metrics/__init__.py:docstring of fairlearn.metrics.selection_rate:: WARNING: py:class reference target not found: Scalar
/home/adrin/Projects/fairlearn/fairlearn/docs/../fairlearn/metrics/__init__.py:docstring of fairlearn.metrics.selection_rate:: WARNING: py:class reference target not found: array_like
/home/adrin/Projects/fairlearn/fairlearn/docs/../fairlearn/metrics/__init__.py:docstring of fairlearn.metrics.true_negative_rate:: WARNING: py:class reference target not found: array-like
/home/adrin/Projects/fairlearn/fairlearn/docs/../fairlearn/metrics/__init__.py:docstring of fairlearn.metrics.true_negative_rate:: WARNING: py:class reference target not found: array-like
/home/adrin/Projects/fairlearn/fairlearn/docs/../fairlearn/metrics/__init__.py:docstring of fairlearn.metrics.true_negative_rate:: WARNING: py:class reference target not found: array-like
/home/adrin/Projects/fairlearn/fairlearn/docs/../fairlearn/metrics/__init__.py:docstring of fairlearn.metrics.true_negative_rate:: WARNING: py:class reference target not found: optional
/home/adrin/Projects/fairlearn/fairlearn/docs/../fairlearn/metrics/__init__.py:docstring of fairlearn.metrics.true_negative_rate:: WARNING: py:class reference target not found: scalar
/home/adrin/Projects/fairlearn/fairlearn/docs/../fairlearn/metrics/__init__.py:docstring of fairlearn.metrics.true_negative_rate:: WARNING: py:class reference target not found: optional
/home/adrin/Projects/fairlearn/fairlearn/docs/../fairlearn/metrics/__init__.py:docstring of fairlearn.metrics.true_positive_rate:: WARNING: py:class reference target not found: array-like
/home/adrin/Projects/fairlearn/fairlearn/docs/../fairlearn/metrics/__init__.py:docstring of fairlearn.metrics.true_positive_rate:: WARNING: py:class reference target not found: array-like
/home/adrin/Projects/fairlearn/fairlearn/docs/../fairlearn/metrics/__init__.py:docstring of fairlearn.metrics.true_positive_rate:: WARNING: py:class reference target not found: array-like
/home/adrin/Projects/fairlearn/fairlearn/docs/../fairlearn/metrics/__init__.py:docstring of fairlearn.metrics.true_positive_rate:: WARNING: py:class reference target not found: optional
/home/adrin/Projects/fairlearn/fairlearn/docs/../fairlearn/metrics/__init__.py:docstring of fairlearn.metrics.true_positive_rate:: WARNING: py:class reference target not found: scalar
/home/adrin/Projects/fairlearn/fairlearn/docs/../fairlearn/metrics/__init__.py:docstring of fairlearn.metrics.true_positive_rate:: WARNING: py:class reference target not found: optional
/home/adrin/Projects/fairlearn/fairlearn/docs/../fairlearn/postprocessing/_threshold_optimizer.py:docstring of fairlearn.postprocessing.ThresholdOptimizer:1: WARNING: py:class reference target not found: sklearn.base.MetaEstimatorMixin
/home/adrin/Projects/fairlearn/fairlearn/docs/../fairlearn/postprocessing/_threshold_optimizer.py:docstring of fairlearn.postprocessing.ThresholdOptimizer:: WARNING: py:class reference target not found: estimator object implementing 'predict' and possibly 'fit'
/home/adrin/Projects/fairlearn/fairlearn/docs/../fairlearn/postprocessing/_threshold_optimizer.py:docstring of fairlearn.postprocessing.ThresholdOptimizer:: WARNING: py:class reference target not found: default='demographic_parity'
/home/adrin/Projects/fairlearn/fairlearn/docs/../fairlearn/postprocessing/_threshold_optimizer.py:docstring of fairlearn.postprocessing.ThresholdOptimizer:: WARNING: py:class reference target not found: default='accuracy_score'
/home/adrin/Projects/fairlearn/fairlearn/docs/../fairlearn/postprocessing/_threshold_optimizer.py:docstring of fairlearn.postprocessing.ThresholdOptimizer:: WARNING: py:class reference target not found: default=1000
/home/adrin/Projects/fairlearn/fairlearn/docs/../fairlearn/postprocessing/_threshold_optimizer.py:docstring of fairlearn.postprocessing.ThresholdOptimizer:: WARNING: py:class reference target not found: default=False
/home/adrin/Projects/fairlearn/fairlearn/docs/../fairlearn/postprocessing/_threshold_optimizer.py:docstring of fairlearn.postprocessing.ThresholdOptimizer:: WARNING: py:class reference target not found: default=False
/home/adrin/Projects/fairlearn/fairlearn/docs/../fairlearn/postprocessing/_threshold_optimizer.py:docstring of fairlearn.postprocessing.ThresholdOptimizer:: WARNING: py:class reference target not found: {'auto'
/home/adrin/Projects/fairlearn/fairlearn/docs/../fairlearn/postprocessing/_threshold_optimizer.py:docstring of fairlearn.postprocessing.ThresholdOptimizer:: WARNING: py:class reference target not found: 'predict_proba'
/home/adrin/Projects/fairlearn/fairlearn/docs/../fairlearn/postprocessing/_threshold_optimizer.py:docstring of fairlearn.postprocessing.ThresholdOptimizer:: WARNING: py:class reference target not found: 'decision_function'
/home/adrin/Projects/fairlearn/fairlearn/docs/../fairlearn/postprocessing/_threshold_optimizer.py:docstring of fairlearn.postprocessing.ThresholdOptimizer:: WARNING: py:class reference target not found: 'predict'            }
/home/adrin/Projects/fairlearn/fairlearn/docs/../fairlearn/postprocessing/_threshold_optimizer.py:docstring of fairlearn.postprocessing.ThresholdOptimizer:: WARNING: py:class reference target not found: default='auto'
/home/adrin/Projects/fairlearn/fairlearn/docs/../fairlearn/postprocessing/_threshold_optimizer.py:docstring of fairlearn.postprocessing.ThresholdOptimizer.predict:: WARNING: py:class reference target not found: RandomState instance
/home/adrin/Projects/fairlearn/fairlearn/docs/../fairlearn/postprocessing/_threshold_optimizer.py:docstring of fairlearn.postprocessing.ThresholdOptimizer.predict:: WARNING: py:class reference target not found: default=None
/home/adrin/Projects/fairlearn/fairlearn/docs/../fairlearn/postprocessing/_threshold_optimizer.py:docstring of fairlearn.postprocessing.ThresholdOptimizer.predict:: WARNING: py:class reference target not found: Scalar
/home/adrin/Projects/fairlearn/fairlearn/docs/../fairlearn/postprocessing/_threshold_optimizer.py:docstring of fairlearn.postprocessing.ThresholdOptimizer.predict:: WARNING: py:class reference target not found: vector as numpy.ndarray
writing output... [ 16%] api_reference/fairlearn.preprocessing .. api_reference/index
/home/adrin/Projects/fairlearn/fairlearn/docs/../fairlearn/reductions/_moments/bounded_group_loss.py:docstring of fairlearn.reductions.BoundedGroupLoss:1: WARNING: py:class reference target not found: fairlearn.reductions.ConditionalLossMoment
/home/adrin/Projects/fairlearn/fairlearn/docs/../fairlearn/reductions/_moments/utility_parity.py:docstring of fairlearn.reductions.DemographicParity:8: WARNING: py:meth reference target not found: signed_weights
/home/adrin/Projects/fairlearn/fairlearn/docs/../fairlearn/reductions/_moments/utility_parity.py:docstring of fairlearn.reductions.EqualizedOdds:20: WARNING: py:meth reference target not found: signed_weights
/home/adrin/Projects/fairlearn/fairlearn/docs/../fairlearn/reductions/_moments/utility_parity.py:docstring of fairlearn.reductions.ErrorRateParity:16: WARNING: py:meth reference target not found: signed_weights
/home/adrin/Projects/fairlearn/fairlearn/docs/../fairlearn/reductions/_exponentiated_gradient/exponentiated_gradient.py:docstring of fairlearn.reductions.ExponentiatedGradient:1: WARNING: py:class reference target not found: sklearn.base.MetaEstimatorMixin
/home/adrin/Projects/fairlearn/fairlearn/docs/../fairlearn/reductions/_exponentiated_gradient/exponentiated_gradient.py:docstring of fairlearn.reductions.ExponentiatedGradient:: WARNING: py:class reference target not found: estimator
/home/adrin/Projects/fairlearn/fairlearn/docs/../fairlearn/reductions/_exponentiated_gradient/exponentiated_gradient.py:docstring of fairlearn.reductions.ExponentiatedGradient.predict:: WARNING: py:class reference target not found: RandomState instance
/home/adrin/Projects/fairlearn/fairlearn/docs/../fairlearn/reductions/_exponentiated_gradient/exponentiated_gradient.py:docstring of fairlearn.reductions.ExponentiatedGradient.predict:: WARNING: py:class reference target not found: default=None
/home/adrin/Projects/fairlearn/fairlearn/docs/../fairlearn/reductions/_exponentiated_gradient/exponentiated_gradient.py:docstring of fairlearn.reductions.ExponentiatedGradient.predict:: WARNING: py:class reference target not found: Scalar
/home/adrin/Projects/fairlearn/fairlearn/docs/../fairlearn/reductions/_exponentiated_gradient/exponentiated_gradient.py:docstring of fairlearn.reductions.ExponentiatedGradient.predict:: WARNING: py:class reference target not found: vector
/home/adrin/Projects/fairlearn/fairlearn/docs/../fairlearn/reductions/_moments/utility_parity.py:docstring of fairlearn.reductions.FalsePositiveRateParity:20: WARNING: py:meth reference target not found: signed_weights
/home/adrin/Projects/fairlearn/fairlearn/docs/../fairlearn/reductions/_grid_search/grid_search.py:docstring of fairlearn.reductions.GridSearch:1: WARNING: py:class reference target not found: sklearn.base.MetaEstimatorMixin
/home/adrin/Projects/fairlearn/fairlearn/docs/../fairlearn/reductions/_grid_search/grid_search.py:docstring of fairlearn.reductions.GridSearch:: WARNING: py:class reference target not found: estimator
/home/adrin/Projects/fairlearn/fairlearn/docs/../fairlearn/reductions/_grid_search/grid_search.py:docstring of fairlearn.reductions.GridSearch.fit:: WARNING: py:class reference target not found: for now
/home/adrin/Projects/fairlearn/fairlearn/docs/../fairlearn/reductions/__init__.py:docstring of fairlearn.reductions.Moment.load_data:: WARNING: py:class reference target not found: pandas.core.series.Series
/home/adrin/Projects/fairlearn/fairlearn/docs/../fairlearn/reductions/__init__.py:docstring of fairlearn.reductions.Moment.load_data:: WARNING: py:class reference target not found: pandas.core.series.Series
/home/adrin/Projects/fairlearn/fairlearn/docs/../fairlearn/reductions/__init__.py:docstring of fairlearn.reductions.Moment.load_data:: WARNING: py:class reference target not found: array
/home/adrin/Projects/fairlearn/fairlearn/docs/../fairlearn/reductions/_moments/utility_parity.py:docstring of fairlearn.reductions.TruePositiveRateParity:26: WARNING: py:meth reference target not found: signed_weights
/home/adrin/Projects/fairlearn/fairlearn/docs/../fairlearn/reductions/__init__.py:docstring of fairlearn.reductions.UtilityParity.load_data:: WARNING: py:class reference target not found: pandas.core.series.Series
/home/adrin/Projects/fairlearn/fairlearn/docs/../fairlearn/reductions/__init__.py:docstring of fairlearn.reductions.UtilityParity.load_data:: WARNING: py:class reference target not found: pandas.core.series.Series
/home/adrin/Projects/fairlearn/fairlearn/docs/../fairlearn/reductions/__init__.py:docstring of fairlearn.reductions.UtilityParity.load_data:: WARNING: py:class reference target not found: pandas.core.series.Series
/home/adrin/Projects/fairlearn/fairlearn/docs/faq.rst:128: WARNING: py:class reference target not found: MetricFrame
/home/adrin/Projects/fairlearn/fairlearn/docs/faq.rst:138: WARNING: py:class reference target not found: ExponentiatedGradient
/home/adrin/Projects/fairlearn/fairlearn/docs/faq.rst:138: WARNING: py:class reference target not found: GridSearch
/home/adrin/Projects/fairlearn/fairlearn/docs/faq.rst:138: WARNING: undefined label: _bounded_group_loss (if the link has no caption the label must precede a section header)
/home/adrin/Projects/fairlearn/fairlearn/docs/user_guide/migrating_versions/migrating_to_v0_5_0.rst:12: WARNING: py:class reference target not found: sklearn.utils.Bunch
/home/adrin/Projects/fairlearn/fairlearn/docs/user_guide/migrating_versions/migrating_to_v0_5_0.rst:21: WARNING: py:class reference target not found: MetricFrame
```

</details>","1","0.978594747488948","Fix warnings","Maintenance"
"https://github.com/fairlearn/fairlearn","913348090","issue","https://github.com/Trusted-AI/AIF360/issues/850","TST find and fix slow tests Some of our tests are really slow. This is issue is to find those tests (with `--duration=N` passed to `pytest`), and to find alternatives so that the run faster.","32","0.5772758904282911","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","911447536","issue","https://github.com/Trusted-AI/AIF360/issues/848","Test for deprecation warnings Our CI should test and fail for `DeprecationWarning` and `FutureWarning`. This can be done by passing `-Werror::DeprecationWarning -Werror::FutureWarning` to `pytest`.

We can enable this for at least a few of the tests if not all.","5","0.4970999053030305","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","911361479","issue","https://github.com/Trusted-AI/AIF360/issues/847","Add code coverage test to the CI We can add https://about.codecov.io/ to the CI, to make sure the PRs are all tested.","32","0.72306696861857","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","911009520","issue","https://github.com/Trusted-AI/AIF360/issues/846","[Website Redesign] Design Language **[Font Ramp](https://www.figma.com/file/wt76z0M87RBDDEoyAEpD2g/OPEN-SOURCE-Fairlearn-Redesign?node-id=2045%3A117)**
_Reconciliation of desktop and mobile sizing in Progress_
Fonts: 
[Umbuntu](https://fonts.google.com/specimen/Ubuntu?category=Sans+Serif,Display,Monospace#standard-styles)
[Font license](https://ubuntu.com/legal/font-licence)

[Open Sans](https://fonts.google.com/specimen/Open+Sans?category=Sans+Serif,Display,Monospace&query=open+sans)
[Font License](https://www.apache.org/licenses/LICENSE-2.0.html)

**[Color palette](https://www.figma.com/file/wt76z0M87RBDDEoyAEpD2g/OPEN-SOURCE-Fairlearn-Redesign?node-id=2046%3A1556)**
Original Blue and Teal color palette. New colors are in exploration.","14","0.5323564183933794","Documentation","Development"
"https://github.com/fairlearn/fairlearn","909932757","issue","https://github.com/Trusted-AI/AIF360/issues/845","[Website Redesign] Mobile Design # Work in Progress

- Not yet reviewed
- Sample pages with navigation example 

## Preview
https://www.figma.com/file/wt76z0M87RBDDEoyAEpD2g/OPEN-SOURCE-Fairlearn-Redesign?node-id=2567%3A3011","14","0.7227969770342649","Documentation","Development"
"https://github.com/fairlearn/fairlearn","909927506","issue","https://github.com/Trusted-AI/AIF360/issues/844","[Website Redesign] About Landing Page https://www.figma.com/file/wt76z0M87RBDDEoyAEpD2g/OPEN-SOURCE-Fairlearn-Redesign?node-id=2535%3A5419

Incomplete: Contributor module at bottom

See Issue #838 ","14","0.8602742118592255","Documentation","Development"
"https://github.com/fairlearn/fairlearn","909926610","issue","https://github.com/Trusted-AI/AIF360/issues/843","[Website Redesign] FAQ Landing Page https://www.figma.com/file/wt76z0M87RBDDEoyAEpD2g/OPEN-SOURCE-Fairlearn-Redesign?node-id=2535%3A5150

Incomplete: Inline expansion view","14","0.6890749601275917","Documentation","Development"
"https://github.com/fairlearn/fairlearn","909925944","issue","https://github.com/Trusted-AI/AIF360/issues/842","[Website Redesign] Community Landing Page https://www.figma.com/file/wt76z0M87RBDDEoyAEpD2g/OPEN-SOURCE-Fairlearn-Redesign?node-id=2535%3A4910

- Gitter module will be ""Featured"" Gitter conversations. Requires curation
- Twitter is a feed","14","0.816847372810675","Documentation","Development"
"https://github.com/fairlearn/fairlearn","909924930","issue","https://github.com/Trusted-AI/AIF360/issues/841","[Website Redesign] Contribute Landing Page https://www.figma.com/file/wt76z0M87RBDDEoyAEpD2g/OPEN-SOURCE-Fairlearn-Redesign?node-id=2535%3A4478

User lands on overview page.

See Issue #838 ","14","0.8602742118592255","Documentation","Development"
"https://github.com/fairlearn/fairlearn","909923709","issue","https://github.com/Trusted-AI/AIF360/issues/840","[Website Redesign] API Docs Landing Page https://www.figma.com/file/wt76z0M87RBDDEoyAEpD2g/OPEN-SOURCE-Fairlearn-Redesign?node-id=2565%3A2687

User lands on overview page

See Issue #838 ","14","0.8724082934609254","Documentation","Development"
"https://github.com/fairlearn/fairlearn","909923353","issue","https://github.com/Trusted-AI/AIF360/issues/839","[Website Redesign] Learn —> User Guide + Other Doc Pages https://www.figma.com/file/wt76z0M87RBDDEoyAEpD2g/OPEN-SOURCE-Fairlearn-Redesign?node-id=2535%3A3632

Incomplete: Overview page: social links module

See Issue #838 ","14","0.8911241854874429","Documentation","Development"
"https://github.com/fairlearn/fairlearn","909922900","issue","https://github.com/Trusted-AI/AIF360/issues/838","[Website Redesign] Document page templates **Learn Docs**
https://www.figma.com/file/wt76z0M87RBDDEoyAEpD2g/OPEN-SOURCE-Fairlearn-Redesign?node-id=2535%3A3632

**API Docs**
https://www.figma.com/file/wt76z0M87RBDDEoyAEpD2g/OPEN-SOURCE-Fairlearn-Redesign?node-id=2565%3A2687

**Contribute Docs**
https://www.figma.com/file/wt76z0M87RBDDEoyAEpD2g/OPEN-SOURCE-Fairlearn-Redesign?node-id=2535%3A4478

**About**
https://www.figma.com/file/wt76z0M87RBDDEoyAEpD2g/OPEN-SOURCE-Fairlearn-Redesign?node-id=2535%3A5899","14","0.9110369752571584","Documentation","Development"
"https://github.com/fairlearn/fairlearn","909921585","issue","https://github.com/Trusted-AI/AIF360/issues/837","[Website Redesign] Learn Landing Page https://www.figma.com/file/wt76z0M87RBDDEoyAEpD2g/OPEN-SOURCE-Fairlearn-Redesign?node-id=2535%3A3249

Redesign in progress as of 6/2/21; see page for sections that will change","14","0.8559688898671949","Documentation","Development"
"https://github.com/fairlearn/fairlearn","909921015","issue","https://github.com/Trusted-AI/AIF360/issues/836","[Website Redesign] Get Started Landing Page https://www.figma.com/file/wt76z0M87RBDDEoyAEpD2g/OPEN-SOURCE-Fairlearn-Redesign?node-id=2535%3A3126

Redesign of page in progress as of 6/2/21; see file for specified WIP modules","14","0.8912895773882321","Documentation","Development"
"https://github.com/fairlearn/fairlearn","909920725","issue","https://github.com/Trusted-AI/AIF360/issues/835","[Website Redesign] Homepage [Homepage Design](https://www.figma.com/file/wt76z0M87RBDDEoyAEpD2g/OPEN-SOURCE-Fairlearn-Redesign?node-id=2535%3A2964)
To see grid, turn on layer ""grid_12-col_1180px""

<img width=""374"" alt=""Screen Shot 2021-06-03 at 6 49 24 PM"" src=""https://user-images.githubusercontent.com/6819397/120739027-b299bd80-c4a5-11eb-8a39-8b93065160ed.png"">
","14","0.4830875339349915","Documentation","Development"
"https://github.com/fairlearn/fairlearn","909912835","issue","https://github.com/Trusted-AI/AIF360/issues/834","[Website Redesign] Responsive Grid https://www.figma.com/file/wt76z0M87RBDDEoyAEpD2g/OPEN-SOURCE-Fairlearn-Redesign?node-id=2336%3A2430

12 column flex grid and content containers","14","0.3847698890926553","Documentation","Development"
"https://github.com/fairlearn/fairlearn","909912130","issue","https://github.com/Trusted-AI/AIF360/issues/833","[Website Redesign] Global UI [Global UI Components](https://www.figma.com/file/wt76z0M87RBDDEoyAEpD2g/?node-id=2028%3A1582)

- [Header](https://www.figma.com/file/wt76z0M87RBDDEoyAEpD2g/OPEN-SOURCE-Fairlearn-Redesign?node-id=2565%3A10919)
- [Search](https://www.figma.com/file/wt76z0M87RBDDEoyAEpD2g/OPEN-SOURCE-Fairlearn-Redesign?node-id=2565%3A14839)
- Navigation
  - [Tier 1](https://www.figma.com/file/wt76z0M87RBDDEoyAEpD2g/OPEN-SOURCE-Fairlearn-Redesign?node-id=2565%3A12134)
  - [Tier 2](https://www.figma.com/file/wt76z0M87RBDDEoyAEpD2g/OPEN-SOURCE-Fairlearn-Redesign?node-id=2565%3A13822)
  - [Tier 3](https://www.figma.com/file/wt76z0M87RBDDEoyAEpD2g/OPEN-SOURCE-Fairlearn-Redesign?node-id=2565%3A14519)
- [Footer](https://www.figma.com/file/wt76z0M87RBDDEoyAEpD2g/OPEN-SOURCE-Fairlearn-Redesign?node-id=2565%3A22305)
- [Buttons & links](https://www.figma.com/file/wt76z0M87RBDDEoyAEpD2g/OPEN-SOURCE-Fairlearn-Redesign?node-id=2582%3A77)","18","0.4192739805738327","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","909051734","issue","https://github.com/Trusted-AI/AIF360/issues/831","Fix comment in postprocessing API documentation Hello! Line249 of fairlearn/fairlearn/postprocessing/_tradeoff_curve_utilities.py accumulates 
`false_positives=count[0]
true_positives=count[1]
`
Shouldn't they be true_negatives=count[0] and false_negatives=count[1]? (since you are accumulating in ascending order how many counts are < threshold, thus yhat= 0 which conflicts with definition on lines244 and 245(P[Y_hat=1 | Y=0], P[Y_hat=1 | Y=1])?

I think this is fine if ROC of two classes's ROC curves intersect, but would be problematic when one class needs to be randomized?
","30","0.424458411628761","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","908580580","issue","https://github.com/Trusted-AI/AIF360/issues/830","Benchmark comparison between Fairlearn's Reductions algorithm and FairTorch's  Constraint algorithm. To understand how [FairTorch](https://github.com/wbawakate/fairtorch)'s algorithm compares to FairLearn's Reduction algorithm, we want to create a workflow to compare the two algorithms on the same dataset.

## Process 

1.) Choose some publicly available datasets (e.g. Adult, German Credit).
2.) Run both Fairlearn's ExponentiatedGradient model (w/ DemographicParity Moment) and a PyTorch model with FairTorch's DemographicParityLoss constraint on the dataset.
3.) Compare the performance of both algorithms, in terms of stability of results, run-time, and correctness of end result.
4.) *End Deliverable*: A Python script or Jupyter notebook that executes the above workflow.","28","0.5475141297059105","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","904161436","issue","https://github.com/Trusted-AI/AIF360/issues/827","Add user guide section on persisting metrics using pickle #### Describe the issue linked to the documentation

Persisting metrics and loading them back into memory is often useful. We don't currently mention this anywhere but `fairlearn.metrics.MetricFrame` can be pickled and then loaded at a later point for further analysis.

#### Suggest a potential alternative/fix

Add a section to the user guide on assessment that shows a short snippet of code for dumping & loading the object.
```
mf = MetricFrame(...)
pickle.dump(mf, open(file_name, 'wb'))
pickle.load(open(file_name, 'rb'))
```
Ideally this should be in a `doctest` block like other code in the user guide so that we run this on every doc build.","14","0.4476754515842465","Documentation","Development"
"https://github.com/fairlearn/fairlearn","903403598","issue","https://github.com/Trusted-AI/AIF360/issues/826","Homepage: link to contributor guide directly #### Describe the issue linked to the documentation

When clicking on ""Contributor guide"" on the homepage we are currently moving to the banner shown below. That's not needed. 
![image](https://user-images.githubusercontent.com/10245648/119792871-ea43ac80-be8a-11eb-8aa9-c25a0970e2f7.png)

#### Suggest a potential alternative/fix

Instead, we should just directly link to the contributor guide. This should be as simple as a single link replacement in https://github.com/fairlearn/fairlearn/blob/f3e23d2cfc66b1677c2d62d1cb13d0fda115a13f/docs/static_landing_page/index.html#L45","14","0.9251748251748256","Documentation","Development"
"https://github.com/fairlearn/fairlearn","903340155","issue","https://github.com/Trusted-AI/AIF360/issues/825","Make navigation bar consistent across website #### Describe the issue linked to the documentation

The navigation bar is currently inconsistent between the homepage and the remainder of the website:

* The order of items differs
* *Getting Started* vs *Quickstart*
* *API docs* versus *API Reference*
* *About* vs *About us*

#### Suggest a potential alternative/fix

Make the navigation bars consistent.

@MiroDudik @LeJit

@romanlutz we might split this up into multiple smaller issues in case we're still looking for good first issues for the SciPy sprint.","14","0.6230804121039334","Documentation","Development"
"https://github.com/fairlearn/fairlearn","900072972","issue","https://github.com/Trusted-AI/AIF360/issues/821","Errors in Frequency Format   
<!--
If your issue is a usage question, please submit it in one of these other
channels instead:
- StackOverflow with the `fairlearn` tag:
  https://stackoverflow.com/questions/tagged/fairlearn
- Gitter: https://gitter.im/fairlearn/community#
The issue tracker is used only to report bugs and feature requests. For
questions, please use either of the above platforms. Most question issues are
closed without an answer on this issue tracker. Thanks for your understanding.
-->
Rather than Error Rates, it would be helpful to have a generalizable way of showing errors in Frequency format. 

#### Is your feature request related to a problem? Please describe.
<!-- A clear and concise description of what the problem is. Ex. I'm always
frustrated when ... -->

Mental processing of Fairlearn output. 

For example, if FPR is 5E-5, that is hard to understand. If instead it were ""Approximately __ errors out of every ___ occurences"", that would be more understandable 

#### Describe the solution you'd like
<!-- A clear and concise description of what you want to happen. -->

Implementation of Frequency format for error metrics

#### Describe alternatives you've considered, if relevant
<!-- A clear and concise description of any alternative solutions or features
you've considered. -->

Application of function that takes error rates, and turns them into frequency format .

#### Additional context
<!-- Add any other context or screenshots about the feature request here. -->

The frequency format hypothesis is the idea that the brain understands and processes information better when presented in frequency formats rather than a numerical or probability format. See [(Frequency Format Hypothesis)](https://en.wikipedia.org/wiki/Frequency_format_hypothesis#:~:text=The%20frequency%20format%20hypothesis%20is,20%25%20leads%20to%20better%20comprehension.)","11","0.6736554571948004","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","899952122","issue","https://github.com/Trusted-AI/AIF360/issues/819","CircleCI doc build does not use PR branch but main branch We set up the CircleCI doc build to build the documentation based on the branch of a Pull Request. However, in the last few weeks it's often (note: not always!) produced the documentation based on the `main` branch rather than the PR branch. This means we can't validate that the new docs will look right. Instead, we have to check out the branch locally and build it which wastes a ton of time.

Ideally we should figure out what's wrong with CircleCI and fix it. Perhaps they've changed something about this behavior recently, perhaps the code checkout logic needs to be adjusted or something like that. This issue requires a bit of investigation from someone who's ideally already somewhat familiar with CircleCI and sphinx doc builds.

Alternatively, we could also switch to a different provider, but the advantage of CircleCI so far was that the artifacts render right in the browser without the need to download anything. GitHub Actions and Azure DevOps don't seem to provide that as of now.","32","0.7380812855673191","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","897068979","issue","https://github.com/Trusted-AI/AIF360/issues/814","DOC: add example for using tensorflow #### Describe the issue linked to the documentation

We're not super clear about supporting any kind of model for our (currently existing reductions and postprocessing) mitigation techniques. This means people don't know they can use tensorflow with Fairlearn.

#### Suggest a potential alternative/fix

We should have a short example that shows how to do that. Perhaps having it as a `plot_tensorflow.py` under `examples` and linking to it from a user guide section makes sense. We've used `literalinclude` for this purpose in parts of the codebase. I am not entirely sure whether including tensorflow into requirements-dev.txt will cause issues, though.

The alternative: Just put a `code-block`, but then the code isn't run during the doc build and we're at risk of this code getting outdated. 

Related: #813","14","0.4205519083120063","Documentation","Development"
"https://github.com/fairlearn/fairlearn","897066196","issue","https://github.com/Trusted-AI/AIF360/issues/813","DOC: add example for using pytorch #### Describe the issue linked to the documentation

We're not super clear about supporting any kind of model for our (currently existing reductions and postprocessing) mitigation techniques. This means people don't know they can use pytorch with Fairlearn.

#### Suggest a potential alternative/fix

We should have a short example that shows how to do that. Perhaps having it as a `plot_pytorch.py` under `examples` and linking to it from a user guide section makes sense. We've used `literalinclude` for this purpose in parts of the codebase. I am not entirely sure whether including pytorch into requirements-dev.txt will cause issues, though.

The alternative: Just put a `code-block`, but then the code isn't run during the doc build and we're at risk of this code getting outdated. ","14","0.4223496678212481","Documentation","Development"
"https://github.com/fairlearn/fairlearn","893989264","issue","https://github.com/Trusted-AI/AIF360/issues/810","Math doesn't render properly anymore on webpage #### Describe the issue linked to the documentation

The following is one of many similar examples from https://fairlearn.org/main/user_guide/fairness_in_machine_learning.html
![image](https://user-images.githubusercontent.com/10245648/118598484-fa5cdd00-b762-11eb-8e31-dcea6e1cce58.png)


#### Suggest a potential alternative/fix

Find a way to make it work again. Perhaps something changed about the way LaTeX is rendered in our sphinx theme (?)
I don't have any idea why it's not working (yet).","14","0.5590649350649353","Documentation","Development"
"https://github.com/fairlearn/fairlearn","893324038","issue","https://github.com/Trusted-AI/AIF360/issues/805","DOC: mention alternatives to `start` for MacOS/Linux in contributor guide  #### Describe the issue linked to the documentation

Our [contributor guide on documentation](https://github.com/bthng/fairlearn/blob/main/docs/contributor_guide/contributing_documentation.rst) uses the `start` command to inspect the HTML flies, which is specific to Windows OS.

#### Suggest a potential alternative/fix
Add alternative commands for MacOS (`open`) and Linux (I believe `xdg-open`, but I am not sure) and note that it is also possible to explore the `_build` directory through any kind of file explorer. ","14","0.7196573060447408","Documentation","Development"
"https://github.com/fairlearn/fairlearn","892783389","issue","https://github.com/Trusted-AI/AIF360/issues/804","Support different costs for false positives and false negatives in ExponentiatedGradient #### Is your feature request related to a problem? Please describe.

In many binary classification problems, the costs of false positives do not equal to the costs of false negatives, but our `ExponentiatedGradient` only allows classification accuracy as the objective, which assumes that these costs are equal.

#### Describe the solution you'd like

We should extend the moment `ErrorRate` to allow for asymmetric costs, and also allow specifying these costs in the constructor to `ExponentiatedGradient`.

I'm thinking that there are two natural API choices. Say, we're considering fairness under demographic parity and the costs for false positives and false negatives are 1.0 and 0.1. Then:

```python
# Current API, which does not allow specifying asymmetric costs for the objective:
expgrad = ExponentiatedGradient(
    learner,
    constraints=DemographicParity())

# API idea (1): Allow specifying the objective via a moment object
expgrad = ExponentiatedGradient(
    learner,
    constraints=DemographicParity(),
    objective=ErrorRate(costs={'fn': 1.0, 'fp': 0.1}))

# API idea (2): Specify the objective costs more directly
expgrad = ExponentiatedGradient(
    learner,
    constraints=DemographicParity(),
    objective_costs={'fn': 1.0, 'fp': 0.1})
```
I slightly prefer API (1), because it allows for extensions into future in case we want to support other objective functions.

The API (2) is a bit simpler and matches #473, but it could be a bit confusing, because `objective_costs` would not be applicable when we are using `ExponentiatedGradient` for regression.","15","0.4384975545228273","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","892713195","issue","https://github.com/Trusted-AI/AIF360/issues/802","DOC: extend documentation for plotting functionality provided through MetricFrame #### Describe the issue linked to the documentation

@hildeweerts [pointed out](https://github.com/fairlearn/fairlearn/pull/770#pullrequestreview-657627887) that our new documentation around plotting using `fairlearn.metrics.MetricFrame` could elaborate a little more on what you can do with the `matplotlib` plotting options.

#### Suggest a potential alternative/fix

The assessment user guide section on plotting should discuss at least the following options:
- `ylim` for bar charts
- how to adjust the colors
- how to use various kinds of charts, e.g., pie charts for the `count` metric, and bar charts for `accuracy_score`
- how to plot multiple metrics (is already included, but could be made clearer)","15","0.5034402852049913","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","891887124","issue","https://github.com/Trusted-AI/AIF360/issues/800","Stop MetricFrame errors on nonscalar metrics #### Is your feature request related to a problem? Please describe.

`MetricFrame` is quite relaxed as to the return values of the metric functions it evaluates. For example, it is perfectly possible to return a confusion matrix or a confidence bound from a metric function.

However such metrics will cause a problem if the user tries calling `group_min()`, `group_max()`, `difference()` or `ratio()` on the resultant `MetricFrame` since those operations are only well defined for scalars.

Currently, we (or rather `pandas`) will throw an exception. This is defensible if only a single metric is being computed by the `MetricFrame` but is likely to lead to a lack of user delight if there are other metrics present for which those operations do make sense. In particular, if a user has constructed a `MetricFrame` consisting of both the `accuracy_score` and `accuracy_score_confidence_interval` then they would reasonably expect `group_min()` to give a useful answer for the `accuracy_score` column.

#### Describe the solution you'd like

Catch any exceptions from `group_min()` and `group_max()` and emplace `NaN` in this case.

#### Describe alternatives you've considered, if relevant

Without this, users are forced to split up related metrics into separate `MetricFrame` objects.

#### Additional context

This is important for #235 
","12","0.2664097806954948","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","891089553","issue","https://github.com/Trusted-AI/AIF360/issues/799","DOC: Measurement modeling - construct reliability #### Describe the issue linked to the documentation
The user guide currently does not include explanations of key concepts from measurement modeling.

Measurement modeling is a framework from quantitative social sciences, involving a process of measuring theoretical constructs that cannot be observed directly (such as fairness) by making inferences from observable properties.

Because fairness itself is unobservable (and is fundamentally contested), users of Fairlearn need to make decisions about what their theoretical understanding of fairness is, and how they operationalize it via observable properties. At each stage, assumptions are made, which need to be tested.

Measurement modeling provides a language and a framework for making those assumptions explicit and testing them.

As many users of Fairlearn may not have quantitative social science backgrounds, we need to provide explanations and examples of the process of measurement modeling, including key concepts such as construct validity and construct reliability. 

This issue is focused on construct reliability (to keep the issues smaller and more achievable). Issue #769 focuses on construct validity.

See Jacobs and Wallach (2021) for more information.

#### Suggest a potential alternative/fix
Create a glossary explaining key terms from construct reliability, with explanations of what they might mean for measuring fairness.

- Test-retest reliability
- Internal consistency reliability","8","0.9134161836631676","Understanding Fairness Definition","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","889629942","issue","https://github.com/Trusted-AI/AIF360/issues/798","DOC: migrate version upgrade guide for v0.2->v0.3 to the corresponding user guide section #### Describe the issue linked to the documentation

The [Development process](https://fairlearn.org/main/contributor_guide/development_process.html) page of our doc has instructions on how to upgrade from v0.2 to v0.3
![image](https://user-images.githubusercontent.com/10245648/117921140-0c430980-b2a5-11eb-8a33-3e27b35cc236.png)

These should live in the [Migrating from prior versions](https://fairlearn.org/main/user_guide/migrating_versions/index.html#migrating-from-prior-versions) section instead.

#### Suggest a potential alternative/fix

Move the docs from https://github.com/fairlearn/fairlearn/blob/main/docs/contributor_guide/development_process.rst to a new file `migrating_to_v0_3 under https://github.com/fairlearn/fairlearn/tree/main/docs/user_guide/migrating_versions

Additionally, we should also make the creation of such a migration file a part of our release process. For that purpose, the [release process instructions](https://fairlearn.org/main/contributor_guide/release.html) should be extended. Specifically, step 9 needs another substep to say ""Create a migration guide from the previous version to the new version under `docs/user_guide/migrating_versions`""
","14","0.5298404956129287","Documentation","Development"
"https://github.com/fairlearn/fairlearn","884558659","issue","https://github.com/Trusted-AI/AIF360/issues/796","Sample Size Needed to Compare k Proportions <!--
If your issue is a usage question, please submit it in one of these other
channels instead:
- StackOverflow with the `fairlearn` tag:
  https://stackoverflow.com/questions/tagged/fairlearn
- Gitter: https://gitter.im/fairlearn/community#
The issue tracker is used only to report bugs and feature requests. For
questions, please use either of the above platforms. Most question issues are
closed without an answer on this issue tracker. Thanks for your understanding.
-->

#### Is your feature request related to a problem? Please describe.
<!-- A clear and concise description of what the problem is. Ex. I'm always
frustrated when ... -->

In many use-cases, it would be valuable to understand the sample size needed to understand the statistical significance of a disparity (or lack thereof) between two groups. Calculators like: http://powerandsamplesize.com/Calculators/Compare-k-Proportions/1-Way-ANOVA-Pairwise

Offer a manual procedure and a formula. I believe that the calculations above can be useful in helping Fairlearn users understand if their sample size is big enough to draw conclusions about the comparison between the proportion values (i.e: FPR, TPR, FNR, TNR) of different groups

#### Describe the solution you'd like
<!-- A clear and concise description of what you want to happen. -->

A function that takes in the subgroups, the proportion values, and performs the powerandsample size calculations (linked above), to output the sample sizes need in each pairwise comparison. 

#### Describe alternatives you've considered, if relevant
<!-- A clear and concise description of any alternative solutions or features
you've considered. -->

Using the power and samplesize calculator manually

#### Additional context
<!-- Add any other context or screenshots about the feature request here. -->
","11","0.6322559309348507","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","883744046","issue","https://github.com/Trusted-AI/AIF360/issues/795","DOC: best practices around demographic data - race and ethnicity _Our documentation doesn't currently say anything about best practices for_
* _collecting demographic data_
* _working with demographic data (including data that may not conform to best practices for data collection)_

_We should probably start a section for this in our user guide. There's likely specific guidance for various kinds of data (e.g., gender, race, age, etc.). This particular task is for race only. If you'd like to work on others please create a new issue and link it at #792._

Race is a multi-dimensional construct and which measurement you choose can impact the conclusions you can draw. I believe that many data sets collected in the US use the census racial categories, which may or may not be good for your purposes (see Hanna et al.). In other countries (e.g., the Netherlands) race is typically not collected at all and the focus is more on ethnicity. Although race and ethnicity are not the same thing, I think it makes sense to discuss both and highlight differences.

Lacking the lived experiences, I personally don't feel very comfortable writing ""best practices"" on collecting and working with data on race. I would be happy to summarize some of the findings of Hanna et al. (2019), but I think it would be much better if we could involve some more experienced experts here.

Sources:
*  Towards a Critical Race Methodology in Algorithmic Fairness by Hanna et al. (2019): https://arxiv.org/pdf/1912.03593.pdf","8","0.6369445914122401","Understanding Fairness Definition","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","879928438","issue","https://github.com/Trusted-AI/AIF360/issues/793","ENH Creating a synthetic example dataset This is based on a Gitter conversation with @adrinjalali @hildeweerts @MiroDudik where we agreed that it would be nice to have a synthetic dataset available for our examples. @adrinjalali suggested the following code using `sklearn`'s `make_classification`:

```
rng = RandomState(seed=42)

X_women, y_women = make_classification(
    n_samples=500,
    n_features=20,
    n_informative=4,
    n_classes=2,
    class_sep=1,
    random_state=rng,
)

X_men, y_men = make_classification(
    n_samples=500,
    n_features=20,
    n_informative=4,
    n_classes=2,
    class_sep=2,
    random_state=rng,
)

X_unspecified, y_unspecified = make_classification(
    n_samples=500,
    n_features=20,
    n_informative=4,
    n_classes=2,
    class_sep=0.5,
    random_state=rng,
)

X = np.r_[X_women, X_men, X_unspecified]
y = np.r_[y_women, y_men, y_unspecified]
gender = np.r_[[""Woman""] * 500, [""Man""] * 500, [""Unspecified""] * 500].reshape(
    -1,
)

X_train, X_test, y_train, y_test, gender_train, gender_test = train_test_split(
    X, y, gender, test_size=0.3, random_state=rng
)
```
@MiroDudik suggested extending this to have at least 2 sensitive features and 1 control feature to allow us to use it in basically all our examples. 

@fairlearn/fairlearn-maintainers any objection with putting this in the `fairlearn.datasets` module?","27","0.6190159819879675","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","879919718","issue","https://github.com/Trusted-AI/AIF360/issues/792","DOC: best practices around demographic data - gender Our documentation doesn't currently say anything about best practices for 
- collecting demographic data
- working with demographic data

We should probably start a section for this in our user guide. There's likely specific guidance for various kinds of data (e.g., gender, race, age, etc.). This particular task is for gender only. If you'd like to work on others please create a new issue and link it.

Regarding the collection @MiroDudik posted the following on Gitter based on a discussion with Queer in AI folks:

> Gender: (please select the options which are applicable to you)
> [ ] Woman
> [ ] Man
> [ ] Non-binary / Genderqueer / Third gender
> [ ] Genderfluid / Gender non-conforming
> [ ] Questioning
> [ ] Prefer not to say
> [ ] Specify your own (open text box)
> 
> The key aspect is that this is a multi-select. When this kind of data is collected, there should be an explanation that the data is collected to audit for fairness and/or mitigate any fairness issues, and that if you pick ""prefer not to say"" you will not be included in any fairness audits.

In some cases people don't start at data collection, but rather already have a dataset with certain demographic data that may not conform to such best practices. Our best practices should mention that as well, and what one can and should do in such a case. A very common situation is to find a dataset with a `sex` column only. Additionally, I've seen plenty of dataset where we see columns like `sex` or `gender` but it's unclear whether these were self-reported or inferred/guessed by someone (or a system) which obviously makes a huge difference.

Further sources:
- https://apastyle.apa.org/style-grammar-guidelines/bias-free-language/gender
- https://medium.com/managing-on-the-margins/respectful-collection-of-demographic-data-56de9fcb80e2
- https://www.morgan-klaus.com/gender-guidelines.html","8","0.4382632293080055","Understanding Fairness Definition","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","879706695","issue","https://github.com/Trusted-AI/AIF360/issues/791","Use icons for Twitter, StackOverflow, and Gitter Our current website has the following in the top right corner:
![image](https://user-images.githubusercontent.com/10245648/117505684-4d4fbc80-af39-11eb-9d3a-3dddc336556e.png)

Pydata-sphinx-theme just added an update that allows us to customize icons: https://pydata-sphinx-theme.readthedocs.io/en/latest/user_guide/configuring.html#configure-icon-links

The GitHub one is already correct. Twitter is also available, but we haven't uncommented that line in `docs/conf.py` yet since we weren't using the account. That changed to day, of course, so we should add the icon.

For StackOverflow and Gitter there were no icons available, but the link above shows how to add them.

This would require upgrading `pydata-sphinx-theme`, probably to the latest version. This may introduce other conflicts.","14","0.735057631530024","Documentation","Development"
"https://github.com/fairlearn/fairlearn","878936408","issue","https://github.com/Trusted-AI/AIF360/issues/786","Guidelines around user-defined disaggregated metrics #### Describe the issue linked to the documentation
As Fairlearn and the associated Fairness dashboard look to expand beyond y_true, y_pred metric formulations (which is geared towards Tabular data), there may need to be documentation around how to think about disaggregated metrics in general with some examples of metrics for unstructured data scenarios that can be disaggregated. 

**Related issues**

Fairlearn: https://github.com/fairlearn/fairlearn/issues/756

RAI Widgets: https://github.com/microsoft/responsible-ai-widgets/issues/498
<!--
Tell us about the confusion introduced in the documentation.
-->

#### Suggest a potential alternative/fix
Documentation that provides some examples and guidelines around choosing user-defined disaggregated metrics, with some examples (both of custom metrics that can be disaggregated, and ones that can not be). 
<!--
Tell us how we could improve the documentation in this regard.
-->
","14","0.3520956827361905","Documentation","Development"
"https://github.com/fairlearn/fairlearn","878638001","issue","https://github.com/Trusted-AI/AIF360/issues/785","ENH Add ""adversarial debiasing"" to mitigations The paper [Mitigating Unwanted Biases with Adversarial Learning](https://arxiv.org/abs/1801.07593) by Zhang, Lemoine, and Mitchell presents an ""adversarial debiasing"" technique. The goal of this task is to implement it.

AIF360 has an [implementation](https://aif360.readthedocs.io/en/v0.2.3/modules/inprocessing.html#aif360.algorithms.inprocessing.AdversarialDebiasing) of this technique, but it uses `tensorflow`. Fairlearn shouldn't depend on `tensorflow` (or `pytorch`, for that matter), but instead have a soft dependency on the package of choice. @adrinjalali has previously noted that Fairlearn should prefer `pytorch` over `tensorflow`, which we should take into account for this.

[Note: The name ""adversarial debiasing"" is in conflict with [our policy](https://fairlearn.org/main/contributor_guide/how_to_talk_about_fairness.html) around the words ""bias""/""debiasing"". What else could we call it? Adversarial fairness? @fairlearn/fairlearn-maintainers ]

Completing this item requires:
- code for the technique in `fairlearn.adversarial` (@fairlearn/fairlearn-maintainers  is that a suitable module name? Other suggestions?)
- unit tests in `test.unit.adversarial`
- descriptive API reference (directly in the docstring)
- ideally a short user guide in docs.user_guide.mitigation.rst

A fully fledged example notebook is not required.

To claim this task please respond below. Of course, you can also ask questions!","28","0.5340524255618595","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","878615073","issue","https://github.com/Trusted-AI/AIF360/issues/784","ENH Add reweighing preprocessing technique based on Kamiran, Calders The paper [Data preprocessing techniques for classification
without discrimination](https://link.springer.com/content/pdf/10.1007%2Fs10115-011-0463-8.pdf) by Kamiran and Calders outlines preprocessing techniques *suppression*, *massaging*, and *reweighing* or *resampling*. The goal of this task is to implement the *reweighing* technique for `fairlearn.preprocessing`.

Even though this technique is from 2012 it is still frequently used as a baseline to compare to, so Fairlearn should provide it to users as well.

Completing this item requires:
- preprocessing technique code in `fairlearn.preprocessing`
- unit tests in `test.unit.preprocessing`
- descriptive API reference (directly in the docstring)
- ideally a short user guide in `docs.user_guide.mitigation.rst`

A fully fledged example notebook is not required.

To claim this task please respond below. Of course, you can also ask questions!","28","0.6275115688908792","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","878581646","issue","https://github.com/Trusted-AI/AIF360/issues/783","DOC Talking points: how to raise fairness issues? Business objectives are not always well aligned with fairness, so prioritizing it may not be encouraged in every company/org/team. In order to help individuals navigate such situations we want to put together a set of talking points on how to raise fairness issues.

The first step in this work is to identify relevant ideas / literature / resources on this topic. All suggestions are welcome! You can participate by posting responses in this thread.

Once we've compiled a set of resources we can capture them in a suitable format on the website.

Note: This is also related to #707 and #769 . The resulting documentation section can certainly refer to these resources.","25","0.5090072894512007","Research","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","878571963","issue","https://github.com/Trusted-AI/AIF360/issues/782","DOC Abstraction traps: Solutionism trap The paper [Fairness and Abstraction in Sociotechnical Systems](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3265913) by Selbst et al. outlines 5 abstraction traps as well as strategies to avoid these traps from a sociotechnical systems perspective.

This task is about adding the relevant information on the **Solutionism Trap** to the Fairlearn documentation. The result should at least define the concept. If possible and suitable it's also nice to add examples of the concept in a particular context.

The place to add this information is the ""Fairness in Machine Learning"" section of our website: https://fairlearn.org/main/user_guide/fairness_in_machine_learning.html
This is part of the following file if you check out the repository: https://github.com/fairlearn/fairlearn/blob/main/docs/user_guide/fairness_in_machine_learning.rst

Website build steps are captured in our contributor guide: https://fairlearn.org/main/contributor_guide/development_process.html#building-the-website

If you'd like to do this task please reply to this issue. You can also ask questions, of course!","25","0.9676758870307256","Research","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","878571364","issue","https://github.com/Trusted-AI/AIF360/issues/781","DOC Abstraction traps: Ripple effect trap The paper [Fairness and Abstraction in Sociotechnical Systems](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3265913) by Selbst et al. outlines 5 abstraction traps as well as strategies to avoid these traps from a sociotechnical systems perspective.

This task is about adding the relevant information on the **Ripple Effect Trap** to the Fairlearn documentation. The result should at least define the concept. If possible and suitable it's also nice to add examples of the concept in a particular context.

The place to add this information is the ""Fairness in Machine Learning"" section of our website: https://fairlearn.org/main/user_guide/fairness_in_machine_learning.html
This is part of the following file if you check out the repository: https://github.com/fairlearn/fairlearn/blob/main/docs/user_guide/fairness_in_machine_learning.rst

Website build steps are captured in our contributor guide: https://fairlearn.org/main/contributor_guide/development_process.html#building-the-website

If you'd like to do this task please reply to this issue. You can also ask questions, of course!","25","0.9639188495120696","Research","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","878570772","issue","https://github.com/Trusted-AI/AIF360/issues/780","DOC Abstraction traps: Formalism trap The paper [Fairness and Abstraction in Sociotechnical Systems](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3265913) by Selbst et al. outlines 5 abstraction traps as well as strategies to avoid these traps from a sociotechnical systems perspective.

This task is about adding the relevant information on the **Formalism Trap** to the Fairlearn documentation. The result should at least define the concept. If possible and suitable it's also nice to add examples of the concept in a particular context.

The place to add this information is the ""Fairness in Machine Learning"" section of our website: https://fairlearn.org/main/user_guide/fairness_in_machine_learning.html
This is part of the following file if you check out the repository: https://github.com/fairlearn/fairlearn/blob/main/docs/user_guide/fairness_in_machine_learning.rst

Website build steps are captured in our contributor guide: https://fairlearn.org/main/contributor_guide/development_process.html#building-the-website

If you'd like to do this task please reply to this issue. You can also ask questions, of course!","25","0.975667749861298","Research","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","878570246","issue","https://github.com/Trusted-AI/AIF360/issues/779","DOC Abstraction traps: Portability trap The paper [Fairness and Abstraction in Sociotechnical Systems](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3265913) by Selbst et al. outlines 5 abstraction traps as well as strategies to avoid these traps from a sociotechnical systems perspective.

This task is about adding the relevant information on the **Portability Trap** to the Fairlearn documentation. The result should at least define the concept. If possible and suitable it's also nice to add examples of the concept in a particular context.

The place to add this information is the ""Fairness in Machine Learning"" section of our website: https://fairlearn.org/main/user_guide/fairness_in_machine_learning.html
This is part of the following file if you check out the repository: https://github.com/fairlearn/fairlearn/blob/main/docs/user_guide/fairness_in_machine_learning.rst

Website build steps are captured in our contributor guide: https://fairlearn.org/main/contributor_guide/development_process.html#building-the-website

If you'd like to do this task please reply to this issue. You can also ask questions, of course!","25","0.9724710047290692","Research","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","878569482","issue","https://github.com/Trusted-AI/AIF360/issues/778","DOC Abstraction traps: Framing trap The paper [Fairness and Abstraction in Sociotechnical Systems](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3265913) by Selbst et al. outlines 5 abstraction traps as well as strategies to avoid these traps from a sociotechnical systems perspective.

This task is about adding the relevant information on the **Framing Trap** to the Fairlearn documentation. The result should at least define the concept. If possible and suitable it's also nice to add examples of the concept in a particular context.

The place to add this information is the ""Fairness in Machine Learning"" section of our website: https://fairlearn.org/main/user_guide/fairness_in_machine_learning.html
This is part of the following file if you check out the repository: https://github.com/fairlearn/fairlearn/blob/main/docs/user_guide/fairness_in_machine_learning.rst

Website build steps are captured in our contributor guide: https://fairlearn.org/main/contributor_guide/development_process.html#building-the-website

If you'd like to do this task please reply to this issue. You can also ask questions, of course!","25","0.9788644949935272","Research","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","878482872","issue","https://github.com/Trusted-AI/AIF360/issues/776","Delete `master` branch I just realized we still have the `master` branch somehow. It should be deleted. People may keep sending PRs to that branch by mistake: https://github.com/fairlearn/fairlearn/pull/771

cc @fairlearn/fairlearn-maintainers ","32","0.7092021128718375","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","877914650","issue","https://github.com/Trusted-AI/AIF360/issues/772","Counterfactual Analysis Documentation with VW Support **Introduction:** Hey all, I am a participant of Microsoft's RL Open Source Fest this summer and I am very excited to begin contributing to FairLearn by writing out a documentation containing counterfactual analysis using logs synthetically generated by Vowpal Wabbit. I am creating this issue to track my progress and to provide the community an opportunity to respond with additional suggestions that would be great to include in the final product.

**Background:** Researchers from CMU have published a study, provided in their paper [Counterfactual Risk Assessments, Evaluation, and Fairness](https://arxiv.org/pdf/1909.00066.pdf), which details an approach to incorporate counterfactual analysis on both synthetic and domain-specific datasets. They have also built out an [R script](https://github.com/mandycoston/counterfactual) to replicate the results shown in the paper.

**Action Items:** On my behalf, I plan to:
- Use Vowpal Wabbit to generate synthetic datasets (detailed in Microsoft Research's paper, [A Contextual Bandit Bake-off](https://arxiv.org/pdf/1802.04064.pdf)) in preparation for documentation. [Python code](https://github.com/albietz/cb_bakeoff) to generate these datasets is additionally provided.
- Convert the provided R script to Python and replicate the results shown in the Counterfactual Risk Assessments, Evaluation, and Fairness paper.
- Build a local Jupyter notebook detailing the steps outlined in Counterfactual Risk Assessments, Evaluation, and Fairness using these generated datasets and reproduceable and ideally well documented code.
- Attach the final product here for community review and potential next steps.

**Extensions to Further Consider:** Given this, here are some follow-up thoughts to consider:
- The Counterfactual Risk Assessments, Evaluation, and Fairness paper currently only supports binary class labeling and naturally, we'd like to extend our analysis to account for applications under the multi-class setting. Is it natural to approach this similar to [logistic regression's multiclass setting](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) in sklearn?
- Similar to the previous paper mentioned, the documentation should also support a domain-specific open source dataset to study how counterfactual analysis will perform. In the [proposal](https://vowpalwabbit.org/rlos/2021/projects#14-extend-fairlearn-to-include-rl-bias-analysis-using-vw), there is an additional note that recommended looking at ""concepts such as Atkinson Index and Simpsons Paradox to identify sources of inequality"", so being able to identify those open source datasets would additionally be great to have. Does anyone know of any great recommendations here?

I'm always happy to hear any additional suggestions or thoughts to further consider in this documentation. Feel free to post and let me know if there are any! 

Thanks again for the warm welcome!


","0","0.2792111105095023","Dataset usage","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","876506859","issue","https://github.com/Trusted-AI/AIF360/issues/769","DOC: Measurement modeling - construct validity #### Describe the issue linked to the documentation!
The user guide currently does not include explanations of key concepts from measurement modeling. 

Measurement modeling is a framework from quantitative social sciences, involving a process of measuring theoretical constructs that cannot be observed directly (such as fairness) by making inferences from observable properties.

Because fairness itself is unobservable (and is fundamentally contested), users of Fairlearn need to make decisions about what their theoretical understanding of fairness is, and how they operationalize it via observable properties. At each stage, assumptions are made, which need to be tested. 

Measurement modeling provides a language and a framework for making those assumptions explicit and testing them.

As many users of Fairlearn may not have quantitative social science backgrounds, we need to provide explanations and examples of the process of measurement modeling, including key concepts such as construct validity and construct reliability. This issue is focused on construct validity (to keep the issues smaller and more achievable), but others should include construct reliability.

See [Jacobs and Wallach (2021)](https://arxiv.org/pdf/1912.05511.pdf) for more information.

#### Suggest a potential alternative/fix

Create a glossary explaining key terms from construct validity, with explanations of what they might mean for measuring fairness. Specifically:

- face validity
- content validity
  - contestedness
  - substantive validity
  - structural validity
- convergent validity
- discriminant validity
- predictive validity
- hypothesis validity
- consequential validity ","8","0.925731126728634","Understanding Fairness Definition","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","875264224","issue","https://github.com/Trusted-AI/AIF360/issues/768","Is there a way to export a report of the metrics? Hi there, thanks for your work :)

Is there a way to export a comprehensive file/report of the metrics showed in the Fairness Dashboard? Or a line of code through Fairlearn.Metrics to do so? 

Many thanks 
","24","0.4062372604973501","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","874686222","issue","https://github.com/Trusted-AI/AIF360/issues/765","Unstructured Data metrics (i.e: WER for Text data) in MetricsFrame 
<!--
If your issue is a usage question, please submit it in one of these other
channels instead:
- StackOverflow with the `fairlearn` tag:
  https://stackoverflow.com/questions/tagged/fairlearn
- Gitter: https://gitter.im/fairlearn/community#
The issue tracker is used only to report bugs and feature requests. For
questions, please use either of the above platforms. Most question issues are
closed without an answer on this issue tracker. Thanks for your understanding.
-->

#### Is your feature request related to a problem? Please describe.
<!-- A clear and concise description of what the problem is. Ex. I'm always
frustrated when ... -->

I would like support for metrics that specifically pertain to unstructured data. For example: 

Speech To Text - Some common metrics used in STT include Word Error Rate (which looks at the difference between the uttered text and the transcribed text). Another example is ""sentence error rate"" where if _any_ mistakes are made in the transcription, the utterance is given a 1, otherwise 0. 

While these metrics are specifically related to the text scenario described above, it would still be very useful to use Fairlearn to see them disaggregated. 

#### Describe the solution you'd like
<!-- A clear and concise description of what you want to happen. -->

These Metrics could either be built into the MetricFrame, or the MetricFrame could support custom metrics for a user's scenario (i.e: the user defines the metric of interest)

#### Describe alternatives you've considered, if relevant
<!-- A clear and concise description of any alternative solutions or features
you've considered. -->

Built a dashboard for these metrics outside of Fairlearn.

#### Additional context
<!-- Add any other context or screenshots about the feature request here. -->
","11","0.6396562641338761","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","873801094","issue","https://github.com/Trusted-AI/AIF360/issues/764","Add notebook execution buttons to the sphinx gallery notebook pages Our notebook pages on the website (such as [this one](https://fairlearn.org/v0.6.1/auto_examples/plot_grid_search_census.html)) have download buttons, but we can't execute them directly on Binder, Azure Notebooks, Google Colab, etc.

Scikit-learn has a button for Binder (e.g., in [this example](https://scikit-learn.org/stable/auto_examples/bicluster/plot_spectral_biclustering.html#sphx-glr-auto-examples-bicluster-plot-spectral-biclustering-py)) so this should be possible to add since we're using a similar set of sphinx extensions such as sphinx-gallery.

The way scikit-learn achieves this is by adding the following lines to the sphinx gallery configuration: https://github.com/scikit-learn/scikit-learn/blob/309f135c3284d7db6e23ca81a87948c7066a3949/doc/conf.py#L338
... and there's a dedicated `requirements.txt` file.

Other than adding that requirements file and the configuration for sphinx gallery, this would require building the website. Instructions are on [this page](https://fairlearn.org/main/contributor_guide/development_process.html#building-the-website), but I suggest going with the ""simple"" method rather than building the website for all versions. After all, we wouldn't expect the button to show up in past versions.","14","0.4593934110063141","Documentation","Development"
"https://github.com/fairlearn/fairlearn","870451744","issue","https://github.com/Trusted-AI/AIF360/issues/759","Support for by_group in Dashboard <!--
If your issue is a usage question, please submit it in one of these other
channels instead:
- StackOverflow with the `fairlearn` tag:
  https://stackoverflow.com/questions/tagged/fairlearn
- Gitter: https://gitter.im/fairlearn/community#
The issue tracker is used only to report bugs and feature requests. For
questions, please use either of the above platforms. Most question issues are
closed without an answer on this issue tracker. Thanks for your understanding.
-->

#### Is your feature request related to a problem? Please describe.
by_group currently supported in MetricFrame, but not yet in Dashboard.

#### Describe the solution you'd like
Ability to select multiple sensitive features in dashboard, and view results across intersections

#### Describe alternatives you've considered, if relevant
API 

#### Additional context
Users of the notebook will likely want to use the dashboard 
","11","0.7341457470143803","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","870449845","issue","https://github.com/Trusted-AI/AIF360/issues/758","plot_roc_curves(scores, y_true, sensitive_features) <!--
If your issue is a usage question, please submit it in one of these other
channels instead:
- StackOverflow with the `fairlearn` tag:
  https://stackoverflow.com/questions/tagged/fairlearn
- Gitter: https://gitter.im/fairlearn/community#
The issue tracker is used only to report bugs and feature requests. For
questions, please use either of the above platforms. Most question issues are
closed without an answer on this issue tracker. Thanks for your understanding.
-->

#### Is your feature request related to a problem? Please describe.
In producing Fairness Assessment results, seeing metrics given a fixed threshold is useful. In addition to that, it helps to also see an ROC curve (TPR vs. FPR) across different thresholds. 

#### Describe the solution you'd like
A function like ""plot_roc_curves(scores, y_true, sensitive_features)"" that takes in sensitive_features (list of sensitive features) as a parameter, and plots roc curves across subgroups

#### Describe alternatives you've considered, if relevant
sklearn plot_roc_curve, though this does not support plot via sensitive groups 

#### Additional context
Creating a Fairness Assessment using Fairlearn, ROC curve plots are missing 
","11","0.4898476887969009","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","868362629","issue","https://github.com/Trusted-AI/AIF360/issues/757","Use the new Azure DevOps organization ""Fairlearn"" https://dev.azure.com/fairlearn/Fairlearn

I created this new org to decouple our CI from Microsoft's internal CI. Also, all Fairlearn maintainers have admin permissions and can approve releases. The task that remains to be done is to point the existing Azure pipelines at this new organization.

@fairlearn/fairlearn-maintainers ","13","0.4938818109839487","Artifact generation and benchmarking","Deployment"
"https://github.com/fairlearn/fairlearn","865258414","issue","https://github.com/Trusted-AI/AIF360/issues/756","MetricFrame should support metrics that don't require y_true and y_pred Right now `MetricFrame` only works with metrics with the signature `metric(y_true, y_pred)`, but its disaggregation functionality should be much more broadly applicable. Two use cases of interest:

1. **Dataset-only metrics or prediction-only metrics**. These are the metrics of the form `metric(y_true)` and `metric(y_pred)`. While in principle these could be handled by the current API, it's a bit confusing.
2. **Metrics for settings beyond classification / regression**. For example, to evaluate a contextual bandit algorithm, the metrics have the signature `metric(actions, rewards, propensities)`. This use case comes up in settings with partial observations, e.g., in lending: we only observe whether the person repays a loan (reward) for the specific loan type we provide (action), including ""no loan"".

I think regardless of how the API is tweaked, there's an obvious first step. And then hopefully a not-too-controversial second step.

## Step 1: Make MetricFrame arguments keyword-only

The suggestion is to change the current API, which is this:
```python
MetricFrame(metric, y_true, y_pred, *, sensitive_features, control_features=None, sample_params=None)
```
To something like this:
```python
MetricFrame(*, metrics, y_true=None, y_pred=None, sensitive_features, control_features=None, sample_params=None)
```
So all arguments would be keyword-only and `metric` would become `metrics` (for consistency with other arguments and with `columns` in pandas). The functionality wouldn't change. 

## Step 2: Allow flexible names of shared sample parameters

Change the signature to:
```python
MetricFrame(*, metrics, sensitive_features, control_features=None, sample_params=None, **shared_sample_params)
```
The idea is that any of the `shared_sample_params` are passed on to all metrics, whereas `sample_params` is (as before) a dictionary. With the new signature, it would still be possible to write things like
```python
mf = MetricFrame(metrics=skm.accuracy_score, y_true=y_true, y_pred=y_pred, sensitive_features=sf)
```
But it would be simple to use other kinds of metrics that do not work with `y_true` and `y_pred`.","2","0.5774472703804739","Performance measurement","Validation"
"https://github.com/fairlearn/fairlearn","864035814","issue","https://github.com/Trusted-AI/AIF360/issues/755","Fetching and underprediction and overprediction values Is there a way to fetch underprediction and overprediction values for each subgroup of a protected attribute (eg, for Gender - subgroups - Male, Female)?
![1](https://user-images.githubusercontent.com/53310020/115583605-08b8f580-a298-11eb-9f74-9e1be86f29bc.PNG)
","30","0.3007759253433993","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","862970213","issue","https://github.com/Trusted-AI/AIF360/issues/753","MetricFrame and multiple/named sensitive features I was writing an example notebook, and I encountered something which I found strange. Basically, this works:

``` python
multi_metric = MetricFrame(
    {
        ""average precision"": skm.average_precision_score,
        ""roc-auc"": skm.roc_auc_score,
        ""count"": lambda x, y: len(x),
    },
    y_test,
    y_pred,
    sensitive_features=gender_test,
)
```

But this doesn't:

``` python
multi_metric = MetricFrame(
    {
        ""average precision"": skm.average_precision_score,
        ""roc-auc"": skm.roc_auc_score,
        ""count"": lambda x, y: len(x),
    },
    y_test,
    y_pred,
    sensitive_features={'gender': gender_test},
)
```
with error: 

    ValueError: If using all scalar values, you must pass an index

To me this error message is not clear at all, and I have no pandas DataFrames to have any indices in the first place. I don't think we should be failing in this case.

cc @riedgar-ms @MiroDudik 

Here's the whole notebook for reproducibility

<details>

``` python
# %%
# First we need to install the requirements:
# pip install model-card-toolkit scikit-learn matplotlib fairlear pandas

# %%
# Here we have all the imports required to run the rest of the script
import numpy as np
import sklearn.metrics as skm
from numpy.random import RandomState
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.experimental import enable_hist_gradient_boosting  # noqa
from sklearn.ensemble import HistGradientBoostingClassifier
from sklearn.decomposition import PCA
from sklearn.pipeline import make_pipeline
from fairlearn.metrics import MetricFrame

# %%
rng = RandomState(seed=42)

X_women, y_women = make_classification(
    n_samples=500,
    n_features=20,
    n_informative=4,
    n_classes=2,
    class_sep=1,
    random_state=rng,
)

X_men, y_men = make_classification(
    n_samples=500,
    n_features=20,
    n_informative=4,
    n_classes=2,
    class_sep=2,
    random_state=rng,
)

X_unspecified, y_unspecified = make_classification(
    n_samples=500,
    n_features=20,
    n_informative=4,
    n_classes=2,
    class_sep=0.5,
    random_state=rng,
)

X = np.r_[X_women, X_men, X_unspecified]
y = np.r_[y_women, y_men, y_unspecified]
gender = np.r_[[""Woman""] * 500, [""Man""] * 500, [""Unspecified""] * 500].reshape(
    -1, 1
)
X_train, X_test, y_train, y_test, gender_train, gender_test = train_test_split(
    X, y, gender, test_size=0.3, random_state=rng
)

# %%
# Now we train a classifier on the data
clf = make_pipeline(PCA(n_components=4), HistGradientBoostingClassifier())
clf.fit(X_train, y_train)

# %%
# We can finally check the performance of our model for different groups.
y_pred = clf.predict_proba(X_test)[:, 0]
multi_metric = MetricFrame(
    {
        ""average precision"": skm.average_precision_score,
        ""roc-auc"": skm.roc_auc_score,
        ""count"": lambda x, y: len(x),
    },
    y_test,
    y_pred,
    sensitive_features={'gender': gender_test},
)
```

</details>","27","0.5506782484531176","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","860672128","issue","https://github.com/Trusted-AI/AIF360/issues/752","Target column with multiple classes (Multi Class Classification) Can we not implement bias detection in fairlearn if we have multiple classes in the target column like 0/1/2? What can be done to resolve this issue? 
","26","0.2786210731164858","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","860355801","issue","https://github.com/Trusted-AI/AIF360/issues/751","Make sure flake8 checks only relevant files From https://github.com/fairlearn/fairlearn/pull/747#issuecomment-821531625:

> Sadly this PR can't be completed since I managed to create a branch with `.py` at the end which means the `.git` folder has a log file ending in `.py` and `flake8` feels responsible for linting it, which results in errors. Wow! Will create a new branch with identical content and better name. Closing this PR

We need to change our CI to make sure `flake8` only checks the relevant files, and not the whole repo folder.

cc @romanlutz ","32","0.2580402565804025","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","857242251","issue","https://github.com/Trusted-AI/AIF360/issues/742","Update links from fairlearn.github.io to fairlearn.org #### Describe the issue linked to the documentation

A variety of our documentation links point to `fairlearn.github.io` rather than `fairlearn.org`. There is a redirect, but it would be good to fix these

#### Suggest a potential alternative/fix

See above
","14","0.874222281903227","Documentation","Development"
"https://github.com/fairlearn/fairlearn","854693050","issue","https://github.com/Trusted-AI/AIF360/issues/738","Move changelog to the main documentation #### Describe the issue linked to the documentation

The `CHANGES.md` file is only accessible in the repo, and not part of the main website (fairlearn.org)

#### Suggest a potential alternative/fix

Move (and convert to ReST) the file so that the file appears on fairlearn.org. Ideally it would be kept prominent in the repo itself, though (which might complicate things a little)
","14","0.6314284854503326","Documentation","Development"
"https://github.com/fairlearn/fairlearn","854368232","issue","https://github.com/Trusted-AI/AIF360/issues/737","Supporting `count` as a metric It's desired to know the number of data points in each group when working with `MetricFrame`, and that can be achieved by passing a function which returns the length of its input. For instance:

``` python
# report the above scores in a table.
from fairlearn.metrics import MetricFrame
import sklearn.metrics as skm
y_pred = np.asarray(df.y_pred).astype(float)
y_true = np.asarray(df.FraudLabel == ""true"").astype(int)
multi_metric = MetricFrame({
    'average precision':skm.average_precision_score,
    'roc-auc': skm.roc_auc_score,
    'count': lambda x, y: len(x)},
    y_true, y_pred,
    sensitive_features=df.gender)
multi_metric.by_group
```

We don't necessarily need to change anything in the code or API for this, but we should document it in the user guide and/or the API reference to let people know how it can be done.","12","0.3086109183437428","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","853940751","issue","https://github.com/Trusted-AI/AIF360/issues/736","Create a table with methods/modules and corresponding dependencies We should start having a very clear table, with methods/modules on one side, and dependencies on the other side, for people to have a clear view about them. Currently `postprocessing` has a soft dependency on `matplotlib`. Soon, this will also be true for `metrics` (once the plotting capabilities are merged). Additionally, #466 seems to converge towards `cvxpy` as another soft dependency.

_Originally posted by @adrinjalali in https://github.com/fairlearn/fairlearn/issues/466#issuecomment-815855327_

The table could either live with our installation instructions or on a separate page of our documentation. The installation instructions are currently part of ""Getting started"", and we may want to avoid adding too many details on that page, hence the suggestion for a ""separate page"".","21","0.3286433622366435","Installation and shell commands","Deployment"
"https://github.com/fairlearn/fairlearn","853490881","issue","https://github.com/Trusted-AI/AIF360/issues/734","Bugfix Release v0.6.1 I would like to put out a release (would be v0.6.1) for #727 (and also #662 , if there is agreement on merging that solution). What are people's thoughts on doing this?","32","0.3742031263645095","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","852536938","issue","https://github.com/Trusted-AI/AIF360/issues/731","Capability to hide prompts and outputs from examples Right now the examples include the prompts and outputs, and the user can't hide them before copy/pasting them:

![Screenshot from 2021-04-07 17-35-42](https://user-images.githubusercontent.com/1663864/113893859-b374e280-97c7-11eb-861b-998f8b477b30.png)

It'd be nice to have something like the one visible in this image, to hide those before copy/pasting.
![Screenshot from 2021-04-07 17-32-18](https://user-images.githubusercontent.com/1663864/113893815-a8ba4d80-97c7-11eb-8a0b-b7590a5d5efa.png)


","13","0.2635915004690241","Artifact generation and benchmarking","Deployment"
"https://github.com/fairlearn/fairlearn","852530785","issue","https://github.com/Trusted-AI/AIF360/issues/730","Add links to user guides from API pages I needed to use the `MetricFrame`, so I duckduckgoed `fairlearn metricframe`, which pointed me to the API page [here](https://fairlearn.org/v0.5.0/api_reference/fairlearn.metrics.html). Then I was expecting to easily find a section with some example code which I could copy/paste/adapt for my use-case, but it's not available.

I think it'd be nice to have links to user guides and example notebooks from the API pages, or even have short examples on those pages, so that users can adapt fairlearn functionality much easier.","14","0.6643508712474231","Documentation","Development"
"https://github.com/fairlearn/fairlearn","851682654","issue","https://github.com/Trusted-AI/AIF360/issues/729","fairlearn.metrics._group_metric_set._create_group_metric_set fails for regression since y_true contains values other than [0, 1] #### Describe the bug
I'm trying to run some fairness analysis on some of our models, where we can produce either classification or regression models. With regression, I found that it was failing to calculate metrics as it was expecting y_test to have values of only 0 and 1, which isn't true for our regression dataset. Strangley, even if I sent in a dummy y_test which only contained 0s and 1s, it still failed with the same error.

#### Steps/Code to Reproduce
```
X_test = <real test data for my model>
y_test = <real test data for my model> 
# I tried synthesizing a y_test with only 0 and 1, and it still failed
y_test = pd.DataFrame(0, index=np.arange(164), columns=[""ERP""])
y_test['ERP'][0] = 1
y_test
y_pred = model.predict(X_test)
dash_dict = _create_group_metric_set(
                                            y_true=y_test.values,
                                            predictions=y_pred,
                                            sensitive_features=[""column_0""],
                                            prediction_type=""regression""
    )
```

#### Expected Results
We would expect to get the dashboard dict returned for this regression model instead of getting an exception.

#### Actual Results
Traceback (most recent call last):
  File ""fairness.py"", line 430, in <module>
    validate_model_ids
  File ""fairness.py"", line 126, in run_fairness_analysis_single_model
    prediction_type=prediction_type
  File ""site-packages\fairlearn\metrics\_group_metric_set.py"", line 164, in _create_group_metric_set
    result[_Y_TRUE], prediction, sensitive_features=g[_BIN_VECTOR])
  File ""site-packages\fairlearn\metrics\_metric_frame.py"", line 151, in __init__
    self._by_group = self._compute_by_group(func_dict, y_t, y_p, sf_list, cf_list)
  File ""site-packages\fairlearn\metrics\_metric_frame.py"", line 169, in _compute_by_group
    return self._compute_dataframe_from_rows(func_dict, y_true, y_pred, rows)
  File ""site-packages\fairlearn\metrics\_metric_frame.py"", line 194, in _compute_dataframe_from_rows
    curr_metric = func_dict[func_name].evaluate(y_true, y_pred, mask)
  File ""site-packages\fairlearn\metrics\_function_container.py"", line 103, in evaluate
    return self.func_(y_true[mask], y_pred[mask], **params)
  File ""\site-packages\fairlearn\metrics\_balanced_root_mean_squared_error.py"", line 36, in _balanced_root_mean_squared_error
    raise ValueError(_Y_TRUE_NOT_0_1)
ValueError: Only 0 and 1 are allowed in y_true and both must be present

#### Versions

I'm running this on a Linux VM in Azure running pyhton 3.6.8 with latest version of fairlearn from pypi. 

Thanks!
","12","0.6330282304514852","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","851510599","issue","https://github.com/Trusted-AI/AIF360/issues/728","ThresholdOptimizer returns different group results when given multiple sensitive features. <!--
Before submitting a bug, please make sure the issue hasn't been already
addressed by searching through the past issues.

If your issue is a usage question, please submit it in one of these other
channels instead:
- StackOverflow with the `fairlearn` tag:
  https://stackoverflow.com/questions/tagged/fairlearn
- Gitter: https://gitter.im/fairlearn/community#
The issue tracker is used only to report bugs and feature requests. For
questions, please use either of the above platforms. Most question issues are
closed without an answer on this issue tracker. Thanks for your understanding.
-->

#### Describe the bug
<!--
A clear and concise description of what the bug is.
-->

The ThresholdOptimizer returns incorrect group performance results when multiple sensitive features are given. If the multiple sensitive feature columns are concatenated into a single column, the results appear more inline with what we intuitively expect.
This bug was found when optimizing for `balanced_accuracy_score` with the fairness constraint of `equalized_odds`.
#### Steps/Code to Reproduce
<!--
Please add a minimal example (in the form of code) that reproduces the error.
Be as succinct as possible, do not depend on external data. In short, we are
going to copy-paste your code and we expect to get the same result as you.

Example:
```python
import pandas as pd
from fairlearn.reductions import ExponentiatedGradient, DemographicParity
from sklearn.linear_model import LinearRegression
from fairlearn.datasets import fetch_adult

data = fetch_adult(as_frame=True)
X = pd.get_dummies(data.data)
y = (data.target == '>50K') * 1
sensitive_features = data.data['sex']
mitigator = ExponentiatedGradient(LinearRegression(), DemographicParity())
mitigator.fit(X, y, sensitive_features=sensitive_features)
```
If the code is too long, feel free to put it in a public gist and link
it in the issue: https://gist.github.com
-->

A script to reproduce the error can be found on the [manandi/scipy_case_study](https://github.com/LeJit/fairlearn/blob/manandi/scipy_case_study/tutorial/scipy/case_study.py) branch.

To reproduce the error, change the `sensitive` variable between the two values specified in lines 23 and 24. 

#### Expected Results
<!-- Example: No error is thrown. Please paste or describe the expected results.-->

<img width=""542"" alt=""Postprocess_bug"" src=""https://user-images.githubusercontent.com/1124464/113729775-51cc4f80-96c5-11eb-803f-468249e493e5.png"">

When the sensitive features are concatenated together, the `ThresholdOptimizer` yields results that mitigate disparity across sensitive attributes that align with intuition.

#### Actual Results
<!-- Please paste or specifically describe the actual output or traceback. -->

<img width=""542"" alt=""Postprocess_separate"" src=""https://user-images.githubusercontent.com/1124464/113729777-51cc4f80-96c5-11eb-8e78-c56a9ab8803c.png"">

Within racial groups, we see the ThresholdOptimizer achieves a false positive rate of 0 and false negative rate of 1 for multiple demographic groups. This result is unintuitive and leads to large between-group disparities.

#### Versions
<!--
Please provide the following information:
- OS: [e.g. Windows]
- Browser (if you're reporting a dashboard bug in jupyter): [e.g. Edge, Firefox, Chrome, Safari]
- Python version: [e.g. 3.7.4]
- Fairlearn version: [e.g. 0.4.5 or installed from main branch in editable mode]
- version of Python packages: please run the following snippet and paste the output:
  ```python
  import fairlearn
  fairlearn.show_versions()
  ```
-->

OS: Windows
Python Version: 3.8.5
Fairlearn version: 0.6.0

<!-- Thanks for contributing! -->
","11","0.4630998579214113","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","841810213","issue","https://github.com/Trusted-AI/AIF360/issues/721","Improve user guide on when to use which fairness metrics #### Describe the issue linked to the documentation

The user guide currently does explain what fairness metrics *mean* in a real-world scenario (i.e., beyond their mathematical definition), do not provide guidance on the scenarios in which they may be relevant, nor state some of the inherent limitations of using these metrics to quantify fairness.

#### Suggest a potential alternative/fix

Add more information to the user guide on the applicability as well as limitations of fairness metrics.

[Adapted from #458]","8","0.4820319376335977","Understanding Fairness Definition","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","841788465","issue","https://github.com/Trusted-AI/AIF360/issues/720","Structure API reference: individual pages and overview tables #### Describe the issue linked to the documentation

The current format puts the documentation of all functions/classes of a module on a single page. As the pages are getting quite long and often contain different types of functionality (e.g., ""base"" metrics versus fairness metrics in the `metrics` module), it can be difficult to get a good overview of the available functionality. This also makes it harder than necessary to find the documentation of a particular function/class.

#### Suggest a potential alternative/fix

I'd suggest to list the functions/classes in tables with a short description of each item. We can put the actual documentation on a separate page for each function/class. We can put all tables on a single page, similar to [scikit-learn](https://scikit-learn.org/stable/modules/classes.html) or on a separate page for each module, similar to [pandas](https://pandas.pydata.org/docs/reference/index.html). My personal preference would go to the latter.

[Adapted from larger issue #601]
","14","0.7263216295474358","Documentation","Development"
"https://github.com/fairlearn/fairlearn","841788150","issue","https://github.com/Trusted-AI/AIF360/issues/719","Improve consistency of reference format in documentation #### Describe the issue linked to the documentation

The usage of references in the documentation is inconsistent.
* In `ThresholdOptimizer` a reference is linked as a clickable *name (year)* in the **Notes**.
* Functions and classes in the reductions module (`DemographicParity`, `EqualizedOdds`, `ErrorRateParity`, `Moment`, `TruePositiveRateParity`, etc.) link the reference as a clickable *name (year)* in the **description**.
* In `GridSearch` the reference is linked as a clickable *name (year)* in the **description**.
* In `fetch_boston()` in the dataset module the **References docstring** is used, 
* In `fetch_adult()` and `fetch_bank_marketing()` in the dataset module the reference (including the full url and explicit 'Source:') is in the **description**. 

#### Suggest a potential alternative/fix

I'd suggest to follow the [docstring guide](https://numpydoc.readthedocs.io/en/latest/format.html) and put proper references in the *References* (i.e. not just links, as they might break).

[Adapted from larger issue #601]","14","0.643598910452843","Documentation","Development"
"https://github.com/fairlearn/fairlearn","839505501","issue","https://github.com/Trusted-AI/AIF360/issues/716","Update ""Developer Calls"" in Contributor Guide We currently describe Developer Calls in the Contributor Guide, including a link to the notes. 

**EDIT based on conversation below**:
* Change ""Developer Calls"" to ""Community Calls"" in Contributor Guide (https://fairlearn.org/v0.6.0/contributor_guide/index.html#)
* Add old developer call notes to HackMD (https://hackmd.io/QcJ8WBAbQTq6iOgoi0JT4g)
* Remove old developer call notes from website (https://fairlearn.org/v0.6.0/contributor_guide/developer_call_notes/index.html#developer-call-notes)
* Add link to HackMD to Contributor Guide.

~~* We should update the text to Community Calls.~~
~~* I don't think we are planning to publish notes of the community calls each week - unless somebody has a good idea to somewhat automate this process? In my opinion, stale content is even worse than no content, so if we don't plan on adding new content we should probably remove the older developer call notes from the website.~~

~~I am not sure about the bigger picture for updating the website content. @MiroDudik is there some sort of to-do list somewhere?~~

@riedgar-ms @MiroDudik @romanlutz @mmadaio @MiroDudik ","20","0.6316661034133154","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","839481986","issue","https://github.com/Trusted-AI/AIF360/issues/715","Order packages in API reference #### Describe the issue linked to the documentation

The current ordering of packages in the API reference seems a bit random to me:

<img width=""756"" alt=""image"" src=""https://user-images.githubusercontent.com/24417440/112278935-a1515b00-8c83-11eb-801f-63f38c257e8c.png"">

#### Suggest a potential alternative/fix

I can see a few options:
* Order alphabetically
* Order based on functionality, e.g.:
   * fairlearn.metrics
   * fairlearn.preprocessing
   * fairlearn.postprocessing
   * fairlearn.reductions
   * fairlearn.widget
   * fairlearn.dataset

My preference would probably be alphabetically, but either is fine with me.

@romanlutz @riedgar-ms @MiroDudik @adrinjalali @mmadaio
","14","0.7339484538540397","Documentation","Development"
"https://github.com/fairlearn/fairlearn","833084966","issue","https://github.com/Trusted-AI/AIF360/issues/714","Add Fairlearn to paperswithcode.com Requirements listed at https://paperswithcode.com/libraries","4","0.3911088911088911","Installation and shell commands","Deployment"
"https://github.com/fairlearn/fairlearn","812438865","issue","https://github.com/Trusted-AI/AIF360/issues/705","Sample for Regression Models using BoundedGroupLoss #### Describe the issue linked to the documentation

<!--
I don't see a sample for regression. Am I looking at the work spot? 
-->

#### Suggest a potential alternative/fix

<!--
Tell us how we could improve the documentation in this regard.
-->
","14","0.4974310579846912","Documentation","Development"
"https://github.com/fairlearn/fairlearn","811789585","issue","https://github.com/Trusted-AI/AIF360/issues/704","CI Warnings in tests We have quite a few warnings which we need to fix.

<details>

```
2021-02-19T05:31:46.6278873Z /opt/hostedtoolcache/Python/3.9.1/x64/lib/python3.9/site-packages/ansiwrap/core.py:6
2021-02-19T05:31:46.6279795Z   /opt/hostedtoolcache/Python/3.9.1/x64/lib/python3.9/site-packages/ansiwrap/core.py:6: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
2021-02-19T05:31:46.6386997Z     import imp
2021-02-19T05:31:46.6387155Z 
2021-02-19T05:31:46.6387827Z /opt/hostedtoolcache/Python/3.9.1/x64/lib/python3.9/site-packages/papermill/iorw.py:50
2021-02-19T05:31:46.6388469Z /opt/hostedtoolcache/Python/3.9.1/x64/lib/python3.9/site-packages/papermill/iorw.py:50
2021-02-19T05:31:46.6389387Z   /opt/hostedtoolcache/Python/3.9.1/x64/lib/python3.9/site-packages/papermill/iorw.py:50: DeprecationWarning: pyarrow.HadoopFileSystem is deprecated as of 2.0.0, please use pyarrow.fs.HadoopFileSystem instead.
2021-02-19T05:31:46.6389944Z     from pyarrow import HadoopFileSystem
2021-02-19T05:31:46.6390230Z 
2021-02-19T05:31:46.6390746Z /opt/hostedtoolcache/Python/3.9.1/x64/lib/python3.9/site-packages/scrapbook/__init__.py:8
2021-02-19T05:31:46.6391692Z   /opt/hostedtoolcache/Python/3.9.1/x64/lib/python3.9/site-packages/scrapbook/__init__.py:8: FutureWarning: 'nteract-scrapbook' package has been renamed to `scrapbook`. No new releases are going out for this old package name.
2021-02-19T05:31:46.6392745Z     warnings.warn(""'nteract-scrapbook' package has been renamed to `scrapbook`. No new releases are going out for this old package name."", FutureWarning)
2021-02-19T05:31:46.6393045Z 
2021-02-19T05:31:46.6393344Z unit/test_random_state.py::test_random_state_threshold_optimizer
2021-02-19T05:31:46.6393729Z unit/test_random_state.py::test_random_state_exponentiated_gradient
2021-02-19T05:31:46.6394632Z   /home/vsts/work/1/s/test/unit/test_random_state.py:69: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
2021-02-19T05:31:46.6395854Z   Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
2021-02-19T05:31:46.6396450Z     y = data['target'].astype(np.int)

2021-02-19T05:31:46.6954615Z   /opt/hostedtoolcache/Python/3.9.1/x64/lib/python3.9/site-packages/sklearn/utils/validation.py:63: FutureWarning: Arrays of bytes/strings is being converted to decimal numbers if dtype='numeric'. This behavior is deprecated in 0.24 and will be removed in 1.1 (renaming of 0.26). Please convert your data to numeric values explicitly instead.
2021-02-19T05:31:46.6955363Z     return f(*args, **kwargs)
2021-02-19T05:31:46.6955496Z 
2021-02-19T05:31:46.6956047Z unit/datasets/test_datasets.py::TestFairlearnDataset::test_dataset_as_bunch[fetch_boston-True]
2021-02-19T05:31:46.6956719Z unit/datasets/test_datasets.py::TestFairlearnDataset::test_dataset_as_bunch[fetch_boston-False]
2021-02-19T05:31:46.6957395Z unit/datasets/test_datasets.py::TestFairlearnDataset::test_dataset_as_X_y[fetch_boston-True]
2021-02-19T05:31:46.6958059Z unit/datasets/test_datasets.py::TestFairlearnDataset::test_dataset_as_X_y[fetch_boston-False]
2021-02-19T05:31:46.6958624Z   /home/vsts/work/1/s/fairlearn/datasets/_fetch_boston.py:114: DataFairnessWarning: You are about to use a dataset with known fairness issues.
2021-02-19T05:31:46.6959098Z     warnings.warn(DataFairnessWarning(msg))

2021-02-19T05:31:46.7155278Z unit/metrics/test_metricframe_smoke.py::test_duplicate_cf_names
2021-02-19T05:31:46.7155932Z   /home/vsts/work/1/s/fairlearn/metrics/_metric_frame.py:533: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. 
2021-02-19T05:31:46.7156904Z   Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
2021-02-19T05:31:46.7157367Z     f_arr = np.squeeze(np.asarray(features, dtype=np.object))
2021-02-19T05:31:46.7157554Z 
2021-02-19T05:31:46.7157872Z unit/metrics/test_metricframe_process_features.py::TestTwoFeatures::test_2d_array
2021-02-19T05:31:46.7158603Z   /home/vsts/work/1/s/test/unit/metrics/test_metricframe_process_features.py:125: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. 
2021-02-19T05:31:46.7159621Z   Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
2021-02-19T05:31:46.7160104Z     rf = np.asarray([a, b], dtype=np.object).transpose()

2021-02-19T05:31:46.7166617Z unit/preprocessing/linear_dep_remover/test_sklearn_compat.py::test_estimator_checks[check_estimators_overwrite_params]
2021-02-19T05:31:46.7167156Z   /home/vsts/work/1/s/fairlearn/preprocessing/_correlation_remover.py:82: RuntimeWarning: Mean of empty slice.
2021-02-19T05:31:46.7167593Z     self.sensitive_mean_ = X_sensitive.mean()

2021-02-19T05:31:46.7173816Z unit/preprocessing/linear_dep_remover/test_sklearn_compat.py::test_estimator_checks[check_estimators_overwrite_params]
2021-02-19T05:31:46.7174659Z   /opt/hostedtoolcache/Python/3.9.1/x64/lib/python3.9/site-packages/numpy/core/_methods.py:188: RuntimeWarning: invalid value encountered in double_scalars
2021-02-19T05:31:46.7175145Z     ret = ret.dtype.type(ret / rcount)
2021-02-19T05:31:46.7175289Z 
2021-02-19T05:31:46.7175786Z unit/reductions/test_smoke.py::test_smoke[10-SVC-EqualizedOdds-ExponentiatedGradient]
2021-02-19T05:31:46.7177228Z   /home/vsts/work/1/s/fairlearn/reductions/_exponentiated_gradient/_lagrangian.py:133: OptimizeWarning: The pivot operation produces a pivot value of: 2.2e-09, which is only slightly greater than the specified tolerance 1.0e-09. This may lead to issues regarding the numerical stability of the simplex method. Removing redundant constraints, changing the pivot strategy via Bland's rule or increasing the tolerance may help reduce the issue.
2021-02-19T05:31:46.7178486Z     result = opt.linprog(c, A_ub=A_ub, b_ub=b_ub, A_eq=A_eq, b_eq=b_eq, method='simplex')

2021-02-19T05:31:47.5691972Z unit/reductions/exponentiated_gradient/test_lagrangian.py::test_lagrangian_eval[False-False-BoundedGroupLoss-0.1]
2021-02-19T05:31:47.5693724Z   /home/vsts/work/1/s/test/unit/reductions/exponentiated_gradient/simple_learners.py:14: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.
2021-02-19T05:31:47.5694735Z     matX = np.array(X) * sqrtW[:, np.newaxis]

2021-02-19T05:31:47.6037584Z unit/reductions/exponentiated_gradient/test_exponentiatedgradient_smoke.py::TestExponentiatedGradientSmoke::test_simple_fit_predict_regression[constraints2]
2021-02-19T05:31:47.6039114Z   /home/vsts/work/1/s/test/unit/reductions/exponentiated_gradient/simple_learners.py:30: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.
2021-02-19T05:31:47.6039881Z     matX = np.array(X) * sqrtW[:, np.newaxis]
2021-02-19T05:31:47.6040066Z 
2021-02-19T05:31:47.6040472Z unit/reductions/exponentiated_gradient/test_exponentiatedgradient_smoke.py::TestExponentiatedGradientSmoke::test_smoke_regression[data0]
2021-02-19T05:31:47.6041154Z unit/reductions/exponentiated_gradient/test_exponentiatedgradient_smoke.py::TestExponentiatedGradientSmoke::test_smoke_regression[data1]
2021-02-19T05:31:47.6041783Z unit/reductions/exponentiated_gradient/test_exponentiatedgradient_smoke.py::TestExponentiatedGradientSmoke::test_smoke_regression[data2]
2021-02-19T05:31:47.6042530Z unit/reductions/exponentiated_gradient/test_exponentiatedgradient_smoke.py::TestExponentiatedGradientSmoke::test_simple_fit_predict_regression[constraints0]
2021-02-19T05:31:47.6044127Z   /home/vsts/work/1/s/fairlearn/reductions/_exponentiated_gradient/_lagrangian.py:139: OptimizeWarning: The pivot operation produces a pivot value of: 2.4e-06, which is only slightly greater than the specified tolerance 1.0e-09. This may lead to issues regarding the numerical stability of the simplex method. Removing redundant constraints, changing the pivot strategy via Bland's rule or increasing the tolerance may help reduce the issue.
2021-02-19T05:31:47.6045047Z     result_dual = opt.linprog(dual_c,
```

</details>","5","0.7787125930164838","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","811785693","issue","https://github.com/Trusted-AI/AIF360/issues/703","CI failing due to removed dataset IDs from OpenML The commit 77e51c5 seems to be triggering a failure in our CI (see https://github.com/fairlearn/fairlearn/pull/702) caused by one or two IDs remvoed from OpenML. We should investigate the issue and figure out why they're removed, and where they can be found now.

<details>

```
2021-02-19T05:31:46.6084363Z =================================== FAILURES ===================================
2021-02-19T05:31:46.6191931Z _________ TestFairlearnDataset.test_dataset_as_bunch[fetch_adult-True] _________
2021-02-19T05:31:46.6192578Z 
2021-02-19T05:31:46.6193185Z self = <test.unit.datasets.test_datasets.TestFairlearnDataset object at 0x7fdc98376040>
2021-02-19T05:31:46.6194599Z as_frame = True, fetch_function = <function fetch_adult at 0x7fdc9e145430>
2021-02-19T05:31:46.6195290Z 
2021-02-19T05:31:46.6196268Z     @pytest.mark.parametrize(""as_frame"", [True, False])
2021-02-19T05:31:46.6197361Z     @pytest.mark.parametrize(""fetch_function"", [fetch_adult, fetch_boston, fetch_bank_marketing])
2021-02-19T05:31:46.6198700Z     def test_dataset_as_bunch(self, as_frame, fetch_function):
2021-02-19T05:31:46.6199841Z >       dataset = fetch_function(as_frame=as_frame)
2021-02-19T05:31:46.6200488Z 
2021-02-19T05:31:46.6201319Z test/unit/datasets/test_datasets.py:18: 
2021-02-19T05:31:46.6203125Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2021-02-19T05:31:46.6204409Z fairlearn/datasets/_fetch_adult.py:79: in fetch_adult
2021-02-19T05:31:46.6205231Z     return fetch_openml(
2021-02-19T05:31:46.6206504Z /opt/hostedtoolcache/Python/3.9.1/x64/lib/python3.9/site-packages/sklearn/utils/validation.py:63: in inner_f
2021-02-19T05:31:46.6213353Z     return f(*args, **kwargs)
2021-02-19T05:31:46.6214531Z /opt/hostedtoolcache/Python/3.9.1/x64/lib/python3.9/site-packages/sklearn/datasets/_openml.py:847: in fetch_openml
2021-02-19T05:31:46.6216700Z     data_description = _get_data_description_by_id(data_id, data_home)
2021-02-19T05:31:46.6217842Z /opt/hostedtoolcache/Python/3.9.1/x64/lib/python3.9/site-packages/sklearn/datasets/_openml.py:437: in _get_data_description_by_id
2021-02-19T05:31:46.6219080Z     json_data = _get_json_content_from_openml_api(
2021-02-19T05:31:46.6220097Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2021-02-19T05:31:46.6220979Z 
2021-02-19T05:31:46.6221694Z url = 'api/v1/json/data/1590'
2021-02-19T05:31:46.6223169Z error_message = 'Dataset with data_id 1590 not found.'
2021-02-19T05:31:46.6223991Z data_home = '/home/vsts/.fairlearn-data/openml'
2021-02-19T05:31:46.6224318Z 
2021-02-19T05:31:46.6224799Z     def _get_json_content_from_openml_api(
2021-02-19T05:31:46.6225233Z         url: str,
2021-02-19T05:31:46.6225693Z         error_message: Optional[str],
2021-02-19T05:31:46.6226170Z         data_home: Optional[str]
2021-02-19T05:31:46.6226760Z     ) -> Dict:
2021-02-19T05:31:46.6227159Z         """"""
2021-02-19T05:31:46.6227588Z         Loads json data from the openml api
2021-02-19T05:31:46.6228012Z     
2021-02-19T05:31:46.6228377Z         Parameters
2021-02-19T05:31:46.6228996Z         ----------
2021-02-19T05:31:46.6229365Z         url : str
2021-02-19T05:31:46.6229793Z             The URL to load from. Should be an official OpenML endpoint
2021-02-19T05:31:46.6230186Z     
2021-02-19T05:31:46.6230549Z         error_message : str or None
2021-02-19T05:31:46.6231014Z             The error message to raise if an acceptable OpenML error is thrown
2021-02-19T05:31:46.6231726Z             (acceptable error is, e.g., data id not found. Other errors, like 404's
2021-02-19T05:31:46.6232227Z             will throw the native error message)
2021-02-19T05:31:46.6232585Z     
2021-02-19T05:31:46.6233132Z         data_home : str or None
2021-02-19T05:31:46.6233587Z             Location to cache the response. None if no cache is required.
2021-02-19T05:31:46.6233996Z     
2021-02-19T05:31:46.6234307Z         Returns
2021-02-19T05:31:46.6234784Z         -------
2021-02-19T05:31:46.6235163Z         json_data : json
2021-02-19T05:31:46.6235708Z             the json result from the OpenML server if the call was successful.
2021-02-19T05:31:46.6236153Z             An exception otherwise.
2021-02-19T05:31:46.6236520Z         """"""
2021-02-19T05:31:46.6236841Z     
2021-02-19T05:31:46.6237212Z         @_retry_with_clean_cache(url, data_home)
2021-02-19T05:31:46.6237609Z         def _load_json():
2021-02-19T05:31:46.6238083Z             with closing(_open_openml_url(url, data_home)) as response:
2021-02-19T05:31:46.6238783Z                 return json.loads(response.read().decode(""utf-8""))
2021-02-19T05:31:46.6239198Z     
2021-02-19T05:31:46.6239512Z         try:
2021-02-19T05:31:46.6239875Z             return _load_json()
2021-02-19T05:31:46.6240283Z         except HTTPError as error:
2021-02-19T05:31:46.6240757Z             # 412 is an OpenML specific error code, indicating a generic error
2021-02-19T05:31:46.6241199Z             # (e.g., data not found)
2021-02-19T05:31:46.6241611Z             if error.code != 412:
2021-02-19T05:31:46.6242010Z                 raise error
2021-02-19T05:31:46.6242508Z     
2021-02-19T05:31:46.6242877Z         # 412 error, not in except for nicer traceback
2021-02-19T05:31:46.6243310Z >       raise OpenMLError(error_message)
2021-02-19T05:31:46.6243799Z E       sklearn.datasets._openml.OpenMLError: Dataset with data_id 1590 not found.
2021-02-19T05:31:46.6244122Z 
2021-02-19T05:31:46.6244826Z /opt/hostedtoolcache/Python/3.9.1/x64/lib/python3.9/site-packages/sklearn/datasets/_openml.py:179: OpenMLError
2021-02-19T05:31:46.6245596Z ____ TestFairlearnDataset.test_dataset_as_bunch[fetch_bank_marketing-True] _____
2021-02-19T05:31:46.6245912Z 
2021-02-19T05:31:46.6246337Z self = <test.unit.datasets.test_datasets.TestFairlearnDataset object at 0x7fdc98293e20>
2021-02-19T05:31:46.6246782Z as_frame = True
2021-02-19T05:31:46.6247203Z fetch_function = <function fetch_bank_marketing at 0x7fdc9e145160>
2021-02-19T05:31:46.6247483Z 
2021-02-19T05:31:46.6247859Z     @pytest.mark.parametrize(""as_frame"", [True, False])
2021-02-19T05:31:46.6248376Z     @pytest.mark.parametrize(""fetch_function"", [fetch_adult, fetch_boston, fetch_bank_marketing])
2021-02-19T05:31:46.6248928Z     def test_dataset_as_bunch(self, as_frame, fetch_function):
2021-02-19T05:31:46.6249411Z >       dataset = fetch_function(as_frame=as_frame)
2021-02-19T05:31:46.6249668Z 
2021-02-19T05:31:46.6250030Z test/unit/datasets/test_datasets.py:18: 
2021-02-19T05:31:46.6250489Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2021-02-19T05:31:46.6251024Z fairlearn/datasets/_fetch_bank_marketing.py:82: in fetch_bank_marketing
2021-02-19T05:31:46.6251455Z     return fetch_openml(
2021-02-19T05:31:46.6252171Z /opt/hostedtoolcache/Python/3.9.1/x64/lib/python3.9/site-packages/sklearn/utils/validation.py:63: in inner_f
2021-02-19T05:31:46.6252687Z     return f(*args, **kwargs)
2021-02-19T05:31:46.6253417Z /opt/hostedtoolcache/Python/3.9.1/x64/lib/python3.9/site-packages/sklearn/datasets/_openml.py:873: in fetch_openml
2021-02-19T05:31:46.6253972Z     features_list = _get_data_features(data_id, data_home)
2021-02-19T05:31:46.6254801Z /opt/hostedtoolcache/Python/3.9.1/x64/lib/python3.9/site-packages/sklearn/datasets/_openml.py:450: in _get_data_features
2021-02-19T05:31:46.6255366Z     json_data = _get_json_content_from_openml_api(
2021-02-19T05:31:46.6257168Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2021-02-19T05:31:46.6257396Z 
2021-02-19T05:31:46.6257820Z url = 'api/v1/json/data/features/1461'
2021-02-19T05:31:46.6258301Z error_message = 'Dataset with data_id 1461 not found.'
2021-02-19T05:31:46.6258783Z data_home = '/home/vsts/.fairlearn-data/openml'
2021-02-19T05:31:46.6259060Z 
2021-02-19T05:31:46.6259316Z     def _get_json_content_from_openml_api(
2021-02-19T05:31:46.6259600Z         url: str,
2021-02-19T05:31:46.6259873Z         error_message: Optional[str],
2021-02-19T05:31:46.6260181Z         data_home: Optional[str]
2021-02-19T05:31:46.6260577Z     ) -> Dict:
2021-02-19T05:31:46.6260885Z         """"""
2021-02-19T05:31:46.6261154Z         Loads json data from the openml api
2021-02-19T05:31:46.6261415Z     
2021-02-19T05:31:46.6261626Z         Parameters
2021-02-19T05:31:46.6261995Z         ----------
2021-02-19T05:31:46.6262254Z         url : str
2021-02-19T05:31:46.6262578Z             The URL to load from. Should be an official OpenML endpoint
2021-02-19T05:31:46.6262866Z     
2021-02-19T05:31:46.6263120Z         error_message : str or None
2021-02-19T05:31:46.6263482Z             The error message to raise if an acceptable OpenML error is thrown
2021-02-19T05:31:46.6264060Z             (acceptable error is, e.g., data id not found. Other errors, like 404's
2021-02-19T05:31:46.6264451Z             will throw the native error message)
2021-02-19T05:31:46.6264720Z     
2021-02-19T05:31:46.6264971Z         data_home : str or None
2021-02-19T05:31:46.6265305Z             Location to cache the response. None if no cache is required.
2021-02-19T05:31:46.6265610Z     
2021-02-19T05:31:46.6265833Z         Returns
2021-02-19T05:31:46.6266169Z         -------
2021-02-19T05:31:46.6266431Z         json_data : json
2021-02-19T05:31:46.6266780Z             the json result from the OpenML server if the call was successful.
2021-02-19T05:31:46.6267278Z             An exception otherwise.
2021-02-19T05:31:46.6267543Z         """"""
2021-02-19T05:31:46.6267744Z     
2021-02-19T05:31:46.6268006Z         @_retry_with_clean_cache(url, data_home)
2021-02-19T05:31:46.6268314Z         def _load_json():
2021-02-19T05:31:46.6268670Z             with closing(_open_openml_url(url, data_home)) as response:
2021-02-19T05:31:46.6269257Z                 return json.loads(response.read().decode(""utf-8""))
2021-02-19T05:31:46.6269565Z     
2021-02-19T05:31:46.6269787Z         try:
2021-02-19T05:31:46.6270033Z             return _load_json()
2021-02-19T05:31:46.6270334Z         except HTTPError as error:
2021-02-19T05:31:46.6270703Z             # 412 is an OpenML specific error code, indicating a generic error
2021-02-19T05:31:46.6271046Z             # (e.g., data not found)
2021-02-19T05:31:46.6271353Z             if error.code != 412:
2021-02-19T05:31:46.6271653Z                 raise error
2021-02-19T05:31:46.6271892Z     
2021-02-19T05:31:46.6272156Z         # 412 error, not in except for nicer traceback
2021-02-19T05:31:46.6272482Z >       raise OpenMLError(error_message)
2021-02-19T05:31:46.6272866Z E       sklearn.datasets._openml.OpenMLError: Dataset with data_id 1461 not found.
2021-02-19T05:31:46.6273089Z 
2021-02-19T05:31:46.6273646Z /opt/hostedtoolcache/Python/3.9.1/x64/lib/python3.9/site-packages/sklearn/datasets/_openml.py:179: OpenMLError
```
</details>","5","0.9610491956818789","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","804930474","issue","https://github.com/Trusted-AI/AIF360/issues/698","Request to host face benchmarking script on fairlearn Per recommendation from Richard Edgar, we're opening a new issue to propose hosting a python script on the fairlearn GitHub that allows developers to benchmark face recognition performance (measured by the same criteria used in industry such as NIST: True Positive, False Positive, and ROC curves) on their own data, using Azure Cognitive Services or other services and their own model. 

Pls let us know what process to follow. Thanks!","13","0.6272083541949788","Artifact generation and benchmarking","Deployment"
"https://github.com/fairlearn/fairlearn","804879396","issue","https://github.com/Trusted-AI/AIF360/issues/697","Documentation Titles don't match selected version #### Describe the issue linked to the documentation

With the versioned documentation, the v0.6.0 documentation has a bad header on its pages:

![image](https://user-images.githubusercontent.com/28632930/107421598-29224200-6ae8-11eb-9ce4-39eda98209f7.png)


#### Suggest a potential alternative/fix

This is probably due to the v0.6.0 tag being at the branch point, rather than the tip of the `release/v0.6.0` branch
","14","0.5506839826839829","Documentation","Development"
"https://github.com/fairlearn/fairlearn","789468451","issue","https://github.com/Trusted-AI/AIF360/issues/683","pip install from source fails due to requirements.txt not being included in .tar.gz dist 
#### Describe the bug
When installing from src dist, fairlearn fails due to missing requirements.txt file.

#### Steps/Code to Reproduce
```shell
pip download --no-binary :all: -d ./ fairlearn==0.5.0
```

#### Expected Results
```shell
Collecting fairlearn==0.4.6
  Downloading fairlearn-0.4.6.tar.gz (10.5 MB)
     |████████████████████████████████| 10.5 MB 16.6 MB/s
```
#### Actual Results
```shell
Collecting fairlearn==0.5.0
  Using cached fairlearn-0.5.0.tar.gz (12.2 MB)
    ERROR: Command errored out with exit status 1:
     command: /usr/bin/python3.7 -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-download-muxh63mo/fairlearn_d4026bee74b547c9957a0a48e60fe570/setup.py'""'""'; __file__='""'""'/tmp/pip-download-muxh63mo/fairlearn_d4026bee74b547c9957a0a48e60fe570/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' egg_info --egg-base /tmp/pip-pip-egg-info-bfv_4scs
         cwd: /tmp/pip-download-muxh63mo/fairlearn_d4026bee74b547c9957a0a48e60fe570/
    Complete output (5 lines):
    Traceback (most recent call last):
      File ""<string>"", line 1, in <module>
      File ""/tmp/pip-download-muxh63mo/fairlearn_d4026bee74b547c9957a0a48e60fe570/setup.py"", line 12, in <module>
        with open('requirements.txt') as f:
    FileNotFoundError: [Errno 2] No such file or directory: 'requirements.txt'
    ----------------------------------------
ERROR: Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.
```

#### Screenshots
![image](https://user-images.githubusercontent.com/9027725/105106478-ee4a6280-5a83-11eb-8dce-d1ac9ff76c39.png)

#### Versions
OS: Ubuntu (Windows Subsystem for Linux)
Python: 3..7.17
pip: 20.3.3
fairlearn: 0.5.0
","4","0.7141085567108658","Installation and shell commands","Deployment"
"https://github.com/fairlearn/fairlearn","783440665","issue","https://github.com/Trusted-AI/AIF360/issues/676","Add Generalized Entropy-based metrics <!--
If your issue is a usage question, please submit it in one of these other
channels instead:
- StackOverflow with the `fairlearn` tag:
  https://stackoverflow.com/questions/tagged/fairlearn
- Gitter: https://gitter.im/fairlearn/community#
The issue tracker is used only to report bugs and feature requests. For
questions, please use either of the above platforms. Most question issues are
closed without an answer on this issue tracker. Thanks for your understanding.
-->

#### Is your feature request related to a problem? Please describe.
<!-- A clear and concise description of what the problem is. Ex. I'm always
frustrated when ... -->

I want to incorporate generalized entropy and other information-theoretic metrics to FairLearn, both for evaluating the distribution of empirical distribution of outputs of a predictive model and the distribution of labels and protected groups in dataset.

#### Describe the solution you'd like
<!-- A clear and concise description of what you want to happen. -->

Create an `Entropy` method with the instantiation of the [Generalized Entropy Index](https://en.wikipedia.org/wiki/Generalized_entropy_index). From there, derived entropy metrics, such as the Atkinson Index, can be defined methods wrapping the `Entropy` function with specific input parameters.


#### Describe alternatives you've considered, if relevant
<!-- A clear and concise description of any alternative solutions or features
you've considered. -->

Add methods directly to `MetricFrame` class to support calculation of entropy-based metrics.


#### Additional context
<!-- Add any other context or screenshots about the feature request here. -->
Here are some use cases that use entropy or information-theoretic methods to assess fairness of a technical system:

- [Fairness in Experimentation: Inequality in A/B Testing as an approach to Responsible Design](https://arxiv.org/abs/2002.05819)
- [Detecting and Reducing Bias in High-Stakes Domains](https://arxiv.org/abs/1908.11474)
","11","0.481029872764153","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","777146519","issue","https://github.com/Trusted-AI/AIF360/issues/668","Add control features to metric plots #### Is your feature request related to a problem? Please describe.
The metric plotting functionality mentioned in the [docs](https://fairlearn.org/main/user_guide/assessment.html#plotting-grouped-metrics) does not mention `control_features`

#### Describe the solution you'd like

Add an example where `control_features` are used. In the same example we could use `age` as a control feature. However, whenever #793 is completed we should switch to the synthetic dataset. After all, it's not ideal to use a real dataset for an abstract purpose of showing this functionality.

#### Bonus

Ideally, a separator line between the control groups or a new line for each set of subgroups within a control group would be nice. Right now they're all treated as equal groups in the plots.
","15","0.5946669412128076","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","777146101","issue","https://github.com/Trusted-AI/AIF360/issues/667","Allow multiple metrics to be passed into plotting functions #### Is your feature request related to a problem? Please describe.
With #561 one can pass in only a single metric. It would be nice to extend that to pass in multiple metrics rather than just one, and plotting all of them in a series of subplots.

#### Describe the solution you'd like
The existing functionality could be extended to take a single metric or a list of metrics. This wouldn't even require an API change (other than the description, of course).

#### Describe alternatives you've considered, if relevant
Alternatively, one could also just call the existing function `n` times. Then you get `n` different plots.
","15","0.6598294926985737","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","776691036","issue","https://github.com/Trusted-AI/AIF360/issues/666","Add model comparison plot #### Is your feature request related to a problem? Please describe.

With the introduction of `matplotlib`-based plotting functions in #561 we are adding ways to plot metrics for all groups as defined by `sensitive_features`. To get to parity with the `FairlearnDashboard` we should also add model comparison visualizations. This would be pretty much identical to the following existing view:
![image](https://user-images.githubusercontent.com/10245648/103387952-7d82dc80-4abb-11eb-9ae9-2b34a0f9626a.png)

Just like the functions in #561  this should live in the `metrics` module.

#### Describe the solution you'd like

```
plot_model_comparison(
    y_true, 
    y_preds={""model 1"": y_pred_1, ""model 2"": y_pred_2, ... }, 
    sensitive_features, show_plot=True)
```","12","0.6136363636363636","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","776688412","issue","https://github.com/Trusted-AI/AIF360/issues/665","Replace `get_dummies` in examples with `Pipeline` This issue is based on the following comment:
could we use a `Pipeline` instead of `get_dummies`? See #649

_Originally posted by @adrinjalali in https://github.com/fairlearn/fairlearn/pull/614#discussion_r547870456_

Using a `Pipeline` with all the preprocessing steps is cleaner and a better practice. However, we currently can't pass a `Pipeline` to mitigation techniques since they run `check_X_y` internally. If there are preprocessing steps, they are not executed before this check and we get a failure.

# Goals
- use `Pipeline` in examples, tests, etc. (we have several places with `get_dummies` which are almost identical
- fix mitigation technique behavior to work with `Pipeline`s that include preprocessing steps
- stretch goal (perhaps out of scope for the initial PR): make it possible to pass our mitigation techniques to `Pipeline`s.","28","0.4900028635437197","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","771142389","issue","https://github.com/Trusted-AI/AIF360/issues/660","Control features for Postprocessing #### Is your feature request related to a problem? Please describe.

While we have `control_features` for reductions, they are not implemented for  `ThresholdOptimizer`

#### Describe the solution you'd like

Add them as an option. Basically, a separate optimiser would need to be constructed for each control feature, and it would have to be available at scoring time too.

#### Describe alternatives you've considered, if relevant

N/A

#### Additional context

N/A","11","0.4853964600800045","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","761803442","issue","https://github.com/Trusted-AI/AIF360/issues/655","Support for streaming metrics #### Is your feature request related to a problem? Please describe.

I am using fairlearn to evaluate deep learning models. In this context, we often compute metrics per batch due to the large size of the dataset. In addition, some tasks such as semantic segmentation are so expensive that we can't hold all the predictions in memory.

#### Describe the solution you'd like
I would like if `MetricFrame` would accept an `update` method where we can add more data. For the first version, we can simply add more data to the internal representation of `ytrue, ypred, sensitive_attribute`. But then I would like it if we could keep a confusion matrix and the `update` method would update this confusion matrix.

#### Describe alternatives you've considered, if relevant

I made a wrapper around Fairlearn metrics that keep tracks of `ytrue, ypred, sensitive_attribute` and then call the underlying metric when needed.

#### Additional context

Example of a metric in Pytorch Lightning, a fairly popular DL framework.
https://pytorch-lightning.readthedocs.io/en/stable/metrics.html#implementing-a-metric

Keras has a similar API

https://keras.io/api/metrics/#as-subclasses-of-metric-stateful
","11","0.2944416802899993","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","761803007","issue","https://github.com/Trusted-AI/AIF360/issues/654","cannot import name 'MetricFrame' (again) There is another issue open for this error but it's closed and the solution is not clear, hence this one.

#### Describe the bug

I have spun up an Azure Compute Instance VM as part of Azure Machine Learning. This VM already comes installed  with Fairlearn 0.4.6 . I have developed a notebook in JupyterLab training a model, and now want to use Fairlearn to look for gender-related biases.

Following the instructions from here: <https://fairlearn.github.io/v0.5.0/quickstart.html#evaluating-fairness-related-metrics>, when I do either `from fairlearn.metrics import MetricFrame` or `from fairlearn.widget import FairlearnDashboard` I get:

```
---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
<ipython-input-127-d59f0567414f> in <module>
----> 1 from fairlearn.widget import FairlearnDashboard

/anaconda/envs/azureml_py36/lib/python3.6/site-packages/fairlearn/widget/__init__.py in <module>
      4 """"""Package for the Fairlearn Dashboard widget.""""""
      5 
----> 6 from ._fairlearn_dashboard import FairlearnDashboard
      7 
      8 

/anaconda/envs/azureml_py36/lib/python3.6/site-packages/fairlearn/widget/_fairlearn_dashboard.py in <module>
      9     _mean_underprediction, _root_mean_squared_error, false_negative_rate,
     10     false_positive_rate, mean_prediction, selection_rate, true_negative_rate)
---> 11 from fairlearn.metrics import MetricFrame
     12 from warnings import warn
     13 

ImportError: cannot import name 'MetricFrame'
```

I have updated to version 0.5.0 with `pip install --upgrade fairlearn`, which succeded with not errors, restarted the kernel, but the error remains.

From the comments in the other bug report, it seems that for this to work the advanced instructions have to be followed from <https://fairlearn.github.io/v0.5.0/contributor_guide/development_process.html#advanced-install>. However this page just says: ""Note that the Fairlearn dashboard is built using nodejs and requires additional steps. "", and these steps are not detailed.

I have also seen the link suggesting using Azure Machine Learning for this, but I want to do it on the VM (or, for that matter, my local laptop).

Guidance appreciated.

#### Versions

Fairlearn is 0.5.0 installed from pip.
Python is what came installed in the VM, show versions outputs:

```
System:
    python: 3.6.9 |Anaconda, Inc.| (default, Jul 30 2019, 19:07:31)  [GCC 7.3.0]
executable: /anaconda/envs/azureml_py36/bin/python
   machine: Linux-4.15.0-1100-azure-x86_64-with-debian-stretch-sid

Python dependencies:
       pip: 20.1.1
setuptools: 50.3.0.post20201006
   sklearn: 0.23.2
     numpy: 1.19.2
     scipy: 1.5.2
    Cython: 0.29.21
    pandas: 0.25.3
matplotlib: 3.3.2
    tempeh: 0.1.12
```
","29","0.4147823882571894","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","761594086","issue","https://github.com/Trusted-AI/AIF360/issues/652","Integration of smartnoise-samples documentation to Fairlearn documentation #### Is your feature request related to a problem? Please describe.
No

#### Describe the solution you'd like
I would like to have the Jupyter Notebooks for smartnoise-samples, a directory of the Microsoft Smartnoise project, added to the documentation for Fairlearn.

#### Describe alternatives you've considered, if relevant
We currently have the smartnoise-samples Jupyter Notebooks documented in the opendifferentialprivacy project, and to continue this would be an alternative. 

#### Additional context
I work for Harvard University in partnership with Microsoft on the Smartnoise project. Our colleagues at Microsoft have mentioned the possibility of integrating our documentation with Fairlearn documentation.
","7","0.4131887267427555","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","756686083","issue","https://github.com/Trusted-AI/AIF360/issues/650","The future of DCO / contributor license agreements in the Fairlearn project As far as I had understood we needed some form of agreement from contributors to contribute. If that's not the case then this is a very simple discussion thread.

The DCO currently introduces a lot of friction. I do not know of a single contributor who got this done from the start without issues. It also regularly trips me and other experienced developers up.

@adrinjalali @hildeweerts @MiroDudik @riedgar-ms wdyt?","20","0.9130637295194258","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","754694117","issue","https://github.com/Trusted-AI/AIF360/issues/648","Case study: chest x-ray classification Today we had a first session regarding chest x-ray classification following the paper ""CheXclusion: Fairness gaps in deep chest X-ray classifiers"" by Seyyed-Kalantari et al. (https://arxiv.org/pdf/2003.00827.pdf). All the notes and open questions are tracked in the scratchpad linked below. This includes several open questions we want to answer before deciding where this will go. A potential set of goals is outlined, but that may change depending on the answers we get. 

Scratchpad (read-only): https://hackmd.io/@nK2k69tBRwSK3K8_DNmRnQ/ByNE2_H9w

If you'd like to contribute to the notes please let @romanlutz know. If there's progress on any of the questions please post a quick note in this thread so that people are alerted.

@riedgar-ms @michaelamoako @brkifle @mmadaio @hildeweerts ","20","0.5552571281164169","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","752450906","issue","https://github.com/Trusted-AI/AIF360/issues/645","Add minimum package requirements re: minimum requirements, they should be in `setup.py` for them to actually take effect when the user installs either from source or a binary. We could also put them in `pyproject.toml` for pip to better understand it, and read those requirements from that file in `setup.py` to have them synced. We can even condition the minimum requirements on architecture and python version there.

This doesn't include the optional requirements though. For those we could have them in `requirements.txt` or any other place in the documentation.

_Originally posted by @adrinjalali in https://github.com/fairlearn/fairlearn/issues/605#issuecomment-734301366_","21","0.3116308311329059","Installation and shell commands","Deployment"
"https://github.com/fairlearn/fairlearn","750904883","issue","https://github.com/Trusted-AI/AIF360/issues/643","Creating group metric sets for regression fails #### Describe the bug

Unable to create group metric set for `regression` prediction types.

#### Steps/Code to Reproduce

Try to create a `regression` group metric set.

```python
dash_dict = _create_group_metric_set(y_true=y,
                                    predictions=y_pred,
                                    sensitive_features=sf,
                                    prediction_type='regression')
```

#### Expected Results
A dictionary containing the group metrics is created.

#### Actual Results
A dictionary containing the group metrics is, however, not created. Instead, we get the error below.

```python
---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
<ipython-input-24-96aab337f351> in <module>
      9                                     predictions=y_pred,
     10                                     sensitive_features=sf,
---> 11                                     prediction_type='regression')
     12 
     13 

/anaconda/envs/azureml_py36/lib/python3.6/site-packages/fairlearn/metrics/_group_metric_set.py in _create_group_metric_set(y_true, predictions, sensitive_features, prediction_type)
    145         function_dict = BINARY_CLASSIFICATION_METRICS
    146     elif prediction_type == REGRESSION:
--> 147         result[_PREDICTION_TYPE] == _PREDICTION_REGRESSION
    148         function_dict = REGRESSION_METRICS
    149     else:

KeyError: 'predictionType'

```

#### Versions

- OS: Linux
- Browser (if you're reporting a dashboard bug in jupyter): Chrome
- Python version: 3.6.9
- Fairlearn version: 0.4.6
- version of Python packages: 
```

System:
    python: 3.6.9 |Anaconda, Inc.| (default, Jul 30 2019, 19:07:31)  [GCC 7.3.0]
executable: /anaconda/envs/azureml_py36/bin/python
   machine: Linux-4.15.0-1098-azure-x86_64-with-debian-stretch-sid

Python dependencies:
       pip: 20.1.1
setuptools: 50.3.2
   sklearn: 0.22.2.post1
     numpy: 1.18.5
     scipy: 1.5.2
    Cython: 0.29.21
    pandas: 0.25.3
matplotlib: 3.2.1
    tempeh: None
```","12","0.498326982398839","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","747638525","issue","https://github.com/Trusted-AI/AIF360/issues/641","citation info, bibtex Hi, great work with the package, especially the interactive dashboard.
Is there any citation info for the software, or a Bibtex to copy?
https://github.com/fairlearn/fairlearn/search?q=citation","30","0.347630796482571","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","744465763","issue","https://github.com/Trusted-AI/AIF360/issues/639","Dashboard does not show up on Google Colab <!--
Before submitting a bug, please make sure the issue hasn't been already
addressed by searching through the past issues.

If your issue is a usage question, please submit it in one of these other
channels instead:
- StackOverflow with the `fairlearn` tag:
  https://stackoverflow.com/questions/tagged/fairlearn
- Gitter: https://gitter.im/fairlearn/community#
The issue tracker is used only to report bugs and feature requests. For
questions, please use either of the above platforms. Most question issues are
closed without an answer on this issue tracker. Thanks for your understanding.
-->

#### Describe the bug
<!--
I'm trying to use Farilearn on Google Colab but the Dashboard does not show up at all. Instead I just get a message <fairlearn.widget._fairlearn_dashboard.FairlearnDashboard at 0x7fc7de84deb8> without any Dashboard being displayed.
-->

#### Steps/Code to Reproduce
<!--
I'm using the sample example from https://fairlearn.github.io/v0.5.0/quickstart.html to get started in Colab but the Dashboard does not show up at all.
-->

```
Sample code to reproduce the problem
```

#### Expected Results
<!-- Example: No error is thrown. Please paste or describe the expected results.-->

#### Actual Results
<!-- Please paste or specifically describe the actual output or traceback. -->

#### Screenshots
<!-- If applicable, add screenshots to help explain your problem. -->

#### Versions
<!--
Please provide the following information:
- OS: [e.g. Windows]
- Browser (if you're reporting a dashboard bug in jupyter): [e.g. Edge, Firefox, Chrome, Safari]
- Python version: [e.g. 3.7.4]
- Fairlearn version: [e.g. 0.4.5 or installed from master branch in editable mode]
- version of Python packages: please run the following snippet and paste the output:
  ```python
  import fairlearn
  fairlearn.show_versions()
  ```
-->

<!-- Thanks for contributing! -->
","29","0.5156336554440818","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","741029239","issue","https://github.com/Trusted-AI/AIF360/issues/633","MetricFrame: missing error message #### Describe the bug


#### Steps/Code to Reproduce
![image](https://user-images.githubusercontent.com/10245648/98857843-7bce5e00-2414-11eb-8ed8-1b123440f733.png)



```
from fairlearn.metrics import MetricFrame
from sklearn.metrics import balanced_accuracy_score
MetricFrame(balanced_accuracy_score, [1], [1], sensitive_features=[0])
```

#### Versions

System:
    python: 3.7.6 (default, Jan  8 2020, 20:23:39) [MSC v.1916 64 bit (AMD64)]
executable: C:\Users\<alias>\Anaconda3\python.exe
   machine: Windows-10-10.0.19041-SP0

Python dependencies:
    Cython: 0.29.15
matplotlib: 3.2.2
     numpy: 1.19.0
    pandas: 1.0.5
       pip: 20.0.2
     scipy: 1.4.1
setuptools: 45.2.0.post20200210
   sklearn: 0.23.1
    tempeh: 0.1.12
","29","0.6814509894867039","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","739498242","issue","https://github.com/Trusted-AI/AIF360/issues/629"," Widgets getting refreshed when different widgets are run I initially installed fairlearn using pip install.
The fair widget was working perfectly.

First I'd run the widget on the base model

from fairlearn.widget import FairlearnDashboard
FairlearnDashboard(sensitive_features=sex,
                   sensitive_feature_names=['sex'],
                   y_true=y_true,
                   y_pred={""initial model"": y_pred}) 
Then I'd run the widget again on the mitigated model

from fairlearn.widget import FairlearnDashboard
FairlearnDashboard(sensitive_features=sex,
                   sensitive_feature_names=['sex'],
                   y_true=y_true,
                   y_pred={""mitigated model"": y_pred_miitgated}) 
the default version using pip install, I was able to see two widgets one above the another and it was super helpful for comparison.

CHANGED BEHAVIOR AFTER INSTALLING FROM GIT
If I run the dashboard now with the mitigated output, the first dashboard also changes. Firstly, the widget does not show up in the cell below and only comes with open in new tab option

Also, the widget above that was displaying the y_pred, changes to start and while selecting them, it shows the new output. So no more comparison in the notebook.

This is not the behavior in the 0.4.7 version installed by pip","29","0.7073642162231815","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","735212871","issue","https://github.com/Trusted-AI/AIF360/issues/621","'GridSearch' object has no attribute 'predictors_' AttributeError                            Traceback (most recent call last)
<ipython-input-502-cb592bed5a23> in <module>
      2           sensitive_features=A_train)
      3 
----> 4 predictors = sweep.predictors_

AttributeError: 'GridSearch' object has no attribute 'predictors_'","11","0.728611813718197","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","734916753","issue","https://github.com/Trusted-AI/AIF360/issues/619","cannot import name 'MetricFrame' ImportError                               Traceback (most recent call last)
<ipython-input-316-af1d762958be> in <module>
     17 
     18 # Metrics
---> 19 from fairlearn.metrics import (
     20     MetricFrame,
     21     selection_rate, demographic_parity_difference, demographic_parity_ratio,

ImportError: cannot import name 'MetricFrame'
","12","0.4359368331199314","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","729832394","issue","https://github.com/Trusted-AI/AIF360/issues/611","Small ratio bound in DP resulted in DummyClassifier in Exponentiated Gradient <!--
Before submitting a bug, please make sure the issue hasn't been already
addressed by searching through the past issues.

If your issue is a usage question, please submit it in one of these other
channels instead:
- StackOverflow with the `fairlearn` tag:
  https://stackoverflow.com/questions/tagged/fairlearn
- Gitter: https://gitter.im/fairlearn/community#
The issue tracker is used only to report bugs and feature requests. For
questions, please use either of the above platforms. Most question issues are
closed without an answer on this issue tracker. Thanks for your understanding.
-->

#### Describe the bug
Small ratio bounds in DP will result in DummyClassifier. 

#### Steps/Code to Reproduce

Change the following line in the first code block under **Exponentiated Gradient** section in notebook [Mitigating Disparities in Ranking from Binary Data](https://github.com/fairlearn/fairlearn/blob/master/notebooks/Mitigating%20Disparities%20in%20Ranking%20from%20Binary%20Data.ipynb)

```
constraints=DemographicParity(difference_bound=0.01),
```
to 
```
constraints=DemographicParity(ratio_bound=0.98),
```  
will trigger the bug. Debug logging need to be enabled to see 
```
redY had single value. Using DummyClassifier
```

My experiments are that ratios <=0.98 will trigger the bug, but 0.99 will not. This causes significant accuracy drop.

Is this intended or is this a bug? 
","11","0.5470512425725075","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","729104288","issue","https://github.com/Trusted-AI/AIF360/issues/610","DemographicParity init parameters The API reference page for [fairlearn.reductions](https://fairlearn.github.io/api_reference/fairlearn.reductions.html#fairlearn.reductions.DemographicParity) indicates that DemographicParity can take multiple parameters when instatiating and the [code on github](https://github.com/fairlearn/fairlearn/blob/master/fairlearn/reductions/_moments/utility_parity.py#L224-L248) indicates that it does as its base class is `UtilityParity`. Yet, when I try to execute the line instantiating dp in the example in the [user guide](https://fairlearn.github.io/user_guide/mitigation.html)  `dp = DemographicParity(difference_bound=0.01)`, I get this error
`TypeError: __init__() got an unexpected keyword argument 'difference_bound'`.

I installed the package using both pip and conda and in both cases, I get the same error. Is there anything I'm doing wrong ? If it's not the case, where would the problem possibly come from ?
","28","0.3681006493506495","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","729032955","issue","https://github.com/Trusted-AI/AIF360/issues/609","selection_rate_group_summary not listed in metrics reference page The function `selection_rate_group_summary` is used in the [quickstart page](https://fairlearn.github.io/quickstart.html). Yet, it is not listed in [API reference for the metrics package](https://fairlearn.github.io/api_reference/fairlearn.metrics.html). Actually, none of the functions `selection_rate_*` are listed in the API reference page. 

Is there a reason for this ? And if no, would it possible to add them, please ?","12","0.5367578465890284","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","728376314","issue","https://github.com/Trusted-AI/AIF360/issues/608","Documentation doesn't list charts When I go to the [a getting started page](https://fairlearn.github.io/auto_examples/plot_grid_search_census.html#training-a-fairness-unaware-predictor) I usually expect to see a full guide on how to use a library. Part of that is there, but ... there's no charts. In order to get the charts I need to run everything locally. This is a bummer of an experience, especially if you never worked with ipython widgets before. 

Wouldn't it be a lot more beginner-friendly just to show simple matplotlib charts in the docs? ","14","0.3364930336131926","Documentation","Development"
"https://github.com/fairlearn/fairlearn","725902627","issue","https://github.com/Trusted-AI/AIF360/issues/606","Doc Build Warning #### Describe the bug

Running sphinx to build, I see:
```
checking consistency... C:\Users\riedgar\source\repos\fairlearn\docs\contributor_guide\developer_call_notes\2020_10_08.rst: WARNING: document isn't included in any toctree
```

#### Steps/Code to Reproduce

See:
https://app.circleci.com/pipelines/github/fairlearn/fairlearn/1040/workflows/223c2925-34ab-4787-9af7-983c61e3ff6f/jobs/1643
as well

#### Expected Results

We shouldn't be getting a warning...

#### Actual Results

Get the above error

#### Screenshots


#### Versions
GitHub master branch
","14","0.3494712039015836","Documentation","Development"
"https://github.com/fairlearn/fairlearn","725566825","issue","https://github.com/Trusted-AI/AIF360/issues/601","Improving API Reference format I think our current API Reference could benefit from restructuring. Some ideas:

#### Structure API Reference in tables
The current format puts the documentation of all functions/classes of a module on a single page. As the pages are getting quite long and often contain different types of functionality (e.g., ""base"" metrics versus fairness metrics in the `metrics` module), it can be difficult to get a good overview of the available functionality. This also makes it harder than necessary to find the documentation of a particular function/class.

Instead, I'd suggest to list the functions/classes in tables with a short description of each item. We can put the actual documentation on a separate page for each function/class. We can put all tables on a single page, similar to [scikit-learn](https://scikit-learn.org/stable/modules/classes.html) or on a separate page for each module, similar to [pandas](https://pandas.pydata.org/docs/reference/index.html). My personal preference would go to the latter.

#### Ordering Attributes/Methods
The attributes and methods of a class are currently ordered alphabetically. 

I'd suggest to group them separately instead, similar to e.g., [this](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html#pandas.DataFrame) or [this](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression)

#### Make References Consistent
The usage of references seems inconsistent. In `fetch_boston()` the *References* docstring is used, in `fetch_adult()` the reference (including the full url and explicit *'Source:'*) is in the description, in `ThresholdOptimizer` a reference is linked as a clickable *name (year)* in the *Notes*, and all functions/classes in the `reductions` module link the reference as a clickable *name (year)* in the description. 

I'd suggest to follow the [docstring guide](https://numpydoc.readthedocs.io/en/latest/format.html) and put implementation details/background theory in the *Notes* and proper references in the *References* (i.e. not just links, as they might break).

#### See Also
I always like the *See Also* sections of classes/functions, as they can lead you to different parts of the package you may not have considered yourself, especially if you're new to the field. This is probably something that's not as relevant with the current size of the package, but might be nice to consider as Fairlearn grows.

---

Do other people have thoughts on this? Tagging @riedgar-ms @MiroDudik @romanlutz","14","0.6292561206996478","Documentation","Development"
"https://github.com/fairlearn/fairlearn","713882114","issue","https://github.com/Trusted-AI/AIF360/issues/593","Widgets getting refreshed when different widgets are run I initially installed fairlearn using pip install.
The fair widget was working perfectly. 

First I'd run the widget on the base model
```
from fairlearn.widget import FairlearnDashboard
FairlearnDashboard(sensitive_features=sex,
                   sensitive_feature_names=['sex'],
                   y_true=y_true,
                   y_pred={""initial model"": y_pred}) 
```

Then I'd run the widget again on the mitigated model

```
from fairlearn.widget import FairlearnDashboard
FairlearnDashboard(sensitive_features=sex,
                   sensitive_feature_names=['sex'],
                   y_true=y_true,
                   y_pred={""mitigated model"": y_pred_miitgated}) 
```

the default version using pip install, I was able to see two widgets one above the another and it was super helpful for comparison.

**CHANGED BEHAVIOR AFTER INSTALLING FROM GIT**
If I run the dashboard now with the mitigated output, the first dashboard also changes. Firstly, the widget does not show up in the cell below and only comes with _open in new tab_ option 

Also, the widget above that was displaying the y_pred, changes to start and while selecting them, it shows the new output. So no more comparison in the notebook.

This is not the behavior in the 0.4.6 version installed by pip","29","0.7047655864098104","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","712199166","issue","https://github.com/Trusted-AI/AIF360/issues/589","pypandoc missing in doc build #### Describe the bug

Likely related to #440 the documentation builds show a warning:

```
WARNING: 'pypandoc' not available. Using Sphinx-Gallery to convert rst text blocks to markdown for .ipynb files.
```


#### Steps/Code to Reproduce

See, a doc build. For example:
https://app.circleci.com/pipelines/github/fairlearn/fairlearn/938/workflows/05589729-ef9f-478f-befe-65f9084ba5f0/jobs/1430/parallel-runs/0/steps/0-104
","14","0.6847792207792209","Documentation","Development"
"https://github.com/fairlearn/fairlearn","711046143","issue","https://github.com/Trusted-AI/AIF360/issues/588","Add random_state to ThresholdOptimizer <!--
If your issue is a usage question, please submit it in one of these other
channels instead:
- StackOverflow with the `fairlearn` tag:
  https://stackoverflow.com/questions/tagged/fairlearn
- Gitter: https://gitter.im/fairlearn/community#
The issue tracker is used only to report bugs and feature requests. For
questions, please use either of the above platforms. Most question issues are
closed without an answer on this issue tracker. Thanks for your understanding.
-->

#### Is your feature request related to a problem? Please describe.
<!-- A clear and concise description of what the problem is. Ex. I'm always
frustrated when ... -->
The results of `ThresholdOptimizer` are currently not reproducible. Although `random_state` in `ThresholdOptimizer.predict()` helps to ensure reproducibility in the predictions, running `fit()` twice will result in a different optimizer object (and hence different predictions).

#### Describe the solution you'd like
<!-- A clear and concise description of what you want to happen. -->
I'd like a `random_state` argument for the `ThresholdOptimizer` object itself.
","11","0.7260132623177186","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","706856644","issue","https://github.com/Trusted-AI/AIF360/issues/583","Datasets module: keep or remove? As discussed in the [last developer call](https://fairlearn.github.io/contributor_guide/developer_call_notes/2020_09_17.html) I wanted to summarize the discussion around the datasets module so that everyone has a chance to comment including folks who didn't happen to be on that call.

Things to be aware of:
- The module is not part of a released version yet, so removing it would be simple since nobody could be using it yet.
- The reasons for including the module are technical:
  - show how to use a function (create ""minimal examples"")
  - benchmark algorithms, e.g., how well they optimize fairness/performance trade-off under specific quantitative definitions of fairness and performance
  
  but including datasets means we should address sociotechnical aspects. So far we've only scratched the surface of that, e.g. with the ""Notes"" and ""References"" in the [API documentation](https://fairlearn.github.io/api_reference/fairlearn.datasets.html)

- We don't add real functionality, just use functionality from `sklearn.datasets`, namely `fetch_openml`
  - e.g., https://github.com/fairlearn/fairlearn/pull/519
  - potential value prop: we can return sensitive features in addition to X, y
- We have been spending a lot of time discussing this every time a dataset gets added since there are no defined criteria for what gets added and what doesn't, and how much documentation there needs to be.

Options:
1. keep module, define inclusion criteria, and write documentation for each added dataset #507 
2. keep module, but don't spend more time on it until someone is willing to pick up tasks like #507, i.e. don't add more datasets for now
3. remove module, but move educational content into examples (i.e. preserve the documentation on why one needs to be careful with the Boston dataset, for example)


Specifically tagging @koaning and @hildeweerts , but also @MiroDudik @riedgar-ms @adrinjalali @kevinrobinson in case they have more to add. The ""default"" option would be (2) since that doesn't require any changes.

---

Stating my own opinion separately from the overall description: I'm in favor of (3) since it's just as easy to specify the one line of code that `sklearn` provides, and I'd rather see us spend time on other aspects of fairness. However, I'm mostly interested in settling this debate so that we can move on, so if there's consensus on any of these that's fine by me. ","25","0.2288509420275647","Research","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","705927637","issue","https://github.com/Trusted-AI/AIF360/issues/581","cannot import name 'BoundedGroupLoss' from 'fairlearn.reductions' Trying to implement BoundedGroupLoss but getting that cannot import  'BoundedGroupLoss' from 'fairlearn.reductions'.

Code was  `from  fairlearn.reductions import BoundedGroupLoss`

Fairlearn package version is latest and other fairlearn classifiers and widgets are working","28","0.4825670783117592","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","704890747","issue","https://github.com/Trusted-AI/AIF360/issues/580","Can't use sklearn.Pipeline with GridSearch. Hello,

I may be wrong, but GridSearch does not allow [sklearn.Pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) for its estimator.
This would be a useful feature have!

## How to reproduce.
In `test/unit/reductions/grid_search/test_grid_search_arguments.py` add:

```python
# Additional Imports
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler

# Set up Pipeline estimator
class TestPipelineEstimator(ConditionalOpportunityTests):
    def setup_method(self, method):
        self.estimator = Pipeline([('scaler', StandardScaler()), ('logistic', LogisticRegression(solver='liblinear'))])
        self.disparity_criterion = DemographicParity()
```

Then : `pytest test/unit/reductions/grid_search/test_grid_search_arguments.py` would fail with:

```
ValueError: Pipeline.fit does not accept the sample_weight parameter. You can pass parameters to specific steps of your pipeline using the stepname__parameter format, e.g. `Pipeline.fit(X, y, logisticregression__sample_weight=sample_weight)`.
```


### Proposed solution

In GridSearch constructor, we could allow specifying the key for `sample_weight`.

Ex. `GridSearch(self.estimator, self.disparity_criterion, grid_size=2, sample_weight_key='logistic__sample_weight')`

I already made the test so I can add the solution too if it is approved.
","23","0.403799819561641","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","698121216","issue","https://github.com/Trusted-AI/AIF360/issues/571","FairLearn does not function in JupyterLab FairLearn does not seem to work in JupyterLab or other custom Jupyter based editors. In a JupyterLab Instance the FairLearn dashboard does not seem to load, and displays-

```
Loading Widgets...
```

However, the same example code works well in a Jupyter Notebook and is able to render the FairLearn Dashboard widget. I also tried updating `ipywidget` which did not seem to solve the problem.

I have also tested with a JupyterLab on Several base OSes, tried using it within a non-GPU instance, and also tried using Colaboratory (based on Jupyter) none of which seems to solve the bug. Thus, I feel this would be a JupyterLab specific issue.","24","0.8137897378694923","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","695690834","issue","https://github.com/Trusted-AI/AIF360/issues/570","Consider changing logo link in the documentation page from contents to index #### Describe the issue linked to the documentation

Clicking the logo leads to contents instead of index. This is because `pydata-sphinx-theme` sets logo href to `master_doc` (which in `fairlearn` case is contents).

#### Suggest a potential alternative/fix
I'm not sure if linking the logo to `index` instead of `content` is really what `fairlearn` wants, but an extra opinion from `fairlearn` maintainers would be awesome.

I have created an issue in `pydata-sphinx-theme` and will mention this issue over there (https://github.com/pandas-dev/pydata-sphinx-theme/issues/245)


","14","0.5989891434917616","Documentation","Development"
"https://github.com/fairlearn/fairlearn","694147047","issue","https://github.com/Trusted-AI/AIF360/issues/569","Picklable objects (currently throws an error) #### Is your feature request related to a problem? Please describe.
```python
from sklearn.tree import DecisionTreeClassifier
from fairlearn.reductions import TruePositiveRateDifference, ExponentiatedGradient
clf = ExponentiatedGradient(DecisionTreeClassifier(), TruePositiveRateDifference())
clf.fit(X, y, sensitive_features=A)

import joblib
joblib.dump(clf, 'file.pkl')

# or
# import pickle; pickle.dumps(clf)
```
Throws the error: ```AttributeError: Can't pickle local object '_Lagrangian.best_h.<locals>.h'```

#### Describe the solution you'd like
I'd like to be able to pickle a fitted classifier/reductions object, or persist it in some other way.
I have searched the API docs and this doesn't seem to be a feature.

#### Additional context
It seems that the problem is some object that was defined locally and thus cannot be imported.
Maybe exposing this object in the package's namespace would be a solution (?)
","11","0.550023132084201","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","683100749","issue","https://github.com/Trusted-AI/AIF360/issues/564","Pre-fit warning/error for post-processing estimator In the [UCI Credit-card Default classification notebook
](https://github.com/fairlearn/fairlearn/blob/master/notebooks/Binary%20Classification%20with%20the%20UCI%20Credit-card%20Default%20Dataset.ipynb) when the fit() function is called on the ThresholdOptimizer() pipeline, it shows the below warning.

This could be expected because the model was already fit before leveraging this post processing estimator, but the error message shown below could lead to confusion and should be addressed with further comments in the notebook.

![image](https://user-images.githubusercontent.com/67665799/90826076-b67d9a00-e2ee-11ea-894f-b3dc7213280c.png)

<!--
Tell us about the confusion introduced in the documentation.
-->

#### Suggest a potential alternative/fix

<!--
Tell us how we could improve the documentation in this regard.
-->
","14","0.2589474128471343","Documentation","Development"
"https://github.com/fairlearn/fairlearn","681586345","issue","https://github.com/Trusted-AI/AIF360/issues/562","Running `predictors = sweep.predictors_` does not work in the GridSearch with Census Data notebook example <!--
Before submitting a bug, please make sure the issue hasn't been already
addressed by searching through the past issues.

If your issue is a usage question, please submit it in one of these other
channels instead:
- StackOverflow with the `fairlearn` tag:
  https://stackoverflow.com/questions/tagged/fairlearn
- Gitter: https://gitter.im/fairlearn/community#
The issue tracker is used only to report bugs and feature requests. For
questions, please use either of the above platforms. Most question issues are
closed without an answer on this issue tracker. Thanks for your understanding.
-->

#### Describe the bug
I am following the GridSearch with Census Data notebook example (https://fairlearn.github.io/auto_examples/plot_grid_search_census.html) but when I tried running the code `predictors = sweep.predictors_` in the notebook example, I get an error.

#### Steps/Code to Reproduce
`predictors = sweep.predictors_`

#### Expected Results
Expected it to run without any errors, as per the notebook example.

#### Actual Results
Getting the following error:
```
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-14-cb592bed5a23> in <module>
      2           sensitive_features=A_train)
      3 
----> 4 predictors = sweep.predictors_

AttributeError: 'GridSearch' object has no attribute 'predictors_'
```

#### Versions

Please provide the following information:
- OS: Windows
- Browser: Chrome
- Python version: 3.7.6
- Fairlearn version: 0.4.6
- version of Python packages: 
```
System:
    python: 3.7.6 (default, Jan  8 2020, 20:23:39) [MSC v.1916 64 bit (AMD64)]
executable: C:\Users\HP\Anaconda3\python.exe
   machine: Windows-7-6.1.7601-SP1

Python dependencies:
       pip: 20.0.2
setuptools: 46.2.0.post20200511
   sklearn: 0.22.1
     numpy: 1.18.1
     scipy: 1.4.1
    Cython: 0.29.17
    pandas: 1.0.3
matplotlib: 3.1.3
    tempeh: None
```

<!-- Thanks for contributing! -->

**EDIT**: It appears this was previously raised here: https://github.com/fairlearn/fairlearn/issues/495. And the solution is to use `predictors = sweep._predictors`. Apologies, I did not see that this issue has been previously raised. In any case, can we update the documentation on this? If not please close this issue.","11","0.5345013826477704","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","679196764","issue","https://github.com/Trusted-AI/AIF360/issues/558","Interactive dashboard doesn't show on Azure Compute Instances, either JupyterLab or Jupyter Notebooks If I setup a local Conda environment and run FairLearn dashboards, it works:

![image](https://user-images.githubusercontent.com/32428960/90260627-0f12ea00-de4c-11ea-9c2d-bbd1f1bd35ed.png)

If I run the same notebook -and libraries- on an Azure Compute Instance, it doesn't. More specifically:

- On JupyterLab, it reports ""Error displaying widget: model not found""

![image](https://user-images.githubusercontent.com/32428960/90260504-df63e200-de4b-11ea-82d2-6b4bd0bf5946.png)


- On a Jupyter Notebook (running on Azure Compute Instance), the dashboard shows up but when I try using it, it gets stuck on this window:

![image](https://user-images.githubusercontent.com/32428960/90259875-066de400-de4b-11ea-8962-899c3a73ad48.png)


I checked the pip libraries, I simply have much more on Azure than locally but the common ones are the same (latest) version, installed today.
I also read that JupyterLab is still non supported, but I supposed that Jupyter Notebooks are supported, even on Azure, aren't they?

Thanks

","24","0.6281378986177824","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","678075047","issue","https://github.com/Trusted-AI/AIF360/issues/556","""GroupLossMoment"" versus ""BoundedGroupLoss"" Hello, I've installed fairlearn using pip and in the init script, ""GroupLossMoment"" is present while ""BoundedGroupLoss"" isn't. Comparing this script to the one found in the Github, they're not consistent, which I'm assuming is because I've installed fairlearn with pip instead of pulling from the repo. Does  ""GroupLossMoment"" work comparably to ""BoundedGroupLoss""?","21","0.3873699803561652","Installation and shell commands","Deployment"
"https://github.com/fairlearn/fairlearn","677307512","issue","https://github.com/Trusted-AI/AIF360/issues/552","'GridSearch' object has no attribute 'predictors_' Hi there, I am running the notebook: Binary Classification with the UCI Credit-Card Default Dataset.
When running the gridsearch component I get an error when trying to get the scores. I have made no alterations to the notebook. 

![image](https://user-images.githubusercontent.com/48265599/89964665-2015fc80-dc8e-11ea-979b-1a01feee2409.png)

#### Versions
- OS: Windows
- Browser: Edge
- Python version: 3.7.4
- Fairlearn version: 0.4.6


","29","0.4202401372212694","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","675060949","issue","https://github.com/Trusted-AI/AIF360/issues/549","Quickstart examples link goes to SciKit-Learn #### Describe the issue linked to the documentation

In the ""What's Next"" section on:
https://fairlearn.github.io/quickstart.html
the ""Examples"" link goes to the SciKitLearn website

#### Suggest a potential alternative/fix

We might want to link to our own examples","14","0.9247129681912294","Documentation","Development"
"https://github.com/fairlearn/fairlearn","672859001","issue","https://github.com/Trusted-AI/AIF360/issues/544","Bank Marketing dataset addition needs documentation review/correction #### Describe the issue linked to the documentation

Link: https://github.com/fairlearn/fairlearn/blob/master/fairlearn/datasets/_fetch_bank_marketing.py#L69
The filename and code within suggests that the dataset incorporated is UCI bank marketing dataset. However, the description is incorrect (it is of UCI adult dataset).

#### Suggest a potential alternative/fix

Correction in the documentation/comments of the code.
The dataset is fetched from https://archive.ics.uci.edu/ml/datasets/bank+marketing
","0","0.5763611215893378","Dataset usage","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","672370488","issue","https://github.com/Trusted-AI/AIF360/issues/543","FalsePositiveRateParity and TruePositiveRateParity can't handle absence of positives/negatives, respectively If there are no positives then there's nothing to take into account for TPRP, and if there are no negatives there's nothing to take into account for FPRP. Currently that results in an exception, but we need to fix that according to Miro's suggestion from #442:

>The fix needs to happen in `UtilityParity`, because this bug occurs whenever we have zero events. In that case, we should just have zero constraints (vacuous constraints). One source of bug is using `size()` to calculate probabilities... this should be replaced by `count()`, which will correctly return an empty data frame.

_Originally posted by @MiroDudik in https://github.com/fairlearn/fairlearn/diffs_","7","0.1723954877239548","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","670028256","issue","https://github.com/Trusted-AI/AIF360/issues/539","Demographic Parity in Grid Search For Regression I modified the plot_grid_search_census example notebook given in the Examples Notebook. I want to modify it run it for regression task.  I used fetch_bostaon and LinearRegression instead of LogisticRegression. I also made required changes to train_text_split.  It 
runs fine  till the point 
sweep = GridSearch(LinearRegression(fit_intercept=True), constraints=DemographicParity(), grid_size=71)
However the next command 
sweep.fit(X_train, Y_train, sensitive_features=A_train)
gives the following error
""ValueError: Supplied y labels are not 0 or 1""
However the Mitigation section on the User Guide says that Grid search is available for regression also. Can you please tell me how to use the fit and predict methods for regression.
Thank you","28","0.4147325141628612","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","669557646","issue","https://github.com/Trusted-AI/AIF360/issues/537","Bug in group_summary function of _metrics_engine.py and _group_metric_set.py <!--
Before submitting a bug, please make sure the issue hasn't been already
addressed by searching through the past issues.

If your issue is a usage question, please submit it in one of these other
channels instead:
- StackOverflow with the `fairlearn` tag:
  https://stackoverflow.com/questions/tagged/fairlearn
- Gitter: https://gitter.im/fairlearn/community#
The issue tracker is used only to report bugs and feature requests. For
questions, please use either of the above platforms. Most question issues are
closed without an answer on this issue tracker. Thanks for your understanding.
-->

#### Describe the bug

When I call  _create_group_metric_set as indicated online at https://docs.microsoft.com/en-us/azure/machine-learning/how-to-machine-learning-fairness-aml , I get the error 
```
Array y_pred is not the same size as y_true
```
, which is clearly not true because I'm passing same-size arrays for y_true and predictions (or even the same variable!).

Please note that the error says ""Array **y_pred** is not the same size as y_true"", rather than ""Array **predictions** is not the same size as y_true"", since the parameter name is **predictions**, not y_pred (as I think it should be in alignment to other functions).

However, if I print **len(y_true)** and **len(y_pred)** at row 133 of **_metrics_engine.py** (in group_summary function), it tells me that y_pred length is 1, rather than the real size I passed, which is the same as y_true.

That function is called by the instruction at row 169 of **_group_metric_set.py**:
>>> gmr = metric_func(result[_Y_TRUE], prediction, sensitive_features=g[_BIN_VECTOR])

where ""prediction"" is a single element of the predictions array. As a result, y_pred array becomes 1 element long, causing the ""not same size"" error reported above.

Thanks,

Mauro Minella, mauro.minella@microsoft.com


#### Steps/Code to Reproduce
<!--
Please add a minimal example (in the form of code) that reproduces the error.
Be as succinct as possible, do not depend on external data. In short, we are
going to copy-paste your code and we expect to get the same result as you.

Example:
```python
import pandas as pd
from fairlearn.reductions import ExponentiatedGradient, DemographicParity
from sklearn.linear_model import LinearRegression
from fairlearn.datasets import fetch_adult

data = fetch_adult(as_frame=True)
X = pd.get_dummies(data.data)
y = (data.target == '>50K') * 1
sensitive_features = data.data['sex']
mitigator = ExponentiatedGradient(LinearRegression(), DemographicParity())
mitigator.fit(X, y, sensitive_features=sensitive_features)
```
If the code is too long, feel free to put it in a public gist and link
it in the issue: https://gist.github.com
-->

Simply run the following command, making sure that Y_test and ys_pred array have the same shape.
```dash_dict = _create_group_metric_set(
    y_true=Y_test,
    predictions=ys_pred,
    sensitive_features=sf_dict,
    prediction_type='binary_classification')
```

#### Expected Results
<!-- Example: No error is thrown. Please paste or describe the expected results.-->
Avoid the error below

#### Actual Results
<!-- Please paste or specifically describe the actual output or traceback. -->
```
ValueError: Array y_pred is not the same size as y_true
```
#### Screenshots
<!-- If applicable, add screenshots to help explain your problem. -->

#### Versions
<!--
Please provide the following information:
- OS: [e.g. Windows]
- Browser (if you're reporting a dashboard bug in jupyter): [e.g. Edge, Firefox, Chrome, Safari]
- Python version: [e.g. 3.7.4]
- Fairlearn version: [e.g. 0.4.5 or installed from master branch in editable mode]
- version of Python packages: please run the following snippet and paste the output:
  ```python
  import fairlearn
  fairlearn.show_versions()
  ```
-->

<!-- Thanks for contributing! -->
","12","0.4185973724301252","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","666010361","issue","https://github.com/Trusted-AI/AIF360/issues/530","Datasets in Fairlearn? I am having trouble with the following when I tried it out in Colab.

```
!pip install fairlearn
from fairlearn.datasets import fetch_adult
```

It does not seem to find datasets module and throws up the following error.

```
      1 get_ipython().system('pip3 install fairlearn')
----> 2 from fairlearn.datasets import fetch_adult
      3 
ModuleNotFoundError: No module named 'fairlearn.datasets'
```

Same issue when I tried using conda in my system. Any help is appreciated. 
","5","0.3704935064935067","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","661414346","issue","https://github.com/Trusted-AI/AIF360/issues/524","Fairlearn Dashboard sensitive feature selection does not carry over from model comparison view to individual model view #### Describe the bug
In the model comparison view we can select a sensitive feature and change that selection using a dropdown:
![image](https://user-images.githubusercontent.com/10245648/87904364-fe20b580-ca12-11ea-82b8-f5639b5cc8a5.png)

However, when switching to the individual model view by clicking on a model point in the chart the selection does not carry over. The same applies when changing the selection and switching back to the comparison view.

#### Expected Results
The selection should carry over.

@chisingh FYI","12","0.5575139146567719","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","661322463","issue","https://github.com/Trusted-AI/AIF360/issues/523","Update dashboard documentation based on new design #### Describe the issue linked to the documentation

With a recent PR (#283 ) the dashboard changed a lot. We need to update the documentation accordingly.


","14","0.4400889630247426","Documentation","Development"
"https://github.com/fairlearn/fairlearn","658598324","issue","https://github.com/Trusted-AI/AIF360/issues/518","Setup fails to install shap due to missing Microsoft Visual C++ <!--
Before submitting a bug, please make sure the issue hasn't been already
addressed by searching through the past issues.

If your issue is a usage question, please submit it in one of these other
channels instead:
- StackOverflow with the `fairlearn` tag:
  https://stackoverflow.com/questions/tagged/fairlearn
- Gitter: https://gitter.im/fairlearn/community#
The issue tracker is used only to report bugs and feature requests. For
questions, please use either of the above platforms. Most question issues are
closed without an answer on this issue tracker. Thanks for your understanding.
-->

#### Describe the bug
<!--
-->
Setup for installing fairlearn throws an error as it's unable to install shap

#### Steps/Code to Reproduce
<!--
Please add a minimal example (in the form of code) that reproduces the error.
Be as succinct as possible, do not depend on external data. In short, we are
going to copy-paste your code and we expect to get the same result as you.

Example:
```python
import pandas as pd
from fairlearn.reductions import ExponentiatedGradient, DemographicParity
from sklearn.linear_model import LinearRegression
from fairlearn.datasets import fetch_adult

data = fetch_adult(as_frame=True)
X = pd.get_dummies(data.data)
y = (data.target == '>50K') * 1
sensitive_features = data.data['sex']
mitigator = ExponentiatedGradient(LinearRegression(), DemographicParity())
mitigator.fit(X, y, sensitive_features=sensitive_features)
```
If the code is too long, feel free to put it in a public gist and link
it in the issue: https://gist.github.com
-->

```
In a new conda env, run these:
git clone git@github.com:fairlearn/fairlearn.git
pip install -e .
pip install -r requirements.txt
```

#### Expected Results
<!-- Example: No error is thrown. Please paste or describe the expected results.-->
Successful install and no errors

#### Actual Results
<!-- Please paste or specifically describe the actual output or traceback. -->

Building wheels for collected packages: shap
  Building wheel for shap (setup.py) ... error
  ERROR: Command errored out with exit status 1:
   command: 'C:\Users\arjsingh\Miniconda3\envs\fair_learn\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'C:\\Users\\arjsingh\\AppData\\Local\\Temp\\pip-install-h00nraer\\shap\\setup.py'""'""'; __file__='""'""'C:\\Users\\arjsingh\\AppData\\Local\\Temp\\pip-install-h00nraer\\shap\\setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d 'C:\Users\arjsingh\AppData\Local\Temp\pip-wheel-f_zgw3dc'
       cwd: C:\Users\arjsingh\AppData\Local\Temp\pip-install-h00nraer\shap\
  Complete output (67 lines):
  running bdist_wheel
  running build
  running build_py
  creating build
  creating build\lib.win-amd64-3.7
  creating build\lib.win-amd64-3.7\shap
  copying shap\common.py -> build\lib.win-amd64-3.7\shap
  copying shap\datasets.py -> build\lib.win-amd64-3.7\shap
  copying shap\__init__.py -> build\lib.win-amd64-3.7\shap
  creating build\lib.win-amd64-3.7\shap\explainers
  copying shap\explainers\additive.py -> build\lib.win-amd64-3.7\shap\explainers
  copying shap\explainers\bruteforce.py -> build\lib.win-amd64-3.7\shap\explainers
  copying shap\explainers\explainer.py -> build\lib.win-amd64-3.7\shap\explainers
  copying shap\explainers\gradient.py -> build\lib.win-amd64-3.7\shap\explainers
  copying shap\explainers\kernel.py -> build\lib.win-amd64-3.7\shap\explainers
  copying shap\explainers\linear.py -> build\lib.win-amd64-3.7\shap\explainers
  copying shap\explainers\mimic.py -> build\lib.win-amd64-3.7\shap\explainers
  copying shap\explainers\partition.py -> build\lib.win-amd64-3.7\shap\explainers
  copying shap\explainers\permutation.py -> build\lib.win-amd64-3.7\shap\explainers
  copying shap\explainers\pytree.py -> build\lib.win-amd64-3.7\shap\explainers
  copying shap\explainers\sampling.py -> build\lib.win-amd64-3.7\shap\explainers
  copying shap\explainers\tf_utils.py -> build\lib.win-amd64-3.7\shap\explainers
  copying shap\explainers\tree.py -> build\lib.win-amd64-3.7\shap\explainers
  copying shap\explainers\__init__.py -> build\lib.win-amd64-3.7\shap\explainers
  creating build\lib.win-amd64-3.7\shap\explainers\other
  copying shap\explainers\other\coefficent.py -> build\lib.win-amd64-3.7\shap\explainers\other
  copying shap\explainers\other\lime.py -> build\lib.win-amd64-3.7\shap\explainers\other
  copying shap\explainers\other\maple.py -> build\lib.win-amd64-3.7\shap\explainers\other
  copying shap\explainers\other\random.py -> build\lib.win-amd64-3.7\shap\explainers\other
  copying shap\explainers\other\treegain.py -> build\lib.win-amd64-3.7\shap\explainers\other
  copying shap\explainers\other\__init__.py -> build\lib.win-amd64-3.7\shap\explainers\other
  creating build\lib.win-amd64-3.7\shap\explainers\deep
  copying shap\explainers\deep\deep_pytorch.py -> build\lib.win-amd64-3.7\shap\explainers\deep
  copying shap\explainers\deep\deep_tf.py -> build\lib.win-amd64-3.7\shap\explainers\deep
  copying shap\explainers\deep\__init__.py -> build\lib.win-amd64-3.7\shap\explainers\deep
  creating build\lib.win-amd64-3.7\shap\plots
  copying shap\plots\bar.py -> build\lib.win-amd64-3.7\shap\plots
  copying shap\plots\colorconv.py -> build\lib.win-amd64-3.7\shap\plots
  copying shap\plots\colors.py -> build\lib.win-amd64-3.7\shap\plots
  copying shap\plots\decision.py -> build\lib.win-amd64-3.7\shap\plots
  copying shap\plots\dependence.py -> build\lib.win-amd64-3.7\shap\plots
  copying shap\plots\embedding.py -> build\lib.win-amd64-3.7\shap\plots
  copying shap\plots\force.py -> build\lib.win-amd64-3.7\shap\plots
  copying shap\plots\force_matplotlib.py -> build\lib.win-amd64-3.7\shap\plots
  copying shap\plots\image.py -> build\lib.win-amd64-3.7\shap\plots
  copying shap\plots\monitoring.py -> build\lib.win-amd64-3.7\shap\plots
  copying shap\plots\partial_dependence.py -> build\lib.win-amd64-3.7\shap\plots
  copying shap\plots\summary.py -> build\lib.win-amd64-3.7\shap\plots
  copying shap\plots\text.py -> build\lib.win-amd64-3.7\shap\plots
  copying shap\plots\waterfall.py -> build\lib.win-amd64-3.7\shap\plots
  copying shap\plots\__init__.py -> build\lib.win-amd64-3.7\shap\plots
  creating build\lib.win-amd64-3.7\shap\benchmark
  copying shap\benchmark\experiments.py -> build\lib.win-amd64-3.7\shap\benchmark
  copying shap\benchmark\measures.py -> build\lib.win-amd64-3.7\shap\benchmark
  copying shap\benchmark\methods.py -> build\lib.win-amd64-3.7\shap\benchmark
  copying shap\benchmark\metrics.py -> build\lib.win-amd64-3.7\shap\benchmark
  copying shap\benchmark\models.py -> build\lib.win-amd64-3.7\shap\benchmark
  copying shap\benchmark\plots.py -> build\lib.win-amd64-3.7\shap\benchmark
  copying shap\benchmark\__init__.py -> build\lib.win-amd64-3.7\shap\benchmark
  creating build\lib.win-amd64-3.7\shap\plots\resources
  copying shap\plots\resources\bundle.js -> build\lib.win-amd64-3.7\shap\plots\resources
  copying shap\plots\resources\logoSmallGray.png -> build\lib.win-amd64-3.7\shap\plots\resources
  copying shap\tree_shap.h -> build\lib.win-amd64-3.7\shap
  running build_ext
  numpy.get_include() C:\Users\arjsingh\Miniconda3\envs\fair_learn\lib\site-packages\numpy\core\include
  building 'shap._cext' extension
  error: Microsoft Visual C++ 14.0 is required. Get it with ""Build Tools for Visual Studio"": https://visualstudio.microsoft.com/downloads/
  ----------------------------------------
  ERROR: Failed building wheel for shap
  Running setup.py clean for shap
Failed to build shap
Installing collected packages: shap, psutil, memory-profiler, tempeh, xlrd, qtpy, qtconsole, jupyter-console, jupyter, nbval, sphinxcontrib-devhelp, sphinxcontrib-jsmath, imagesize, alabaster, sphinxcontrib-qthelp, sphinxcontrib-serializinghtml, sphinxcontrib-htmlhelp, babel, sphinxcontrib-applehelp, sphinx, sphinx-gallery, pydata-sphinx-theme
    Running setup.py install for shap ... error
    ERROR: Command errored out with exit status 1:
     command: 'C:\Users\arjsingh\Miniconda3\envs\fair_learn\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'C:\\Users\\arjsingh\\AppData\\Local\\Temp\\pip-install-h00nraer\\shap\\setup.py'""'""'; __file__='""'""'C:\\Users\\arjsingh\\AppData\\Local\\Temp\\pip-install-h00nraer\\shap\\setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' install --record 'C:\Users\arjsingh\AppData\Local\Temp\pip-record-cp20g2kq\install-record.txt' --single-version-externally-managed --compile --install-headers 'C:\Users\arjsingh\Miniconda3\envs\fair_learn\Include\shap'
         cwd: C:\Users\arjsingh\AppData\Local\Temp\pip-install-h00nraer\shap\
    Complete output (67 lines):
    running install
    running build
    running build_py
    creating build
    creating build\lib.win-amd64-3.7
    creating build\lib.win-amd64-3.7\shap
    copying shap\common.py -> build\lib.win-amd64-3.7\shap
    copying shap\datasets.py -> build\lib.win-amd64-3.7\shap
    copying shap\__init__.py -> build\lib.win-amd64-3.7\shap
    creating build\lib.win-amd64-3.7\shap\explainers
    copying shap\explainers\additive.py -> build\lib.win-amd64-3.7\shap\explainers
    copying shap\explainers\bruteforce.py -> build\lib.win-amd64-3.7\shap\explainers
    copying shap\explainers\explainer.py -> build\lib.win-amd64-3.7\shap\explainers
    copying shap\explainers\gradient.py -> build\lib.win-amd64-3.7\shap\explainers
    copying shap\explainers\kernel.py -> build\lib.win-amd64-3.7\shap\explainers
    copying shap\explainers\linear.py -> build\lib.win-amd64-3.7\shap\explainers
    copying shap\explainers\mimic.py -> build\lib.win-amd64-3.7\shap\explainers
    copying shap\explainers\partition.py -> build\lib.win-amd64-3.7\shap\explainers
    copying shap\explainers\permutation.py -> build\lib.win-amd64-3.7\shap\explainers
    copying shap\explainers\pytree.py -> build\lib.win-amd64-3.7\shap\explainers
    copying shap\explainers\sampling.py -> build\lib.win-amd64-3.7\shap\explainers
    copying shap\explainers\tf_utils.py -> build\lib.win-amd64-3.7\shap\explainers
    copying shap\explainers\tree.py -> build\lib.win-amd64-3.7\shap\explainers
    copying shap\explainers\__init__.py -> build\lib.win-amd64-3.7\shap\explainers
    creating build\lib.win-amd64-3.7\shap\explainers\other
    copying shap\explainers\other\coefficent.py -> build\lib.win-amd64-3.7\shap\explainers\other
    copying shap\explainers\other\lime.py -> build\lib.win-amd64-3.7\shap\explainers\other
    copying shap\explainers\other\maple.py -> build\lib.win-amd64-3.7\shap\explainers\other
    copying shap\explainers\other\random.py -> build\lib.win-amd64-3.7\shap\explainers\other
    copying shap\explainers\other\treegain.py -> build\lib.win-amd64-3.7\shap\explainers\other
    copying shap\explainers\other\__init__.py -> build\lib.win-amd64-3.7\shap\explainers\other
    creating build\lib.win-amd64-3.7\shap\explainers\deep
    copying shap\explainers\deep\deep_pytorch.py -> build\lib.win-amd64-3.7\shap\explainers\deep
    copying shap\explainers\deep\deep_tf.py -> build\lib.win-amd64-3.7\shap\explainers\deep
    copying shap\explainers\deep\__init__.py -> build\lib.win-amd64-3.7\shap\explainers\deep
    creating build\lib.win-amd64-3.7\shap\plots
    copying shap\plots\bar.py -> build\lib.win-amd64-3.7\shap\plots
    copying shap\plots\colorconv.py -> build\lib.win-amd64-3.7\shap\plots
    copying shap\plots\colors.py -> build\lib.win-amd64-3.7\shap\plots
    copying shap\plots\decision.py -> build\lib.win-amd64-3.7\shap\plots
    copying shap\plots\dependence.py -> build\lib.win-amd64-3.7\shap\plots
    copying shap\plots\embedding.py -> build\lib.win-amd64-3.7\shap\plots
    copying shap\plots\force.py -> build\lib.win-amd64-3.7\shap\plots
    copying shap\plots\force_matplotlib.py -> build\lib.win-amd64-3.7\shap\plots
    copying shap\plots\image.py -> build\lib.win-amd64-3.7\shap\plots
    copying shap\plots\monitoring.py -> build\lib.win-amd64-3.7\shap\plots
    copying shap\plots\partial_dependence.py -> build\lib.win-amd64-3.7\shap\plots
    copying shap\plots\summary.py -> build\lib.win-amd64-3.7\shap\plots
    copying shap\plots\text.py -> build\lib.win-amd64-3.7\shap\plots
    copying shap\plots\waterfall.py -> build\lib.win-amd64-3.7\shap\plots
    copying shap\plots\__init__.py -> build\lib.win-amd64-3.7\shap\plots
    creating build\lib.win-amd64-3.7\shap\benchmark
    copying shap\benchmark\experiments.py -> build\lib.win-amd64-3.7\shap\benchmark
    copying shap\benchmark\measures.py -> build\lib.win-amd64-3.7\shap\benchmark
    copying shap\benchmark\methods.py -> build\lib.win-amd64-3.7\shap\benchmark
    copying shap\benchmark\metrics.py -> build\lib.win-amd64-3.7\shap\benchmark
    copying shap\benchmark\models.py -> build\lib.win-amd64-3.7\shap\benchmark
    copying shap\benchmark\plots.py -> build\lib.win-amd64-3.7\shap\benchmark
    copying shap\benchmark\__init__.py -> build\lib.win-amd64-3.7\shap\benchmark
    creating build\lib.win-amd64-3.7\shap\plots\resources
    copying shap\plots\resources\bundle.js -> build\lib.win-amd64-3.7\shap\plots\resources
    copying shap\plots\resources\logoSmallGray.png -> build\lib.win-amd64-3.7\shap\plots\resources
    copying shap\tree_shap.h -> build\lib.win-amd64-3.7\shap
    running build_ext
    numpy.get_include() C:\Users\arjsingh\Miniconda3\envs\fair_learn\lib\site-packages\numpy\core\include
    building 'shap._cext' extension
    error: Microsoft Visual C++ 14.0 is required. Get it with ""Build Tools for Visual Studio"": https://visualstudio.microsoft.com/downloads/
    ----------------------------------------
ERROR: Command errored out with exit status 1: 'C:\Users\arjsingh\Miniconda3\envs\fair_learn\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'C:\\Users\\arjsingh\\AppData\\Local\\Temp\\pip-install-h00nraer\\shap\\setup.py'""'""'; __file__='""'""'C:\\Users\\arjsingh\\AppData\\Local\\Temp\\pip-install-h00nraer\\shap\\setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' install --record 'C:\Users\arjsingh\AppData\Local\Temp\pip-record-cp20g2kq\install-record.txt' --single-version-externally-managed --compile --install-headers 'C:\Users\arjsingh\Miniconda3\envs\fair_learn\Include\shap' Check the logs for full command output.

#### Screenshots
<!-- If applicable, add screenshots to help explain your problem. -->

#### Versions
<!--
Please provide the following information:
- OS: [e.g. Windows]
- Browser (if you're reporting a dashboard bug in jupyter): [e.g. Edge, Firefox, Chrome, Safari]
- Python version: [e.g. 3.7.4]
- Fairlearn version: [e.g. 0.4.5 or installed from master branch in editable mode]
- version of Python packages: please run the following snippet and paste the output:
  ```python
  import fairlearn
  fairlearn.show_versions()
  ```
-->

<!-- Thanks for contributing! -->
","4","0.8277815857311019","Installation and shell commands","Deployment"
"https://github.com/fairlearn/fairlearn","655483741","issue","https://github.com/Trusted-AI/AIF360/issues/516","Split up requirements file A good example: https://github.com/scikit-image/scikit-image/tree/master/requirements

Goal: right now we direct people to install requirements.txt in many places. That brings a lot of extra packages into an env that one may not particularly care about. Additionally, we want to have tests to validate that Fairlearn works with other ML packages like tensorflow, pytorch, xgboost, etc., so adding all of these to requirements.txt is infeasible. Related to #231 as a prerequisite.

The alternative would be to just do custom installations in the pipeline definition, but then it's harder to point people at a file for an installation on their local machine.","21","0.3854198898446687","Installation and shell commands","Deployment"
"https://github.com/fairlearn/fairlearn","654809607","issue","https://github.com/Trusted-AI/AIF360/issues/514","GridSearch Object has no attribute 'predictors_' ---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-58-eecd0beaf4b3> in <module>
----> 1 sweep_preds = [predictor.predict(df_test) for predictor in sweep.predictors_]
      2 sweep_scores = [predictor.predict_proba(df_test)[:, 1] for predictor in sweep.predictors_]

AttributeError: 'GridSearch' object has no attribute 'predictors_'","11","0.7643982683982685","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","647559667","issue","https://github.com/Trusted-AI/AIF360/issues/510","Show progress during unfairness mitigation #### Is your feature request related to a problem? Please describe.
When running `GridSearch` or `ExponentiatedGradient` it may take a while to run all the iterations. It would be nice to have an idea about the progress. 

#### Describe the solution you'd like
For `GridSearch` this could be something simple like `completed 7/100 iterations`, or perhaps something more sophisticated like `completed 7/100 iterations, projected to finish in 4:32`. For `ExponentiatedGradient` we can't know the number of iterations ahead of time, but we can tell the user how close we are to the stopping criterion as well as the current iteration number (in case we hit the maximum number of iterations).
We should perhaps also think about solutions other packages have for this.
","10","0.2613562333432042","Model development","Development"
"https://github.com/fairlearn/fairlearn","644150873","issue","https://github.com/Trusted-AI/AIF360/issues/508","Interactive dashboard doesn't show on website in examples Neither examples nor user guides can currently use the dashboard. This makes it harder to imagine how it works, but also a maintenance burden. Every time any changes are made to related files we have to update all screenshots.","14","0.4138579293675304","Documentation","Development"
"https://github.com/fairlearn/fairlearn","644138972","issue","https://github.com/Trusted-AI/AIF360/issues/507","Documentation for fetch_boston This is a follow-up to #435 and #494 to add documentation for `fetch_boston`. We need to find a good way to highlight the issues with this dataset as an educational example. Depending on the chosen way this may end up as a user guide or notebook (whatever is more appropriate).

There are plenty of resources to base this on, e.g. https://github.com/fairlearn/fairlearn/pull/434#discussion_r437683926 (comment from the original PR).

A great first step would be to compile a list of related sources and to post them in this thread. After that we can summarize the info and decide how to add it to the docs.","25","0.3177165849365022","Research","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","643147645","issue","https://github.com/Trusted-AI/AIF360/issues/504","Structure of project boards and labels The current organization of issues and the (lack of) the current organization of PRs make it hard to:
* identify the themes pursued in the package short-term
* identify opportunities to contribute
* tell if somebody is already working to address a specific issue

On this issue, I'd like to brainstorm the right structure for project boards / labels. I think that together with @romanlutz , we will play a bit with the project boards and keep this issue open until we reach a bit of stability.

Some ideas:

If there is an obvious set of items / tasks that comes up within an issue or a proposal, then it's probably not too much overhead to create more granular issues and tag them as ""high-priority"" (if that's the case) and/or ""help wanted""--and put them in the same project as the proposal. We probably shouldn't mark ""TODOs"" in the proposal and instead just rely on issues (that was my mistake...). If there ends up being a single issue attached to implementation of a proposal, I think that Kevin's suggestion might be easily done by presenting the most up-to-date TODO list at the bottom of the issue--this could be the same comment that's re-edited, or simply the latest comment.

That said--I don't want to force the author of an issue / proposal to split it into smaller tasks if that presents an overhead. I'd rather postpone splitting until there's somebody who wants to help and let them figure it out with whoever submitted the issue (or maybe somebody like the ""project owner"" if we end up having those).

I feel similar about having a ""high-level"" issue tagged as ""epic"" in a project--I'm not opposed, but also don't want to force it, for fear of creating additional overhead.

My current preference is to experiment with project boards and labels. This will help us manage the workflow and ever-so-slightly help with the ""discoverability"" of the more granular tasks in support of the already planned work.

The project boards should complement the ""Short-term roadmap"". I like the approach suggested by @kevinrobinson in [#500](https://github.com/fairlearn/fairlearn/pull/500#discussion_r443040820) -- if we use the (short-term?) roadmap to describe the real-world problems we're trying to address (both short-term and long-term), the contributors will be able to come up with issues and/or projects themselves and we will have an easier time identifying the ""high-priority"" issues. I'm not yet sure if we should use these ""problems"" as ""projects"" in our project boards... I think we'll need to experiment a bit.

_Originally posted by @MiroDudik in https://github.com/fairlearn/fairlearn/pull/500/review_comment/create_","20","0.5277544178809046","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","642218013","issue","https://github.com/Trusted-AI/AIF360/issues/501","Fairlearn widget crashes when run inside of VS code <!--
Before submitting a bug, please make sure the issue hasn't been already
addressed by searching through the past issues.

If your issue is a usage question, please submit it in one of these other
channels instead:
- StackOverflow with the `fairlearn` tag:
  https://stackoverflow.com/questions/tagged/fairlearn
- Gitter: https://gitter.im/fairlearn/community#
The issue tracker is used only to report bugs and feature requests. For
questions, please use either of the above platforms. Most question issues are
closed without an answer on this issue tracker. Thanks for your understanding.
-->

#### Describe the bug
<!--
A clear and concise description of what the bug is.
-->
Fairlearn widget will crash when loaded into VS code in some circumstances. 

#### Steps/Code to Reproduce
<!--
Please add a minimal example (in the form of code) that reproduces the error.
Be as succinct as possible, do not depend on external data. In short, we are
going to copy-paste your code and we expect to get the same result as you. 
-->

1. Install VS code
2. Install the ms-python.python extension (it's used for opening notebooks)

Using the notebook in this zip file, opening the notebook and running it will cause a crash in the fairlearn javascript:
[sql_query2.zip](https://github.com/fairlearn/fairlearn/files/4806678/sql_query2.zip)

See this bug on the ms-python.python extension for VS code for more information:
https://github.com/microsoft/vscode-python/issues/12394

It should also be noted that VS code tries to load the fairlearn widget js from http://unpkg.com or http://jsdelivr.net. We're unable to find this js so we load the fairlearn js from the jupyter nbextensions folder. This might be what's causing the problem. JS loaded from the nbextensions folder can sometimes have implicit dependencies upon jupyter JS, whereas JS packaged on the CDN is explicit about those.

#### Screenshots
Here's a screenshot of the console log:

![image](https://user-images.githubusercontent.com/19672699/85179252-5e1a1580-b235-11ea-8cf9-eaec9c87226e.png)


#### Versions
<!--
Please provide the following information:
- OS: [e.g. Windows]
- Browser (if you're reporting a dashboard bug in jupyter): [e.g. Edge, Firefox, Chrome, Safari]
- Python version: [e.g. 3.7.4]
- Fairlearn version: [e.g. 0.4.5 or installed from master branch in editable mode]
- version of Python packages: please run the following snippet and paste the output:
  ```python
  import fairlearn
  fairlearn.show_versions()
  ```
-->
System:
    python: 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)]
executable: C:\Users\rchiodo.REDMOND\AppData\Local\Continuum\miniconda3\envs\jupyter\python.exe
   machine: Windows-10-10.0.19041-SP0

Python dependencies:
       pip: 20.0.2
setuptools: 46.1.3.post20200330
   sklearn: 0.22.1
     numpy: 1.18.1
     scipy: 1.4.1
    Cython: None
    pandas: 0.25.1
matplotlib: 3.1.3
    tempeh: None
<!-- Thanks for contributing! -->
","29","0.5994871370667549","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","642102814","issue","https://github.com/Trusted-AI/AIF360/issues/498","Brainstorming: Website for ""fairness as a sociotechnical challenge"" community This is an attempt to tackle https://github.com/fairlearn/fairlearn/issues/413#issuecomment-646703267 in a different way.  I'm going to keep this work private and outside of fairlearn for now, to respect everyone's bandwidth for collaborating, but wanted to at least communicating that I am doing this :)

So I'll just share a screenshot here to illustrate the rough direction of what I'm talking about, and if this is something folks want to discuss more in a month or two when the various WIP bits of documentation have landed, and we have put together an example notebook or two, I'd be interested in chatting more and seeing if there are ways to merge some of these ideas into fairlearn!

<img width=""845"" alt=""Screen Shot 2020-06-19 at 12 31 15 PM"" src=""https://user-images.githubusercontent.com/1056957/85157279-4341a400-b229-11ea-8804-d3ad2d6cf478.png"">

This prototype aspires towards similar goals as articulated in the [microrubric](https://github.com/fairlearn/fairlearn/pull/490) for critiquing notebooks, but isn't centered on Fairlearn.  @romanlutz the theory of change here is similar to what we talked about the other day, where the idea is to take ideas at the heart of communities like FAccT, and do the translational work to reach a broader audience of developers.  As [Barocas and Boyd (2017)](https://cacm.acm.org/magazines/2017/11/222176-engaging-the-ethics-of-data-science-in-practice/fulltext) say:

> All too often, the data scientists we have encountered are quite sympathetic to the sentiment behind the critiques they hear, but feel maligned and misunderstood, unacknowledged for their efforts, and frustrated by vague recommendations that are not actionable... The gaps between data scientists and critics are wide, but critique divorced from practice only increases them. Data scientists, as the ones closest to the work, are often the best positioned to address ethical concerns, but they often need help from those who are willing to take time to understand what they are doing and the challenges of their practice.

I know there's a lot of threads going on right now but I'll be excited to chat more down the line whenever the timing seems right.  I'm just sharing this as a way to try to demonstrate support and show that this line of work could be possible.  👍 

EDIT: The hackpad [Fairlearn: Sociotechnical “talking points”](https://hackmd.io/nDiDafJ6TMKi2cYDHnujtA) is another smaller way to try working forward on this.","20","0.3756219474097272","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","641856266","issue","https://github.com/Trusted-AI/AIF360/issues/495","AttributeError: 'GridSearch' object has no attribute 'predictors_' I'm using `sklearn` **0.23.1** and `fairlearn` **0.4.6**. The following [example](https://github.com/fairlearn/fairlearn/blob/master/notebooks/Grid%20Search%20with%20Census%20Data.ipynb) from fairlearn/notebooks fails at line:

`predictors = sweep.predictors_`
 
with the error message:

`AttributeError: 'GridSearch' object has no attribute 'predictors_'`
","11","0.6889884763124197","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","640018407","issue","https://github.com/Trusted-AI/AIF360/issues/486","Documentation: Comparison to Seldonian approach in Thomas et al. (2019) The paper [Preventing undesirable behavior of intelligent machines (Thomas et al. 2019)](https://science.sciencemag.org/content/sci/366/6468/999.full.pdf?ijkey=sfV6zLc.FeEZI&keytype=ref&siteid=sci) seems like it trying to provide a similar value to fairlearn, and directly references this project.  I've read through the paper but there are also 90 pages of [supplemental materials](https://science.sciencemag.org/content/sci/suppl/2019/11/20/366.6468.999.DC1/aag3311_Thomas_SM.pdf) so I do not fully understand everything in there yet :)

This is building on previous discussions like https://github.com/fairlearn/fairlearn/issues/444, https://github.com/fairlearn/fairlearn/issues/439, and https://github.com/fairlearn/fairlearn/issues/460, with the hope that we can roll the discussion in these issues forward into user guides. 😄 


Here's the most relevant comparison sections from the paper:

> Unlike our approach, fairness-aware classification algorithms designed using the standard ML approach do not provide probabilistic guarantees that the resulting classifier is acceptably fair when applied to unseen data. We observed that two state-of-the-art fairness-aware algorithms that we ran for comparison, Fairlearn (24) and Fairness Constraints (25), each produced unfair behavior under at least one definition of fairness.

and in the supplemental bits:

> As described previously,Fairlearn(FL) can enforce definitions of fairness that can be written as a set of linear constraints on conditional moments, such as confusion rates [24].  It includes a tolerance parameter that determines the maximum amount by which fairness can be violated, analogous to in our formulation.  We used the implementation provided by Agarwalet al.[24], which includes code to enforce demographic parity and equalized odds.  For other definitions of fairness,  we set Fairlearn to enforce equalized odds as a surrogate fairness definition, and tested several settings of the tolerance parameter to assess the performance of the approach.  When applying FL to definitions besides demographic parity and equalized odds, we evaluate FL using tolerance values of 0.01, 0.1, and 1.0.

There are some very abstracted charts of different algorithms on different constraints for a single dataset:

<img width=""682"" alt=""Screen Shot 2020-06-16 at 5 48 19 PM"" src=""https://user-images.githubusercontent.com/1056957/84832920-22483b80-affc-11ea-8e7b-7f2110d341fc.png"">

The paper doesn't reference real deployment contexts, so there's no way to really critique or evaluate these approaches from a sociotechnical perspective or whether they do anything to mitigate real harms.  And there are some interesting takes on how their method contributes to shifting responsibility (and power) from users to designers.  But if we ignore that :) it might be helpful to understand how the approaches compare in the context of a narrow technical benchmark.

I'm assuming this paper is familiar to folks on the project, because I came across it when I noticed the Stanford students referencing it in the teams chat :)  Thanks!","6","0.2971229788441155","API expansion","Development"
"https://github.com/fairlearn/fairlearn","639840815","issue","https://github.com/Trusted-AI/AIF360/issues/484","Dashboard did not even show #### Describe the issue linked to the documentation

when I was trying to access the dashboard, the user interactive dashboard did not show up, I only see [out]:<fairlearn.widget._fairlearn_dashboard.FairlearnDashboard at 0x7f0206c42cc0>


<!--
Tell us about the confusion introduced in the documentation.
-->

#### Suggest a potential alternative/fix

<!--
Tell us how we could improve the documentation in this regard.
-->
","14","0.6746019937509303","Documentation","Development"
"https://github.com/fairlearn/fairlearn","639146923","issue","https://github.com/Trusted-AI/AIF360/issues/482","Documentation needs to be versioned This is something we've wanted from the start and only excluded since it added complexity. #472 shows that this is important. ","25","0.4059034145489476","Research","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","637945602","issue","https://github.com/Trusted-AI/AIF360/issues/478","Notebook: Examples of sociotechnical perspectives on pre-trial detention In the same spirit as https://github.com/fairlearn/fairlearn/issues/413, here's some examples of work on pre-trial detention that may be relevant if the project chooses to use the COMPAS dataset in any examples illustrating the value of its approach.  There's an existing [Binary Classification on COMPAS dataset](https://fairlearn.github.io/auto_examples/notebooks/plot_binary_classification_COMPAS.html) example, and discussion in https://github.com/fairlearn/fairlearn/issues/466#issuecomment-640854619 about potentially using the COMPAS dataset as part of an example or evaluation as well.

I've pulled out a quote from each to show the punchline.  I view the first one as the most important.

### [Understanding the Content and Consequences of Pre-Trial Detention (Lum, Bender and Wilkerson 2018)](https://facctconference.org/2018/livestream_vh210.html)
> Please, if you study algorithmic fairness & have used the COMPAS dataset as an example, watch our session. We need to hear the voices of those affected by our “fair” tech. 

### [Sample COMPAS Risk Assessment questions](https://www.documentcloud.org/documents/2702103-Sample-Risk-Assessment-COMPAS-CORE.html)
Excerpted questions:
> If you lived with both parents and they later separated, how old were you at the time?  Do you live with friends?  How many times did you skip classes while at school?  Do you often become bored with your usual activities?  How often do you feel left out of things?

### [Big Data:  New Tricks for Econometrics (Varian 2013)](http://people.ischool.berkeley.edu/~hal/Papers/2013/ml.pdf)
> The machine learning models we have described so far have been entirely about prediction.  If our data was generated by policymakers who assigned police to areas with high crime, then the observed relationship between police and crime rates could be highly predictive for the historical data,  but not useful in predicting the causal impact of explicitly assigning additional police to a precinct.

### [An algorithm for removing sensitive information: application to race-independent recidivism prediction (Johndrow and Lum 2017)](https://arxiv.org/pdf/1703.04957.pdf):
> For drug crimes, African American drug users are arrested at a rate that is several times that of Caucasian drug users despite the fact that African American and Caucasian populations are estimated by public health researchers to use drugs at roughly the same rate [Langan,1995]... Given that the outcome variable is observed with bias with respect to the protected variable, we believe that the second set of approaches designed with the objective of achieving equivalent predictive accuracy by race are inappropriate for this particular application, as they ultimately rely upon comparing the  model’s  predictions  of  re-offense  to  a  fundamentally  flawed  and  biased  measure  of  re-offense:re-arrest. 

### [Fairness, Equality, and Power in Algorithmic Decision-Making (Kasy and Adebe 2020)](https://maxkasy.github.io/home/files/papers/fairness_equality_power.pdf)
> The first asks:what is the causal impact of the introduction of an algorithm on inequality, both within and between groups?  In contrast to fairness, this perspective is consequentialist.  It depends on the distribution of outcomes affected by the algorithm rather than treatment, and it does so for the full population rather than only for individuals who are part of the algorithm. 

and

> ""A perspective focused on the distribution of power propels us to consider the design of the algorithms themselves: Don’t just ask how the algorithm treats different people differently, but also who gets todo the treating. Specifically, we ask what implicit distribution of social power justifies the current choice of objectives. Such a question foregrounds how power gets allocated, and what is the process that leads some groups to have more control over data in decision making processes.""

### [Human Decision Making with Machine Assistance: An Experiment on Bailing and Jailing (Grgić-Hlača et al. 2019)](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3465622)
> In multiple jurisdictions in the US, judges have access to a machine prediction about a defendant’s recidivism risk. In our study, we explore how receiving machine advice influences people’s bail decisions... We run a vignette experiment with laypersons whom we test on a subsample of cases from the database of this prediction tool. In study 1, we ask them to predict whether defendants will recidivate before tried, and manipulate whether they have access to machine advice. We find that receiving machine advice has a small effect, which is biased in the direction of predicting no recidivism.

### [What Do Criminal Justice Professionals Think About Risk Assessment at Pretrial?](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3168490)
> We describe findings from surveys with [people] in 30 jurisdictions about their perceptions and use of risk assessments in making pretrial release decisions... [their] view and use of these tools is critical because there is substantial discretion in whether and how they implement recommendations... respondents were asked about the extent to which they agree with recommendations from the tool and how frequently they use it. Virtually [no one] indicated that they “always” or “never” agreed with the PSA recommendation. Judges (63%) and pretrial staff (72%) were more likely to indicate they agreed with it “often.” Half of defenders and 38% of prosecutors indicated they agree with the recommendation “sometimes.” Nearly one in three prosecutors “rarely” agree with it.

### [Layers of Bias: A Unified Approach for Understanding Problems With Risk Assessment (Eckhouse et al. 2018)](https://journals.sagepub.com/doi/abs/10.1177/0093854818811379):
> we propose a framework for understanding the relationships among these debates: layers of bias. In the top layer, we identify challenges to fairness within the risk-assessment models themselves. We explain types of statistical fairness and the tradeoffs between them. The second layer covers biases embedded in data. Using data from a racially biased criminal justice system can lead to unmeasurable biases in both risk scores and outcome measures. The final layer engages conceptual problems with risk models: Is it fair to make criminal justice decisions about individuals based on groups? We show that each layer depends on the layers below it: Without assurances about the foundational layers, the fairness of the top layers is irrelevant.

### [In Pursuit of Interpretable, Fair and Accurate Machine Learning for Criminal Recidivism Prediction (Wang et al. 2020)](https://arxiv.org/pdf/2005.04176.pdf)
> We compare against two existing risk assessments... and train models that output probabilities rather than binary predictions.
> We also observed that machine learning models do not generalize well across states, perhaps due to the differences in the feature distributions between regions.

If folks want to work on this I'm happy to chat more, but my intention with the issue is more to document a critique and aspirational direction, so that if other folks want to collaborate and follow this would be a helpful starting point for that work :)  In other words, I'm trying to share a starting point for what it might look like to do what is proposed in the fairness styleguide in https://github.com/fairlearn/fairlearn/pull/426.

EDIT: added two more papers with research on how humans interpret information from pre-trial detention prediction systems.  These were from a toy experiment showing the impact of adding small amounts of noise to visualizations of fairness metrics in subsets of the COMPAS dataset ([demo](https://fairvis-noise-experiment.netlify.app/), [code](https://github.com/kevinrobinson/fairvis)).

EDIT 10/15/20: added Wang et al. 2020 and link to the actual COMPAS survey questions.","8","0.5216816649922693","Understanding Fairness Definition","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","637416743","issue","https://github.com/Trusted-AI/AIF360/issues/477","Replacing master as the default branch Context: the word master is triggering or at least making people uncomfortable. Haven’t seen a good reason not to do this yet.

https://twitter.com/mislav/status/1270388510684598272?s=21

The new default isn’t really set yet. Could be
- main
- default

Any thoughts?","32","0.4920007528703181","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","636372283","issue","https://github.com/Trusted-AI/AIF360/issues/473","include costs for ThresholdOptimizer <!--
If your issue is a usage question, please submit it in one of these other
channels instead:
- StackOverflow with the `fairlearn` tag:
  https://stackoverflow.com/questions/tagged/fairlearn
- Gitter: https://gitter.im/fairlearn/community#
The issue tracker is used only to report bugs and feature requests. For
questions, please use either of the above platforms. Most question issues are
closed without an answer on this issue tracker. Thanks for your understanding.
-->

I suggest that the constructor `ThresholdOptimizer()` include the possibility of considering asymmetric cost in False Positives and False Negatives. 

As by actual implementation, thresholds are chosen such that given the constraints, the results minimize `P(Yhat = 1, Y = 0) + P(Yhat = 0, Y = 1)`, that is the total fraction of false positives and false negatives (which is equivalent to maximize accuracy).
I simply suggest to add `Cfp * P(Yhat = 1, Y = 0) + Cfn * P(Yhat = 0, Y = 1)`, with default `Cfp=Cfn=1`; or even a single parameter (corresponding e.g. to Cfp/Cfn.

The most general case would be to compute the error as:
`Ctp * P(Yhat = 1, Y = 1) + Ctn * P(Yhat = 0, Y = 0) + Cfp * P(Yhat = 1, Y = 0) + Cfn * P(Yhat = 0, Y = 1)` but it is pretty reasonable to assume that both true positives and true negatives have no cost at all.

By the way, this is also coherent with Hardt et al. (2016) which is the main reference for `ThresholdOptimizer()`.

This issue may be related to PR https://github.com/fairlearn/fairlearn/pull/381, but I cannot see references to cost modification in the code of this PR. 

The changes should be minimal, just add these two parameters in the class `__init__` and then use them in the definition of the error

What do you think?","11","0.4205959653978032","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","635687939","issue","https://github.com/Trusted-AI/AIF360/issues/472","How to proceed with PR Gate that runs notebooks against the last released Fairlearn version? This keeps causing confusion, e.g. most recently https://github.com/fairlearn/fairlearn/pull/434#issuecomment-640945245

Any suggestions for how to rename the pipeline? This is complex because in general it should be green/passing. If we make the conscious decision to break something compared to the last version it gets red. From that point onward it stays red until the next release. So we definitely do want to be alerted when it goes red for the first time. That's what makes this tricky. 

@adrinjalali suggested in the past that things like this could perhaps be fixed just before a release. [I hope I'm not rephrasing his ideas in a way that misrepresent them...]

So I'm curious how people feel about this. In a way I feel like solving the documentation versioning in the way scikit-learn does it, i.e. the webpage can be viewed based on whatever version of the package you're using incl. notebook, and therefore you will have no trouble finding the compatible notebook. That would perhaps address the issue sufficiently.

FYI @kevinrobinson helped out some folks over the past week or so who raised this issue here on GitHub, so perhaps you also have thoughts.

FYI @koaning who faced this issue as well.

FYI @riedgar-ms @MiroDudik ","32","0.350873042694604","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","635196032","issue","https://github.com/Trusted-AI/AIF360/issues/471","Expand Makefile Different folks have different ways of working but it might help some folks if the Makefile was expanded a bit. 

For example locally I've added this; 

```
commit:
	git commit --amend --signoff
```

This handles the part of the signoff. I also think it'd be nice to add a `check` command that will run pytest/flake8 locally with all the correct configurations as well as a `install` command that installs everything locally for development. I personally also like adding a [help command](https://calmcode.io/makefiles/help-commands.html) that explains what commands are in the file. 

I wouldn't mind making the PR. Just wanted to check what folks think of it first. ","21","0.4282169228906913","Installation and shell commands","Deployment"
"https://github.com/fairlearn/fairlearn","634896744","issue","https://github.com/Trusted-AI/AIF360/issues/468","`constraint` is not documented in the `ThresholdOptimizer` The docstring parameter seems to be missing here: https://fairlearn.github.io/api_reference/fairlearn.postprocessing.html#fairlearn.postprocessing.ThresholdOptimizer","14","0.4018529241459179","Documentation","Development"
"https://github.com/fairlearn/fairlearn","634588543","issue","https://github.com/Trusted-AI/AIF360/issues/466","Proposal: Port EqualOpportunityClassifier and DemographicParityClassifier I'm making this issue so that we can disucss how to move the `EqualOpportunityClassifier` and `DemographicParityClassifier from scikit-fairness to this project.

The methods are discussed in a notebook here: https://scikit-lego.readthedocs.io/en/latest/fairness.html. The implementations can be found here: https://github.com/koaning/scikit-fairness/tree/master/skfair/linear_model

The `DemographicParityClassifier` is an implementation of the method described in M. Zafar et al. (2017), Fairness Constraints: Mechanisms for Fair Classification. The optimization itself is done using `cvxpy`. The `EqualOpportunityClassifier` is not described in a paper afaik, but is a simple modification on the constraint of the `DemographicParityClassifier`

Is this something you'd be open to having in this repo? If so, I can start working on a PR. ","6","0.5893354315458181","API expansion","Development"
"https://github.com/fairlearn/fairlearn","632317356","issue","https://github.com/Trusted-AI/AIF360/issues/463","Image on Landing Page of Documentation  #### Describe the issue linked to the documentation

Currently, this is the main image on the documentation page: 

![image](https://user-images.githubusercontent.com/1019791/83940251-7aed2c80-a7e3-11ea-997b-d2dd3b94f88b.png)

I think the image communicates a few things well; it's machine learning, there's a sense of gender equality and I think the wheel chair is a nice touch. But as far as fairness goes, I think the image is at risk of doing this project a dis-service by not having folks of different color also represented. The goal here is fairness and fairness towards race will certainly be a theme. The front page is a lot more welcoming to a diverse crowd if the main image also represents a diverse crowd.

#### Suggest a potential alternative/fix

Come to think of it; there's other angles of fairness to consider as well. Age discrimination and sexual orientation come to mind. I am no designer and I woulnd't want to suggest that designing a proper image is easy, but it feels like a missed opportunity to not iterate on the image. 
","14","0.3144838212634821","Documentation","Development"
"https://github.com/fairlearn/fairlearn","631104129","issue","https://github.com/Trusted-AI/AIF360/issues/460","Documentation: Understanding generalization hello!  in building on https://github.com/fairlearn/fairlearn/issues/444, I was curious what would happen if I took the quickstart example, split the data, and looked at how well the constraints held up on the test set.

Here's what I see in the training dataset using `ExponentiatedGradient` to satisfy the `DemographicParity` constraint on a `DecisionTreeClassifier`.  This is with a 70/30 train/test split, and shows the differences in these metrics between the predictions of the original model, and the predictions of the mitigated model:


```
TRAINING
   accuracy: 0.980 to 0.931
  selection: 0.230 to 0.275
   for male: 0.289 to 0.282
 for female: 0.110 to 0.261

TEST
   accuracy: 0.813 to 0.759
  selection: 0.233 to 0.292
   for male: 0.288 to 0.319
 for female: 0.119 to 0.236
```

This is just a single run with one seed, but the demographic parity difference for selection rate changes quite a lot - from `0.021` (28.2% vs. 26.1%) to `0.083` (31.9% vs. 23.6%).

Of course in the real world there's distribution drift and many other things to consider, but I'm trying to understand even if the training and test data were drawn from the same underlying distribution with no noise, is the mitigation approach just overfitting?  Code for the modeling is below.

Setup:
```
X, y_true = adult()
trial_one = attempt(X, y_true, seed=11)
trial_two = attempt(X, y_true, seed=22)
```

Splitting, mitigating, and predicting:
```
from fairlearn.metrics import group_summary
from sklearn.metrics import accuracy_score
from sklearn.tree import DecisionTreeClassifier
from fairlearn.reductions import ExponentiatedGradient, DemographicParity, ConditionalSelectionRate
from sklearn.model_selection import train_test_split

def attempt(X, y, seed=42):
  np.random.seed(seed)
  (X_train, X_test, y_train, y_test) = train_test_split(X, y, test_size=0.30, random_state=seed)
  A_train = sex_for(X_train)
  A_test = sex_for(X_test)
  del X_train['Sex']
  del X_test['Sex']

  # train
  print('training...')
  (classifier, mitigator) = go(X_train, y_train, A_train)
  classifier.fit(X_train, y_train)

  # predict
  print('predicting...')
  y_train_predictions = classifier.predict(X_train)
  y_train_mitigated = mitigator.predict(X_train)

  # predict with mitigated
  print('predicting mitigated...')
  y_test_predictions = classifier.predict(X_test)
  y_test_mitigated = mitigator.predict(X_test)
  return (
    (X_train, y_train, A_train, y_train_predictions, y_train_mitigated),
    (X_test, y_test, A_test, y_test_predictions, y_test_mitigated)
  )

def sex_for(X):
  return X['Sex'].apply(lambda sex: ""female"" if sex == 0 else ""male"")

def go(X, y, A):
  constraint = DemographicParity()
  classifier = DecisionTreeClassifier()
  mitigator = ExponentiatedGradient(classifier, constraint)
  mitigator.fit(X, y, sensitive_features=A)
  return (classifier, mitigator)
```

Printing results:
```
from fairlearn import metrics

def printout(trial):
  train, test = trial
  (X_train, y_train, A_train, y_train_predictions, y_train_mitigated) = train
  (X_test, y_test, A_test, y_test_predictions, y_test_mitigated) = test
  output(""TRAINING"", y_train, y_train_predictions, y_train_mitigated, A_train)
  print('')
  output(""TEST"", y_test, y_test_predictions, y_test_mitigated, A_test)

def output(title, truth, predictions, mitigated, sensitive):
  print(title)
  print('   accuracy: {} to {}'.format(
    rate(accuracy_score(truth, predictions)),
    rate(accuracy_score(truth, mitigated)))
  )

  print('  selection: {} to {}'.format(
    rate(metrics.selection_rate(truth, predictions)),
    rate(metrics.selection_rate(truth, mitigated)))
  )
  predictions_summary = group_summary(metrics.selection_rate, truth, predictions, sensitive_features=sensitive)
  mitigated_summary = group_summary(metrics.selection_rate, truth, mitigated, sensitive_features=sensitive)
  print('   for male: {} to {}'.format(
    rate(predictions_summary['by_group']['male']),
    rate(mitigated_summary['by_group']['male'])
  ))
  print(' for female: {} to {}'.format(
    rate(predictions_summary['by_group']['female']),
    rate(mitigated_summary['by_group']['female'])
  ))

def rate(f):
  return '{0:.3f}'.format(f)
```

Thanks!","10","0.7921608676437218","Model development","Development"
"https://github.com/fairlearn/fairlearn","630964484","issue","https://github.com/Trusted-AI/AIF360/issues/459","'GridSearch' object has no attribute 'predictors_' Hi

I'm walking through the ""Binary Classification with the UCI Credit-card Default Dataset"" notebook and get an error **'GridSearch' object has no attribute 'predictors_'** when executing the following code cell after successfully training GridSearch:

`sweep_preds = [predictor.predict(df_test) for predictor in sweep.predictors_]`
`sweep_scores = [predictor.predict_proba(df_test)[:, 1] for predictor in sweep.predictors_]`

Thanks for looking into this :wink:","11","0.5320077247693297","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","630856703","issue","https://github.com/Trusted-AI/AIF360/issues/458","Roadmap for interpretability of fairness metrics? The correct interpretation of almost any fairness metric relies on which label is positive and which sensitive groups are compared. At the moment, for most metrics, these considerations are implicit in the API. For example, all confusion_matrix related performance metrics will return labels as: “those that appear at least once in y_true or y_pred are used in sorted order” due to the dependency on scikit-learn, but this is not explicitly mentioned in the fairlearn documentation. 

I was wondering whether there are plans to help users (particularly those who do not use the dashboard) interpret the metrics correctly.

I do not have a strong opinion on what it should look like, but I can imagine e.g. consistent usage of ``pos_label`` and an equivalent argument for privileged group membership might help (although the latter is tricky for polytomous sensitive features). Or perhaps some accompanying textual information, e.g. if demographic_parity_difference = 0.10, one could retrieve an 'explanation' along the lines of ""selection rate of group=1 (largest) is 0.10 higher than the selection rate of group=2 (smallest)"".","3","0.4304350435043507","Bias detection metrics validation","Validation"
"https://github.com/fairlearn/fairlearn","630779301","issue","https://github.com/Trusted-AI/AIF360/issues/457","Separate functions for different aggregations of the same fairness metric? I was wondering whether there is a particular rationale for having separate functions for different types of aggregation of the same fairness metric, e.g. demographic_parity_difference and demographic_parity_ratio.

From a user perspective, I would expect ""demographic_parity"" to be a single function and the way it aggregated (ratio/difference/etc.) a function argument. Especially if we would like to include some more information in the documentation on the rationale behind different metrics, the current API would require us to copy this multiple times.
","3","0.4810634391786225","Bias detection metrics validation","Validation"
"https://github.com/fairlearn/fairlearn","630772040","issue","https://github.com/Trusted-AI/AIF360/issues/456","Add links to source code to API reference #### Describe the issue linked to the documentation
It would be nice to be able to quickly navigate from the API reference to the source code on Github.

#### Suggest a potential alternative/fix
I believe scikit-learn uses the sphinx extension linkcode to achieve this.","14","0.791041861918151","Documentation","Development"
"https://github.com/fairlearn/fairlearn","630768868","issue","https://github.com/Trusted-AI/AIF360/issues/455","Improve organization of metrics module #### Describe the issue
I think the organization of files in the fairlearn.metrics module could be improved, 

~~both in API reference and the repository itself. My main issue regarding the documentation is that the distinction between performance metrics (e.g. recall) and fairness metrics (e.g. demographic parity) is not clear in the [API reference](https://fairlearn.github.io/api_reference/fairlearn.metrics.html). Additionally,~~ 

navigating the repository was not very intuitive for me. For example, some of the performance metrics not directly available in scikit-learn have their own file (e.g. _balanced_root_mean_squared_error.py), whereas others are grouped in (e.g. _extra_metrics.py).

#### Suggest a potential alternative/fix
I think merging and/or renaming some of the seperate files could help to improve. E.g. something like this:
* _fairness.py
    * _disparities.py
*  _base_metrics.py ( ~~_performance.py (or perhaps _classification.py and _regression.py to mimic scikit-learn)~~
    * _extra_metrics.py
    * _balanced_root_mean_squared_error.py 
    * _mean_predictions.py
    * _selection_rate.py
* _input_manipulations.py might be better off in a utils folder in the main repository.
* I am not sure what the best place would be for _group_metric_set.py. 

~~PS. I suppose giving each function their own page as in the scikit-learn API reference is already implicit in the documentation proposal, right?~~

EDIT: documentation issues moved to issue #720 ","14","0.4287331715903144","Documentation","Development"
"https://github.com/fairlearn/fairlearn","628921608","issue","https://github.com/Trusted-AI/AIF360/issues/448","AttributeError: 'GridSearch' object has no attribute 'predictors_' I have done pip install -r requirements.txt
And run the ""Grid Search with Census Data"" example notebook.
Everything go well until below cell.

------------------------------------------
sweep.fit(X_train, Y_train,
          sensitive_features=A_train)
predictors = sweep.predictors_

------------------------------------------

AttributeError                            Traceback (most recent call last)
<ipython-input-24-49c8aaf5d801> in <module>
----> 1 predictors = sweep.predictors_

AttributeError: 'GridSearch' object has no attribute 'predictors_'

-------------------------------

Could anyone quide me how to solve this problem?


Environment: Python 3.6




","11","0.5839353408196836","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","627792515","issue","https://github.com/Trusted-AI/AIF360/issues/444","Documentation: Understanding ExponentiatedGradient's termination and guarantees I'm trying to better understand what leads `ExponentiatedGradient` to terminate.  In particular, I'm trying to understand this in terms that I could translate to a non-technical person who wants to understand if they can control how much slack there is around the selection rates being exactly equal.  To make this concrete, I'm looking at the example in the quickstart guide, where a `DemographicParity` constraint for the `sex` column is applied to a task predicting which people earned over $50k.  I'd like to understand how to apply the `ExponentiatedGradient` so that it will enforce that the value of `demographic_parity_difference` is < 0.05 across the male and female groups.

I looked into understanding how the `eps` and `nu` params influence this.  I also noticed the `_MIN_ITER` check, and that the example scenario would terminate earlier if this is disabled.

To experiment, I set `eps=0.05` but found that this produced a demographic parity difference of `0.072`, so outside of the bounds I was expecting.  Similarly, setting `eps=0.10` produced a demographic disparity of `0.149` which was out of the bounds of what I was expecting.  So I'm figuring I'm misunderstanding something important here.

If it's helpful, this is the code from the quickstart I'm starting from, and an example of what that case shows with debug logging enabled.  The examples above all use the same `seed(0)` from the quickstart.

```
np.random.seed(0)
constraint = DemographicParity()
classifier = DecisionTreeClassifier()
mitigator = ExponentiatedGradient(classifier, constraint, eps=whatever)
mitigator.fit(X, y_true, sensitive_features=sex)
y_pred_mitigated = mitigator.predict(X)
...
Classification problem detected
...Exponentiated Gradient STARTING
...iter=000
...eps=0.010, B=100.0, nu=0.000421, max_iter=50, eta_min=0.000002
         eta=0.020000, L_low=0.024, L=0.024, L_high=10.862, gap=10.837989, disp=0.118, err=0.024, gap_LP=inf
...iter=001
         eta=0.020000, L_low=0.030, L=0.030, L_high=10.652, gap=10.622350, disp=0.116, err=0.024, gap_LP=77.863978
...iter=002
         eta=0.020000, L_low=0.036, L=0.036, L_high=10.486, gap=10.450200, disp=0.115, err=0.024, gap_LP=0.069860
...iter=003
         eta=0.020000, L_low=0.041, L=0.042, L_high=10.248, gap=10.206413, disp=0.112, err=0.024, gap_LP=0.000108
...iter=004
         eta=0.020000, L_low=0.047, L=0.047, L_high=10.091, gap=10.043995, disp=0.111, err=0.025, gap_LP=0.000000
...iter=005
         eta=0.020000, L_low=0.052, L=0.074, L_high=0.133, gap=0.059818, disp=0.011, err=0.074, gap_LP=0.000000
...eps=0.010, B=100.0, nu=0.000421, max_iter=50, eta_min=0.000002
...last_iter=5, best_iter=5, best_gap=0.000000, n_oracle_calls=26, n_hs=12
```

And I am running this to see what the demographic parity difference is afterward:
```
print(metrics.demographic_parity_difference(y_true, y_pred_mitigated, sensitive_features=sex))
0.010575424150881252
```

Thanks! :)","10","0.6707241677772129","Model development","Development"
"https://github.com/fairlearn/fairlearn","626838581","issue","https://github.com/Trusted-AI/AIF360/issues/439","Documentation: In what situations would this approach be preferable to TFCO? hello!  I was reading the TFCO paper ([Cotter et al. 2018](https://arxiv.org/pdf/1809.04198.pdf)), and it has a great section on past work, including this section:
> Agarwal et al. (2018) recently addressed training classifiers with fairness constraints. Like this work, their proposed algorithm is based on the two-player game perspective. Unlike this paper, they assume a zero-sum game, which works because they also assume oracle solvers for the two players, side-stepping the practical issues of dealing with the non-differentible non-convex indicators in the constraints, which is the focus of our algorithmic and theoretical contributions. Similar to this work, they output a stochastic classifier, but do not provide the sparse m+1 solution that we present in this work. They also consider a deterministic solution, which they produce by searching over a grid of values for λ for the best λ. They noted in their experimental section that the resulting deterministic solution was generally as good as their stochastic solutions on test data for those experiments they tried it on. As they note, a grid-search over λ is less ideal as the number of constraints grows.

To check my understanding, the essential differences is that fairlearn asks you to pick the `n` for the grid search over the values of λ, and then the intent of the dashboard visualizations is to pick a model from that grid search based on the (disparity, accuracy) tradeoff.  In constrast, TFCO allows generating a stochastic classifier that essentially picks from different points in that search, but also allows ""shrinking"" to a smaller model.  And it supports a wider range of ""rate constraints"" than the metrics described in `fairlearn`.

I'm sure there's also a range of factors related to the ecosystem and developer experience (eg, TF vs. emphasis on sci-kit compatability), so would be curious to hear folks' thoughts on that as well.  If folks want to comment here, I could afterwards repackage thoughts into a PR for https://github.com/fairlearn/fairlearn/pull/436.  Thanks! 👍 ","28","0.3912968366093366","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","623425779","issue","https://github.com/Trusted-AI/AIF360/issues/435","Feature Request: fetch_boston The `load_boston` dataset is moving out of scikit-learn (for good reason) but as an educational example it would be good to host it here and to properly document the issues with it. 

There is a WIP PR for this issue https://github.com/fairlearn/fairlearn/pull/434. ","0","0.4927443448570208","Dataset usage","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","623397963","issue","https://github.com/Trusted-AI/AIF360/issues/433","Microsoft Copyright Notice When you look at `setup.py` you'll notice these two lines: 

```
# Copyright (c) Microsoft Corporation. All rights reserved.
# Licensed under the MIT License.
```

Just to check; does the project intend on keeping the copyright line? It appears in every file. ","20","0.6792444384763437","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","622629147","issue","https://github.com/Trusted-AI/AIF360/issues/432","Make link to docs more visible.  Often, the documentation of a github project is immediately next to the title. See the example from [here](https://github.com/koaning/scikit-lego).

<img width=""1169"" alt=""image"" src=""https://user-images.githubusercontent.com/1019791/82582042-bc4dcd00-9b91-11ea-885a-2c612daedc2b.png"">

Fairlearn doesn't do that yet, but since it is common on github (and it gets more people to see an overview of the project) I figured it'd be good if the change was made. ","13","0.4950986974620652","Artifact generation and benchmarking","Deployment"
"https://github.com/fairlearn/fairlearn","622626321","issue","https://github.com/Trusted-AI/AIF360/issues/431","Proposal: Datasets Module Just like scikit-learn offers datasets that make it easy for people to play around, it would be nice if this package also offers datasets where fairness is a theme. Over at scikit-fairness we've got a [few of these implemented already](https://scikit-fairness.netlify.app/api/datasets.html) and we could port them over here. 

There's one feature about these `load_`/`fetch_` methods that is worth pointing out; they all raise a `FairnessWarning`. Note that this is a feature, not a bug! It makes people aware that they should not play with this data naively. 

Another benefit of having these datasets around is that it potentially makes writing guides a lot easier as well. ","15","0.5112173667433324","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","622619965","issue","https://github.com/Trusted-AI/AIF360/issues/430","Proposal: InformationFilter I'm making this issue so that we can disucss how to move the `InformationFilter` from [scikit-fairness](https://scikit-fairness.netlify.app/api/preprocessing.html#skfair.preprocessing.InformationFilter) to this project. While moving it to this project I could also add a [guide](https://scikit-fairness.netlify.app/fairness_boston_housing.html#How-it-Works) to the docs on the boston housing dataset (the same we have on scikit-lego side). The details of the approach can also be seen on youtube [here](https://youtu.be/Z8MEFI7ZJlA?t=948). 

The goal of this issue is to first gather feedback on the implementation and, assuming people agree this tool is useful, to discuss the steps to make it compatible for this project. Once agreed on the path forward I'll open up a PR. 
","20","0.4504515911201151","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","619918226","issue","https://github.com/Trusted-AI/AIF360/issues/425","Assign labels to values in FairlearnDashboard ![image](https://user-images.githubusercontent.com/1550763/82177036-b4afcf00-98d8-11ea-9a45-14e30b3579c0.png)

#### Is your feature request related to a problem? Please describe.
When I have a categorical feature in my dataset with just numbers, I get just numbers on the dashboard. Now I have to explain those numbers to the viewers of my `FairlearnDashboard`.

#### Describe the solution you'd like
I would like to be able to assign labels to values of my `sensitive_features` so it's clear what we're talking about in the `FairlearnDashboard`.

#### Describe alternatives you've considered, if relevant
I've considered the following options:

* Turning the feature into a `category` type in Pandas. This requires me to include additional 
  preprocessing in my machine learning pipeline, to turn it back into numeric values.

","11","0.3642026150519922","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","616775675","issue","https://github.com/Trusted-AI/AIF360/issues/419","Notebook: Sociotechnical context in law school admissions This is in reference to the notebook [Mitigating Disparities in Ranking from Binary Data](https://github.com/fairlearn/fairlearn/blob/23d3817f0c2214534325ce033b47a9c23caae629/notebooks/Mitigating%20Disparities%20in%20Ranking%20from%20Binary%20Data.ipynb).

There is a rich sociotechnical context surrounding decisions about which applicants are accepted to law school.  In the US, this has been contested for many years, and constitutional challenges to law school admissions policies in particular shaped the legal environment related to admissions for all higher education in the US.  See [Hopwood v. Texas](https://law.justia.com/cases/federal/appellate-courts/F3/78/932/504514/) which speaks directly to this issue, and subsequent landmark cases like [Grutter v. Bollinger](https://en.wikipedia.org/wiki/Grutter_v._Bollinger) and [Fisher v. University of Texas](https://en.wikipedia.org/wiki/Fisher_v._University_of_Texas_(2013)).  The legal context within the US has changed in critically important ways between those time periods.

This papers uses data from a 1998 paper.  That paper describes its motivation as:

> [the LSAC study] was undertaken primarily in response to rumors and anecdotal reports suggesting bar passage rates were so low among examinees of color that potential applicants were questioning the wisdom of investing the time and resources necessary to obtain a legal education

It also notes in the abstract that the study explicitly makes an normative argument about what admissions policies should be, in relation to broader questions of fairness.

In the example notebook, none of this context is currently mentioned.  In fact, the notebook seems to acknowledge that is is modeling doing something that it understands the authors of the original paper would object to:

> This notebook is intended as an example of Fairlearn functionality and not a fully realistic case study of an admission scenario. In real world, one should think carefully about whether it is appropriate to rank or score individuals. Also, additional features beyond the two considered here (GPA and LSAT scores) should be considered in practice, as recommended by the authors of the LSAC study. Finally, in real-world settings, it would be inappropriate to restrict attention to only two of the subgroups without evaluating the impacts on other individuals.

And:

> The authors of the LSAC study conclude that this justifies admission practices that look beyond LSAT and UGPA. However, in this simplified example, we build predictors of bar passage from these two variables alone.

In other words, this example uses the dataset from a 1998 study to model how to use `fairlearn`, in ways that the authors of that study would not endorse.  This isn't a minor issue - the main point of the study is to argue that law school admissions officer should not use the kinds of approaches modeled in this `fairlearn` notebook.  It may also be important to note that the LSAC study starts with a ""Historical introduction"" to situate its work within the broader sociotechnical context.

While the notebook cites the 1998 study, I'm assuming that the deployment context implied here is actually 2020 US.  I'm also assuming the notebook is implying that questions about generalization are not important to fairness work, and perhaps this is not intentional.

🟠 Minimal suggestion: Link to relevant Supreme Court cases.  Add an excerpt the ""Historical Introduction"" from the LSAC study where this data comes from.  Add a deployment context for the notebook scenario (eg, who is using this model, and what are they using it do, where and when is it being deployed).

But I'm also trying to understand if there are ways to demonstrate using `fairlearn` in a law school admissions scenario that would consider the extraordinarily contested sociotechnical context around college admissions.  Are there folks working within higher education admissions using the project that could collaborate on such work?  If not, perhaps other example notebooks drawn from real use cases might better demonstrate how fairlearn fits within the work of teams that view fairness as a sociotechnical challenge.

Thanks! 👍 ","8","0.6508399379289519","Understanding Fairness Definition","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","616775645","issue","https://github.com/Trusted-AI/AIF360/issues/418","Notebook: Sociotechnical context in credit card application decisions This is in reference to the notebook [Binary Classification with the UCI Credit-card Default Dataset](https://github.com/fairlearn/fairlearn/blob/master/notebooks/Binary%20Classification%20with%20the%20UCI%20Credit-card%20Default%20Dataset.ipynb).

hello!  To start off figuring out how to collaborate and contribute on notebooks, I thought it might be useful to share a critique of one of the existing notebooks.  This includes a wider critique about modeling ""fairness is a sociotechnical challenge"" along with more tightly scoped minimal suggestions for improvement.  I hope this lets us align on questions related to vision discussed elsewhere in a more concrete and productive way, while also adding ""minimal suggestions"" where team members can envision ""let's go make that change to the notebook right now"" and some small feelings of progress in what may otherwise feel overwhelming. :)  Thanks! 👍 

### Taiwan in 2006
Let's start with the source, the first paragraph of the paper has some pretty important sociotechnical context:

> In recent years, the credit card issuers in Taiwan faced the cash and credit card debt crisis and the delinquency is expected to peak in the third quarter of 2006 (Chou,2006). In order to increase market share, card-issuing banks in Taiwan over-issued cash and credit cards to unqualified applicants. At the same time, most cardholders, irrespective of their repayment ability, overused credit card for consumption and accumulated heavy credit and cash–card debts. The crisis caused the blow to consumer finance confidence and it is a big challenge for both banks and card holders.

And about where the data came from*:

> Our study took payment data in October, 2005, from an important bank (a cash and credit card issuer) in Taiwan and the targets were credit card holders of the bank.  Among the total 25,000 observations, 5529 observations(22.12%) are the cardholders with default payment. 

And that this work is done from the perspective of lenders aiming to reduce their risk:

> From the perspective of risk control, estimating the probability of default will be more meaningful than classfying customers into the binary results – risky and non-risky.

🟠 Minimal suggestion: Quote the original paper for context.  Add a deployment context for the notebook scenario (eg, who is using this model, and what are they using it do, where and when is it being deployed).

(* Note also that the original paper says the dataset contains 25,000 total records, but the UCI CSV actually contains 30,000 records.  I don't know what where this discrepancy comes from but it's off by 20% so not a small or insignificant data quality question.  This is probably worth understanding separately.)

### Deployment context
It's not clear to me why the training dataset would be used, as most of this data wouldn't be available in what I assume the deployment context is.  The paper acknowledges that this is an open question:

> Therefore, whether or not the estimated probability of default produced from data mining methods can represent the ‘‘real” probability of default is an important problem. 

It then cites 8 papers suggesting this is still an open problem (I didn't look at any of them).  And as best as I can tell, the original paper is not addressing questions about whether data from these current credit card holders, within the context of a national credit card debt crisis, will be relevant for a deployment context that would make decisions about credit risk for new people without past credit history.

More importantly, any lender using such an algorithm would have to make predictions without access to most of the columns in the dataset (eg, credit limit, past payment history, etc).  Perhaps the scenario assumes there are credit reporting agencies, and that they would be receiving that kind of information from other lenders the person applying for the card has previously worked with?  My assumption for now is that this is what the current notebook is assuming.  If so, one relevant data point from the US suggests it might be worth understanding how accurate these data sources tend to be (assuming they work in similar ways in Taiwan), since a [2015 FTC report](https://www.ftc.gov/sites/default/files/documents/reports/section-319-fair-and-accurate-credit-transactions-act-2003-fifth-interim-federal-trade-commission/130211factareport.pdf) found that ""26% of the 1,001 participants in the study identified at least one potentially material error on at least one of their three credit reports.""

🟠 Minimal suggestion: Add a deployment context for the notebook scenario (eg, this is a credit card company, they are using it to review applications, this is where and when it is being deployed).


### US law, Equal Credit Opportunity Act
Since many folks on the project team are in the US, here's some related sociotechnical context.  This is definitely US-centric, and banking and credit work very differently in different parts of the world :) but these are illustrative of the kinds of things I'd hope to see in work that viewed fairness as a sociotechnical challenge.

Here's some starting points.  First, the FTC: [Your Equal Credit Opportunity Rights](https://www.consumer.ftc.gov/articles/0347-your-equal-credit-opportunity-rights/)
<div>
  <img width=""45%"" src=""https://user-images.githubusercontent.com/1056957/81698772-7330a700-9434-11ea-967a-bd770573939c.png"">
  <img width=""45%"" alt=""Screen Shot 2020-05-12 at 9 38 21 AM"" src=""https://user-images.githubusercontent.com/1056957/81698727-6613b800-9434-11ea-8cce-9dbbf9a734ec.png"">
</div>

It seems that many of these attributes are included in the model within the current example notebook.  I'm just a layperson, so maybe I'm misunderstanding that guidance.

Here's a few other reference points from the Consumer Financial Protection Bureau: [Can a card issuer consider my age when deciding whether to issue a credit card to me?](https://www.consumerfinance.gov/ask-cfpb/can-a-card-issuer-consider-my-age-when-deciding-whether-to-issue-a-credit-card-to-me-en-20/), [My credit application was denied because of my credit report. What can I do?](https://www.consumerfinance.gov/ask-cfpb/my-credit-application-was-denied-because-of-my-credit-report-what-can-i-do-en-1253/) and [Can a card issuer consider my sex or marital status when deciding whether to issue a credit card to me?](https://www.consumerfinance.gov/ask-cfpb/can-a-card-issuer-consider-my-sex-or-marital-status-when-deciding-whether-to-issue-a-credit-card-to-me-en-22/):
  <img width=""60%"" alt=""Screen Shot 2020-05-12 at 8 49 04 AM"" src=""https://user-images.githubusercontent.com/1056957/81693134-78d6be80-942d-11ea-8eb6-ef788838305c.png"">

> Typically, card issuers may not even ask your sex on an application form, and the form has to disclose that you do not have to indicate Mr., Miss, Mrs., or Ms. on the application. There are also restrictions on asking for information about your marital status. A creditor may not ask about your marital status if you are applying for individual, unsecured credit unless

> If a lender rejects your application based on your credit report, the lender is also required to: Provide you the numerical credit score it used in taking the adverse action and the key factors that affected your score

My interpretation as a layperson is that the notebook is demonstrating practices that would introduce legal and regulatory risk for organizations operating within the US.  That being said,  what is currently legal in different places is certainly only one part of understanding the wider sociotechnical context, especially when what is legal is contested :)

🟠 Minimal suggestion: Add a note that indicates using fields like marriage, age and sex may be illegal within the United States, and link to CFPB guidance.


### Legal and political context
The notebook uses the term ""discrimination"" but it may be worth talking about key aspects of how the term itself is contested politically and legally.  For example, my assumption is that discrimination in this sense is closer to the legal concept of ""disparate impact"" rather than ""disparate treatment.""

This [guidance](https://files.consumerfinance.gov/f/documents/cfpb_supervision-and-examination-manual_ecoa-baseline-exam-procedures_2019-04.pdf) from the [CFPB compliance guides](https://www.consumerfinance.gov/policy-compliance/guidance/other-applicable-requirements/equal-credit-opportunity-act/) may be a helpful short summary:

<img width=""820"" alt=""Screen Shot 2020-05-12 at 10 16 14 AM"" src=""https://user-images.githubusercontent.com/1056957/81702788-be00ed80-9439-11ea-9b35-ce73b12bbb8c.png"">


These concepts are contested, and there's a case history in the US Supreme Court about this (eg, [Texas...](https://www.supremecourt.gov/opinions/14pdf/13-1371_8m58.pdf)), and HUD has recently [proposed rule changes](https://www.hud.gov/press/press_releases_media_advisories/HUD_No_19_122) related to its interpretation of disparate impact in the context of mortgage lending.  There are legal challenges related to Section 1071 in the Dodd-Frank Act that are specifically about monitoring, reporting and enforcement related to gender, race and ethnicity (recent [CFPB symposium](https://www.consumerfinance.gov/about-us/events/archive-past-events/cfpb-symposium-section-1071-dodd-frank-act/), [legal challenge](https://www.consumerfinancemonitor.com/wp-content/uploads/sites/14/2020/03/CA_Reinvestment_Coalition_v._Kraninger_-_order_granting_stip_of_settlement-1.pdf)).

Within algorithmic models in particular, it seems the Consumer Finance Protection Bureau issued a [report to Congress](https://www.consumerfinance.gov/about-us/blog/protecting-consumers-and-encouraging-innovation-2019-fair-lending-report-congress/) a few weeks ago that included some commentary on ""the use of alternative data, such as cash flow data, in credit underwriting.""  Additionally there was a ""Request for Information regarding “Tech Sprints” to encourage regulatory innovation and collaborate with stakeholders in developing viable solutions to regulatory compliance challenges.""  So depending on when the context of this notebook is taking place, that may be relevant for folks looking to this project for examples of how to do fairness work within financial services organizations.  Mortgage lending seems much more contested than credit cards, but as another data points here's a [letter from some senators](https://www.consumerfinancemonitor.com/wp-content/uploads/sites/14/2019/06/2019.6.1020Letter20to20Regulators20on20Fintech20FINAL1.pdf):

<img width=""783"" alt=""Screen Shot 2020-05-12 at 10 13 38 AM"" src=""https://user-images.githubusercontent.com/1056957/81702513-61053780-9439-11ea-80b1-e8b2f868131d.png"">

So there's a lot of data suggesting that fairness and specifically algorithmic approaches to fairness are likely to be contested in different ways depending on the legal and political context.

🟠 Minimal suggestion: Add a paragraph about discrimination, disparate treatment, and disparate impact within the US legal tradition, drawing from the CFPB guidance.

### Wider economic context
When talking about fairness in these kinds of sociotechnical contexts, particularly related to gender or racial categories, other background context might be important.  Here's some data from the Fed: [Recent Trends in Wealth-Holding by Race and Ethnicity: Evidence from the Survey of Consumer Finances, 2017](https://www.federalreserve.gov/econres/notes/feds-notes/recent-trends-in-wealth-holding-by-race-and-ethnicity-evidence-from-the-survey-of-consumer-finances-20170927.htm):

<div>
  <img width=""45%"" alt=""Screen Shot 2020-05-12 at 9 56 06 AM"" src=""https://user-images.githubusercontent.com/1056957/81700734-12ef3480-9437-11ea-9fce-a8072ba67570.png"">
  <img width=""45%"" alt=""Screen Shot 2020-05-12 at 9 56 37 AM"" src=""https://user-images.githubusercontent.com/1056957/81700737-14206180-9437-11ea-99d2-a9e9f2002e73.png"">
</div>

These may be important for understanding how the isolated decision of ""approve or reject credit card application"" fits within other aspects of peoples' experiences within the financial system or credit markets.  There is a lot of context there, but maybe adding one data point would be a start.

Separately, in the US [the NY Fed](https://www.newyorkfed.org/microeconomics/sce/credit-access.html#/) reports information about credit cards it seems, including things like:

> [In February 2020] the rejection rate for credit card applications dropped to 9.7 percent, a new series low.

That seems a useful baseline rate to understand in trying to interpret data on credit card lending decisions.  If the deployment context of the notebook is 2006 Taiwan, that context may be very different (as implied in the original paper's intro).

🟠 Minimal suggestion: Add one data point illustrating the wider sociotechnical context of credit decisions (eg, sociotechnical leading to wealth differences at a given point in time).  Add baseline rate of credit approval or denial for the deployment context.


### De-abstracting
In the US, consumer complaints related to denied credit card applications refused are published by the Consumer Financial Protection Bureau [here](https://www.consumerfinance.gov/data-research/consumer-complaints/).  Here's an overview of complaints with getting a credit card across US states over the last three years:
![image](https://user-images.githubusercontent.com/1056957/81698284-bd655880-9433-11ea-93fe-41590d639d40.png)

These are searchable and filterable and some even include narratives from real people, like [this one](https://www.consumerfinance.gov/data-research/consumer-complaints/search/detail/3562815):

> Reference # XXXX I contacted <company> credit card to discuss the reasoning for their decision. After being transferred to 3 different people I ended up talking to a supervisor named XXXX. My first question was about the credit score. She could not tell me what score that they were using. Examples FICO XXXX, Vantage 3.0, etc. The reason I was asking is because the number reflected was not even close to the actual numbers that I obtained from the credit bureau myself. In addition as we looked over my credit report there was conflicting information through out the report and she could not answer my specific questions. How can a company be following the FCRA if they cant even explain their own information to the consumer?

I understand that an example notebook might want to center the perspective of lenders, as those are the clients that users of this library might be working with.  In that circumstance, these are perhaps examples of the kind of brand or reputational damage that is part of the consideration of fairness work.  Of course the scenario could also choose to include more diverse stakeholders as well (eg, people applying for credit cards).

🟠 Minimal suggestion: Add one paragraph highlighting the experience of being approved or denied a credit card application.

---

Thanks for reading! 😄  There are many other ways that a project might model approaching working on a ML system that influenced credit decisions, and wider scope that could be tackled here in exploring how libraries and tools could support data scientists and practioners who view fairness as a sociotechnical challenge (particularly in supporting communication and collaboration with non-technical stakeholders). I'd be excited if there were ways to work on that as well, but have shared this in the spirit of opening those wider questions while also providing smaller specific suggestions for improvement.
","8","0.7117641412207502","Understanding Fairness Definition","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","614175188","issue","https://github.com/Trusted-AI/AIF360/issues/413","Notebook: Example notebooks aligned with vision whitepaper hi pals!

I think that de-abstracting would really help make productive progress on what excellence in a sociotechnical approach to ML would be.  I think looking at the same scenario and talking about it can help the project to confront the core tension in the work: between the mission on one hand, and the realities of who practioners are, what they believe, and what they need on the other hand.  For me, that's the super unique and compelling opportunity here. :)

There are many media to make educational materials, but I think a narrative Python notebook is the best way to confront this tension, because it's closer to where practitioners live (which is not to suggest that notebooks or software are all that matter!).  There are of course many aspects of a sociotechnical approach to fairness that won't fit nicely in there, and attempts at resolving that tension can be super generative!   I also found @hannawallach's comparison to accessibility super compelling.  That work requires both the mission and vision component, but also the tooling that encodes values related to that vision (eg, linter rules that fail the build if you leave out accessibility attributes in your HTML).

There may be early adopter partners who are aligned with the mission aspect of the project, and that may be a source of potential case study topics to make a notebook about.  But there also may be sources that come from the existing culture of data scientists that are not aligned with the mission.  One that I started hacking around on is the Boston housing dataset, since I happened to come upon it earlier in the week and was a bit blown away by how some educational materials approached it.  I started talking about that with @koaning in https://github.com/koaning/scikit-fairness/issues/31, and that might be worth checking out.  I'll put something together and share it in this repo at some point.

But if folks have other ideas of a specific real scenario where we could collectively make the case that there is more to be done to create tooling, process and educational support for fairness as a sociotechnical issue, it would be super helpful if you could share them here. 👍 ","25","0.3991524568661085","Research","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","611418402","issue","https://github.com/Trusted-AI/AIF360/issues/406","From scikit-lego to something fair ... and then we heard about this project. Hi all, 

my name is Vincent Warmerdam and together with Matthijs Brouns (@MBrouns) we've started a project called [scikit-lego](https://scikit-lego.readthedocs.io/en/latest/) a while ago. It's a set of scikit-learn compatible tools that we wanted to open source. It's taken off, we hear it is in production, it has 2K downloads a month and we use sklearn's testing framework to keep all the parts compatible. 

Part of that library contained tools that might make models more fair and a while ago we decided it may be best to have these tools in a seperate package. We figured scikit-fairness might be a nice name. Potentially we could make it part of the official scikit-learn contrib landscape and that would perhaps help more folks consider more fair approaches. So we started working ... preview is [here](https://github.com/koaning/scikit-fairness).

On our side, it is our long term goal to make it as simple as possible for somebody new to scikit-learn to be able to pick up tools for fairness. The project will be heavily tutorial driven and there's been interest from DataKind UK to help create content. Odds are that we prefer simpler tooling that is easier to use over state of the art tooling that requires one to be knowledgeable in the field.

Then somebody on github made us aware of this project. We weren't aware of this effort and since there may be overlap in the goals we figured it'd be good to reach out. We were happy to see other efforts to make scikit-learn compatible tools for fairness but it made us wonder what to do with our own package. 

It might make sense to collaborate, then again it might make sense to have multiple packages for fairness but either way we figured it'd be good to check in. 

So out of wonder; 

1. Are you planning on making fairness-learn something that exists inside of scikit-learn-contrib? 
2. Would you argue that your effort is more industry focussed or academia focussed? 
3. What would you say is the long term goal of your package? ","20","0.332369433832169","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","608695007","issue","https://github.com/Trusted-AI/AIF360/issues/395","Handle exceptions when relabeling results in single class #### Describe the bug
Our reductions methods relabel samples occasionally, so it's possible that a class (label) is entirely erased, resulting in a single class as opposed to two classes (for binary classification). Certain estimators are unable to handle this and throw and exception. Since this isn't unintended from a Fairlearn perspective we should catch this error and handle it appropriately.

#### Steps/Code to Reproduce
Example with `LogisticRegression` as a representative of classifiers that don't work with a single class.
```python
>>> from sklearn.linear_model import LogisticRegression
>>> LogisticRegression().fit([[1],[2],[3]], [0,0,0])
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""~\AppData\Local\Continuum\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py"", line 1558, in fit
    "" class: %r"" % classes_[0])
ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 0
```

Another example with different error message
```python
>>> from sklearn.ensemble import GradientBoostingClassifier
>>> GradientBoostingClassifier().fit([[1],[2],[3]], [0,0,0])
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""~\AppData\Local\Continuum\anaconda3\lib\site-packages\sklearn\ensemble\_gb.py"", line 1455, in fit
    y = self._validate_y(y, sample_weight)
  File ""~\AppData\Local\Continuum\anaconda3\lib\site-packages\sklearn\ensemble\_gb.py"", line 2097, in _validate_y
    % n_trim_classes)
ValueError: y contains 1 class after sample_weight trimmed classes with zero weights, while a minimum of 2 classes are required.
```

#### Expected vs. Actual Results
Fairlearn should just skip fitting in this case and use a classifier that always returns the one remaining class.
We received a `ValueError` in the case above. However, even within `sklearn` the returned error messaged isn't consistent as shown above. For that reason we can't assume that we'll be able to check for those error messages. `ValueError` is too generic to simply catch, either. It may be best to first check for the number of unique classes in `y` before calling `fit`.

#### Versions

System:
    python: 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)]
executable: ~\AppData\Local\Continuum\anaconda3\python.exe
   machine: Windows-10-10.0.18362-SP0

Python dependencies:
       pip: 19.2.3
setuptools: 41.4.0
   sklearn: 0.22.2.post1
     numpy: 1.18.1
     scipy: 1.4.1
    Cython: 0.29.13
    pandas: 0.25.1
matplotlib: 3.1.3
    tempeh: 0.1.12
","29","0.3269321005891451","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","603390900","issue","https://github.com/Trusted-AI/AIF360/issues/380","ENH - ExponentiatedGradient: use only classifiers with non-zero weights for predictions #### Is your feature request related to a problem? Please describe.
Some classifiers within ExponentiatedGradient have weights of 0. That means they don't contribute to the prediction, yet we calculate their predictions for every sample. This feature request is about optimizing ExponentiatedGradient to not waste time on this. The more iterations EG runs, the more potential there is for improvement due to the larger number of resulting classifiers.
Example based on EG tests in the repo:
```>>> eg = ExponentiatedGradient(LogisticRegression(), DemographicParity())
>>> eg.fit(X, y, sensitive_features=A)
>>> eg._last_t
5
// i.e. 6 iterations
>>> eg.predict(X)
predictions per classifier (column) per sample (row)
    0  1  2  3  4  5
0   0  0  1  1  1  0
1   1  1  0  1  1  0
2   1  1  0  1  1  0
3   0  0  1  1  1  0
4   1  1  1  0  1  1
5   0  0  1  1  1  0
6   1  1  0  1  1  0
7   0  0  1  1  1  0
8   1  1  0  1  1  0
9   1  1  0  1  1  0
10  1  1  0  1  1  0
11  1  1  0  1  1  0
12  1  0  1  0  0  1
13  1  1  1  0  1  1
14  1  0  1  0  0  1
15  1  0  1  0  0  1
16  1  1  1  0  1  1
17  1  0  1  0  0  1
18  1  1  1  0  1  1
19  1  1  1  0  1  1


aggregated product of predictions multiplied times the weight of the classifiers 
           0
0   0.430000
1   0.713846
2   0.713846
3   0.430000
4   1.000000
5   0.430000
6   0.713846
7   0.430000
8   0.713846
9   0.713846
10  0.713846
11  0.713846
12  0.286154
13  1.000000
14  0.286154
15  0.286154
16  1.000000
17  0.286154
18  1.000000
19  1.000000
output from predict(X):
array([0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1])

>>> eg._weights
0    0.000000
1    0.283846
2    0.000000
3    0.000000
4    0.430000
5    0.286154
```
The last output shows that half the weights are actually zero, so we could have avoided using them for predictions.


#### Describe the solution you'd like
Ignore classifiers with weights of zero for predictions.
","23","0.29685977666192","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","601614503","issue","https://github.com/Trusted-AI/AIF360/issues/376","Enable GridSearch with more than 2 sensitive feature values #### Is your feature request related to a problem? Please describe.
GridSearch can't be run with more than 2 sensitive feature values due to a check in the validation stage.

#### Describe the solution you'd like
Enable GridSearch for >2 sensitive feature values.","9","0.4256774796422374","Feature engineering methodology","Design"
"https://github.com/fairlearn/fairlearn","597950867","issue","https://github.com/Trusted-AI/AIF360/issues/371","Simplify Release Pipeline Our release pipeline currently release first to Test-PyPI and then to PyPI itself. In order to avoid 'burning' version numbers in Test-PyPI (because PyPI does not separate 'upload package' from 'publish package' and once a version is published, it can never be replaced), there is some elaborate machinery using a `DEV_VERSION` pipeline variable to append `.dev[n]` to the package uploaded to Test-PyPI.

This is confusing, and actually doesn't get us much benefit.

@adrinjalali  has pointed out that once the wheel is created, with can just `pip install` that file, and run our validation tests against it. If we do this, we can dump all the machinery associated with `DEV_VERSION` which requires some supporting Python scripts for the build and also infects `setup.py`. We would skip the upload to Test-PyPI entirely (or perhaps make it available, if we wanted to keep versions on Test-PyPI and PyPI in sync... although we might be unique in that).

A final (minor) benefit is that we would also then recover the `.dev[n]` option for mainline versioning.","32","0.5955734654392643","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","596575382","issue","https://github.com/Trusted-AI/AIF360/issues/370","Mismatched labels in Dashboard Testing out the v0.4.5 dashboard, I noticed the following when on the model details tab:
![image](https://user-images.githubusercontent.com/28632930/78788983-86a2ab00-797a-11ea-8c0e-8137821f6de3.png)
The large title ""Disparity in accuracy"" doesn't match the smaller ones (I'd selected precision as the metric to plot).

Shouldn't the large title match the metric chosen for comparing models?","12","0.4457718872503267","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","589456003","issue","https://github.com/Trusted-AI/AIF360/issues/353","Add a GitHub Action to test notebooks against the last released Fairlearn version Currently we can break notebooks for the last released version without noticing because we don't actually test against the last released version, but the current master version.

#351  addresses this gap and will tell us that it's failing, so that we can create a new release","32","0.7274766507606347","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","589455591","issue","https://github.com/Trusted-AI/AIF360/issues/352","Add postprocessing tests for scenarios with prefit=True and prefit=False We created a small testing gap in the move away from a PostProcessing parent class, and away from `estimator` and `unconstrained_predictor` to be more compatible with `sklearn`.

#350 addresses this","23","0.5113468510809721","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","587457341","issue","https://github.com/Trusted-AI/AIF360/issues/345","Combine ExponentiatedGradient and GridSearch into a hybrid mitigation technique Idea: run EG for a few iterations, then plug result into GS

Tasks: 
- store lambda vector in EG (#344 completed)
- recenter grid for GS around lambda vector result from EG
- adjust grid limit to be fairly restricted
- use only subset of data for EG to save time
","28","0.3773052048914117","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","585437457","issue","https://github.com/Trusted-AI/AIF360/issues/339","Bug: class labels not preserved ```
>>> eg = ExponentiatedGradient(constraints=DemographicParity(), estimator=LogisticRegression())
>>> eg.fit(pd.DataFrame([[1], [2], [3]]), [2, 0, 0], sensitive_features=[3,2,4])
>>> eg.predict([[1], [2], [3]])
0    0
1    0
2    1
dtype: int32
```","12","0.4473433890474249","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","580793707","issue","https://github.com/Trusted-AI/AIF360/issues/331","Add ratios for demographic parity & equalized odds moments Currently DP & EO are only about the difference. Ratio would make this more general.","3","0.7671876146452415","Bias detection metrics validation","Validation"
"https://github.com/fairlearn/fairlearn","576422195","issue","https://github.com/Trusted-AI/AIF360/issues/322","Approaches to end-to-end product development I'm curious how folks are doing end-to-end development!

I see the different sub-projects within the repo, including the JS test app and how it has a mix of static fixture data and randomly generated values.  This works great for some kinds of changes, but not super well for developing semantically meaningful changes.  Earlier I worked around this by writing a script to process a dataset and output more metrics into a static file in [visualization/test/__mock-data](https://github.com/fairlearn/fairlearn/tree/master/) and then updated [generateRandomMetrics](https://github.com/fairlearn/fairlearn/blob/master/visualization/test/test-page/App.jsx#L36) to read from those new fields.  But this was just hacked-together to understand how the pieces fit together :)

To improve on this, I first tried to see if there ways to develop UI elements within Jupyter.  That would enable using realistic data fed in from Python (as in the demo notebooks).  Installing a local fairlearn package worked on the Python side, but couldn't get this to load updated JS in the visualization folder.  So I looked into the build steps and started trying to see where the problem was.

I changed some localization text within [visualization/FairnessDashboard](https://github.com/fairlearn/fairlearn/tree/master/visualization/FairnessDashboard), ran `build_widget.py` and worked through differences on OSX.  But this didn't update the artifacts in the `static` folder that get bundled with the Python package, and it seems like this is because when it builds [fairlearn/widget/js](https://github.com/fairlearn/fairlearn/tree/master/fairlearn/widget/js) that project is pinned to a specific published version of the `fairlearn-dashboard` npm package.

So I backed up and tried to use `npm link` to build [fairlearn/widget/js](https://github.com/fairlearn/fairlearn/tree/master/fairlearn/widget/js) with the local modified copy of `fairlearn-dashboard`, but I can't get that working.  I can share more specific error messages (some around not being able to `npm install` because of plot.ly type defs, and others around that suggest I'm probably not doing `npm link` right since the build process can't resolve `fairlearn-dashboard`).  but I thought this might be a good point to instead ask how others folks are doing this :)

@chisingh it seems like you must have been  https://github.com/fairlearn/fairlearn/pull/283 so thought you in particular might have thoughts on this.  Thanks!","13","0.4214363333745994","Artifact generation and benchmarking","Deployment"
"https://github.com/fairlearn/fairlearn","576336801","issue","https://github.com/Trusted-AI/AIF360/issues/320","Remove matplotlib TKAgg special case with matplotlib>3.2.0 related to #318 

With matplotlib release 3.2.0 the TKAgg backend stopped working on Linux causing our tests to fail. #319 fixes that by using TKAgg only on MacOS. We should check for future matplotlib releases whether they fixed that and revert the special case, i.e. use TKAgg everywhere like it used to be.","32","0.6641990912100149","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","576324350","issue","https://github.com/Trusted-AI/AIF360/issues/318","matplotlib 3.2.0 breaks linux tests https://dev.azure.com/responsibleai/fairlearn/_build/results?buildId=4508&view=logs&j=d0d954b5-f111-5dc4-4d76-03b6c9d0cf7e&t=2a79e97e-143e-5ca8-c651-c67f5c62e229","13","0.4940127077223855","Artifact generation and benchmarking","Deployment"
"https://github.com/fairlearn/fairlearn","575711458","issue","https://github.com/Trusted-AI/AIF360/issues/317","Tease apart eps in ExponentiatedGradient According to @MiroDudik , the `eps` parameter in `ExponentiatedGradient` is used for two slightly different purposes. It should be split into two independent parameters.","10","0.3815735833673053","Model development","Development"
"https://github.com/fairlearn/fairlearn","575710315","issue","https://github.com/Trusted-AI/AIF360/issues/316","Improve ExponentiatedGradient Testing The smoke tests for `ExponentiatedGradient` are overly self-validating. In particular, when the [expected disparity and error are calculated by the test](https://github.com/fairlearn/fairlearn/blob/23d535a628856ba4dec37a09096355977b44a694/test/unit/reductions/exponentiated_gradient/test_exponentiatedgradient_smoke.py#L87), the `Moment` objects are used. Since the `Moment` objects drive `ExponentiatedGradient` itself, these checks only verify consistency, not actual correctness.

The tests should be augmented to compute disparity and errors using the `metrics` portion of the package, and then to compare with the result from `ExponentiatedGradient`","28","0.3329615170105967","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","575339112","issue","https://github.com/Trusted-AI/AIF360/issues/314","Question: should we move to numpydoc format? Many libraries use the [`numpydoc`](https://numpydoc.readthedocs.io/en/latest/format.html) format, which is understood by many tools in the ecosystem as well.

Would there be any interest in moving to it instead of what we have?","20","0.3815735833673054","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","574648690","issue","https://github.com/Trusted-AI/AIF360/issues/313","Documentation for Build & Release Pipelines Our `devops` directory is rather sparsely documented. Since the build and release pipelines are getting quite involved, adding a ReadMe file would be good, especially combined with more comments in the YAML files.","32","0.4359368331199316","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","572782302","issue","https://github.com/Trusted-AI/AIF360/issues/312","GridSearch incompatible with models such as LightGBM that rely on dataframe metadata The `GridSearch` estimator removes the metadata from input dataframes via `_validate_and_reformat_input` which calls the following methods:
```Python 
X = check_array(X)
...
return pd.DataFrame(X) 
```
Estimators such as `LightGBM` rely on metadata for categorical columns (column type = `category`) which they process internally more efficiently than by user crafted dummies. The preprocessing function will erase the metadata causing `LightGBM` to process the columns as numerical rather than categorical.

Furthermore, execution of GridSearch fails if there is metadata because of the following two calls:
```
objective.load_data(X_train, y_train, **kwargs) # ln 197
...
current_estimator.fit(X, y_reduction, sample_weight=weights) # ln 237
```
The current estimator is trained on `X` which contains the metadata and the ` objective.gamma` function calls predict on `X_train` which doesn't contain the metadata, which leads to the following error:

```Python
# General imports
import numpy as np
import pandas as pd
import lightgbm as lgb
from fairlearn.reductions import GridSearch, EqualizedOdds

# Data
n = 100
X0 = np.random.normal(size=n)
X1 = np.random.choice([1, 2, 3], size=n)
Y = np.random.choice([0, 1], size=n)
A = np.random.choice([0, 1], size=n)
df = pd.DataFrame({""X0"": X0, ""X1"": X1})
# Set X1 as categorical
df['X1'] = df['X1'].astype('category')

# Model
model = lgb.LGBMClassifier()

# Fairlearn
sweep = GridSearch(model,
                   constraints=EqualizedOdds())
sweep.fit(df, Y, sensitive_features=A)
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-2-e41612319198> in <module>
     22 sweep = GridSearch(model,
     23                    constraints=EqualizedOdds())
---> 24 sweep.fit(df, Y, sensitive_features=A)

~\AppData\Local\Continuum\anaconda3\lib\site-packages\fairlearn\reductions\_grid_search\grid_search.py in fit(self, X, y, **kwargs)
    242             nxt = GridSearchResult(current_estimator,
    243                                    lambda_vec,
--> 244                                    objective.gamma(predict_fct)[0],
    245                                    self.constraints.gamma(predict_fct),
    246                                    oracle_call_execution_time)

~\AppData\Local\Continuum\anaconda3\lib\site-packages\fairlearn\reductions\_moments\error_rate.py in gamma(self, predictor)
     19     def gamma(self, predictor):
     20         """"""Return the gamma values for the given predictor.""""""
---> 21         pred = predictor(self.X)
     22         error = pd.Series(data=(self.tags[_LABEL] - pred).abs().mean(),
     23                           index=self.index)

~\AppData\Local\Continuum\anaconda3\lib\site-packages\fairlearn\reductions\_grid_search\grid_search.py in predict_fct(X)
    239             logger.debug(""Call to underlying estimator complete"")
    240 
--> 241             def predict_fct(X): return current_estimator.predict(X)
    242             nxt = GridSearchResult(current_estimator,
    243                                    lambda_vec,

~\AppData\Local\Continuum\anaconda3\lib\site-packages\lightgbm\sklearn.py in predict(self, X, raw_score, num_iteration, pred_leaf, pred_contrib, **kwargs)
    812         """"""Docstring is inherited from the LGBMModel.""""""
    813         result = self.predict_proba(X, raw_score, num_iteration,
--> 814                                     pred_leaf, pred_contrib, **kwargs)
    815         if raw_score or pred_leaf or pred_contrib:
    816             return result

~\AppData\Local\Continuum\anaconda3\lib\site-packages\lightgbm\sklearn.py in predict_proba(self, X, raw_score, num_iteration, pred_leaf, pred_contrib, **kwargs)
    861         """"""
    862         result = super(LGBMClassifier, self).predict(X, raw_score, num_iteration,
--> 863                                                      pred_leaf, pred_contrib, **kwargs)
    864         if self._n_classes > 2 or raw_score or pred_leaf or pred_contrib:
    865             return result

~\AppData\Local\Continuum\anaconda3\lib\site-packages\lightgbm\sklearn.py in predict(self, X, raw_score, num_iteration, pred_leaf, pred_contrib, **kwargs)
    663                              % (self._n_features, n_features))
    664         return self.booster_.predict(X, raw_score=raw_score, num_iteration=num_iteration,
--> 665                                      pred_leaf=pred_leaf, pred_contrib=pred_contrib, **kwargs)
    666 
    667     @property

~\AppData\Local\Continuum\anaconda3\lib\site-packages\lightgbm\basic.py in predict(self, data, num_iteration, raw_score, pred_leaf, pred_contrib, data_has_header, is_reshape, **kwargs)
   2413         return predictor.predict(data, num_iteration,
   2414                                  raw_score, pred_leaf, pred_contrib,
-> 2415                                  data_has_header, is_reshape)
   2416 
   2417     def refit(self, data, label, decay_rate=0.9, **kwargs):

~\AppData\Local\Continuum\anaconda3\lib\site-packages\lightgbm\basic.py in predict(self, data, num_iteration, raw_score, pred_leaf, pred_contrib, data_has_header, is_reshape)
    502         if isinstance(data, Dataset):
    503             raise TypeError(""Cannot use Dataset instance for prediction, please use raw data instead"")
--> 504         data = _data_from_pandas(data, None, None, self.pandas_categorical)[0]
    505         predict_type = C_API_PREDICT_NORMAL
    506         if raw_score:

~\AppData\Local\Continuum\anaconda3\lib\site-packages\lightgbm\basic.py in _data_from_pandas(data, feature_name, categorical_feature, pandas_categorical)
    322         else:
    323             if len(cat_cols) != len(pandas_categorical):
--> 324                 raise ValueError('train and valid dataset categorical_feature do not match.')
    325             for col, category in zip_(cat_cols, pandas_categorical):
    326                 if list(data[col].cat.categories) != list(category):

ValueError: train and valid dataset categorical_feature do not match.

```
","17","0.6712491966978098","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","572520158","issue","https://github.com/Trusted-AI/AIF360/issues/311","Purpose of FairLearn with AIF360 What do you see as the purpose or future of FairLearn with AIF360 gaining support? Advantages, disadvantages and the like. ","13","0.4059034145489473","Artifact generation and benchmarking","Deployment"
"https://github.com/fairlearn/fairlearn","569996601","issue","https://github.com/Trusted-AI/AIF360/issues/308","ThresholdOptimizer as a scikit-learn meta-estimator `ThresholdOptimizer`'s current implementation looks like a meta-estimator, but does not implement it as meta-estimators are implemented in sklearn.

One of the issues right now, is that it implements two different cases, one is when the child estimator is already fitted, one is when it's not.

As mentioned in https://github.com/fairlearn/fairlearn/pull/282#discussion_r377255359

> Sorry for a delay on this, but I think that `warm_star` is confusing in this context.
> 
> Just to state the obvious: the high-level semantics of `ThresholdOptimizer` (after it is fitted) is really of a two-stage pipeline. The first step is implemented by `self._estimator.predict()`. It takes as input `X` and returns `y`. The returned `y` is then transformed into `y'` (inside `self.predict()`).
> 
> We would like to enable two styles of fitting this pipeline: (1) fitting both steps from the provided data; or (2) fitting only the second step from the provided data (and using the provided `unconstrained_predictor` as the first step). Now if I think about a warm start, I would expect re-fitting a previously fitted pipeline, using the previous pipeline as a warm start. This to me is quite different from (2), which is just fitting the second stage of the pipeline (from scratch) while keeping the first stage of the pipeline untouched.
> 
> I'm not quite sure I understand the issue with the interaction between the cloning behavior and the `unconstrained_predictor` argument.

This makes it tricky to have a usual sklearn meta-estimator, since cloning the estimator (`ThresholdOptimizer` in this case) does not keep the fitted child estimator. For example, `Pipeline`:

``` python
In [1]: from sklearn.pipeline import Pipeline                   

In [2]: from sklearn.svm import SVC                              

In [3]: from sklearn.datasets import make_classification

In [4]: X, y = make_classification()

In [5]: clf = SVC().fit(X, y)

In [6]: Pipeline([('svm', clf)]).predict(X)
Out[6]: 
array([..., 0, 1, 1])

In [7]: from sklearn import clone

In [8]: clone(Pipeline([('svm', clf)])).predict(X)
...

NotFittedError: This SVC instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.

```

And that's important since `GridSearchCV` would `clone` the estimators before fitting them.

Another point regarding `warm_start`, is that right now the user has to call `fit` even if the given estimator is pre-fitted. In `sklearn`, we usually assume `warm_start` means the user has called a fit on the estimator already; but here we could use it with a different meaning and assume that the user has passed a pre-fitted estimator. The `warm_start` semantics of sklearn are not very well-defined anyway, and we're still working on them.

__EDIT__

So my proposal is to implement a usual meta-estimator as is in sklearn, and include the `warm_start` parameter to support a pre-fitted estimator.

WDYT @MiroDudik @romanlutz ","23","0.7809663202210233","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","566482773","issue","https://github.com/Trusted-AI/AIF360/issues/300","Title copy in WizardReport should vary depending on the disparity metric hello!  I think this is a UX bug, but perhaps I'm misunderstanding.  I'll walk through what I see to try to understand if this is an issue or I'm misunderstanding.

### 1. Pick ""precision"" for a disparity metric
Let's say I want to look at my binary classifier and see how precision changes for sensitive attributes.  I think the root UX issue might be that the title copy is asking how I want to measure ""accuracy"" which is an overloaded term on this screen where it has a precise technical meaning:
<img width=""945"" alt=""Screen Shot 2020-02-17 at 1 53 59 PM"" src=""https://user-images.githubusercontent.com/1056957/74681384-43556880-5191-11ea-9f25-5630f859ab04.png"">

### 2. Look at model comparisons
This matches what I expect; I see the disparity in precision, which is what I chose.
<img width=""959"" alt=""Screen Shot 2020-02-17 at 1 53 42 PM"" src=""https://user-images.githubusercontent.com/1056957/74681492-8a435e00-5191-11ea-9231-468afe529e5a.png"">

### 3. Look at precision by sensitive attribute
On this screen, the header copy says ""Disparity in accuracy,"" which isn't what I expect.  I care about precision, and have been looking at precision, and looking closer, I realize that the chart is actually showing precision.
<img width=""981"" alt=""Screen Shot 2020-02-17 at 1 53 34 PM"" src=""https://user-images.githubusercontent.com/1056957/74681527-9af3d400-5191-11ea-8f22-b765ea142454.png"">

I think the root issue is using the term ""accuracy"" in a more informal way, but in this context it already has a precise technical meaning.  If I'm understanding this properly, then one approach could be to rename this ""disparity metric"" or ""focus metric"" or something like that.  Alternately, the initial prompt could be ""How do you want to measure model performance?"" or something that is more generic and doesn't have a specific technical meaning.

Let me know what y'all think! 👍 ","12","0.3979161420297155","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","566476885","issue","https://github.com/Trusted-AI/AIF360/issues/298","Support python 3.8 ... and have validation for the support through test pipelines","32","0.7601417399804496","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","563968641","issue","https://github.com/Trusted-AI/AIF360/issues/293","tests fail with coverage==5.0.3 User reported having to manually downgrade. v4.4.1 worked. I can confirm v4.5.4 works for me. Will check >=5 today","32","0.4919656614571868","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","563880126","issue","https://github.com/Trusted-AI/AIF360/issues/290","SECURITY.md file It seems the `SECURITY.md` file is from the time where `fairlearn` was under the _microsoft_ org. Now that it's not, does it need to be there?","20","0.579531974549768","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","559798206","issue","https://github.com/Trusted-AI/AIF360/issues/285","Automatic reviewer addition to PRs I've seen this in other repositories, e.g. https://github.com/pytorch/pytorch/blob/master/CODEOWNERS

and we've previously set up the CODEOWNERS file, so this should be possible.

@adrinjalali if you've seen a different way of handling this lmk!","20","0.489938446969697","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","557506291","issue","https://github.com/Trusted-AI/AIF360/issues/278","RFC: have benchmarks instead of/on top of current perf tests Currently the performance tests are benchmarks which are supposed to be run on azure-ml. There are two issues with this:

- most people don't have an azure-ml subscription, and the results are not public.
- running the benchmarks independently is kinda impossible, and it's cryptic code which I'm not sure where I'm supposed to extract the actual benchmark from. I guess `script_generation.py`?

I'm also not sure if they're tests, they/it seems to be a benchmark, but I may be missing something.

If it's not a test suite which is run by people who are going to package the lib, should it be moved to a benchmark folder instead maybe? And have standalone `.py` files which do the tests locally?

p.s. I was gonna check the performance and experiment with `numba`, and benchmarks seemed like a good place to start. ","13","0.628206206374569","Artifact generation and benchmarking","Deployment"
"https://github.com/fairlearn/fairlearn","556861721","issue","https://github.com/Trusted-AI/AIF360/issues/275","Enhancements for GroupMetricResult and GroupMetricSet Both `GroupMetricResult` and `GroupMetricSet` should have comparators implemented, to ease equality checking in tests.

Also, `GroupMetricSet` should take care of ensuring that the `groups` property is a set of sequential integers from zero, and fill out the `group_names` property. Given this, consider turning `group_names` into a simple list, rather than a dictionary.","17","0.5297316017316018","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","555844297","issue","https://github.com/Trusted-AI/AIF360/issues/273","Widget needs minimum padding for over-/underprediction charts If the overprediction bars are very large compared to the underprediction bars the label of the underprediction bars (e.g. 0.1%) may be cut off.

![image](https://user-images.githubusercontent.com/10245648/73215241-a2d7cf80-4121-11ea-9ed3-3d62a3e4039b.png)
","12","0.3388382090804235","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","555758909","issue","https://github.com/Trusted-AI/AIF360/issues/270","Validate that checked in widget files aren't stale This is a reminder to set --assert-no-changes in build-widget-job-template.yml

Currently some of the files, particularly extension.js, change every time it's built which renders the check in the existing form useless.","32","0.509081498211933","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","555643826","issue","https://github.com/Trusted-AI/AIF360/issues/269","Unify string-metric mappings Currently the `GroupMetricSet` and `FairlearnDashboard` implement the same mapping of strings to metric functions, but do so separately. This is obviously a source of potential bugs an heartache in the future, so some common mapping would be desirable.","15","0.3434492636856003","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","554895505","issue","https://github.com/Trusted-AI/AIF360/issues/266","Performance tests fail in image build stage https://ml.azure.com/experiments/id/737565a7-514b-4dde-9e69-6b2644929eb1/runs/perftest_1579846054_95b06609?wsid=/subscriptions/cecafb73-04ae-4432-9f96-d96925d28058/resourcegroups/fairlearn-automation/workspaces/fairlearnperf&tid=72f988bf-86f1-41af-91ab-2d7cd011db47#logs","32","0.5419803126809495","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","554839872","issue","https://github.com/Trusted-AI/AIF360/issues/265","test_bgl_unfair fails consistently on Windows with Python3.6 Example: https://dev.azure.com/responsibleai/fairlearn/_build/results?buildId=3657&view=logs&jobId=a846d25a-e32c-5640-1b53-e815fab94407&j=f2ae15cd-30d2-5fb2-07f7-558e9be6b891&s=778b61a2-25e7-5600-2cb0-dd2dc8e23627

The affected code:
```
assert logging_all_close([[3.2, 11.2],
                              [-3.47346939, 10.64897959],
                              [-2.68, 10.12],
                              [-1.91764706, 9.61176471],
                              [-1.18461538,  9.12307692],
                              [-0.47924528,  8.65283019],
                              [0.2, 0.7]],
                             all_predict)
```
and only the first entry fails.
```
E        +  where False = logging_all_close([[3.2, 11.2], [-3.47346939, 10.64897959], [-2.68, 10.12], [-1.91764706, 9.61176471], [-1.18461538, 9.12307692], [-0.47924528, 8.65283019], ...], [array([ 3.03010885, 11.2       ]), array([-3.47346939, 10.64897959]), array([-2.68, 10.12]), array([-1.91764706,  9.61176471]), array([-1.18461538,  9.12307692]), array([-0.47924528,  8.65283019]), ...])

test\unit\reductions\grid_search\test_grid_search_regression.py:72: AssertionError
---------------------------- Captured stdout call -----------------------------
a mismatches:  [3.2]
b mismatches:  [3.03010885]
mismatch indices:  (array([0], dtype=int64), array([0], dtype=int64))
```","16","0.3199205166418281","Testing","Maintenance"
"https://github.com/fairlearn/fairlearn","554498103","issue","https://github.com/Trusted-AI/AIF360/issues/264","Support for Equality of Opportunity constraint Is there a way to use Equality of Opportunity or True Positive Rate equality constraint?
Any quick change to [ConditionalSelectionRate](https://github.com/fairlearn/fairlearn/blob/master/fairlearn/reductions/_moments/conditional_selection_rate.py#L12) implementation?","3","0.8506964050442316","Bias detection metrics validation","Validation"
"https://github.com/fairlearn/fairlearn","554417173","issue","https://github.com/Trusted-AI/AIF360/issues/263","Dashboard has no tests We need at least basic validation to make sure the dashboard shows up at all. This could be as simple as checking that there's some HTML field in the browser, or maybe this can be checked with papermill/nbformat/scrapbook. Any direction that seems promising is appreciated.","32","0.3959316346859599","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","553664042","issue","https://github.com/Trusted-AI/AIF360/issues/258","Release versioning We currently push a version to test pypi with `dev0` appended, and then another one to pypi. See

https://test.pypi.org/project/fairlearn/#history
and
https://pypi.org/project/fairlearn/#history
for examples.

@adrinjalali has proposed to simply do a rc release to pypi, and once it's stable release without rc. If there are still issues then release with patch. Let's use this thread as a discussion forum.

At the end we need to update the documentation (CONTRIBUTING.md) accordingly.","32","0.7121918156400915","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","553545738","issue","https://github.com/Trusted-AI/AIF360/issues/256","Naming Conventions We should review our property naming conventions. For example, `sklearn` puts trailing underscores on attributes which have been calculated by operation of the class. See e.g.:
https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html","30","0.7549124053030304","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","553542682","issue","https://github.com/Trusted-AI/AIF360/issues/254","Documentation - argument types Our documentation:
https://fairlearn.readthedocs.io/en/latest/index.html
does not consistently provide links to definitions of Estimators (which should come from `sklearn`), `numpy.ndarray` and `pandas`. These should be remedied.","2","0.3896780303030304","Performance measurement","Validation"
"https://github.com/fairlearn/fairlearn","553541489","issue","https://github.com/Trusted-AI/AIF360/issues/253","Removed NotFittedException Our `NotFittedException` should be retired in favour of the `NotFittedError` from `sklearn`; having our own type doesn't achieve anything.

It would also be good to make sure that all our Estimators implement this check (and are tested for it).","23","0.4777630163851459","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","553540654","issue","https://github.com/Trusted-AI/AIF360/issues/252","Postprocessing - Separate plotting Having the decision on plotting done in the constructor of the `ThresholdOptimizer` object is rather odd. The plotting should be moved to a separate routine","15","0.5160371987094329","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","550931315","issue","https://github.com/Trusted-AI/AIF360/issues/248","python tests instead of notebooks Would it be okay to replace the notebooks tests with `.py` tests? It'd be much easier to work with, and it'll also remove a bunch of test dependencies we have right now.","13","0.4670002780094521","Artifact generation and benchmarking","Deployment"
"https://github.com/fairlearn/fairlearn","550930185","issue","https://github.com/Trusted-AI/AIF360/issues/247","cannot run tests w/o azure authentication Running tests fail with:

```
______________________ ERROR at setup of test_perf[[dataset:adult_uci,predictor:rbm_svm,mitigator:ThresholdOptimizer,disparity_metric:equalized_odds]] ______________________

self = <msrestazure.azure_active_directory.AdalAuthentication object at 0x7fd9e7c793a0>, session = <requests.sessions.Session object at 0x7fd9e7c791c0>

    def signed_session(self, session=None):
        """"""Create requests session with any required auth headers applied.
    
        If a session object is provided, configure it directly. Otherwise,
        create a new session and return it.
    
        :param session: The session to configure for authentication
        :type session: requests.Session
        :rtype: requests.Session
        """"""
        session = super(AdalAuthentication, self).signed_session(session)
    
        try:
>           raw_token = self._adal_method(*self._args, **self._kwargs)

../.venv/lib/python3.8/site-packages/msrestazure/azure_active_directory.py:448: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <adal.authentication_context.AuthenticationContext object at 0x7fd9e563ed30>, resource = 'https://management.core.windows.net/', client_id = None
client_secret = None

    def acquire_token_with_client_credentials(self, resource, client_id, client_secret):
        '''Gets a token for a given resource via client credentials.
    
        :param str resource: A URI that identifies the resource for which the
            token is valid.
        :param str client_id: The OAuth client id of the calling application.
        :param str client_secret: The OAuth client secret of the calling application.
        :returns: dict with several keys, include ""accessToken"".
        '''
        def token_func(self):
            token_request = TokenRequest(self._call_context, self, client_id, resource)
            return token_request.get_token_with_client_credentials(client_secret)
    
>       return self._acquire_token(token_func)

../.venv/lib/python3.8/site-packages/adal/authentication_context.py:179: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <adal.authentication_context.AuthenticationContext object at 0x7fd9e563ed30>
token_func = <function AuthenticationContext.acquire_token_with_client_credentials.<locals>.token_func at 0x7fd9e45d6790>, correlation_id = None

    def _acquire_token(self, token_func, correlation_id=None):
        self._call_context['log_context'] = log.create_log_context(
            correlation_id or self.correlation_id, self._call_context.get('enable_pii', False))
        self.authority.validate(self._call_context)
>       return token_func(self)

../.venv/lib/python3.8/site-packages/adal/authentication_context.py:128: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <adal.authentication_context.AuthenticationContext object at 0x7fd9e563ed30>

    def token_func(self):
        token_request = TokenRequest(self._call_context, self, client_id, resource)
>       return token_request.get_token_with_client_credentials(client_secret)

../.venv/lib/python3.8/site-packages/adal/authentication_context.py:177: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <adal.token_request.TokenRequest object at 0x7fd9e45cad00>, client_secret = None

    def get_token_with_client_credentials(self, client_secret):
        self._log.debug(""Getting token with client credentials."")
        try:
            token = self._find_token_from_cache()
            if token:
                return token
        except AdalError:
            self._log.exception('Attempt to look for token in cache resulted in Error')
    
        oauth_parameters = self._create_oauth_parameters(OAUTH2_GRANT_TYPE.CLIENT_CREDENTIALS)
        oauth_parameters[OAUTH2_PARAMETERS.CLIENT_SECRET] = client_secret
    
>       token = self._oauth_get_token(oauth_parameters)

../.venv/lib/python3.8/site-packages/adal/token_request.py:310: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <adal.token_request.TokenRequest object at 0x7fd9e45cad00>
oauth_parameters = {'client_secret': None, 'grant_type': 'client_credentials', 'resource': 'https://management.core.windows.net/'}

    def _oauth_get_token(self, oauth_parameters):
        client = self._create_oauth2_client()
>       return client.get_token(oauth_parameters)

../.venv/lib/python3.8/site-packages/adal/token_request.py:112: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <adal.oauth2_client.OAuth2Client object at 0x7fd9e45ca550>
oauth_parameters = {'client_secret': None, 'grant_type': 'client_credentials', 'resource': 'https://management.core.windows.net/'}

    def get_token(self, oauth_parameters):
        token_url = self._create_token_url()
        url_encoded_token_request = urlencode(oauth_parameters)
        post_options = util.create_request_options(self, _REQ_OPTION)
    
        operation = ""Get Token""
    
        try:
            resp = requests.post(token_url.geturl(),
                                 data=url_encoded_token_request,
                                 headers=post_options['headers'],
                                 verify=self._call_context.get('verify_ssl', None),
                                 proxies=self._call_context.get('proxies', None),
                                 timeout=self._call_context.get('timeout', None))
    
            util.log_return_correlation_id(self._log, operation, resp)
        except Exception:
            self._log.exception(""%(operation)s request failed"", {""operation"": operation})
            raise
    
        if util.is_http_success(resp.status_code):
            return self._handle_get_token_response(resp.text)
        else:
            if resp.status_code == 429:
                resp.raise_for_status()  # Will raise requests.exceptions.HTTPError
            return_error_string = _ERROR_TEMPLATE.format(operation, resp.status_code)
            error_response = """"
            if resp.text:
                return_error_string = u""{} and server response: {}"".format(return_error_string,
                                                                           resp.text)
                try:
                    error_response = resp.json()
                except ValueError:
                    pass
>           raise AdalError(return_error_string, error_response)
E           adal.adal_error.AdalError: Get Token request returned http error: 400 and server response: {""error"":""invalid_request"",""error_description"":""AADSTS900144: The request body must contain the following parameter: 'client_id'.\r\nTrace ID: 9ff1d754-5f79-403a-9ae7-31cd01d02800\r\nCorrelation ID: 8eeb21da-25ab-4993-8ed7-76b06b6837b4\r\nTimestamp: 2020-01-16 16:47:32Z"",""error_codes"":[900144],""timestamp"":""2020-01-16 16:47:32Z"",""trace_id"":""9ff1d754-5f79-403a-9ae7-31cd01d02800"",""correlation_id"":""8eeb21da-25ab-4993-8ed7-76b06b6837b4"",""error_uri"":""https://login.microsoftonline.com/error?code=900144""}

../.venv/lib/python3.8/site-packages/adal/oauth2_client.py:289: AdalError

During handling of the above exception, another exception occurred:

    @pytest.fixture(scope=""session"")
    def workspace():
>       return get_workspace()

test/perf/conftest.py:71: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
../.venv/lib/python3.8/site-packages/tempeh/execution/azureml/workspace.py:48: in get_workspace
    resource_management_client = resource_client_factory(auth, SUBSCRIPTION_ID)
../.venv/lib/python3.8/site-packages/azureml/_base_sdk_common/common.py:354: in resource_client_factory
    return auth._get_service_client(ResourceManagementClient, subscription_id)
../.venv/lib/python3.8/site-packages/azureml/core/authentication.py:146: in _get_service_client
    return _get_service_client_using_arm_token(self, client_class, subscription_id,
../.venv/lib/python3.8/site-packages/azureml/core/authentication.py:1590: in _get_service_client_using_arm_token
    adal_auth_object = auth._get_adal_auth_object()
../.venv/lib/python3.8/site-packages/azureml/core/authentication.py:191: in _get_adal_auth_object
    token = self.get_authentication_header()[""Authorization""].split("" "")[1]
../.venv/lib/python3.8/site-packages/azureml/core/authentication.py:88: in get_authentication_header
    auth_header = {""Authorization"": ""Bearer "" + self._get_arm_token()}
../.venv/lib/python3.8/site-packages/azureml/core/authentication.py:863: in wrapper
    new_token = actual_function(self, *args, **kwargs)
../.venv/lib/python3.8/site-packages/azureml/core/authentication.py:962: in _get_arm_token
    header = execute_func(self._get_sp_credential_object().signed_session).headers['Authorization']
../.venv/lib/python3.8/site-packages/azureml/_restclient/clientbase.py:51: in execute_func
    return ClientBase._execute_func_internal(
../.venv/lib/python3.8/site-packages/azureml/_restclient/clientbase.py:294: in _execute_func_internal
    left_retry = cls._handle_retry(back_off, left_retry, total_retry, error, logger, func)
../.venv/lib/python3.8/site-packages/azureml/_restclient/clientbase.py:345: in _handle_retry
    raise error
../.venv/lib/python3.8/site-packages/azureml/_restclient/clientbase.py:292: in _execute_func_internal
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <msrestazure.azure_active_directory.AdalAuthentication object at 0x7fd9e7c793a0>, session = <requests.sessions.Session object at 0x7fd9e7c791c0>

    def signed_session(self, session=None):
        """"""Create requests session with any required auth headers applied.
    
        If a session object is provided, configure it directly. Otherwise,
        create a new session and return it.
    
        :param session: The session to configure for authentication
        :type session: requests.Session
        :rtype: requests.Session
        """"""
        session = super(AdalAuthentication, self).signed_session(session)
    
        try:
            raw_token = self._adal_method(*self._args, **self._kwargs)
        except adal.AdalError as err:
            # pylint: disable=no-member
            if 'AADSTS70008:' in ((getattr(err, 'error_response', None) or {}).get('error_description') or ''):
                raise Expired(""Credentials have expired due to inactivity."")
            else:
>               raise AuthenticationError(err)
E               msrest.exceptions.AuthenticationError: Get Token request returned http error: 400 and server response: {""error"":""invalid_request"",""error_description"":""AADSTS900144: The request body must contain the following parameter: 'client_id'.\r\nTrace ID: 9ff1d754-5f79-403a-9ae7-31cd01d02800\r\nCorrelation ID: 8eeb21da-25ab-4993-8ed7-76b06b6837b4\r\nTimestamp: 2020-01-16 16:47:32Z"",""error_codes"":[900144],""timestamp"":""2020-01-16 16:47:32Z"",""trace_id"":""9ff1d754-5f79-403a-9ae7-31cd01d02800"",""correlation_id"":""8eeb21da-25ab-4993-8ed7-76b06b6837b4"",""error_uri"":""https://login.microsoftonline.com/error?code=900144""}

../.venv/lib/python3.8/site-packages/msrestazure/azure_active_directory.py:454: AuthenticationError

```","31","0.9779048448705414","Testing","Maintenance"
"https://github.com/fairlearn/fairlearn","550894796","issue","https://github.com/Trusted-AI/AIF360/issues/245","Test dependencies `conftest.py` under `test/perf` checks for `azureml`, but would also give the same error if the user doesn't have `tempeh` installed.

I encountered this error since I avoid installing everything which is in `requirements.txt` not to bloat my env unnecessarily.

Also, we could have the performance tests optional and skip them if the user doesn't have required dependencies. They're not an integral part of the functionality test anyway, are they?","21","0.3729048177906172","Installation and shell commands","Deployment"
"https://github.com/fairlearn/fairlearn","550874944","issue","https://github.com/Trusted-AI/AIF360/issues/244","Question: are other types of fairness in scope? The terminology doc says:

> There are many approaches to conceptualizing fairness. In fairlearn, we follow the approach known as group fairness, which asks: Which groups of individuals are at risk for experiencing harms?

Does that mean other definitions of fairness are out of scope, or is that they're not included yet?

Also, are mitigation methods included here inherently incapable of working with other forms of fariness definitions?

Shouldn't it be up to the ethical counsel and lawyers and the local laws to define which types of fairness should be enforced? I'm not sure why this library should take a stand on that.","6","0.3476576685631282","API expansion","Development"
"https://github.com/fairlearn/fairlearn","550870116","issue","https://github.com/Trusted-AI/AIF360/issues/243","DOC How is randomization handled The docs say:

> Randomization. In contrast with scikit-learn, estimators in fairlearn can produce randomized predictors. Randomization of predictions is required to satisfy many definitions of fairness. Because of randomization, it is possible to get different outputs from the predictor's predict method on identical data. For each of our methods, we provide explicit access to the probability distribution used for randomization.

`sklearn` does have randomization in many estimators (`RandomForests` as an example :P). But randomness is always controlled by a `random_seed` parameter. Reproducibility requires setting this parameter, to be able to go back and reproduce the results.

It is understandable if in the context of fairness the RNG shouldn't be fixed, but shouldn't the user be able to feed in a seed or a seed and have reproducible results?

Also, the user can set the RNG, and still get probabilistic output given the same input. I could have:

``` python
clf = MyClassifier(random_seed=42)
clf.fit(X, y)
clf.predict(x0) -> returns 0
clf.predict(x0) -> returns 1
```

but if the user runs the same script again, they'll get the same output as before.","23","0.7879280811367027","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","550373007","issue","https://github.com/Trusted-AI/AIF360/issues/242","Create makefile and replace powershell All popular repos provide makefiles, so we should, too.

Apart from that we should move away from powershell since we want to be inclusive to all kinds of users & contributors.","13","0.6614790915417224","Artifact generation and benchmarking","Deployment"
"https://github.com/fairlearn/fairlearn","549093275","issue","https://github.com/Trusted-AI/AIF360/issues/238","Enable switching between metrics within a report Allow user to select multiple metrics of interest and switch between them inside of the report view.","24","0.4966707433075146","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","549092747","issue","https://github.com/Trusted-AI/AIF360/issues/237","Allow grouping by multiple sensitive features Multiple sensitive features are a common scenario. The only other way to capture intersectionality is by creating a 1-dimensional grouping vector manually and providing that to Fairlearn. We want to make this easier and allow users to provide an m-dimensional matrix. This needs to be enabled on three levels:

- [x] mitigation techniques
- [x] metrics
- [ ] dashboard

The current implementation for mitigation techniques simply considers all combinations from the `sensitive_features` matrix, e.g. if column A has 4 distinct values and column B has 3 distinct values and all combinations occur we have 4*3=12 groups.

It is possible that users want more fine-grained control, but we'll wait for that feedback. This could mean specifying multiple constraints (demographic parity w.r.t. column A and demographic parity w.r.t. column B, but not intersections of A and B, for example).","9","0.3348278463355967","Feature engineering methodology","Design"
"https://github.com/fairlearn/fairlearn","549092201","issue","https://github.com/Trusted-AI/AIF360/issues/236","Add CDF curves to report For regression models, add view with CDF curves and minimum distance between sub-group metric","30","0.4555646147574398","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","549091495","issue","https://github.com/Trusted-AI/AIF360/issues/235","Add Error bars Show error bars in reports when available","7","0.7403805496828751","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","548259761","issue","https://github.com/Trusted-AI/AIF360/issues/234","simplex method failed Sometimes I get this error. Sometimes it's fine. It seems the error is from ```/scipy/optimized/_linprog.py```

```ValueError: Phase 1 of the simplex method failed to find a feasible solution. The pseudo-objective function evaluates to 1.7e-12 which exceeds the required tolerance of 1e-12 for a solution to be considered 'close enough' to zero to be a basic solution. Consider increasing the tolerance to be greater than 1.7e-12. If this tolerance is unacceptably  large the problem may be infeasible.```","10","0.6582893784258974","Model development","Development"
"https://github.com/fairlearn/fairlearn","547725396","issue","https://github.com/Trusted-AI/AIF360/issues/232","project_lambda, signed_weights, gamma need descriptions This is for exponentiated gradient.","10","0.506948465547192","Model development","Development"
"https://github.com/fairlearn/fairlearn","547710220","issue","https://github.com/Trusted-AI/AIF360/issues/231","Add tests with DNNs We don't currently have tests with pytorch or tensorflow (or other popular packages for that matter). We should figure out a standard way to test all of them with all mitigation techniques to ensure that always works and we catch breaking changes.","13","0.3625452561622776","Artifact generation and benchmarking","Deployment"
"https://github.com/fairlearn/fairlearn","546446934","issue","https://github.com/Trusted-AI/AIF360/issues/226","Run perf tests on large datasets / stress test We should test the limits. If you're interested in tackling this please reach out.","13","0.4720070661774698","Artifact generation and benchmarking","Deployment"
"https://github.com/fairlearn/fairlearn","546446421","issue","https://github.com/Trusted-AI/AIF360/issues/225","Extend perf metrics to show overhead without estimator fit time absolute/relative time metrics are currently not correct","30","0.2788906009244991","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","545624473","issue","https://github.com/Trusted-AI/AIF360/issues/222","try to pass sklearn's check_estimator [This page](https://scikit-learn.org/dev/developers/develop.html#rolling-your-own-estimator) explains how you can write a test to pass tests included in `check_estimator`. We can see which ones fail and see if we can fix them.","32","0.4871837642479842","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","544949034","issue","https://github.com/Trusted-AI/AIF360/issues/220","setup a readthedocs documentation It may be useful to have the API docs compiled on readthedocs for people to easily go through them.","14","0.2685354990830493","Documentation","Development"
"https://github.com/fairlearn/fairlearn","537982861","issue","https://github.com/Trusted-AI/AIF360/issues/207","Make fairlearn available through conda-forge https://conda-forge.org/","13","0.5188087774294674","Artifact generation and benchmarking","Deployment"
"https://github.com/fairlearn/fairlearn","534316108","issue","https://github.com/Trusted-AI/AIF360/issues/200","Release Pipeline Improvements In addition to the `pip freeze` proposal, the 0.4.0 release has highlighted a weakness in our release automation. This is around the
- Required updates to `ReadMe.md` (not helped by GitHub and PyPI having difference concepts of markdown syntax)
- Branching and tagging of the release

We need to get all these automated so they are done and done consistently.","32","0.4741356294219732","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","534315551","issue","https://github.com/Trusted-AI/AIF360/issues/199","Pin all pip packages for release pipelines We were just burned by a bad version of the `colorama` package coming out during a release. Since each job does its own `pip install` this bad package got picked up mid-release, and killed it.

We should have the ""Prevalidation"" stage run `pip freeze` to get a complete package list, and share this as a Build artifact for subsequent steps.","21","0.5037419865006072","Installation and shell commands","Deployment"
"https://github.com/fairlearn/fairlearn","534075362","issue","https://github.com/Trusted-AI/AIF360/issues/195","Trouble with Linux and Python 3.5 in builds For some reason `shap` is failing to install on the ""Install Requirements"" stage on Linux with Python 3.5. Have turned off for now, but need to track this down","32","0.4737281067556297","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","511022058","issue","https://github.com/Trusted-AI/AIF360/issues/96","rationale for default values of constants in exponentiated gradient Would be awesome to have a description of where these numbers come from or why they might be reasonable.","26","0.4671451355661882","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","511008694","issue","https://github.com/Trusted-AI/AIF360/issues/95","float as supported type for labels in classification scenarios We should think about whether that's something we want to support, or whether we want to stick with int. If we don't want to support it we should at least check for the type and raise an exception with clear instructions for the user.","32","0.4777630163851459","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","510132792","issue","https://github.com/Trusted-AI/AIF360/issues/90","Specific cutoff for Fair Regression  I just read the new paper (Fair Regression). Is it possible to just optimize some specific cutoff in Fair Regression? (e.g. 0.65 approval cutoff in lending business). ","6","0.5725834449238709","API expansion","Development"
"https://github.com/fairlearn/fairlearn","504877839","issue","https://github.com/Trusted-AI/AIF360/issues/80","Chg PyPi badge in readme to autoupdate the version number Modify badge in readme.md","13","0.6022328548644339","Artifact generation and benchmarking","Deployment"
"https://github.com/fairlearn/fairlearn","504860519","issue","https://github.com/Trusted-AI/AIF360/issues/78","ModuleNotFoundError: No module named 'azureml.contrib.explain' Hi guys,

I am testing the tool using the Threshold Optimization Post-Processing for Binary notebook and i am continuously getting the following error:

![image](https://user-images.githubusercontent.com/45678322/66514119-864c5480-eaaa-11e9-9b3e-afe8f1d01fe6.png)


I read about a similar issue and they recommend to:
pip install 'azureml-sdk[notebooks]'
But i get the same error.
Could you let me know what i could do? Please","21","0.4215098495044659","Installation and shell commands","Deployment"
"https://github.com/fairlearn/fairlearn","501617671","issue","https://github.com/Trusted-AI/AIF360/issues/65","Expectations for predict, predict_proba, etc. Since fairlearn uses randomized classifiers we need to set expectations for what `predict`, `predict_proba` etc. return.","23","0.8027001247340231","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","501559011","issue","https://github.com/Trusted-AI/AIF360/issues/64","Moments use dataX/A/Y instead of X, group_data, and y e.g.
```python
def init(self, dataX, dataA, dataY):
```
For consistency this should be adjusted.","2","0.3321593660576711","Performance measurement","Validation"
"https://github.com/fairlearn/fairlearn","501263761","issue","https://github.com/Trusted-AI/AIF360/issues/62","Terminology - differentiate between group criteria and individual criteria BGL is a group criterion, while SP is an individual criterion. Make that clear in the terminology section.","6","0.5813733666944673","API expansion","Development"
"https://github.com/fairlearn/fairlearn","501251632","issue","https://github.com/Trusted-AI/AIF360/issues/61","ExponentiatedGradient needs comprehensive tests The current set of tests is too limited. We need to extend that asap.

UPDATE: argument tests are now there thanks to @riedgar-ms but we still need functional tests","13","0.5400686553030306","Artifact generation and benchmarking","Deployment"
"https://github.com/fairlearn/fairlearn","499160881","issue","https://github.com/Trusted-AI/AIF360/issues/56","How to handle unknown/missing value in sensitive features In real life application, sometimes we have some unknown/missing value in the class attribute. 
For example, `dataA = pd.Series([np.nan, 0, 0, 1, np.nan])`. 

What if I just care about non-missing values disparity but still want to optimize on the whole data? Is it equal to change the loss function by 
`min ERROR` (using the whole data) subject to `fairness constraints`(using the non-missing data) ?  
Could you provide some suggestion? 
","6","0.2940850815850817","API expansion","Development"
"https://github.com/fairlearn/fairlearn","496518710","issue","https://github.com/Trusted-AI/AIF360/issues/38","add github badges at top of readme add usual GitHub badges at top of readme.
Additional badges can be added as needed.","13","0.8172985459980975","Artifact generation and benchmarking","Deployment"
"https://github.com/fairlearn/fairlearn","490569044","issue","https://github.com/Trusted-AI/AIF360/issues/23","What if I have my own train sample weights?  For Fairlearn, we can simply regard it as changing the sample weights and targets(Y) over iterations to make our classifier fair. What if I have my own train sample weights and want the returned algorithm also considers  this? Thanks.","23","0.3496612083568606","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","453896859","issue","https://github.com/Trusted-AI/AIF360/issues/5","How should the probabilities generated by best_classifier be turned in predictions? The returned best_classifier gives the probability of a sample belonging to class 1. However, I am conflicted in how I should turn this probability into a prediction.

Intuitively, it would make sense to binarize this probability at a 0.5 threshold. Not doing this would result in [probability matching](https://en.wikipedia.org/wiki/Probability_matching), which would result in suboptimal accuracy.

However, in practice, I find that simply sampling from the probability (predicting class 0 if a random number in [0,1] exceeds the probability) results in better fairness. 

I am wondering what the correct method is.","23","0.7720469124603124","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","420772669","issue","https://github.com/Trusted-AI/AIF360/issues/4","Mapping unweighted estimators to weighted estimators I am wondering if there are any underlying requisites to apply **any** sklearn model, when I applied `sklearn.neural_network.MLPClassifier()` it says
![fairlearn](https://user-images.githubusercontent.com/36497361/54322048-dca6f280-45b0-11e9-940e-c400c4017215.png)
But when I applied `sklearn.svm.SVC()`, this issues **doe not** arise. 
It seems to me that the input object should have `class_weight` attribute, which `MLPClassifier()` does not have. So is this true?
","23","0.4660592711253504","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","375124598","issue","https://github.com/Trusted-AI/AIF360/issues/3","The returned best_classifier  When I used the returned best_classifier to do prediction. The prediction scores are binary not probability based. How could I generate the probability based prediction? Thanks ","23","0.7620738636363638","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","364576712","issue","https://github.com/Trusted-AI/AIF360/issues/1","Could fairlearn work with random forest or xgboost? I'm looking to use fairlearn with the most powerful algorithm possible for binary classification. How can I integrate the weights to work with one of these algorithms?","23","0.8470907623449997","Bias mitigation methodology","Design"
"https://github.com/algofairness/BlackBoxAuditing","501440849","issue","https://github.com/Trusted-AI/AIF360/issues/12","listing scikit-learn as a dependency Your setup.py should have scikit-learn as a dependency listed as it is used at least in model_factories","4","0.4587372281896772","Installation and shell commands","Deployment"
"https://github.com/algofairness/BlackBoxAuditing","442990732","issue","https://github.com/Trusted-AI/AIF360/issues/10","Is it correct that the implemented partial repair is combinatorial repair, not geometric repair?  The original paper [Certifying and removing disparate impact](https://arxiv.org/pdf/1412.3756.pdf) describes two methods for partial repair: Combinatorial repair and Geometric repair. From observing the repair code, it seems to be that the Combinatorial repair was actually implemented. However, no documentation explicitly mentions this. It would be good to mention this in the documentation.","16","0.5035251549737939","Testing","Maintenance"
"https://github.com/algofairness/BlackBoxAuditing","299872932","issue","https://github.com/Trusted-AI/AIF360/issues/8","Newer Version of Orange3 are incompatible.  Using BlackBoxAuditing with Orange3 version 3.10.0 Gives the following error:
```
/usr/lib/python3.6/site-packages/Orange/data/variable.py in <module>()
      9 import numpy as np
     10
---> 11 from Orange.data import _variable
     12 from Orange.util import Registry, color_to_hex, hex_to_color, Reprable
     13

ImportError: cannot import name '_variable'
```
Older version of Orange3 also gives the same error. The version which seem to work is 3.3.5.

You might want to make code of BlackBoxAuditing compatible with newer version or add a note that the Orange3 version 3.3.5 works.
","21","0.7278846807642619","Installation and shell commands","Deployment"
"https://github.com/algofairness/BlackBoxAuditing","280884276","issue","https://github.com/Trusted-AI/AIF360/issues/7","Document Java requirement We should add a Java requirement since we're running Weka for a number of things..","21","0.4693039909178235","Installation and shell commands","Deployment"
"https://github.com/algofairness/BlackBoxAuditing","266365575","issue","https://github.com/Trusted-AI/AIF360/issues/6","Error during installation- BlackBoxAuditing  I am using Python 3.6.3 on windows 10.

Below is the error : 
 File ""C:\Users\AppData\Local\Temp\pip-build-ccruyk_l\BlackBoxAuditing\setup.py"", line 46, in run
        f.write(WEKA_PATH)
    TypeError: write() argument must be str, not None

Thanks!","21","0.3925507116996477","Installation and shell commands","Deployment"
"https://github.com/algofairness/BlackBoxAuditing","220324594","issue","https://github.com/Trusted-AI/AIF360/issues/5","No license The code doesn't specify any license which makes it hard to determine to what extent one is allowed to use the code.","20","0.6566471163245359","Opinion","Requirement Analysis"
"https://github.com/algofairness/BlackBoxAuditing","195321083","issue","https://github.com/Trusted-AI/AIF360/issues/4","Failing test on latest master The test suite fails on the latest master, this indicates bad code, bad tests or both. 
```
#########################################################################
### Running all *.py files now. #########################################
### No tests should be False nor should there be Traceback exceptions. ##
#########################################################################
________________________________
Running tests for: ./disparate_impact.py
Traceback (most recent call last):
  File ""./disparate_impact.py"", line 92, in <module>
    test()
  File ""./disparate_impact.py"", line 88, in test
    di = disparate_impact(feature_to_repair, response, groups, outcomes)
TypeError: disparate_impact() takes exactly 3 arguments (4 given)
________________________________
Running tests for: ./loggers.py
________________________________
Running tests for: ./splitters.py
75:25 data split correct? True
50:50 data split correct? True
________________________________
Running tests for: ./audit_reading.py
image file generated? -- True
data file generated? -- True
ranked features sorted? -- True
ranked image file generated? -- True
________________________________
Running tests for: ./repairers/GeneralRepairer.py
________________________________
Running tests for: ./repairers/binning/BinSizes.py
FreedmanDiaconisBinSize -- correct # of bins?  False
________________________________
Running tests for: ./repairers/binning/Binner.py
make_histogram_bins -- no entries lost -- True
make_histogram_bins -- correct # of bins -- True
homogenous feature yields one bin?  True
bins being bucketed by value? --  True
________________________________
Running tests for: ./repairers/calculators.py
median value is correct? True
________________________________
Running tests for: ./repairers/CategoricalFeature.py
Traceback (most recent call last):
  File ""./repairers/CategoricalFeature.py"", line 114, in <module>
    if __name__==""__main__"": test()
  File ""./repairers/CategoricalFeature.py"", line 96, in test
    DG = test_feature.create_graph()
TypeError: create_graph() takes exactly 2 arguments (1 given)
________________________________
Running tests for: ./repairers/SparseList.py
SparseList size correct? True
SparseList accessed correctly? True
Sorted SparseList size correct? True
Sorted SparseList accessed correct? True
Resorted SparseList size correct? True
Resorted SparseList accessed correct? True
Big SparseList size correct? False
________________________________
Running tests for: ./repairers/NumericRepairer.py
repaired_data altered? True
median replaces column? False
repaired_data unaltered for repair level=0? True
________________________________
Running tests for: ./repairers/CategoricRepairer.py
HERE
Minimal Dataset -- repaired_data altered? True
Minimal Dataset -- mode is true mode? True
Minimal Dataset -- mode value as feature_to_repair? True
Test get_group_data -- group features correct? True
Test get_categories_count -- category counts correct? True
Test get_categories_count_norm -- normalized category counts correct? True
Test get_median_per_category -- medians are correct? True
Test gen_desired_count -- desired count correct for feature with category with no values? True
Test gen_desired_count -- desired count correct for standard feature? True
Test gen_desired_count -- desired count correct for mode category when repairing feature to remove? True
Test gen_desired_dist -- desired distribution correct for feature with category with no values? True
Test gen_desired_dist -- desired distribution correct for standard feature? True
Test gen_desired_dist -- desired distribution correct for mode category when repairing feature to remove? True
Test assign_overflow -- updated group features correct? True
Test assign_overflow -- assigned overflow correctly? True
Test assign_overflow -- distribution correct? True
Categorical Minimal Dataset -- full repaired_data altered? True
Categorical Minimal Dataset -- full repaired_data correct? True
Categorical Minimal Dataset -- partial (.5) repaired_data altered? True
Categorical Minimal Dataset -- partial (.5) repaired_data correct? True
Categorical Minimal Dataset -- partial (.2) repaired_data altered? True
Categorical Minimal Dataset -- partial (.2) repaired_data correct? True
Test unique values -- .5 repaired_data altered? True
Test unique values -- .5 repaired_data correct? True
Test repeated values -- .5 repaired_data altered? True
Test repeated values -- .5 repaired_data correct? True
________________________________
Running tests for: ./model_factories/TensorFlowModelFactory.py
Traceback (most recent call last):
  File ""./model_factories/TensorFlowModelFactory.py"", line 260, in <module>
factory settings valid? --  True
factory builds ModelVisitor? --  True
    test()
  File ""./model_factories/TensorFlowModelFactory.py"", line 192, in test
    test_categorical_model()
  File ""./model_factories/TensorFlowModelFactory.py"", line 254, in test_categorical_model
    predictions = model.test(test_set)
  File ""./model_factories/TensorFlowModelFactory.py"", line 151, in test
    self.model_saver.restore(tf_session, self.checkpoint)
  File ""/home/hargup/anaconda/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1342, in restore
    ""File path is: %r"" % (save_path, file_path))
ValueError: Restore called with invalid save path: 'tmp//TensorFlow_Network_test_1481650127.91_1481650137.88.model-98'. File path is: 'tmp//TensorFlow_Network_test_1481650127.91_1481650137.88.model-98'
________________________________
Running tests for: ./model_factories/J48_ModelFactory.py
factory builds ModelVisitor? --  True
predicting correctly? --  True
________________________________
Running tests for: ./model_factories/SVM_ModelFactory.py
factory builds ModelVisitor? --  True
predicting correctly? --  True
________________________________
Running tests for: ./model_factories/RecidivismTensorFlowModelFactory.py
Traceback (most recent call last):
  File ""./model_factories/RecidivismTensorFlowModelFactory.py"", line 309, in <module>
    test()
  File ""./model_factories/RecidivismTensorFlowModelFactory.py"", line 260, in test
    test_list_to_tf_input()
  File ""./model_factories/RecidivismTensorFlowModelFactory.py"", line 266, in test_list_to_tf_input
    tf_matrix, tf_onehot = list_to_tf_input(data, 1, 3)
  File ""./model_factories/RecidivismTensorFlowModelFactory.py"", line 120, in list_to_tf_input
    data = expand_to_one_hot(data)
  File ""./model_factories/RecidivismTensorFlowModelFactory.py"", line 146, in expand_to_one_hot
    if entry[header_dict[""SEX1""]] == ""FEMALE"":
IndexError: list index out of range
________________________________
Running tests for: ./disparate_impact_evaluator.py
Traceback (most recent call last):
  File ""./disparate_impact_evaluator.py"", line 146, in <module>
    graph_disparate_impact_accuracy(""audits/1455586474.33"", ""disparate_impact_graphs/DI_Accuracy"")
  File ""./disparate_impact_evaluator.py"", line 18, in graph_disparate_impact_accuracy
    only_files = [f for f in listdir(directory) if isfile(join(directory, f))]
OSError: [Errno 2] No such file or directory: 'audits/1455586474.33'
________________________________
Running tests for: ./GradientFeatureAuditor.py
Audit files dumped to: audits/1481650141.25
correct # of audit files produced? -- True
correct # of lines per file? -- True
all audit files not empty? -- True
________________________________
Running tests for: ./make_graphs.py
proper usage: make_graphs.py <directory/with/audits>
________________________________
Running tests for: ./repair.py
usage: repair.py [-h] -r RESPONSE -p PROTECTED [PROTECTED ...]
                 [-i IGNORED [IGNORED ...]]
                 input_csv output_csv repair_level
repair.py: error: too few arguments
________________________________
Running tests for: ./consistency_graph.py
Traceback (most recent call last):
  File ""./consistency_graph.py"", line 86, in <module>
    directory = sys.argv[1]
IndexError: list index out of range
________________________________
Running tests for: ./experiments/ricci/load_data.py
load_data unpacks correctly? --  True
load_data types are correct? --  True
________________________________
Running tests for: ./experiments/sample_2/load_data.py
load_data -- unpacks correctly? --  True
Needs better test False
________________________________
Running tests for: ./experiments/adult/load_data.py
load_data unpacks correctly? --  True
load_data types are correct? --  True
load_data train size correct? --  True
load_data test size correct? --  True
________________________________
Running tests for: ./experiments/diabetes/load_data.py
load_data unpacks correctly? --  True
load_data types are correct? --  True
all headers given types? --  True
________________________________
Running tests for: ./experiments/german/load_data.py
load_data unpacks correctly? --  True
load_data types are correct? --  False
all headers given types? --  True
________________________________
Running tests for: ./experiments/DRP/example_headers.py
________________________________
Running tests for: ./experiments/DRP/load_data.py
Traceback (most recent call last):
  File ""./experiments/DRP/load_data.py"", line 60, in <module>
    test()
  File ""./experiments/DRP/load_data.py"", line 42, in test
    headers, train, test = load_data()
  File ""./experiments/DRP/load_data.py"", line 12, in load_data
    with open(train_filename) as f:
IOError: [Errno 2] No such file or directory: 'test_data/DRP_old_train.arff'
________________________________
Running tests for: ./experiments/sample/load_data.py
load_data -- unpacks correctly? --  True
________________________________
Running tests for: ./experiments/arrests/converter.py
Traceback (most recent call last):
  File ""./experiments/arrests/converter.py"", line 2, in <module>
    import get_data as gd
ImportError: No module named get_data
________________________________
Running tests for: ./experiments/arrests/load_data.py
0.666666666667
load_data unpacks correctly? --  True
Test and train not empty? --  True
load_data types are correct? --  True
________________________________
Running tests for: ./experiments/glass/load_data.py
load_data unpacks correctly? --  True
load_data types are correct? --  True
all headers given types? --  True
________________________________
Running tests for: ./measurements.py
confusion matrix correct? --  True
measurements -- accuracy correct? --  True
measurements -- 1-BER correct? --  True
measurements -- accuracy correct? --  True
measurements -- 1-BER correct? --  True
measurements -- accuracy correct? --  True
measurements -- 1-BER correct? --  True
```","16","0.9655967171347276","Testing","Maintenance"
"https://github.com/algofairness/BlackBoxAuditing","195317507","issue","https://github.com/Trusted-AI/AIF360/issues/3","Nothing breaks in repair.py if garbage is passed as a protected feature In project home I ran the following command and nothing breaks
```
 python repair.py test_data/adult.csv out.csv 0.5 -r education -p blah
```
Also, reading the code it doesn't look like the protected feature parameter is being used anywhere so it is not clear what is this code supposed to do.

/cc @geomblog
","16","0.2964557468962756","Testing","Maintenance"
"https://github.com/algofairness/BlackBoxAuditing","195303484","issue","https://github.com/Trusted-AI/AIF360/issues/2","WEKA_PATH is hard coded in model_factories/AbstractWekaModelFactory.py It should be taken from the environment variables or should be specified in a separate config file which user should be asked to create at time of installation.","4","0.6614790915417224","Installation and shell commands","Deployment"
"https://github.com/dssg/aequitas","1025641953","issue","https://github.com/Trusted-AI/AIF360/issues/114","issue regarding uploading the data set and run it on compas notebook I want to run a data set on Web Api of Aequitas but am not able to run it, after uploading it is showing a local error, please provide a solution regarding this with proper steps on how can we upload different data sets.","0","0.4065970261622436","Dataset usage","Requirement Analysis"
"https://github.com/dssg/aequitas","1025625622","issue","https://github.com/Trusted-AI/AIF360/issues/113","how to use web api on different data set  I want to run a data set on Web Api of Aequitas but am not able to run it, after uploading it is showing a local error, please provide a solution regarding this with proper steps on how can we upload different data sets.","13","0.4022753486421306","Artifact generation and benchmarking","Deployment"
"https://github.com/dssg/aequitas","1025080121","issue","https://github.com/Trusted-AI/AIF360/issues/112","Issue regarding running my data set on compass file as a reference  Actually, I am trying to run my data set on Aequitas(Compass notebook ) but after reading my data set whenever I am trying to initialize group class and giving my df as the value in the function get_crosstabs( ) it is generating an error (Exception: get_crosstabs: input df was not preprocessed. There are non-string cols within attr_cols!).
Please help me to resolve this
![stack overflow 1](https://user-images.githubusercontent.com/53338376/137117644-9ca3b7c4-efcb-48fb-b988-ad305eba8f89.JPG)
![stack overflow 2](https://user-images.githubusercontent.com/53338376/137117655-f2c86fbf-1bc6-4740-b96e-bf16f432595e.JPG)
![stack overflow](https://user-images.githubusercontent.com/53338376/137117657-b1384ae1-cc5b-41d6-8233-c22f34741252.JPG)
, how can I run my data set on fraud deduction and resolve my issue
![stack overflow](https://user-images.githubusercontent.com/53338376/137117397-29222d44-ac59-4106-83f0-c560af5d17b9.JPG)
","17","0.5122981641229816","Troubleshooting","Maintenance"
"https://github.com/dssg/aequitas","996205148","issue","https://github.com/Trusted-AI/AIF360/issues/111","Change the way to launch the web-app (or update the doc) Hello the community !

I'm using aequitas for the first time today and i was facing with some issues for launching the audit web-app locally.

To make the command `python -m serve` work, I had to clone the repository and add the following shell environment variable in my terminal `HOST=localhost` because it used in the server.py script. 

I don't know if it was the best way to do it but finally it worked. So i was wondering if it would be possible to update the documentation or to simplify the process for the audit web-app

Thanks !","4","0.3968313843832518","Installation and shell commands","Deployment"
"https://github.com/dssg/aequitas","905182669","issue","https://github.com/Trusted-AI/AIF360/issues/108","Different setups Projects that use `aequitas` might not require the `cli` package and the `webapp` package. Because of this, making a setup where only the `aequitas` package is installed might be of use. This also has the advantage of removing some heavier dependencies, such as `Flask` and `SQLAlchemy.`","4","0.7706194514705157","Installation and shell commands","Deployment"
"https://github.com/dssg/aequitas","829156348","issue","https://github.com/Trusted-AI/AIF360/issues/107","xhtml2pdf broken in python 3.8 - AttributeError: module 'cgi' has no attribute 'escape' When using the CLI with python 3.8, the following error occurs:
```
Traceback (most recent call last):
  File ""c:\users\sscher\miniconda3\envs\ai-certification\lib\runpy.py"", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File ""c:\users\sscher\miniconda3\envs\ai-certification\lib\runpy.py"", line 87, in _run_code
    exec(code, run_globals)
  File ""C:\Users\sscher\Miniconda3\envs\ai-certification\Scripts\aequitas-report.exe\__main__.py"", line 7, in <module>
  File ""c:\users\sscher\miniconda3\envs\ai-certification\lib\site-packages\aequitas_cli\aequitas_audit.py"", line 235, in main
    push_topdf(args.input_file, report)
  File ""c:\users\sscher\miniconda3\envs\ai-certification\lib\site-packages\aequitas_cli\utils\io.py"", line 97, in push_topdf
    pisaStatus = pisa.CreatePDF(report, dest=result_pdf)
  File ""c:\users\sscher\miniconda3\envs\ai-certification\lib\site-packages\xhtml2pdf\document.py"", line 136, in pisaDocument
    doc.build(context.story)
  File ""c:\users\sscher\miniconda3\envs\ai-certification\lib\site-packages\reportlab\platypus\doctemplate.py"", line 1080, in build
    self.handle_flowable(flowables)
  File ""c:\users\sscher\miniconda3\envs\ai-certification\lib\site-packages\reportlab\platypus\doctemplate.py"", line 932, in handle_flowable
    self.afterFlowable(f)
  File ""c:\users\sscher\miniconda3\envs\ai-certification\lib\site-packages\xhtml2pdf\xhtml2pdf_reportlab.py"", line 122, in afterFlowable
    cgi.escape(copy.deepcopy(flowable.text), 1),
AttributeError: module 'cgi' has no attribute 'escape'
```

It is due to a problem in xhtml2pdf which uses cgi.escape, which has been removed in python 3.8. This is a known issue in xthml2pdf which has not yet been fixed (https://github.com/xhtml2pdf/xhtml2pdf/issues/501)
workaround solution: downgrade to python 3.7
","16","0.4403720518215037","Testing","Maintenance"
"https://github.com/dssg/aequitas","782596426","issue","https://github.com/Trusted-AI/AIF360/issues/104","Bug on min_metric disparities when there are no positive predictions ## Bug
When there are multiple protected attributes, and you assess disparities with `get_disparity_min_metric`, aequitas enters the if on [line 102 of bias.py](https://github.com/dssg/aequitas/blob/edf8313bae4a153133ff5289ad6ebeac041e1043/src/aequitas/bias.py#L102) and returns the wrong disparities matrix (as seen in the bottom figure).


## Steps to reproduce bug:

```py
import pandas as pd
import aequitas
import numpy as np

# Fake DF to demonstrate bug
n_samples = 1000

aequitas_df = pd.DataFrame({
    'label_value': (np.random.random((n_samples,)) > 0.95).astype(int),
    'score': np.zeros((n_samples,)).astype(int),  # All negative predictions  # -> leads to bug
#    'score': (np.random.random((n_samples,)) > 0.90).astype(int),  # -> no bug with these scores
    'gender': (np.random.random((n_samples,)) > 0.6).astype(str),
    'age_group': (4 * np.random.random((n_samples,))).astype(int).astype(str),
})


from aequitas.group import Group
from aequitas.bias import Bias

attr_cols = list(set(aequitas_df.columns) - {
    'entity_id', 'score', 'label_value', 'as_of_date'
})

# Initialize aequitas objects
g = Group()
b = Bias()

# Get confusion matrix and metrics for each individual group and attribute
confusion_matrix_metrics, _ = g.get_crosstabs(
    aequitas_df, attr_cols=attr_cols,
)

disparities_matrix = b.get_disparity_min_metric(
    confusion_matrix_metrics,
    original_df=aequitas_df,
    fill_divbyzero=1e3,
)
```

`confusion_matrix_metrics`
![image](https://user-images.githubusercontent.com/13498941/104091314-84a4aa00-5274-11eb-869d-9ac56ff0a026.png)

`disparities_matrix`
![image](https://user-images.githubusercontent.com/13498941/104091321-925a2f80-5274-11eb-9918-21f018b5220c.png)
","17","0.5635055164005114","Troubleshooting","Maintenance"
"https://github.com/dssg/aequitas","726378485","issue","https://github.com/Trusted-AI/AIF360/issues/95","[Feature] Add Group-Size adjusted False Positives and False Negatives columns to get_crosstabs method Add two new columns to the result of the `Group.get_crosstabs` method, representing Group-Size adjusted False Positives and Group-Size adjusted False Negatives respectively. Additionally, make disparities of the two columns available with `Bias.getdisparity_predefined_groups`.","3","0.4879953379953383","Bias detection metrics validation","Validation"
"https://github.com/dssg/aequitas","684279795","issue","https://github.com/Trusted-AI/AIF360/issues/94","[Feature] Expose the required cols in get_crosstabs signature attrs_cols = ['age']
score_col = 'predict_proba'
label_col = 'label'","17","0.5164621998532032","Troubleshooting","Maintenance"
"https://github.com/dssg/aequitas","665045197","issue","https://github.com/Trusted-AI/AIF360/issues/90","Absolute vs. relative disparity and the implications for fairness results According to [Verma & Rubin 2018](https://ieeexplore.ieee.org/abstract/document/8452913?casa_token=ML5mZqZWGiwAAAAA:B8V-_Kxl-nitYu9omc5Nk5YRcW9S0FadL3t6V6kQ233Le-LHQY-uCKowf-lHpKMmesBaLzq7yKXZ) PPV + FDR = 1 and thus if there is PPV Parity there is also FDR Parity. 
However, Aequitas sais there is no FDR Parity, but PPV Parity:
![image](https://user-images.githubusercontent.com/5471273/88378262-f3fb1000-cda0-11ea-9c90-15ad5290637e.png)
I understand that using Aequitas it looks like there was no FDR Parity because Aequitas looks at relative disparities while Verma & Rubin talk about absolute differences. Why does Aequitas prefer relative disparities?","3","0.9341802516277462","Bias detection metrics validation","Validation"
"https://github.com/dssg/aequitas","665029305","issue","https://github.com/Trusted-AI/AIF360/issues/89","Confusion of Statistical Parity and Impact Parity? Here: https://github.com/dssg/aequitas/blob/master/docs/source/examples/compas_demo.ipynb
Under ""How do I assess model fairness?"" > ""Pairities Calcuated"" it sais 

Predicted Positive Ratio k Parity | Statistical Parity
-- | --

According to [your definitions](https://dssg.github.io/aequitas/metrics.html) Predicted Positive Ratio k is PPR and Predicted Positive Ratio g is PPrev. **So Aequitas defines Statistical Parity as PPR Parity** That is not the definition according to [Verma & Rubin 2018](https://ieeexplore.ieee.org/abstract/document/8452913?casa_token=ML5mZqZWGiwAAAAA:B8V-_Kxl-nitYu9omc5Nk5YRcW9S0FadL3t6V6kQ233Le-LHQY-uCKowf-lHpKMmesBaLzq7yKXZ), who say Statistical Parity was PPrev Parity. Then again, if taking Statistical Parity synonymous to Demographic Parity, your definition of it being PPR Parity makes more sense I think. 

So, any thoughts on this?","3","0.885470424771735","Bias detection metrics validation","Validation"
"https://github.com/dssg/aequitas","659051782","issue","https://github.com/Trusted-AI/AIF360/issues/87","Details on how significance is calculated? Hi, sorry to be bothering you again!

I am currently looking at src/aequitas/bias.py to find out how the significance is calculated. Here is what I think I understood:

1. Check if sample group is normally distributed
2. a. If sample group is normally distributed, calculate whether sample group and ref. group have equal variances using levene
b. If sample group is not normally distributed, calculate whether sample group and ref. group have equal variances using bartlett
3. a. if both groups have equal variances, perform [independent 2 sample t-test](https://en.wikipedia.org/wiki/Student%27s_t-test#Independent_two-sample_t-test)
b. if both groups have different variances, perform [Welch's t-test](https://en.wikipedia.org/wiki/Welch%27s_t-test)

I have two questions:
<s>1. Is this correct?</s>
I didn't understand what you mean with ""sample"" group at first. Now I understand that you mean the lists of binary encoded values that say for each entry of each group whether the entry belongs to whichever measure is relevant, fpr or fnr. It canalso be a list of scores, right?

<s>2. Do we not check whether the ref. group is normally distributed? If not, why not? If we do, where?</s> 
I found it! It is in an if condition https://github.com/dssg/aequitas/blob/a61ef33a55a8e21611425f13c5688bae6743f041/src/aequitas/bias.py#L521","3","0.5553284368070951","Bias detection metrics validation","Validation"
"https://github.com/dssg/aequitas","653914708","issue","https://github.com/Trusted-AI/AIF360/issues/86","[Error] get_disparity_predefined_group() raises AttributeError I try executing the following code:

```
bdf = b.get_disparity_predefined_groups(xtab, original_df=df, 
                                        ref_groups_dict={'race':'Caucasian'}, 
                                        alpha=0.05, check_significance=True, 
                                        mask_significance=False)
bdf.style
```

but it raises an Attribute Error with the following details:

```
get_disparity_predefined_group()
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-22-8a5ae26f1e35> in <module>
      2                                         ref_groups_dict={'race':'Caucasian'},
      3                                         alpha=0.05, check_significance=True,
----> 4                                         mask_significance=False)
      5 bdf.style

C:\Program_Files\Anaconda3\lib\site-packages\aequitas\bias.py in get_disparity_predefined_groups(self, df, original_df, ref_groups_dict, key_columns, input_group_metrics, fill_divbyzero, check_significance, alpha, mask_significance, selected_significance)
    439             self._get_statistical_significance(
    440                 original_df, df, ref_dict=full_ref_dict, score_thresholds=None,
--> 441                 attr_cols=None, alpha=5e-2, selected_significance=selected_significance)
    442 
    443             # if specified, apply T/F mask to significance columns

C:\Program_Files\Anaconda3\lib\site-packages\aequitas\bias.py in _get_statistical_significance(cls, original_df, disparity_df, ref_dict, score_thresholds, attr_cols, alpha, selected_significance)
    745                 for name, func in binary_col_functions.items():
    746                     func = func(thres_unit, 'label_value', thres_val)
--> 747                     original_df.loc[:, name] = original_df.apply(func, axis=1)
    748 
    749         # add columns for error-based significance

C:\Program_Files\Anaconda3\lib\site-packages\pandas\core\frame.py in apply(self, func, axis, broadcast, raw, reduce, result_type, args, **kwds)
   6485                          args=args,
   6486                          kwds=kwds)
-> 6487         return op.get_result()
   6488 
   6489     def applymap(self, func):

C:\Program_Files\Anaconda3\lib\site-packages\pandas\core\apply.py in get_result(self)
    149             return self.apply_raw()
    150 
--> 151         return self.apply_standard()
    152 
    153     def apply_empty_result(self):

C:\Program_Files\Anaconda3\lib\site-packages\pandas\core\apply.py in apply_standard(self)
    255 
    256         # compute the result using the series generator
--> 257         self.apply_series_generator()
    258 
    259         # wrap results

C:\Program_Files\Anaconda3\lib\site-packages\pandas\core\apply.py in apply_series_generator(self)
    284             try:
    285                 for i, v in enumerate(series_gen):
--> 286                     results[i] = self.f(v)
    287                     keys.append(v.name)
    288             except Exception as e:

C:\Program_Files\Anaconda3\lib\site-packages\aequitas\bias.py in <lambda>(x)
    734 
    735         binary_score = lambda rank_col, label_col, thres: lambda x: (
--> 736                 x[rank_col] <= thres).astype(int)
    737 
    738         binary_col_functions = {'binary_score': binary_score,

AttributeError: (""'bool' object has no attribute 'astype'"", 'occurred at index 0')
```

It works if I set `check_significance=False`. 

My data frame:
```
entity_id        int64
race            object
score          float64
label_value    float64
rank_abs         int32
rank_pct       float64
dtype: object
```

Any ideas why this is? I have the up to date Aequitas version this time.","17","0.8381151247748705","Troubleshooting","Maintenance"
"https://github.com/dssg/aequitas","653029842","issue","https://github.com/Trusted-AI/AIF360/issues/85","Question: Can we use the fairness tree for regression tasks? On the fairness tree, it asks whether we trust the labels. 

Could we use this for regression tasks? I thought discrete class labels are for classification tasks.

Kind regards,
Alex","28","0.3667419536984754","Bias mitigation methodology","Design"
"https://github.com/dssg/aequitas","651577541","issue","https://github.com/Trusted-AI/AIF360/issues/84","[Solved] meaning of get_crosstabs() param ""score_thresholds"", ""score"" Hi everyone,

I am analyzing whether an API performs equally well for all groups. So I have a data set where I have continuous ""score"" values between 0 and 1. They are predictions made by the API for it's result being correct. Then there is the ""label_value"" which is either 0 (meaning that the API result was incorrect) or 1 (meaning that the API result was correct). 

Now if I calculate the true positives for instance for women by hand I do:
`f_tp = df[((df['sex'] == 'Female') & (df['label_value'] == 1) & (df['score'] >= t))]`
where t is my treshold of 0.8. So I consider a score of 0.8 to be a prediction of being correct.

I do similar stuff to calculate tn, fp, fn:
```
f_tn = df[((df['sex'] == 'Female') & (df['label_value'] == 0) & (df['score'] < t))]
f_fp = df[((df['sex'] == 'Female') & (df['label_value'] == 0) & (df['score'] >= t))]
f_fn = df[((df['sex'] == 'Female') & (df['label_value'] == 1) & (df['score'] < t))]
```

And I obtain:
* tp = 1185
* tn = 43
* fp = 63
* fn = 104

Now I would like to make these calculations automatically using Aequitas.
```
g = Group()
xtab, _ = g.get_crosstabs(df, attr_cols=[""sex""], score_thresholds= {'score': [0.8]})
```
In the result, I get values similar to those I got by hand but they are switched around :
* tp = 104
* tn = 63
* fp = 43
* fn = 1185

Here, tp is what I considered to be fn and fn is what I considered to be tp. What I considered to be tn is fp and what I considered to be fp is tn. 

I can't quite wrap my mind around this. What am I missing? I think I might be confusing something? I'd be so happy if you could give me a hint. 

Thanks!","30","0.8236568420874482","Troubleshooting","Maintenance"
"https://github.com/dssg/aequitas","647242494","issue","https://github.com/Trusted-AI/AIF360/issues/83","Data formatting: Documentation-link not found Hi, 

in the Data Formatting section to preprocess input-data for Aequitas the documentation link provided does not work. Coud you please update it? Thanks.
","14","0.5048919690175295","Documentation","Development"
"https://github.com/dssg/aequitas","640935274","issue","https://github.com/Trusted-AI/AIF360/issues/81","[COMPAS Example] Switched variable names https://github.com/dssg/aequitas/blob/master/docs/source/examples/compas_demo.ipynb

Under ""Levels of recidivism"" you write `label_by_age = sns.countplot(x=""sex"", hue=""label_value"", data=df, palette=aq_palette)` and then `label_by_sex = sns.countplot(x=""age_cat"", hue=""label_value"", data=df, palette=aq_palette)`. It seems like the names were switched.

Also I can make a pull request for the issues I am filing.","22","0.5248273287287661","Bias mitigation methodology","Design"
"https://github.com/dssg/aequitas","639832547","issue","https://github.com/Trusted-AI/AIF360/issues/80","[COMPAS Example] Inconsistency in ""Visualizing default absolute group metrics across all population groups"" In https://github.com/dssg/aequitas/blob/master/docs/source/examples/compas_demo.ipynb
Under ""Visualizing default absolute group metrics across all population groups""
Under ""Default absolute group metrics""

You say ""We can also see that the model is equally likely to predict a woman as 'high' risk as it is for a man (false positive rate FPR of 0.32 for both Male and Female).""

![image](https://user-images.githubusercontent.com/5471273/84805462-9850a080-b004-11ea-861d-9e4a327558f1.png)

As far as I understood the rate that measures the likelyhood to predict someone as 'high' risk is the PPR (Predicted Positive Rate) and the FPR measures how likely it is that someone was incorrectly predicted as 'high' risk. Did you mean to say ""equally likely to incorrectly predict""?
","3","0.7870645336398762","Bias detection metrics validation","Validation"
"https://github.com/dssg/aequitas","639822919","issue","https://github.com/Trusted-AI/AIF360/issues/79","[COMPAS Example] Inconsistency in ""Visualizing a single absolute group metric across all population groups"" About https://github.com/dssg/aequitas/blob/master/docs/source/examples/compas_demo.ipynb

In the part ""Visualizing a single absolute group metric across all population groups"" you say ""We can see from the longer bars that across 'age_cat', 'sex', and 'race' attributes, the groups COMPAS incorrectly predicts as 'low' or 'medium' risk most often are 25-45, Male, and African American."". 

![image](https://user-images.githubusercontent.com/5471273/84803084-cbddfb80-b001-11ea-8f4f-5b8e71737972.png)

You display the FNR, which evaluates which groups have incorrectly been assigned 'low' or 'medium' scores as you say -  but what you say about the bar length doesn't add up. Are you talking about the absolute numbers instead of the bar lengths? Or am I missing something?
","3","0.3891094349609634","Bias detection metrics validation","Validation"
"https://github.com/dssg/aequitas","639435123","issue","https://github.com/Trusted-AI/AIF360/issues/78","[Resolved] get_disparity_predefined_group() raises TypeError Hi everyone,

I am a CS student and I am trying out your Aequitas COMPAS analysis example.

When I run https://github.com/dssg/aequitas/blob/master/docs/source/examples/compas_demo.ipynb on my local machine (Python 3), when trying to execute the cell with code

```
bdf = b.get_disparity_predefined_groups(xtab, original_df=df, 
                                        ref_groups_dict={'race':'Caucasian', 'sex':'Male', 'age_cat':'25 - 45'}, 
                                        alpha=0.05, check_significance=True, 
                                        mask_significance=True)
bdf.style
```

I get the error

`TypeError: Input must be Index or array-like`

With the following details:

```
get_disparity_predefined_group()
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-46-a7be4e5ab5de> in <module>
      2                                         ref_groups_dict={'race':'Caucasian', 'sex':'Male', 'age_cat':'25 - 45'},
      3                                         alpha=0.05, check_significance=True,
----> 4                                         mask_significance=True)
      5 bdf.style

C:\Program_Files\Anaconda3\lib\site-packages\aequitas\bias.py in get_disparity_predefined_groups(self, df, original_df, ref_groups_dict, key_columns, input_group_metrics, fill_divbyzero, check_significance, alpha, mask_significance)
    370             # for predefined groups, use the largest of the predefined groups as
    371             # ref group for score and label value
--> 372             check_significance = df_cols.intersection(check_significance).tolist()
    373 
    374             # compile dictionary of reference groups based on bias-augmented crosstab

C:\Program_Files\Anaconda3\lib\site-packages\pandas\core\indexes\base.py in intersection(self, other, sort)
   2391         """"""
   2392         self._validate_sort_keyword(sort)
-> 2393         self._assert_can_do_setop(other)
   2394         other = ensure_index(other)
   2395 

C:\Program_Files\Anaconda3\lib\site-packages\pandas\core\indexes\base.py in _assert_can_do_setop(self, other)
   2590     def _assert_can_do_setop(self, other):
   2591         if not is_list_like(other):
-> 2592             raise TypeError('Input must be Index or array-like')
   2593         return True
   2594 
```

Does this run on your own Notebook? If so, any guesses why it doesn't work for me?

-----------

EDIT:
The problem seems to be that when I installed Aequitas using pip it did not install the up to date version? My version is 38.0 and apparently there is a newer version 38.1. 

EDIT2:
Yup, it works now, updating the version of aequitas was it :)","17","0.7451543825612679","Troubleshooting","Maintenance"
"https://github.com/dssg/aequitas","450021238","issue","https://github.com/Trusted-AI/AIF360/issues/71","get_disparity_min_metric squashes any score_threshold configuration in groups model If multiple 'score_thresholds' are configured in group crosstabs, they get squashed if passed to 'get_disparity_min_metric'.

```
from aequitas.preprocessing import preprocess_input_df
from aequitas.bias import Bias
from aequitas.group import Group
import pandas as pd

protected_df = pd.read_csv('compas_for_aequitas.csv')
g = Group()
score_thresholds = {'rank_abs': [25], 'rank_pct': [50]}
df, attr_cols = preprocess_input_df(protected_df)
groups_model, attr_cols = g.get_crosstabs(df, score_thresholds=score_thresholds, model_id=45, attr_cols=attr_cols)
bias = Bias()
bias_df = bias.get_disparity_min_metric(groups_model, df)
bias_df
```

The bias_df in this case does end up with more rows than if we were to only give it one score threshold, but they all have the same resulting score_threshold and k values.

This problem does not seem to apply to get_disparity_major_group.","17","0.8162984726067388","Troubleshooting","Maintenance"
"https://github.com/dssg/aequitas","439695599","issue","https://github.com/Trusted-AI/AIF360/issues/68","specifying columns check_significance when trying to specify the 'for' and 'fpr' columns for the check_significance argument of the get_disparity_predefined_groups method of the Bias class, I am running into a number of key errors for columns that have not been included ('fpr', 'precision', and 'tpr' to start). It seems that the user cannot specify a small number of cols for check significance","17","0.533116668171286","Troubleshooting","Maintenance"
"https://github.com/dssg/aequitas","439678660","issue","https://github.com/Trusted-AI/AIF360/issues/67","clarify rank_pct values in documentation Edit documentation to clarify that rank_pct input values must be floats between 0 and 1 ","17","0.4241791611311445","Troubleshooting","Maintenance"
"https://github.com/dssg/aequitas","439678280","issue","https://github.com/Trusted-AI/AIF360/issues/66","create tables with replace Issue with writing new rows to table when previous rows in column had null value. Postgres expects double precision when col should be boolean per error message below.

```
  File ""/home/ubuntu/.local/lib/python3.6/site-packages/ohio/ext/pandas.py"", line 96, in to_sql_method_pg_copy_to
    cursor.copy_expert(sql, csv_buffer)
psycopg2.errors.InvalidTextRepresentation: invalid input syntax for type double precision: ""True""
CONTEXT:  COPY aequitas_group, line 1, column FOR Parity: ""True""
```","12","0.2410736895416561","Metrics operation","Validation"
"https://github.com/dssg/aequitas","432266969","issue","https://github.com/Trusted-AI/AIF360/issues/62","Aequitas CLI Config requires secrets Aequitas CLI config file currently requires the DB credentials in the file. Should be updated to reference an external secrets file. ","4","0.5798993394149103","Installation and shell commands","Deployment"
"https://github.com/dssg/aequitas","430984303","issue","https://github.com/Trusted-AI/AIF360/issues/61","Documentation clarity I'm working through using Aequitas for a bias audit and am noticing a few issues in the documentation:

- The Python API preprocessing input statement is incorrect, it should be `from Aequitas.preprocessing import preprocess_input_df`, i.e. preprocess_input_df should not be callable

- The documentation says that attribute columns can be categorical or continuous, this is confusing in terms of when cleaning functions are called in the pre-processing step. For instance I had made age categories using the cut function that returned a categorical datatype, which caused the discretize cleaning function to be called which then threw an error. It should be clear that categorical columns must be converted to strings.

- Import statements are missing for the Plot and Bias modules; the docs should include `from aequitas.bias import Bias` and `from aequitas.plotting import Plot` in those example calls. 

- The call for the visualization of the disparity treemaps doesn't work as is, it should be `j = p.plot_disparity_all(....` rather than `j = aqp.plot_disparity_all(...` to match the earlier use of Plot().

","17","0.6471390244975149","Troubleshooting","Maintenance"
"https://github.com/dssg/aequitas","413058180","issue","https://github.com/Trusted-AI/AIF360/issues/58","Plotting all disparities of a specific model_id when there are multiple models fails _assemble_ref_groups needs to create a list of reference groups per model_id otherwise it crashes when we have multiple model_ids in the bias df","17","0.7472164443900654","Troubleshooting","Maintenance"
"https://github.com/dssg/aequitas","410884005","issue","https://github.com/Trusted-AI/AIF360/issues/57","Treatment of Group Metric of Zero and/ or NA Determine desired behavior of Aequitas methods including visualization for when group metrics equal zero and when they are NA","3","0.4469696969696971","Bias detection metrics validation","Validation"
"https://github.com/dssg/aequitas","410883751","issue","https://github.com/Trusted-AI/AIF360/issues/56","Separate Statistical Significance DF ### To Explore 
May want to have an option return of information based on statistical significance parameters and outputs such as:
- Group size 
- P-value 
- T/F significant
- Which method used for equal variance calculation 
- Which method used for t-test, equal variance with ref group Y/N 
- Which group compared to if not ref group.
  - Currently can only compares to ref group, may want to change that as well","3","0.4829649762773285","Bias detection metrics validation","Validation"
"https://github.com/dssg/aequitas","402498103","issue","https://github.com/Trusted-AI/AIF360/issues/53","update readme to list different ways aequitas can be used and requirements for each we can copy this from the documentation but would be good for people at first glance to see how it can be used (cmd line, web demo, python)","4","0.4165243246662872","Installation and shell commands","Deployment"
"https://github.com/dssg/aequitas","386308500","issue","https://github.com/Trusted-AI/AIF360/issues/51","Statistical Significance of Metrics Add methods for calculating statistical significance of scores, labels, TP, TF, FP, FN","7","0.3705953058771431","Opinion","Requirement Analysis"
"https://github.com/dssg/aequitas","364059115","issue","https://github.com/Trusted-AI/AIF360/issues/48","Implement individual fairness metrics This issue it's about creating a new class maybe named ""Individual"" that implements individual notions of fairness based on label differences (impurities) for similar individuals. Each method of the class just needs a list of dataframes as input (let's consider that in the future we might want to compare multiple train/test sets labels) and finds similar data points and then look to the label distribution of the pair/cluster.

1. Cynthia Dwork's notion of individual fairness (Lipschitz condition). 
sub methods:
- [ ] create pairwise distance metric in feature space
- [ ] create pairwise distance metric in output space
- [ ] some sort of aggregator
 e.g. count number of times the lipshitz condition is not met for each point, normalize and average?

2. Matching methods to find similar data points and then calculate label purity.
sub methods:
- [ ] Create clusters (start with k-means)
- [ ] Calculate purity metric of labels within a cluster (output k metrics)
- [ ]  Visualize clusters (if not 2-d use principal components?)
- [ ] Visualize the purity metric per cluster

","6","0.4846022855501531","API expansion","Development"
"https://github.com/dssg/aequitas","354072406","issue","https://github.com/Trusted-AI/AIF360/issues/45","Test fails Test: test_all_0_scores_4 fails.
The reason for it is that in group.get_crosstabs there is a line calling for `df['score'].value_counts()[1.0]`, however the values are all 0s and so there is no threshold. 
Not sure if the fix is to change the test or the check in group.","17","0.3739650021340162","Troubleshooting","Maintenance"
"https://github.com/dssg/aequitas","336383179","issue","https://github.com/Trusted-AI/AIF360/issues/43","Pandas Warnings in CLI There is a warning associated with copying dataframes. See the bottom of the CLI output. 

**CLI OUTPUT:**

`############################################################################
##   Center for Data Science and Public Policy                            ##
##   http://dsapp.uchicago.edu                                            ##
##                                                                        ##
##   Copyright © 2018. The University of Chicago. All Rights Reserved.    ##
############################################################################
____________________________________________________________________________

                    ___                    _ __            
                   /   | ___  ____ ___  __(_) /_____ ______
                  / /| |/ _ \/ __ `/ / / / / __/ __ `/ ___/
                 / ___ /  __/ /_/ / /_/ / / /_/ /_/ (__  ) 
                /_/  |_\___/\__, /\__,_/_/\__/\__,_/____/  
                              /_/    



____________________________________________________________________________

                      Bias and Fairness Audit Tool                           
____________________________________________________________________________
/mnt/data/users/aanisfeld/venv/aequitas_env/lib/python3.4/site-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use ""pip install psycopg2-binary"" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.
  """""")
Welcome to Aequitas-Audit
Fairness measures requested: Statistical Parity,Impact Parity,FPR Parity,FDR Parity
model_id, score_thresholds 1 {'rank_abs': [150, 300, 500, 1000], 'rank_pct': [1.0, 2.0, 5.0, 10.0]}
COUNTS::: gender
F     82601
M    310418
dtype: int64
COUNTS::: race
Black        90769
Hispanic    206528
Other        42483
White        53239
dtype: int64
COUNTS::: age
18-25    138154
26-35    129956
36-45     75097
46-55     34431
>55       15381
dtype: int64
audit: df shape from the crosstabs: (88, 26)
get_disparity_predefined_group()
Any NaN?:  True
bias_df shape: (88, 46)
Fairness Threshold: 0.8
Fairness Measures: ['Statistical Parity', 'Impact Parity', 'FPR Parity', 'FDR Parity']
get_group_value_fairness: No Parity measure input found on bias_df
{'Unsupervised Fairness': False, 'Supervised Fairness': False, 'Overall Fairness': False}
****************** 0     0.21
1     0.79
2     0.21
3     0.79
4     0.21
5     0.79
6     0.21
7     0.79
8     0.21
9     0.79
10    0.21
11    0.79
12    0.21
13    0.79
14    0.21
15    0.79
16    0.23
17    0.53
18    0.11
19    0.14
20    0.23
21    0.53
22    0.11
23    0.14
24    0.23
25    0.53
26    0.11
27    0.14
28    0.23
29    0.53
      ... 
58    0.35
59    0.33
60    0.19
61    0.09
62    0.04
63    0.35
64    0.33
65    0.19
66    0.09
67    0.04
68    0.35
69    0.33
70    0.19
71    0.09
72    0.04
73    0.35
74    0.33
75    0.19
76    0.09
77    0.04
78    0.35
79    0.33
80    0.19
81    0.09
82    0.04
83    0.35
84    0.33
85    0.19
86    0.09
87    0.04
Name: group_size_pct, Length: 88, dtype: object
/mnt/data/users/aanisfeld/venv/aequitas_env/lib/python3.4/site-packages/pandas-0.21.0-py3.4-linux-x86_64.egg/pandas/core/indexing.py:194: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame

See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy
  self._setitem_with_indexer(indexer, value)
/mnt/data/users/aanisfeld/venv/aequitas_env/lib/python3.4/site-packages/aequitas-0.23.0-py3.4.egg/aequitas_cli/utils/report.py:126: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame

See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy
  aux_df.loc[idx, col] = 'Ref'`","17","0.3817680423678353","Troubleshooting","Maintenance"
"https://github.com/dssg/aequitas","320063695","issue","https://github.com/Trusted-AI/AIF360/issues/39","split out ""extra"" installation requirements A basic installation of the `aequitas` distribution should only satisfy the requirements of the `aequitas` package:

    pip install aequitas

Having done the above, `import aequitas_cli` should raise an `ImportError` (either because the package is not installed or because it raises this error due to unsatisfied dependencies).

To install the CLI, _(etc.)_:

    pip install aequitas[cli]","21","0.5248273287287663","Installation and shell commands","Deployment"
"https://github.com/dssg/aequitas","315720026","issue","https://github.com/Trusted-AI/AIF360/issues/34","Fairness Measures with type (supervised vs unsupervised) and retrieve supported fairness.get_fairness_measures_supported(self, input_df)
make it more flexible and less hardcoded. 

actually the all fairness class should be agnostic of the actual names of columns","6","0.4355221341927396","API expansion","Development"
"https://github.com/dssg/aequitas","309160311","issue","https://github.com/Trusted-AI/AIF360/issues/13","Support threshold by score value, e.g. 'rank_val': 0.6 every data points with score > rank_val will be considered 1s","30","0.5732854864433812","Troubleshooting","Maintenance"
"https://github.com/tensorflow/fairness-indicators","996242669","issue","https://github.com/Trusted-AI/AIF360/issues/266","Hakuna  **Describe the feature and the current behavior/state.**

**Will this change the current api? How?**

**Who will benefit with this feature?**

**Are you willing to contribute it (Yes/No).**

**Any Other info.**
","14","0.3782501423420005","Documentation","Development"
"https://github.com/tensorflow/fairness-indicators","939859914","issue","https://github.com/Trusted-AI/AIF360/issues/261","Image in Fairness_Indicators_Lineage_Case_Study.ipynb is not publicly accessible ## URL with the issue:

https://github.com/tensorflow/fairness-indicators/blob/master/g3doc/tutorials/Fairness_Indicators_Lineage_Case_Study.ipynb

## Description of issue (what needs changing):

The link to the image on line 899 in the notebook is not publicly accessible, you need to sign in with a Google account to view it.

```
        ""![Type I and Type II errors](http://services.google.com/fh/gumdrop/preview/blogs/type_i_type_ii.png)\n"",
```

### Correct links

Link to a publicly accessible `type_i_type_ii.png`.","13","0.4924759540800497","Artifact generation and benchmarking","Deployment"
"https://github.com/tensorflow/fairness-indicators","920242916","issue","https://github.com/Trusted-AI/AIF360/issues/260","Performance of CelebA constrained model - Have I written custom code (as opposed to using stock example code provided): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab
- Fairness Indicators version: 0.30.1
- TensorFlow version: 2.5.0
- Python version: 3.7.10

- TFMA version: 0.30.0


Hey there,

I have noticed that the constrained model in the CelebA example Notebook has a horrible positive rate: 0.1 @ 0.5 threshold.

I expected that the model would perform at least equally good on this metric.


","29","0.5310329573103294","Troubleshooting","Maintenance"
"https://github.com/tensorflow/fairness-indicators","759527631","issue","https://github.com/Trusted-AI/AIF360/issues/197","Update example and tutorial links in blog posts hello! 👋 

In looking at some blog posts that are ranked highly when typing ""fairness indicators"" in search engines ([Google AI blog](https://ai.googleblog.com/2019/12/fairness-indicators-scalable.html), [TF blog](https://blog.tensorflow.org/2019/12/fairness-indicators-fair-ML-systems.html)), I found that many of the links are broken.

### In the blog posts:
<img width=""637"" alt=""Screen Shot 2020-12-08 at 9 53 28 AM"" src=""https://user-images.githubusercontent.com/1056957/101499576-6c6f0180-393b-11eb-9387-f77e9f176f51.png"">
<img width=""659"" alt=""Screen Shot 2020-12-08 at 9 53 32 AM"" src=""https://user-images.githubusercontent.com/1056957/101499573-6bd66b00-393b-11eb-89f1-074a4d68ced0.png"">

I submitted a fix for this in the REAMDE in This is the same problem as https://github.com/tensorflow/fairness-indicators/pull/196, but since the blog posts from December 2019 come up so highly in search engines, it might be good to backport the link fixes and republish those posts too.  Thanks! 👍 ","14","0.664852908648529","Documentation","Development"
"https://github.com/tensorflow/fairness-indicators","705417342","issue","https://github.com/Trusted-AI/AIF360/issues/143","Widget doesn't show up in Jupyter Notebook **System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): RHEL 7
- Fairness Indicators version: 
```
fairness-indicators                    0.24.0
tensorboard-plugin-fairness-indicators 0.24.0
```
- Python version: 3.7
- Pip version: 20.2
- NPM version: npm@6.14.4
- Jupyter Notebook version: 
```
jupyter                                1.0.0
jupyter-client                         5.2.3
jupyter-console                        6.1.0
jupyter-contrib-core                   0.3.3
jupyter-contrib-nbextensions           0.5.1
jupyter-core                           4.4.0
jupyter-highlight-selected-word        0.2.0
jupyter-latex-envs                     1.4.6
jupyter-nbextensions-configurator      0.4.1
jupyterlab                             2.2.4
jupyterlab-launcher                    0.11.2
jupyterlab-pygments                    0.1.1
jupyterlab-server                      1.2.0
```


**Describe the problem**
```python
tfma.addons.fairness.view.widget_view.render_fairness_indicator(eval_result)
```
does not display any widget in Jupyter Notebook. It shows these errors in the browser console,
```
manager-base.js:273 Could not instantiate widget
        (anonymous) @ manager-base.js:273
        (anonymous) @ manager-base.js:44
        (anonymous) @ manager-base.js:25
        a @ manager-base.js:17
        Promise.then (async)
        u @ manager-base.js:18
        (anonymous) @ manager-base.js:19
        A @ manager-base.js:15
        t._make_model @ manager-base.js:257
        (anonymous) @ manager-base.js:246
        (anonymous) @ manager-base.js:44
        (anonymous) @ manager-base.js:25
        (anonymous) @ manager-base.js:19
        A @ manager-base.js:15
        t.new_model @ manager-base.js:232
        t.handle_comm_open @ manager-base.js:144
        L @ underscore.js:762
        (anonymous) @ underscore.js:775
        (anonymous) @ underscore.js:122
        (anonymous) @ comm.js:89
        Promise.then (async)
        CommManager.comm_open @ comm.js:85
        i @ jquery.min.js:2
        Kernel._handle_iopub_message @ kernel.js:1223
        Kernel._finish_ws_message @ kernel.js:1015
        (anonymous) @ kernel.js:1006
        Promise.then (async)
        Kernel._handle_ws_message @ kernel.js:1006
        i @ jquery.min.js:2

utils.js:119 Error: Could not create a model.
        at utils.js:119
        (anonymous) @ utils.js:119
        Promise.catch (async)
        t.handle_comm_open @ manager-base.js:149
        L @ underscore.js:762
        (anonymous) @ underscore.js:775
        (anonymous) @ underscore.js:122
        (anonymous) @ comm.js:89
        Promise.then (async)
        CommManager.comm_open @ comm.js:85
        i @ jquery.min.js:2
        Kernel._handle_iopub_message @ kernel.js:1223
        Kernel._finish_ws_message @ kernel.js:1015
        (anonymous) @ kernel.js:1006
        Promise.then (async)
        Kernel._handle_ws_message @ kernel.js:1006
        i @ jquery.min.js:2

2kernel.js:1007 Couldn't process kernel message TypeError: Cannot read property 'FairnessIndicatorModel' of undefined
        at manager.js:153
        (anonymous) @ kernel.js:1007
        Promise.catch (async)
        Kernel._handle_ws_message @ kernel.js:1007
        i @ jquery.min.js:2

manager.js:153 Uncaught (in promise) TypeError: Cannot read property 'FairnessIndicatorModel' of undefined
    at manager.js:153
    (anonymous)	@	manager.js:153
    Promise.then (async)		
    t.register_model	@	manager-base.js:208
    (anonymous)	@	manager-base.js:248
    (anonymous)	@	manager-base.js:44
    (anonymous)	@	manager-base.js:25
    (anonymous)	@	manager-base.js:19
    A	@	manager-base.js:15
    t.new_model	@	manager-base.js:232
    t.handle_comm_open	@	manager-base.js:144
    L	@	underscore.js:762
    (anonymous)	@	underscore.js:775
    (anonymous)	@	underscore.js:122
    (anonymous)	@	comm.js:89
    Promise.then (async)		
    CommManager.comm_open	@	comm.js:85
    i	@	jquery.min.js:2
    Kernel._handle_iopub_message	@	kernel.js:1223
    Kernel._finish_ws_message	@	kernel.js:1015
    (anonymous)	@	kernel.js:1006
    Promise.then (async)		
    Kernel._handle_ws_message	@	kernel.js:1006
    i	@	jquery.min.js:2
```


**Provide the exact sequence of commands / steps that you executed before running into the problem**
I was just following an example from TF docs which uses bar_pass_prediction.csv dataset. Below code shows how the config is created from sample data.

```python
# Specify Fairness Indicators in eval_config.
eval_config = text_format.Parse(""""""
  model_specs {
    prediction_key: 'dnn_bar_pass_prediction',
    label_key: 'pass_bar'
  }
  metrics_specs {
    metrics {class_name: ""AUC""}
    metrics {
      class_name: ""FairnessIndicators""
      config: '{""thresholds"": [0.50, 0.90]}'
    }
  }
  slicing_specs {
    feature_keys: 'race1'
  }
  slicing_specs {}
  """""", tfma.EvalConfig())

# Run TensorFlow Model Analysis.
eval_result = tfma.analyze_raw_data(
  data=_LSAT_DF,
  eval_config=eval_config,
  output_path=_DATA_ROOT)
```

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
None","29","0.9103938616457417","Troubleshooting","Maintenance"
"https://github.com/tensorflow/fairness-indicators","612135707","issue","https://github.com/Trusted-AI/AIF360/issues/71","AttributeError in Facessd Fairness Indicators Example Colab.ipynb **System information**
- Running ""Facessd Fairness Indicators Example Colab.ipynb"" on Colab
- TensorFlow version: 2.2.0-rc3
- Python version: 3.6.9


**Describe the current behavior**

Getting the following error when running cell 3 line 2:

```python
AttributeError: module 'tfx_bsl.coders.example_coder' has no attribute 'ExamplesToRecordBatchDecoder' [while running 'DecodeData/BatchSerializedExamplesToArrowTables/BatchDecodeExamples']
```

**Standalone code to reproduce the issue**
The error is easy reproduced running the ""Facessd Fairness Indicators Example Colab.ipynb"" on Colab.

**Other info / logs** 
```python
IndexError                                Traceback (most recent call last)
/usr/local/lib/python3.6/dist-packages/apache_beam/runners/worker/sdk_worker.py in get(self, instruction_id, bundle_descriptor_id)
    311       # pop() is threadsafe
--> 312       processor = self.cached_bundle_processors[bundle_descriptor_id].pop()
    313     except IndexError:

IndexError: pop from empty list

During handling of the above exception, another exception occurred:

AttributeError                            Traceback (most recent call last)
/usr/local/lib/python3.6/dist-packages/apache_beam/runners/common.cpython-36m-x86_64-linux-gnu.so in apache_beam.runners.common.DoFnRunner._invoke_lifecycle_method()

/usr/local/lib/python3.6/dist-packages/apache_beam/runners/common.cpython-36m-x86_64-linux-gnu.so in apache_beam.runners.common.DoFnInvoker.invoke_setup()

/usr/local/lib/python3.6/dist-packages/tensorflow_data_validation/utils/batch_util.py in setup(self)
    106   def setup(self):
--> 107     self._decoder = example_coder.ExamplesToRecordBatchDecoder()
    108 

AttributeError: module 'tfx_bsl.coders.example_coder' has no attribute 'ExamplesToRecordBatchDecoder'

During handling of the above exception, another exception occurred:

AttributeError                            Traceback (most recent call last)
<ipython-input-3-31ccd38caa04> in <module>()
      1 data_location = tf.keras.utils.get_file('lfw_dataset.tf', 'https://storage.googleapis.com/facessd_dataset/lfw_dataset.tfrecord')
      2 
----> 3 stats = tfdv.generate_statistics_from_tfrecord(data_location=data_location)
      4 tfdv.visualize_statistics(stats)

/usr/local/lib/python3.6/dist-packages/tensorflow_data_validation/utils/stats_gen_lib.py in generate_statistics_from_tfrecord(data_location, output_path, stats_options, pipeline_options, compression_type)
    113             shard_name_template='',
    114             coder=beam.coders.ProtoCoder(
--> 115                 statistics_pb2.DatasetFeatureStatisticsList)))
    116   return load_statistics(output_path)
    117 

/usr/local/lib/python3.6/dist-packages/apache_beam/pipeline.py in __exit__(self, exc_type, exc_val, exc_tb)
    501   def __exit__(self, exc_type, exc_val, exc_tb):
    502     if not exc_type:
--> 503       self.run().wait_until_finish()
    504 
    505   def visit(self, visitor):

/usr/local/lib/python3.6/dist-packages/apache_beam/pipeline.py in run(self, test_runner_api)
    481       return Pipeline.from_runner_api(
    482           self.to_runner_api(use_fake_coders=True), self.runner,
--> 483           self._options).run(False)
    484 
    485     if self._options.view_as(TypeOptions).runtime_type_check:

/usr/local/lib/python3.6/dist-packages/apache_beam/pipeline.py in run(self, test_runner_api)
    494       finally:
    495         shutil.rmtree(tmpdir)
--> 496     return self.runner.run_pipeline(self, self._options)
    497 
    498   def __enter__(self):

/usr/local/lib/python3.6/dist-packages/apache_beam/runners/direct/direct_runner.py in run_pipeline(self, pipeline, options)
    128       runner = BundleBasedDirectRunner()
    129 
--> 130     return runner.run_pipeline(pipeline, options)
    131 
    132 

/usr/local/lib/python3.6/dist-packages/apache_beam/runners/portability/fn_api_runner.py in run_pipeline(self, pipeline, options)
    553 
    554     self._latest_run_result = self.run_via_runner_api(
--> 555         pipeline.to_runner_api(default_environment=self._default_environment))
    556     return self._latest_run_result
    557 

/usr/local/lib/python3.6/dist-packages/apache_beam/runners/portability/fn_api_runner.py in run_via_runner_api(self, pipeline_proto)
    563     # TODO(pabloem, BEAM-7514): Create a watermark manager (that has access to
    564     #   the teststream (if any), and all the stages).
--> 565     return self.run_stages(stage_context, stages)
    566 
    567   @contextlib.contextmanager

/usr/local/lib/python3.6/dist-packages/apache_beam/runners/portability/fn_api_runner.py in run_stages(self, stage_context, stages)
    704               stage,
    705               pcoll_buffers,
--> 706               stage_context.safe_coders)
    707           metrics_by_stage[stage.name] = stage_results.process_bundle.metrics
    708           monitoring_infos_by_stage[stage.name] = (

/usr/local/lib/python3.6/dist-packages/apache_beam/runners/portability/fn_api_runner.py in _run_stage(self, worker_handler_factory, pipeline_components, stage, pcoll_buffers, safe_coders)
   1071         cache_token_generator=cache_token_generator)
   1072 
-> 1073     result, splits = bundle_manager.process_bundle(data_input, data_output)
   1074 
   1075     def input_for(transform_id, input_id):

/usr/local/lib/python3.6/dist-packages/apache_beam/runners/portability/fn_api_runner.py in process_bundle(self, inputs, expected_outputs)
   2332 
   2333     with UnboundedThreadPoolExecutor() as executor:
-> 2334       for result, split_result in executor.map(execute, part_inputs):
   2335 
   2336         split_result_list += split_result

/usr/lib/python3.6/concurrent/futures/_base.py in result_iterator()
    584                     # Careful not to keep a reference to the popped future
    585                     if timeout is None:
--> 586                         yield fs.pop().result()
    587                     else:
    588                         yield fs.pop().result(end_time - time.monotonic())

/usr/lib/python3.6/concurrent/futures/_base.py in result(self, timeout)
    430                 raise CancelledError()
    431             elif self._state == FINISHED:
--> 432                 return self.__get_result()
    433             else:
    434                 raise TimeoutError()

/usr/lib/python3.6/concurrent/futures/_base.py in __get_result(self)
    382     def __get_result(self):
    383         if self._exception:
--> 384             raise self._exception
    385         else:
    386             return self._result

/usr/local/lib/python3.6/dist-packages/apache_beam/utils/thread_pool_executor.py in run(self)
     42       # If the future wasn't cancelled, then attempt to execute it.
     43       try:
---> 44         self._future.set_result(self._fn(*self._fn_args, **self._fn_kwargs))
     45       except BaseException as exc:
     46         # Even though Python 2 futures library has #set_exection(),

/usr/local/lib/python3.6/dist-packages/apache_beam/runners/portability/fn_api_runner.py in execute(part_map)
   2329           self._registered,
   2330           cache_token_generator=self._cache_token_generator)
-> 2331       return bundle_manager.process_bundle(part_map, expected_outputs)
   2332 
   2333     with UnboundedThreadPoolExecutor() as executor:

/usr/local/lib/python3.6/dist-packages/apache_beam/runners/portability/fn_api_runner.py in process_bundle(self, inputs, expected_outputs)
   2243             process_bundle_descriptor_id=self._bundle_descriptor.id,
   2244             cache_tokens=[next(self._cache_token_generator)]))
-> 2245     result_future = self._worker_handler.control_conn.push(process_bundle_req)
   2246 
   2247     split_results = []  # type: List[beam_fn_api_pb2.ProcessBundleSplitResponse]

/usr/local/lib/python3.6/dist-packages/apache_beam/runners/portability/fn_api_runner.py in push(self, request)
   1557       self._uid_counter += 1
   1558       request.instruction_id = 'control_%s' % self._uid_counter
-> 1559     response = self.worker.do_instruction(request)
   1560     return ControlFuture(request.instruction_id, response)
   1561 

/usr/local/lib/python3.6/dist-packages/apache_beam/runners/worker/sdk_worker.py in do_instruction(self, request)
    413       # E.g. if register is set, this will call self.register(request.register))
    414       return getattr(self, request_type)(
--> 415           getattr(request, request_type), request.instruction_id)
    416     else:
    417       raise NotImplementedError

/usr/local/lib/python3.6/dist-packages/apache_beam/runners/worker/sdk_worker.py in process_bundle(self, request, instruction_id)
    442     # type: (...) -> beam_fn_api_pb2.InstructionResponse
    443     bundle_processor = self.bundle_processor_cache.get(
--> 444         instruction_id, request.process_bundle_descriptor_id)
    445     try:
    446       with bundle_processor.state_handler.process_instruction_id(

/usr/local/lib/python3.6/dist-packages/apache_beam/runners/worker/sdk_worker.py in get(self, instruction_id, bundle_descriptor_id)
    316           self.state_handler_factory.create_state_handler(
    317               self.fns[bundle_descriptor_id].state_api_service_descriptor),
--> 318           self.data_channel_factory)
    319     self.active_bundle_processors[
    320         instruction_id] = bundle_descriptor_id, processor

/usr/local/lib/python3.6/dist-packages/apache_beam/runners/worker/bundle_processor.py in __init__(self, process_bundle_descriptor, state_handler, data_channel_factory)
    741     self.ops = self.create_execution_tree(self.process_bundle_descriptor)
    742     for op in self.ops.values():
--> 743       op.setup()
    744     self.splitting_lock = threading.Lock()
    745 

/usr/local/lib/python3.6/dist-packages/apache_beam/runners/worker/operations.cpython-36m-x86_64-linux-gnu.so in apache_beam.runners.worker.operations.DoOperation.setup()

/usr/local/lib/python3.6/dist-packages/apache_beam/runners/worker/operations.cpython-36m-x86_64-linux-gnu.so in apache_beam.runners.worker.operations.DoOperation.setup()

/usr/local/lib/python3.6/dist-packages/apache_beam/runners/common.cpython-36m-x86_64-linux-gnu.so in apache_beam.runners.common.DoFnRunner.setup()

/usr/local/lib/python3.6/dist-packages/apache_beam/runners/common.cpython-36m-x86_64-linux-gnu.so in apache_beam.runners.common.DoFnRunner._invoke_lifecycle_method()

/usr/local/lib/python3.6/dist-packages/apache_beam/runners/common.cpython-36m-x86_64-linux-gnu.so in apache_beam.runners.common.DoFnRunner._reraise_augmented()

/usr/local/lib/python3.6/dist-packages/future/utils/__init__.py in raise_with_traceback(exc, traceback)
    417         if traceback == Ellipsis:
    418             _, _, traceback = sys.exc_info()
--> 419         raise exc.with_traceback(traceback)
    420 
    421 else:

/usr/local/lib/python3.6/dist-packages/apache_beam/runners/common.cpython-36m-x86_64-linux-gnu.so in apache_beam.runners.common.DoFnRunner._invoke_lifecycle_method()

/usr/local/lib/python3.6/dist-packages/apache_beam/runners/common.cpython-36m-x86_64-linux-gnu.so in apache_beam.runners.common.DoFnInvoker.invoke_setup()

/usr/local/lib/python3.6/dist-packages/tensorflow_data_validation/utils/batch_util.py in setup(self)
    105 
    106   def setup(self):
--> 107     self._decoder = example_coder.ExamplesToRecordBatchDecoder()
    108 
    109   def process(self, batch: List[bytes]) -> Iterable[pa.Table]:

AttributeError: module 'tfx_bsl.coders.example_coder' has no attribute 'ExamplesToRecordBatchDecoder' [while running 'DecodeData/BatchSerializedExamplesToArrowTables/BatchDecodeExamples']
```","18","0.8803068767527574","Troubleshooting","Maintenance"
"https://github.com/tensorflow/fairness-indicators","591288148","issue","https://github.com/Trusted-AI/AIF360/issues/51","Widget Not Working in Jupyter Notebook Environment Hi Dev team,

I am studying the fairness indicator when trying to use it locally instead of colab. When I am trying to render the widget from the following example, the results are not showing up:

event_handlers={'slice-selected':
                wit.create_selection_callback(wit_data, DEFAULT_MAX_EXAMPLES)}
widget_view.render_fairness_indicator(eval_result=eval_result,
                                      slicing_column=slice_selection,
                                      event_handlers=event_handlers
                                      )
Later I checked the source code and find out when the environment is not colab, it will pass to a empty class called FairnessIndicatorViewer. I wonder is it intended or it will be fixed later? Many thanks!


","29","0.4315106088997408","Troubleshooting","Maintenance"
"https://github.com/adebayoj/fairml","621668019","issue","https://github.com/Trusted-AI/AIF360/issues/13","ValueError: cannot reshape array of size 34712 into shape (17356,) Hi @adebayoj  

I just tried to use Fairml to explain my XGBoost Model, and I got the aforementioned error. 

I think the error is logical, because in the mse function, I think you are doing the reshape as if it was only a one class situation. However, for my case the output_constant_col and the normal_black_box_output is of size (17356,2).  So I am working in a 2 class classification scenario. 

Any ideas how to overcome the error mentioned?

![Capture (1)](https://user-images.githubusercontent.com/45386621/82438882-03589700-9a9a-11ea-9f9a-e1113845299d.PNG)

Thanks.


","17","0.2823480200780813","Troubleshooting","Maintenance"
"https://github.com/adebayoj/fairml","492620377","issue","https://github.com/Trusted-AI/AIF360/issues/12","Need Clarification for function detect_feature_sign  Hi Julius,

I need clarification in function detect feature sign.
1. For calculating feature dependence value we are transforming input by 
    obtain_orthogonal_transformed_matrix function and obtain the output from model by sending 
    the transformed input to model.
2. For calculating feature sign we are transforming the input by different transformation 
    and output from model by sending the transformed input to model.

Output from model for step 1 and 2 is different for same feature.

So can you please clarify why we are using two different transformation for calculating feature dependence value and sign respectively. 
Hoping to hear from you soon.

Thankyou in Advance","9","0.9194951700410215","Feature engineering methodology","Design"
"https://github.com/adebayoj/fairml","467947004","issue","https://github.com/Trusted-AI/AIF360/issues/11","cannot import name 'plot generic dependency dictionary' Hi,
I have installed fairml but when I do ""from fairml import plot_generic_dependence_dictionary"". I got an error: 'cannot import name 'plot generic dependency dictionary'. Can you please let me know how to fix it?
Thanks","9","0.585702091335894","Feature engineering methodology","Design"
"https://github.com/adebayoj/fairml","443017094","issue","https://github.com/Trusted-AI/AIF360/issues/10","Data in public domain or can I use it? I am a law professor at the University of Houston Law Center publishing a book on Data Analysis using the Wolfram Language. I would like to place your data with some minor optimizations for the Wolfram Language on the Wolfram Data Repository for use by other researchers and readers of my book.  I would of course give you full credit for developing the simplified data and cite ProPublica and Northpointe (now Equivant) as originators of data. I am not sure of the licensing status of your dataset and want to be sure there is not a problem.  I can be reached directly at schandler @ uh . edu 

Thanks. A very interesting and important topic!","8","0.372069563813905","Understanding Fairness Definition","Requirement Analysis"
"https://github.com/adebayoj/fairml","364626227","issue","https://github.com/Trusted-AI/AIF360/issues/9","Update Sign Detection and add to Readme.  Clarify that fairml does not necessarily match regression signs. 

Bug in https://github.com/adebayoj/fairml/blob/master/fairml/utils.py#L80, why add the range? ","9","0.607895236287721","Feature engineering methodology","Design"
"https://github.com/adebayoj/fairml","340163201","issue","https://github.com/Trusted-AI/AIF360/issues/8","constant-zero no matter what Hi,
  I tried using this package, and in the total dictionary I only got constant vectors. I digged a bit in the code and it seems like in line 217 in orthogonal_projection.py, you pass ptb_strategy=""constant-zero""to replace_column_of_matrix no matter what the user has selected as direct_input_pertubation_strategy. This means the algorithm produces the exact same number in each iteration no matter what.
So in line 217 in orthogonal_projection.py, ""constant-zero"" should be replaced by direct_input_pertubation_strategy.

In addition the comments are not consistent with what the algorithm actually does.
In line 132 in orthogonal_projection.py you have:
direct_input_pertubation_strategy -> This is referring to how to zero out a
                            single variable. One of three different options
                            1) replace with a random constant value
                            2) replace with median constant value
                            3) replace all values with a random permutation of
                               the column.  options = [constant-zero,
                               constant-median, global-permutation]

While in reality, you should have:

direct_input_pertubation_strategy -> This is referring to how to zero out a
                            single variable. One of three different options
                            1) replace with 0 constant value
                            2) replace with median constant value
                            3) replace with one random sample from the data
                               options = [constant-zero, constant-median, random-sample]

Also the first option, which is the default is not very smart, given that 0 might not even be in the range of the variable.

Otherwise IOFP is a smart idea ;)","9","0.7842616173919463","Feature engineering methodology","Design"
"https://github.com/adebayoj/fairml","339879675","issue","https://github.com/Trusted-AI/AIF360/issues/7","Code not consistent with thesis Hi,
  In your thesis, somewhere you say that : four different algorithms, listed below, are run to rank the importance of all features. Each ranking algorithm produces a score for each feature. The four sets of scores obtained from the ranking algorithms are then aggregated into a combined score for each feature. The final ranking of a feature is determined by the combined score obtained. The results can then be scaled and ploted. The ranking algorithms consist of:
1.	 Iterative Orthogonal Feature Projection Algorithm (IOFP)
2.	minimum Redundancy, Maximum Relevance Feature Selection (mRMR)
3.	Least Absolute Shrinkage and Selection Operator (LASSO)
4.	Random Forest (RF)

However in the audit_model function which seems to be the main function, which is implemented in orthogonal_projection.py, you seem to only implement 1 and 2 and then return the results as 2 dictionaries. Am I correct and there is an inconsistency between the documentation and the code or have I misunderstood something?
","9","0.4777749023377329","Feature engineering methodology","Design"
"https://github.com/adebayoj/fairml","213662022","issue","https://github.com/Trusted-AI/AIF360/issues/4","readme.md example code not consistent with example.py Hi, I noticed that some lines in the example are different from example.py, and seem to call (presumably) deprecated functions.

Regarding `from fairml import plot_generic_dependence_dictionary`:
There is no `plot_generic_dependence_dictionary`. Importing `plot_dependencies` worked for me.

Also,
`fig = plot_dependencies(
    total.get_compress_dictionary_into_key_median(),
    reverse_values=False,
    title=""FairML feature dependence""
)` results in: 
`AttributeError: 'AuditResult' object has no attribute 'get_compress_dictionary_into_key_median'`, while `total.median()` works.","9","0.5879135745207176","Feature engineering methodology","Design"
"https://github.com/adebayoj/fairml","213607391","issue","https://github.com/Trusted-AI/AIF360/issues/3","`direct_input_pertubation_strategy=` isn't passed down Nice method.

I am examining the code and the thesis more closely as it appears to be very useful.

I don't fully understand the point of perturbation strategy and it's not fully expanded on in the thesis.

I started reading the code and I spotted some bugs.

Firstly

https://github.com/adebayoj/fairml/blob/master/fairml/orthogonal_projection.py#L120

takes the strategy but ignores it, see:

https://github.com/adebayoj/fairml/blob/master/fairml/orthogonal_projection.py#L217

Also, I think that with constant_zero and median perturbation strategies this loop is redundant:

https://github.com/adebayoj/fairml/blob/master/fairml/orthogonal_projection.py#L205

As each run ignores `random_sample_selected` anyway, so each run *should* produce the same `output_difference_col` and `total_difference`. (because `data_col_ptb` and `total_ptb_data` are identical each run).

Finally, it would be great if you could explain more in the documentation the purpose of `direct_input_pertubation_strategy`. Is it necessary at all to ""zero-out"" a column? Why?

It appears to me that just by orthogonalising other columns you already take away the effect of the subject column. Not clear to me why zero'ing out is required on top. Is it to be certain the effect of the column is not present? 

Many thanks for the code by the way!","9","0.5907044197279627","Feature engineering methodology","Design"
"https://github.com/adebayoj/fairml","213122129","issue","https://github.com/Trusted-AI/AIF360/issues/2","Figure broken Cool project! (I'm currently reading the white paper.)

I saw that the png on ln133 of the README is not rendering.","25","0.4062372604973501","Research","Requirement Analysis"
"https://github.com/algofairness/fairness-comparison","658541311","issue","https://github.com/Trusted-AI/AIF360/issues/7","UndefinedMetricWarning and run parameters {} failed using python 3.7 on Windows 10.   installed with pip3 utilized 

>>> from fairness.benchmark import run
Available algorithms:
  SVM
  GaussianNB
  LR
  DecisionTree
  Kamishima
  Calders
  ZafarBaseline
  ZafarFairness
  ZafarAccuracy
  Kamishima-accuracy
  Kamishima-DIavgall
  Feldman-SVM
  Feldman-GaussianNB
  Feldman-LR
  Feldman-DecisionTree
  Feldman-SVM-DIavgall
  Feldman-SVM-accuracy
  Feldman-GaussianNB-DIavgall
  Feldman-GaussianNB-accuracy

>>> run()
Datasets: '['ricci', 'adult', 'german', 'propublica-recidivism', 'propublica-violent-recidivism']'

Evaluating dataset:ricci
Sensitive attribute:Race
    Algorithm: SVM
       supported types: {'numerical', 'numerical-binsensitive'}
C:\Users\kevin\AppData\Local\Programs\Python\Python37-32\lib\site-packages\sklearn\metrics\_classification.py:1221: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
C:\Users\kevin\AppData\Local\Programs\Python\Python37-32\lib\site-packages\sklearn\metrics\_classification.py:1221: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.

-------------------------------------------

C:\Users\kevin\Documents\GitHub\fairness-comparison-master\fairness\algorithms\kamishima\KamishimaAlgorithm.py:99: UserWarning: loadtxt: Empty input file: ""C:\Users\kevin\AppData\Local\Temp\tmps24_t_kk""
  m = numpy.loadtxt(output_name)
run for parameters {} failed: too many indices for array
C:\Users\kevin\Documents\GitHub\fairness-comparison-master\fairness\algorithms\kamishima\KamishimaAlgorithm.py:99: UserWarning: loadtxt: Empty input file: ""C:\Users\kevin\AppData\Local\Temp\tmp86i67yum""
  m = numpy.loadtxt(output_name)
run for parameters {} failed: too many indices for array
C:\Users\kevin\Documents\GitHub\fairness-comparison-master\fairness\algorithms\kamishima\KamishimaAlgorithm.py:99: UserWarning: loadtxt: Empty input file: ""C:\Users\kevin\AppData\Local\Temp\tmp3q3uqt1g""
  m = numpy.loadtxt(output_name)
run for parameters {} failed: too many indices for array","16","0.4244862504857476","Testing","Maintenance"
"https://github.com/algofairness/fairness-comparison","572056470","issue","https://github.com/Trusted-AI/AIF360/issues/6","python version Hi,

In which python version those this work?","29","0.6933066933066934","Troubleshooting","Maintenance"
"https://github.com/algofairness/fairness-comparison","292654374","issue","https://github.com/Trusted-AI/AIF360/issues/1","File ""pandas/_libs/hashtable_class_helper.pxi"", line 817, in pandas._libs.hashtable.Int64HashTable.get_item KeyError: 0 Above error while using actual[i] in  ***calc_fp_fn(actual, predicted, sensitive, unprotected_vals, positive_pred)**  in utility.py.
Can't figure out why. The code is available on my branch.","4","0.3675951918232203","Installation and shell commands","Deployment"
"https://github.com/cosmicBboy/themis-ml","572378985","issue","https://github.com/Trusted-AI/AIF360/issues/42","Consider Rewording Your Description - Minority vs Majority not Minority vs Race, or Race vs Majority. ","26","0.5186257479251111","Bias mitigation methodology","Design"
"https://github.com/cosmicBboy/themis-ml","446212023","issue","https://github.com/Trusted-AI/AIF360/issues/41","ACF algo Hi I am trying to implement ACF also for fairness, 
but I am getting the following error: index %s is not in continuous_index_ or binary_index_","6","0.4564531513684055","API expansion","Development"
"https://github.com/cosmicBboy/themis-ml","369630740","issue","https://github.com/Trusted-AI/AIF360/issues/40","Fair ANN with adversarial networks Implement the architecture as described in this post: https://blog.godatadriven.com/fairness-in-ml","22","0.6704304188380622","Bias mitigation methodology","Design"
"https://github.com/cosmicBboy/themis-ml","261959072","issue","https://github.com/Trusted-AI/AIF360/issues/32","Add docstrings to the data_types classes These classes are currently lacking in documentation","26","0.3905180840664712","Bias mitigation methodology","Design"
"https://github.com/cosmicBboy/themis-ml","261867351","issue","https://github.com/Trusted-AI/AIF360/issues/31","Add census income data https://archive.ics.uci.edu/ml/datasets/Census-Income+%28KDD%29","0","0.428513321130771","Dataset usage","Requirement Analysis"
"https://github.com/cosmicBboy/themis-ml","261866391","issue","https://github.com/Trusted-AI/AIF360/issues/30","Add logo to favicon and upper left navigation home button Need to add stuff to `conf.py`
http://www.sphinx-doc.org/en/1.5.1/config.html","14","0.4478468899521532","Documentation","Development"
"https://github.com/cosmicBboy/themis-ml","261865791","issue","https://github.com/Trusted-AI/AIF360/issues/29","Add data from Bureau of Labor Statistics wage data Tables can be found here: https://www.bls.gov/cps/cpswktabs.htm","8","0.1880728998166719","Understanding Fairness Definition","Requirement Analysis"
"https://github.com/cosmicBboy/themis-ml","261864707","issue","https://github.com/Trusted-AI/AIF360/issues/28","Create stratified metrics for mean difference and normalized mean difference The purpose of this issue is to add support for stratified mean difference and normalized mean difference so that we can control for other explanatory (or confounding) factors that may be driving the mean difference in outcome `y` between the advantaged and disadvantaged groups `s_+ and s_-`","3","0.470859333578042","Bias detection metrics validation","Validation"
"https://github.com/cosmicBboy/themis-ml","253876044","issue","https://github.com/Trusted-AI/AIF360/issues/14","Implement ""Discrimination-aware Ensemble Classification"" postprocessing DAEC is like #10, with a similar relabelling rule as ROC but re-assigns any prediction where classifiers disagree on the predicted label.

For example, if the an observation was positively labelled and the ensemble classifiers disagree on the predicted label, then the prediction would be negative.","23","0.6316246162061141","Bias mitigation methodology","Design"
"https://github.com/cosmicBboy/themis-ml","253875676","issue","https://github.com/Trusted-AI/AIF360/issues/13","Implement ""Prejudice Remover Regularized"" Estimator PRR as an optimization technique that extends the standard L1/L2-norm regularization method by adding a prejudice index term to the objective function. This term is equivalent to normalized mutual information, which measures the degree to which predictions ŷ and s are dependent on each other.

With values ranging from 0 to 1, 0 means that ŷ and s are independent and a value of 1 means that they are dependent. The goal of the objective function is to find model parameters that minimize the difference between ŷ and y in addition to the degree to which ŷ depends on s. See reference below for exact implementation details.

```
Reference:
Kamishima, T., Akaho, S., Asoh, H., & Sakuma, J. (2012). Fairness-aware classifier with prejudice remover regularizer. Machine Learning and Knowledge Discovery in Databases, 35-50.
http://www.kamishima.net/archive/2011-ws-icdm_padm.pdf
```
","6","0.3537725778214273","API expansion","Development"
"https://github.com/cosmicBboy/themis-ml","253874219","issue","https://github.com/Trusted-AI/AIF360/issues/12","Implement ""Sampling"" fairness-aware preprocessing Sampling is composed of two methods:

# Uniform Sampling

- uniformly sample (with replacement) `n` observations from each group, where `n` is the expected size of that group assuming a uniform distribution (conditioned on the protected class `s`).

# Preferential Sampling

- sample observations using a ranker `R`, similar to the massaging method.
- the procedure is to duplicate the top-ranked `X_s1_y+` and `X_s0_y–` while removing
  top-ranked `X_s1_y–` and `X_s0_y+`.","23","0.5935771283105692","Bias mitigation methodology","Design"
"https://github.com/cosmicBboy/themis-ml","253872439","issue","https://github.com/Trusted-AI/AIF360/issues/11","Implement ""Reweighting"" fairness-aware preprocessing Reweighting takes a dataset `D` and assigns a weight to each observation using conditional probabilities based on target labels and protected class membership. 

`s1` - disadvantaged group
`s2` - advantaged group
`+` - positive label
`-` - negative label

- large weights are assigned to `X_s1_y+` and `X_s0_y–`:
  - weights for `s1 | +`: `(p(s1) * p(+)) / p(s1 and +)`
  - weights for `s1 | -`: `(p(s1) * p(-)) / p(s1 and -)`
- small weights are assigned to `Xs1_y–` and `X_s0_y+`
  - weights for `s0 | +`: `(p(s0) * p(+)) / p(s0 and +)`
  - weights for `s0 | -`: `(p(s0) * p(-)) / p(s0 and -)`
- the weights are then used as input to model types that support weighted observations

NOTE: The above weighting scheme works because e.g. the numerator `p(s1) * p(+)` denotes the
expected probability of an observation being disadvantaged and positively labelled if the two variables are independent, and the denominator `p(s1 and +)` denotes the actual probability. Therefore, in a discriminatory dataset the term `(p(s1) * p(+)) / p(s1 and +)` will evaluate to `> 1` since the actual probability of being `s1` and `+` is less than the expected probability under the independence assumption.

Conversly, `(p(s1) * p(-)) / p(s1 and -)` will evaluate to `< 1` since the actual probability of being `s1` and `-` is greater than the expected probability under the independence assumption.","23","0.371567835717582","Bias mitigation methodology","Design"
"https://github.com/cosmicBboy/themis-ml","253870578","issue","https://github.com/Trusted-AI/AIF360/issues/10","Implement ""Reject Option Classification"" post-processing technique **Single Classifier Setting**

-  training an initial classifier on dataset `D`
- generating predicted probabilities on the test set
- computing the proximity of each prediction to the decision boundary learned by the
  classifier
- within the critical region threshold `theta` around the decision boundary,
  where `0.5 < theta < 1`, `X_s1` (disadvantaged observations) are assigned as `y+` and
  X_s0 (advantaged observations are assigned as `y –`.

**Multi-classifier Setting**

ROC in the multiple classifier setting is similar to the single classifier setting, except that predicted probabilities are defined as the weighted average of probabilities generated by each classifier `C_k` (k is the number of different classifiers trained), where the weights can be defined as:

- the accuracy of the classifier on the data.
- uniform (take the mean of the predictions)
","23","0.8385053121408348","Bias mitigation methodology","Design"
"https://github.com/cosmicBboy/themis-ml","253868339","issue","https://github.com/Trusted-AI/AIF360/issues/9","Implement ""Additive Counterfactually Fair"" estimator The main idea is to:

- train linear models using some linear estimator `M` to predict each feature `x_i` using
  the protected class attribute `s` as input.
- then compute the residuals `epsilon_ij` between the predicted feature values 
  and true feature values for each observation i for each feature j.
- The final model is then trained using `epsilon_ij` as input features to predict the target `y`.","0","0.3110310283085152","Dataset usage","Requirement Analysis"
"https://github.com/cosmicBboy/themis-ml","253867985","issue","https://github.com/Trusted-AI/AIF360/issues/8","Implement ""Massaging""/""Relabelling"" data transformation technique This technique essentially relabels the target variables using a function that can compute a decision boundary in input data space.

- the top `n` `-ve` labelled observations in the disadvantaged group `s1` that are closest to the 
  decision boundary are ""promoted"" to the `+ve` label.
- the top `n` `+ve` labelled observations in the advantaged group `s0` closest to the
  decision boundary are ""demoted' to the `-ve` label. 

- `n` is the number of promotions/demotions needed to make `p(+|s0) = p(+|s1)`","23","0.3686226513451645","Bias mitigation methodology","Design"
"https://github.com/cosmicBboy/themis-ml","253500090","issue","https://github.com/Trusted-AI/AIF360/issues/2","Build benchmark classifiers using the german credit data # Benchmark classifiers

Train models with the following specifications using the [german_credit dataset](https://github.com/cosmicBboy/themis-ml/blob/master/themis_ml/datasets/datasets.py#L28)

## Models
- logistic regression
- random forest
- svc
- neural net

## Feature Sets
- All `german credit dataset` features
- Non-protected class features

## Performance Metrics
- AUC, F1-score, Accuracy

Pick the best model type and hyper-parameter setting to establish performance benchmark","22","0.3080429894589188","Bias mitigation methodology","Design"
"https://github.com/cosmicBboy/themis-ml","253129899","issue","https://github.com/Trusted-AI/AIF360/issues/1","Create utility function to load German Credit dataset Create a function `german_dataset` that outputs a pandas dataframe with the german credit data found here:

https://archive.ics.uci.edu/ml/datasets/Statlog+(German+Credit+Data)","22","0.4910873440285204","Bias mitigation methodology","Design"
"https://github.com/predictive-analytics-lab/EthicML","920261270","issue","https://github.com/Trusted-AI/AIF360/issues/436","Merge mypy.ini into pyproject.toml The latest mypy version (version 0.900) finally has support for `pyproject.toml`. We should make use of that.

Docs: https://mypy.readthedocs.io/en/latest/config_file.html#using-a-pyproject-toml-file","32","0.6997532738660085","Dependency and Release","Deployment"
"https://github.com/predictive-analytics-lab/EthicML","920259454","issue","https://github.com/Trusted-AI/AIF360/issues/435","The release packaged with poetry contains the raw CSV files, making it quite big Now that #429 has been merged, I created a new release and built and published it with poetry. However, while with setup.py, the wheel was 38MB, with poetry it's 88MB. This is most likely because of `ethicml/data/csvs/raw/health.csv`, which is a very large file and which was previously not included in the wheel.","4","0.5925887445887449","Installation and shell commands","Deployment"
"https://github.com/predictive-analytics-lab/EthicML","890102503","issue","https://github.com/Trusted-AI/AIF360/issues/431","Support for multi-class classification Hi,

I have been working on a multi-class classification problem. I used the following metrics with the pos_class parameter for all the classes one by one but the values are same for all the classes.

1. AbsCV
2. Accuracy
3. Average Odds Difference
4. Balanced Classification Rate
5. CV
6. F1-score
7. NMI
8. RenyiCorrelation
9. SklearnMetric (for few metrics)
10. Theil Index
11. Yanovich 

On further investigation, I figured out that the Theil index is calculated for the class having maximum instances while other metrics are calculated for Y=1.

Please let me know whether the above mentioned metrics support multi-class classification or not. If yes, then how can I use it and if not, are you planning to have it in the next release.   ","3","0.4138199621675753","Bias detection metrics validation","Validation"
"https://github.com/predictive-analytics-lab/EthicML","839554863","issue","https://github.com/Trusted-AI/AIF360/issues/427","add ""too relaxed to be fair"" method implementation: https://github.com/mlohaus/SearchFair","6","0.7403805496828751","API expansion","Development"
"https://github.com/predictive-analytics-lab/EthicML","830935402","issue","https://github.com/Trusted-AI/AIF360/issues/423","Avoid division by zero in TPR (and similar metrics) TPR is defined like this:

```python
return t_pos / (t_pos + f_neg)
```

If both `t_pos` and `f_neg` are 0, then numpy produces `NaN`. But I'd argue that the result should be 0 instead.

cc @olliethomas @MylesBartlett ","19","0.3689927707233021","Metrics operation","Validation"
"https://github.com/predictive-analytics-lab/EthicML","803453153","issue","https://github.com/Trusted-AI/AIF360/issues/414","Add FairBatch algorithm. As proposed in https://arxiv.org/abs/2012.01696","6","0.3508158508158507","API expansion","Development"
"https://github.com/predictive-analytics-lab/EthicML","757190786","issue","https://github.com/Trusted-AI/AIF360/issues/403","Looking at People Dataset http://chalearnlap.cvc.uab.es/","0","0.1578198088265203","Dataset usage","Requirement Analysis"
"https://github.com/predictive-analytics-lab/EthicML","752170047","issue","https://github.com/Trusted-AI/AIF360/issues/395","Add toxic comments dataset As used in [this post](https://blog.tensorflow.org/2020/11/applying-mindiff-to-improve-model.html)","22","0.5321345321345321","Bias mitigation methodology","Design"
"https://github.com/predictive-analytics-lab/EthicML","711857522","issue","https://github.com/Trusted-AI/AIF360/issues/387","Add a __version__ attribute to the package Everyone has this.","29","0.2562538133007932","Troubleshooting","Maintenance"
"https://github.com/predictive-analytics-lab/EthicML","695649892","issue","https://github.com/Trusted-AI/AIF360/issues/385","Metrics - Mutation Test revealed untested code so we're not completely bombarded, here's results for the first few files of `ethicml.metrics`

## accuracy.py
```diff
---- ethicml/metrics/accuracy.py (6) ----                                                                                               [3/1889]

# mutant 6500
--- ethicml/metrics/accuracy.py
+++ ethicml/metrics/accuracy.py
@@ -21,7 +21,7 @@
         name: str,
         pos_class: Optional[int] = None,
     ):
-        if pos_class is not None:
+        if pos_class is  None:
             super().__init__(pos_class=pos_class)
         else:
             super().__init__()

# mutant 6501
--- ethicml/metrics/accuracy.py
+++ ethicml/metrics/accuracy.py
@@ -25,7 +25,7 @@
             super().__init__(pos_class=pos_class)
         else:
             super().__init__()
-        self._metric = sklearn_metric
+        self._metric = None
         self._name = name

     @implements(Metric)

# mutant 6502
--- ethicml/metrics/accuracy.py
+++ ethicml/metrics/accuracy.py
@@ -26,7 +26,7 @@
         else:
             super().__init__()
         self._metric = sklearn_metric
-        self._name = name
+        self._name = None

     @implements(Metric)
     def score(self, prediction: Prediction, actual: DataTuple) -> float:

# mutant 6503
--- ethicml/metrics/accuracy.py
+++ ethicml/metrics/accuracy.py
@@ -28,7 +28,6 @@
         self._metric = sklearn_metric
         self._name = name

-    @implements(Metric)
     def score(self, prediction: Prediction, actual: DataTuple) -> float:
         return self._metric(actual.y, prediction.hard)


# mutant 6504
--- ethicml/metrics/accuracy.py
+++ ethicml/metrics/accuracy.py
@@ -37,7 +37,7 @@
     """"""Classification accuracy.""""""

     def __init__(self, pos_class: Optional[int] = None):
-        super().__init__(accuracy_score, ""Accuracy"", pos_class=pos_class)
+        super().__init__(accuracy_score, ""XXAccuracyXX"", pos_class=pos_class)


 class F1(SklearnMetric):

# mutant 6505
--- ethicml/metrics/accuracy.py
+++ ethicml/metrics/accuracy.py
@@ -44,5 +44,5 @@
     """"""F1 score: harmonic mean of precision and recall.""""""

     def __init__(self, pos_class: Optional[int] = None):
-        super().__init__(f1_score, ""F1"", pos_class=pos_class)
+        super().__init__(f1_score, ""XXF1XX"", pos_class=pos_class)
```

## anti_spur.py
```diff
---- ethicml/metrics/anti_spur.py (9) ----

# mutant 6506
--- ethicml/metrics/anti_spur.py
+++ ethicml/metrics/anti_spur.py
@@ -14,7 +14,7 @@
     Computes :math:`P(\hat{y}=y|y\neq s)`.
     """"""

-    _name: str = ""anti_spurious""
+    _name: str = ""XXanti_spuriousXX""

     @implements(Metric)
     def score(self, prediction: Prediction, actual: DataTuple) -> float:

# mutant 6507
--- ethicml/metrics/anti_spur.py
+++ ethicml/metrics/anti_spur.py
@@ -14,7 +14,7 @@
     Computes :math:`P(\hat{y}=y|y\neq s)`.
     """"""

-    _name: str = ""anti_spurious""
+    _name: str = None

     @implements(Metric)
     def score(self, prediction: Prediction, actual: DataTuple) -> float:

# mutant 6508
--- ethicml/metrics/anti_spur.py
+++ ethicml/metrics/anti_spur.py
@@ -16,7 +16,6 @@

     _name: str = ""anti_spurious""

-    @implements(Metric)
     def score(self, prediction: Prediction, actual: DataTuple) -> float:
         preds = prediction.hard.to_numpy()[:, np.newaxis]
         sens = actual.s.to_numpy()

# mutant 6509
--- ethicml/metrics/anti_spur.py
+++ ethicml/metrics/anti_spur.py
@@ -18,7 +18,7 @@

     @implements(Metric)
     def score(self, prediction: Prediction, actual: DataTuple) -> float:
-        preds = prediction.hard.to_numpy()[:, np.newaxis]
+        preds = None
         sens = actual.s.to_numpy()
         labels = actual.y.to_numpy()
         s_uneq_y = sens != labels

# mutant 6510
--- ethicml/metrics/anti_spur.py
+++ ethicml/metrics/anti_spur.py
@@ -19,7 +19,7 @@
     @implements(Metric)
     def score(self, prediction: Prediction, actual: DataTuple) -> float:
         preds = prediction.hard.to_numpy()[:, np.newaxis]
-        sens = actual.s.to_numpy()
+        sens = None
         labels = actual.y.to_numpy()
         s_uneq_y = sens != labels
         return (preds[s_uneq_y] == labels[s_uneq_y]).mean()


# mutant 6511
--- ethicml/metrics/anti_spur.py
+++ ethicml/metrics/anti_spur.py
@@ -20,7 +20,7 @@
     def score(self, prediction: Prediction, actual: DataTuple) -> float:
         preds = prediction.hard.to_numpy()[:, np.newaxis]
         sens = actual.s.to_numpy()
-        labels = actual.y.to_numpy()
+        labels = None
         s_uneq_y = sens != labels
         return (preds[s_uneq_y] == labels[s_uneq_y]).mean()


# mutant 6512
--- ethicml/metrics/anti_spur.py
+++ ethicml/metrics/anti_spur.py
@@ -21,6 +21,6 @@
         preds = prediction.hard.to_numpy()[:, np.newaxis]
         sens = actual.s.to_numpy()
         labels = actual.y.to_numpy()
-        s_uneq_y = sens != labels
+        s_uneq_y = sens == labels
         return (preds[s_uneq_y] == labels[s_uneq_y]).mean()


# mutant 6513
--- ethicml/metrics/anti_spur.py
+++ ethicml/metrics/anti_spur.py
@@ -21,6 +21,6 @@
         preds = prediction.hard.to_numpy()[:, np.newaxis]
         sens = actual.s.to_numpy()
         labels = actual.y.to_numpy()
-        s_uneq_y = sens != labels
+        s_uneq_y = None
         return (preds[s_uneq_y] == labels[s_uneq_y]).mean()


# mutant 6514
--- ethicml/metrics/anti_spur.py
+++ ethicml/metrics/anti_spur.py
@@ -22,5 +22,5 @@
         sens = actual.s.to_numpy()
         labels = actual.y.to_numpy()
         s_uneq_y = sens != labels
-        return (preds[s_uneq_y] == labels[s_uneq_y]).mean()
+        return (preds[s_uneq_y] != labels[s_uneq_y]).mean()
```

## balanced_accuracy.py

```diff
---- ethicml/metrics/balanced_accuracy.py (13) ----

# mutant 6169
--- ethicml/metrics/balanced_accuracy.py
+++ ethicml/metrics/balanced_accuracy.py
@@ -10,7 +10,7 @@
 class BalancedAccuracy(Metric):
     """"""Accuracy that is balanced with respect to the class labels.""""""

-    _name: str = ""Balanced Accuracy""
+    _name: str = ""XXBalanced AccuracyXX""

     @implements(Metric)
     def score(self, prediction: Prediction, actual: DataTuple) -> float:

# mutant 6170
--- ethicml/metrics/balanced_accuracy.py
+++ ethicml/metrics/balanced_accuracy.py
@@ -10,7 +10,7 @@
 class BalancedAccuracy(Metric):
     """"""Accuracy that is balanced with respect to the class labels.""""""

-    _name: str = ""Balanced Accuracy""
+    _name: str = None

     @implements(Metric)
     def score(self, prediction: Prediction, actual: DataTuple) -> float:

# mutant 6171
--- ethicml/metrics/balanced_accuracy.py
+++ ethicml/metrics/balanced_accuracy.py
@@ -12,7 +12,6 @@

     _name: str = ""Balanced Accuracy""

-    @implements(Metric)
     def score(self, prediction: Prediction, actual: DataTuple) -> float:
         t_neg, f_pos, f_neg, t_pos = confusion_matrix(prediction, actual, self.positive_class)
         tpr = t_pos / (t_pos + f_neg)

# mutant 6172
--- ethicml/metrics/balanced_accuracy.py
+++ ethicml/metrics/balanced_accuracy.py
@@ -14,7 +14,7 @@

     @implements(Metric)
     def score(self, prediction: Prediction, actual: DataTuple) -> float:
-        t_neg, f_pos, f_neg, t_pos = confusion_matrix(prediction, actual, self.positive_class)
+        t_neg, f_pos, f_neg, t_pos = None
         tpr = t_pos / (t_pos + f_neg)
         tnr = t_neg / (t_neg + f_pos)
         return 0.5 * (tpr + tnr)

# mutant 6173
--- ethicml/metrics/balanced_accuracy.py
+++ ethicml/metrics/balanced_accuracy.py
@@ -15,7 +15,7 @@
     @implements(Metric)
     def score(self, prediction: Prediction, actual: DataTuple) -> float:
         t_neg, f_pos, f_neg, t_pos = confusion_matrix(prediction, actual, self.positive_class)
-        tpr = t_pos / (t_pos + f_neg)
+        tpr = t_pos * (t_pos + f_neg)
         tnr = t_neg / (t_neg + f_pos)
         return 0.5 * (tpr + tnr)

# mutant 6174
--- ethicml/metrics/balanced_accuracy.py
+++ ethicml/metrics/balanced_accuracy.py
@@ -15,7 +15,7 @@
     @implements(Metric)
     def score(self, prediction: Prediction, actual: DataTuple) -> float:
         t_neg, f_pos, f_neg, t_pos = confusion_matrix(prediction, actual, self.positive_class)
-        tpr = t_pos / (t_pos + f_neg)
+        tpr = t_pos / (t_pos - f_neg)
         tnr = t_neg / (t_neg + f_pos)
         return 0.5 * (tpr + tnr)


# mutant 6175
--- ethicml/metrics/balanced_accuracy.py
+++ ethicml/metrics/balanced_accuracy.py
@@ -15,7 +15,7 @@
     @implements(Metric)
     def score(self, prediction: Prediction, actual: DataTuple) -> float:
         t_neg, f_pos, f_neg, t_pos = confusion_matrix(prediction, actual, self.positive_class)
-        tpr = t_pos / (t_pos + f_neg)
+        tpr = None
         tnr = t_neg / (t_neg + f_pos)
         return 0.5 * (tpr + tnr)


# mutant 6176
--- ethicml/metrics/balanced_accuracy.py
+++ ethicml/metrics/balanced_accuracy.py
@@ -16,6 +16,6 @@
     def score(self, prediction: Prediction, actual: DataTuple) -> float:
         t_neg, f_pos, f_neg, t_pos = confusion_matrix(prediction, actual, self.positive_class)
         tpr = t_pos / (t_pos + f_neg)
-        tnr = t_neg / (t_neg + f_pos)
+        tnr = t_neg * (t_neg + f_pos)
         return 0.5 * (tpr + tnr)


# mutant 6177
--- ethicml/metrics/balanced_accuracy.py
+++ ethicml/metrics/balanced_accuracy.py
@@ -16,6 +16,6 @@
     def score(self, prediction: Prediction, actual: DataTuple) -> float:
         t_neg, f_pos, f_neg, t_pos = confusion_matrix(prediction, actual, self.positive_class)
         tpr = t_pos / (t_pos + f_neg)
-        tnr = t_neg / (t_neg + f_pos)
+        tnr = t_neg / (t_neg - f_pos)
         return 0.5 * (tpr + tnr)


# mutant 6178
--- ethicml/metrics/balanced_accuracy.py
+++ ethicml/metrics/balanced_accuracy.py
@@ -16,6 +16,6 @@
     def score(self, prediction: Prediction, actual: DataTuple) -> float:
         t_neg, f_pos, f_neg, t_pos = confusion_matrix(prediction, actual, self.positive_class)
         tpr = t_pos / (t_pos + f_neg)
-        tnr = t_neg / (t_neg + f_pos)
+        tnr = None
         return 0.5 * (tpr + tnr)


# mutant 6179
--- ethicml/metrics/balanced_accuracy.py
+++ ethicml/metrics/balanced_accuracy.py
@@ -17,5 +17,5 @@
         t_neg, f_pos, f_neg, t_pos = confusion_matrix(prediction, actual, self.positive_class)
         tpr = t_pos / (t_pos + f_neg)
         tnr = t_neg / (t_neg + f_pos)
-        return 0.5 * (tpr + tnr)
+        return 1.5 * (tpr + tnr)

# mutant 6180
--- ethicml/metrics/balanced_accuracy.py
+++ ethicml/metrics/balanced_accuracy.py
@@ -17,5 +17,5 @@
         t_neg, f_pos, f_neg, t_pos = confusion_matrix(prediction, actual, self.positive_class)
         tpr = t_pos / (t_pos + f_neg)
         tnr = t_neg / (t_neg + f_pos)
-        return 0.5 * (tpr + tnr)
+        return 0.5 / (tpr + tnr)


# mutant 6181
--- ethicml/metrics/balanced_accuracy.py
+++ ethicml/metrics/balanced_accuracy.py
@@ -17,5 +17,5 @@
         t_neg, f_pos, f_neg, t_pos = confusion_matrix(prediction, actual, self.positive_class)
         tpr = t_pos / (t_pos + f_neg)
         tnr = t_neg / (t_neg + f_pos)
-        return 0.5 * (tpr + tnr)
+        return 0.5 * (tpr - tnr)
```

## bcr.py

```diff
---- ethicml/metrics/bcr.py (10) ----

# mutant 6182
--- ethicml/metrics/bcr.py
+++ ethicml/metrics/bcr.py
@@ -11,7 +11,7 @@
 class BCR(Metric):
     """"""Balanced Classification Rate.""""""

-    _name: str = ""BCR""
+    _name: str = ""XXBCRXX""

     @implements(Metric)
     def score(self, prediction: Prediction, actual: DataTuple) -> float:

# mutant 6183
--- ethicml/metrics/bcr.py
+++ ethicml/metrics/bcr.py
@@ -11,7 +11,7 @@
 class BCR(Metric):
     """"""Balanced Classification Rate.""""""

-    _name: str = ""BCR""
+    _name: str = None

     @implements(Metric)
     def score(self, prediction: Prediction, actual: DataTuple) -> float:

# mutant 6184
--- ethicml/metrics/bcr.py
+++ ethicml/metrics/bcr.py
@@ -13,7 +13,6 @@

     _name: str = ""BCR""

-    @implements(Metric)
     def score(self, prediction: Prediction, actual: DataTuple) -> float:
         tpr_metric = TPR()
         tpr = tpr_metric.score(prediction, actual)

# mutant 6185
--- ethicml/metrics/bcr.py
+++ ethicml/metrics/bcr.py
@@ -15,7 +15,7 @@

     @implements(Metric)
     def score(self, prediction: Prediction, actual: DataTuple) -> float:
-        tpr_metric = TPR()
+        tpr_metric = None
         tpr = tpr_metric.score(prediction, actual)

         tnr_metric = TNR()

# mutant 6186
--- ethicml/metrics/bcr.py
+++ ethicml/metrics/bcr.py
@@ -16,7 +16,7 @@
     @implements(Metric)
     def score(self, prediction: Prediction, actual: DataTuple) -> float:
         tpr_metric = TPR()
-        tpr = tpr_metric.score(prediction, actual)
+        tpr = None

         tnr_metric = TNR()
         tnr = tnr_metric.score(prediction, actual)

# mutant 6187
--- ethicml/metrics/bcr.py
+++ ethicml/metrics/bcr.py
@@ -18,7 +18,7 @@
         tpr_metric = TPR()
         tpr = tpr_metric.score(prediction, actual)

-        tnr_metric = TNR()
+        tnr_metric = None
         tnr = tnr_metric.score(prediction, actual)

         return (tpr + tnr) / 2

# mutant 6188
--- ethicml/metrics/bcr.py
+++ ethicml/metrics/bcr.py
@@ -19,7 +19,7 @@
         tpr = tpr_metric.score(prediction, actual)

         tnr_metric = TNR()
-        tnr = tnr_metric.score(prediction, actual)
+        tnr = None

         return (tpr + tnr) / 2


# mutant 6189
--- ethicml/metrics/bcr.py
+++ ethicml/metrics/bcr.py
@@ -21,5 +21,5 @@
         tnr_metric = TNR()
         tnr = tnr_metric.score(prediction, actual)

-        return (tpr + tnr) / 2
+        return (tpr - tnr) / 2


# mutant 6190
--- ethicml/metrics/bcr.py
+++ ethicml/metrics/bcr.py
@@ -21,5 +21,5 @@
         tnr_metric = TNR()
         tnr = tnr_metric.score(prediction, actual)

-        return (tpr + tnr) / 2
+        return (tpr + tnr) * 2


# mutant 6191
--- ethicml/metrics/bcr.py
+++ ethicml/metrics/bcr.py
@@ -21,5 +21,5 @@
         tnr_metric = TNR()
         tnr = tnr_metric.score(prediction, actual)

-        return (tpr + tnr) / 2
+        return (tpr + tnr) / 3
```

## confusion_matrix.py

```diff
---- ethicml/metrics/confusion_matrix.py (23) ----

# mutant 6347
--- ethicml/metrics/confusion_matrix.py
+++ ethicml/metrics/confusion_matrix.py
@@ -16,7 +16,7 @@
     prediction: Prediction, actual: DataTuple, pos_cls: int
 ) -> Tuple[int, int, int, int]:
     """"""Apply sci-kit learn's confusion matrix.""""""
-    actual_y: np.ndarray[np.int32] = actual.y.to_numpy(dtype=np.int32)
+    actual_y: np.ndarray[np.int32] = None
     labels: np.ndarray[np.int32] = np.unique(actual_y)
     if labels.size == 1:
         labels = np.array([0, 1], dtype=np.int32)

# mutant 6348
--- ethicml/metrics/confusion_matrix.py
+++ ethicml/metrics/confusion_matrix.py
@@ -17,7 +17,7 @@
 ) -> Tuple[int, int, int, int]:
     """"""Apply sci-kit learn's confusion matrix.""""""
     actual_y: np.ndarray[np.int32] = actual.y.to_numpy(dtype=np.int32)
-    labels: np.ndarray[np.int32] = np.unique(actual_y)
+    labels: np.ndarray[np.int32] = None
     if labels.size == 1:
         labels = np.array([0, 1], dtype=np.int32)
     conf_matr: np.ndarray = conf_mtx(y_true=actual_y, y_pred=prediction.hard, labels=labels)

# mutant 6349
--- ethicml/metrics/confusion_matrix.py
+++ ethicml/metrics/confusion_matrix.py
@@ -18,7 +18,7 @@
     """"""Apply sci-kit learn's confusion matrix.""""""
     actual_y: np.ndarray[np.int32] = actual.y.to_numpy(dtype=np.int32)
     labels: np.ndarray[np.int32] = np.unique(actual_y)
-    if labels.size == 1:
+    if labels.size != 1:
         labels = np.array([0, 1], dtype=np.int32)
     conf_matr: np.ndarray = conf_mtx(y_true=actual_y, y_pred=prediction.hard, labels=labels)


# mutant 6350
--- ethicml/metrics/confusion_matrix.py
+++ ethicml/metrics/confusion_matrix.py
@@ -18,7 +18,7 @@
     """"""Apply sci-kit learn's confusion matrix.""""""
     actual_y: np.ndarray[np.int32] = actual.y.to_numpy(dtype=np.int32)
     labels: np.ndarray[np.int32] = np.unique(actual_y)
-    if labels.size == 1:
+    if labels.size == 2:
         labels = np.array([0, 1], dtype=np.int32)
     conf_matr: np.ndarray = conf_mtx(y_true=actual_y, y_pred=prediction.hard, labels=labels)


# mutant 6351
--- ethicml/metrics/confusion_matrix.py
+++ ethicml/metrics/confusion_matrix.py
@@ -19,7 +19,7 @@
     actual_y: np.ndarray[np.int32] = actual.y.to_numpy(dtype=np.int32)
     labels: np.ndarray[np.int32] = np.unique(actual_y)
     if labels.size == 1:
-        labels = np.array([0, 1], dtype=np.int32)
+        labels = np.array([1, 1], dtype=np.int32)
     conf_matr: np.ndarray = conf_mtx(y_true=actual_y, y_pred=prediction.hard, labels=labels)

     if pos_cls not in labels:

# mutant 6352
--- ethicml/metrics/confusion_matrix.py
+++ ethicml/metrics/confusion_matrix.py
@@ -19,7 +19,7 @@
     actual_y: np.ndarray[np.int32] = actual.y.to_numpy(dtype=np.int32)
     labels: np.ndarray[np.int32] = np.unique(actual_y)
     if labels.size == 1:
-        labels = np.array([0, 1], dtype=np.int32)
+        labels = np.array([0, 2], dtype=np.int32)
     conf_matr: np.ndarray = conf_mtx(y_true=actual_y, y_pred=prediction.hard, labels=labels)

     if pos_cls not in labels:

# mutant 6353
--- ethicml/metrics/confusion_matrix.py
+++ ethicml/metrics/confusion_matrix.py
@@ -19,7 +19,7 @@
     actual_y: np.ndarray[np.int32] = actual.y.to_numpy(dtype=np.int32)
     labels: np.ndarray[np.int32] = np.unique(actual_y)
     if labels.size == 1:
-        labels = np.array([0, 1], dtype=np.int32)
+        labels = None
     conf_matr: np.ndarray = conf_mtx(y_true=actual_y, y_pred=prediction.hard, labels=labels)

     if pos_cls not in labels:

# mutant 6354
--- ethicml/metrics/confusion_matrix.py
+++ ethicml/metrics/confusion_matrix.py
@@ -20,7 +20,7 @@
     labels: np.ndarray[np.int32] = np.unique(actual_y)
     if labels.size == 1:
         labels = np.array([0, 1], dtype=np.int32)
-    conf_matr: np.ndarray = conf_mtx(y_true=actual_y, y_pred=prediction.hard, labels=labels)
+    conf_matr: np.ndarray = None

     if pos_cls not in labels:
         raise LabelOutOfBounds(""Positive class specified must exist in the test set"")

# mutant 6355
--- ethicml/metrics/confusion_matrix.py
+++ ethicml/metrics/confusion_matrix.py
@@ -22,7 +22,7 @@
         labels = np.array([0, 1], dtype=np.int32)
     conf_matr: np.ndarray = conf_mtx(y_true=actual_y, y_pred=prediction.hard, labels=labels)

-    if pos_cls not in labels:
+    if pos_cls  in labels:
         raise LabelOutOfBounds(""Positive class specified must exist in the test set"")

     tp_idx: np.int64 = (labels == pos_cls).nonzero()[0].item()

# mutant 6356
--- ethicml/metrics/confusion_matrix.py
+++ ethicml/metrics/confusion_matrix.py
@@ -23,7 +23,7 @@
     conf_matr: np.ndarray = conf_mtx(y_true=actual_y, y_pred=prediction.hard, labels=labels)

     if pos_cls not in labels:
-        raise LabelOutOfBounds(""Positive class specified must exist in the test set"")
+        raise LabelOutOfBounds(""XXPositive class specified must exist in the test setXX"")

     tp_idx: np.int64 = (labels == pos_cls).nonzero()[0].item()
     tp_idx = int(tp_idx)

# mutant 6357
--- ethicml/metrics/confusion_matrix.py
+++ ethicml/metrics/confusion_matrix.py
@@ -25,7 +25,7 @@
     if pos_cls not in labels:
         raise LabelOutOfBounds(""Positive class specified must exist in the test set"")

-    tp_idx: np.int64 = (labels == pos_cls).nonzero()[0].item()
+    tp_idx: np.int64 = (labels != pos_cls).nonzero()[0].item()
     tp_idx = int(tp_idx)

     true_pos = conf_matr[tp_idx, tp_idx]

# mutant 6358
--- ethicml/metrics/confusion_matrix.py
+++ ethicml/metrics/confusion_matrix.py
@@ -25,7 +25,7 @@
     if pos_cls not in labels:
         raise LabelOutOfBounds(""Positive class specified must exist in the test set"")

-    tp_idx: np.int64 = (labels == pos_cls).nonzero()[0].item()
+    tp_idx: np.int64 = (labels == pos_cls).nonzero()[1].item()
     tp_idx = int(tp_idx)

     true_pos = conf_matr[tp_idx, tp_idx]

# mutant 6359
--- ethicml/metrics/confusion_matrix.py
+++ ethicml/metrics/confusion_matrix.py
@@ -25,7 +25,7 @@
     if pos_cls not in labels:
         raise LabelOutOfBounds(""Positive class specified must exist in the test set"")

-    tp_idx: np.int64 = (labels == pos_cls).nonzero()[0].item()
+    tp_idx: np.int64 = None
     tp_idx = int(tp_idx)

     true_pos = conf_matr[tp_idx, tp_idx]

# mutant 6360
--- ethicml/metrics/confusion_matrix.py
+++ ethicml/metrics/confusion_matrix.py
@@ -26,7 +26,7 @@
         raise LabelOutOfBounds(""Positive class specified must exist in the test set"")

     tp_idx: np.int64 = (labels == pos_cls).nonzero()[0].item()
-    tp_idx = int(tp_idx)
+    tp_idx = None

     true_pos = conf_matr[tp_idx, tp_idx]
     false_pos = conf_matr[:, tp_idx].sum() - true_pos

# mutant 6361
--- ethicml/metrics/confusion_matrix.py
+++ ethicml/metrics/confusion_matrix.py
@@ -28,7 +28,7 @@
     tp_idx: np.int64 = (labels == pos_cls).nonzero()[0].item()
     tp_idx = int(tp_idx)

-    true_pos = conf_matr[tp_idx, tp_idx]
+    true_pos = None
     false_pos = conf_matr[:, tp_idx].sum() - true_pos
     false_neg = conf_matr[tp_idx, :].sum() - true_pos
     true_neg = conf_matr.sum() - true_pos - false_pos - false_neg

# mutant 6362
--- ethicml/metrics/confusion_matrix.py
+++ ethicml/metrics/confusion_matrix.py
@@ -29,7 +29,7 @@
     tp_idx = int(tp_idx)

     true_pos = conf_matr[tp_idx, tp_idx]
-    false_pos = conf_matr[:, tp_idx].sum() - true_pos
+    false_pos = conf_matr[:, tp_idx].sum() + true_pos
     false_neg = conf_matr[tp_idx, :].sum() - true_pos
     true_neg = conf_matr.sum() - true_pos - false_pos - false_neg
     return true_neg, false_pos, false_neg, true_pos

# mutant 6363
--- ethicml/metrics/confusion_matrix.py
+++ ethicml/metrics/confusion_matrix.py
@@ -29,7 +29,7 @@
     tp_idx = int(tp_idx)

     true_pos = conf_matr[tp_idx, tp_idx]
-    false_pos = conf_matr[:, tp_idx].sum() - true_pos
+    false_pos = None
     false_neg = conf_matr[tp_idx, :].sum() - true_pos
     true_neg = conf_matr.sum() - true_pos - false_pos - false_neg
     return true_neg, false_pos, false_neg, true_pos

# mutant 6364
--- ethicml/metrics/confusion_matrix.py
+++ ethicml/metrics/confusion_matrix.py
@@ -30,7 +30,7 @@

     true_pos = conf_matr[tp_idx, tp_idx]
     false_pos = conf_matr[:, tp_idx].sum() - true_pos
-    false_neg = conf_matr[tp_idx, :].sum() - true_pos
+    false_neg = conf_matr[tp_idx, :].sum() + true_pos
     true_neg = conf_matr.sum() - true_pos - false_pos - false_neg
     return true_neg, false_pos, false_neg, true_pos


# mutant 6365
--- ethicml/metrics/confusion_matrix.py
+++ ethicml/metrics/confusion_matrix.py
@@ -30,7 +30,7 @@

     true_pos = conf_matr[tp_idx, tp_idx]
     false_pos = conf_matr[:, tp_idx].sum() - true_pos
-    false_neg = conf_matr[tp_idx, :].sum() - true_pos
+    false_neg = None
     true_neg = conf_matr.sum() - true_pos - false_pos - false_neg
     return true_neg, false_pos, false_neg, true_pos


# mutant 6366
--- ethicml/metrics/confusion_matrix.py
+++ ethicml/metrics/confusion_matrix.py
@@ -31,6 +31,6 @@
     true_pos = conf_matr[tp_idx, tp_idx]
     false_pos = conf_matr[:, tp_idx].sum() - true_pos
     false_neg = conf_matr[tp_idx, :].sum() - true_pos
-    true_neg = conf_matr.sum() - true_pos - false_pos - false_neg
+    true_neg = conf_matr.sum() + true_pos - false_pos - false_neg
     return true_neg, false_pos, false_neg, true_pos

# mutant 6367
--- ethicml/metrics/confusion_matrix.py
+++ ethicml/metrics/confusion_matrix.py
@@ -31,6 +31,6 @@
     true_pos = conf_matr[tp_idx, tp_idx]
     false_pos = conf_matr[:, tp_idx].sum() - true_pos
     false_neg = conf_matr[tp_idx, :].sum() - true_pos
-    true_neg = conf_matr.sum() - true_pos - false_pos - false_neg
+    true_neg = conf_matr.sum() - true_pos + false_pos - false_neg
     return true_neg, false_pos, false_neg, true_pos


# mutant 6368
--- ethicml/metrics/confusion_matrix.py
+++ ethicml/metrics/confusion_matrix.py
@@ -31,6 +31,6 @@
     true_pos = conf_matr[tp_idx, tp_idx]
     false_pos = conf_matr[:, tp_idx].sum() - true_pos
     false_neg = conf_matr[tp_idx, :].sum() - true_pos
-    true_neg = conf_matr.sum() - true_pos - false_pos - false_neg
+    true_neg = conf_matr.sum() - true_pos - false_pos + false_neg
     return true_neg, false_pos, false_neg, true_pos


# mutant 6369
--- ethicml/metrics/confusion_matrix.py
+++ ethicml/metrics/confusion_matrix.py
@@ -31,6 +31,6 @@
     true_pos = conf_matr[tp_idx, tp_idx]
     false_pos = conf_matr[:, tp_idx].sum() - true_pos
     false_neg = conf_matr[tp_idx, :].sum() - true_pos
-    true_neg = conf_matr.sum() - true_pos - false_pos - false_neg
+    true_neg = None
     return true_neg, false_pos, false_neg, true_pos
```

cv.py

```diff
---- ethicml/metrics/cv.py (18) ----

# mutant 6376
--- ethicml/metrics/cv.py
+++ ethicml/metrics/cv.py
@@ -10,7 +10,7 @@
 class CV(Metric):
     """"""Calder-Verwer.""""""

-    _name: str = ""CV""
+    _name: str = ""XXCVXX""

     @implements(Metric)
     def score(self, prediction: Prediction, actual: DataTuple) -> float:

# mutant 6377
--- ethicml/metrics/cv.py
+++ ethicml/metrics/cv.py
@@ -10,7 +10,7 @@
 class CV(Metric):
     """"""Calder-Verwer.""""""

-    _name: str = ""CV""
+    _name: str = None

     @implements(Metric)
     def score(self, prediction: Prediction, actual: DataTuple) -> float:

# mutant 6378
--- ethicml/metrics/cv.py
+++ ethicml/metrics/cv.py
@@ -12,7 +12,6 @@

     _name: str = ""CV""

-    @implements(Metric)
     def score(self, prediction: Prediction, actual: DataTuple) -> float:
         # has to be imported on demand because otherwise we get circular imports
         from ethicml.evaluators.per_sensitive_attribute import (

# mutant 6379
--- ethicml/metrics/cv.py
+++ ethicml/metrics/cv.py
@@ -20,7 +20,7 @@
             diff_per_sensitive_attribute,
         )

-        per_sens = metric_per_sensitive_attribute(prediction, actual, ProbPos())
+        per_sens = None
         diffs = diff_per_sensitive_attribute(per_sens)

         return 1 - list(diffs.values())[0]

# mutant 6380
--- ethicml/metrics/cv.py
+++ ethicml/metrics/cv.py
@@ -21,7 +21,7 @@
         )

         per_sens = metric_per_sensitive_attribute(prediction, actual, ProbPos())
-        diffs = diff_per_sensitive_attribute(per_sens)
+        diffs = None

         return 1 - list(diffs.values())[0]

# mutant 6381
--- ethicml/metrics/cv.py
+++ ethicml/metrics/cv.py
@@ -23,7 +23,7 @@
         per_sens = metric_per_sensitive_attribute(prediction, actual, ProbPos())
         diffs = diff_per_sensitive_attribute(per_sens)

-        return 1 - list(diffs.values())[0]
+        return 2 - list(diffs.values())[0]

     @property
     def apply_per_sensitive(self) -> bool:

# mutant 6382
--- ethicml/metrics/cv.py
+++ ethicml/metrics/cv.py
@@ -23,7 +23,7 @@
         per_sens = metric_per_sensitive_attribute(prediction, actual, ProbPos())
         diffs = diff_per_sensitive_attribute(per_sens)

-        return 1 - list(diffs.values())[0]
+        return 1 + list(diffs.values())[0]

     @property
     def apply_per_sensitive(self) -> bool:

# mutant 6383
--- ethicml/metrics/cv.py
+++ ethicml/metrics/cv.py
@@ -23,7 +23,7 @@
         per_sens = metric_per_sensitive_attribute(prediction, actual, ProbPos())
         diffs = diff_per_sensitive_attribute(per_sens)

-        return 1 - list(diffs.values())[0]
+        return 1 - list(diffs.values())[1]

     @property
     def apply_per_sensitive(self) -> bool:

# mutant 6384
--- ethicml/metrics/cv.py
+++ ethicml/metrics/cv.py
@@ -25,7 +25,6 @@

         return 1 - list(diffs.values())[0]

-    @property
     def apply_per_sensitive(self) -> bool:
         """"""Can this metric be applied per sensitive attribute group?""""""
         return False

# mutant 6385
--- ethicml/metrics/cv.py
+++ ethicml/metrics/cv.py
@@ -28,7 +28,7 @@
     @property
     def apply_per_sensitive(self) -> bool:
         """"""Can this metric be applied per sensitive attribute group?""""""
-        return False
+        return True

 class AbsCV(CV):

# mutant 6386
--- ethicml/metrics/cv.py
+++ ethicml/metrics/cv.py
@@ -37,7 +37,7 @@
     This metric is supposed to make it easier to compare results.
     """"""

-    _name: str = ""CV absolute""
+    _name: str = ""XXCV absoluteXX""

     @implements(Metric)
     def score(self, prediction: Prediction, actual: DataTuple) -> float:

# mutant 6387
--- ethicml/metrics/cv.py
+++ ethicml/metrics/cv.py
@@ -37,7 +37,7 @@
     This metric is supposed to make it easier to compare results.
     """"""

-    _name: str = ""CV absolute""
+    _name: str = None

     @implements(Metric)
     def score(self, prediction: Prediction, actual: DataTuple) -> float:

# mutant 6388
--- ethicml/metrics/cv.py
+++ ethicml/metrics/cv.py
@@ -39,7 +39,6 @@

     _name: str = ""CV absolute""

-    @implements(Metric)
     def score(self, prediction: Prediction, actual: DataTuple) -> float:
         cv_score = super().score(prediction, actual)
         # the following is equivalent to 1 - abs(diff)

# mutant 6389
--- ethicml/metrics/cv.py
+++ ethicml/metrics/cv.py
@@ -41,7 +41,7 @@

     @implements(Metric)
     def score(self, prediction: Prediction, actual: DataTuple) -> float:
-        cv_score = super().score(prediction, actual)
+        cv_score = None
         # the following is equivalent to 1 - abs(diff)
         if cv_score > 1:
             return 2 - cv_score

# mutant 6390
--- ethicml/metrics/cv.py
+++ ethicml/metrics/cv.py
@@ -43,7 +43,7 @@
     def score(self, prediction: Prediction, actual: DataTuple) -> float:
         cv_score = super().score(prediction, actual)
         # the following is equivalent to 1 - abs(diff)
-        if cv_score > 1:
+        if cv_score >= 1:
             return 2 - cv_score
         return cv_score

# mutant 6391
--- ethicml/metrics/cv.py
+++ ethicml/metrics/cv.py
@@ -43,7 +43,7 @@
     def score(self, prediction: Prediction, actual: DataTuple) -> float:
         cv_score = super().score(prediction, actual)
         # the following is equivalent to 1 - abs(diff)
-        if cv_score > 1:
+        if cv_score > 2:
             return 2 - cv_score
         return cv_score


# mutant 6392
--- ethicml/metrics/cv.py
+++ ethicml/metrics/cv.py
@@ -44,6 +44,6 @@
         cv_score = super().score(prediction, actual)
         # the following is equivalent to 1 - abs(diff)
         if cv_score > 1:
-            return 2 - cv_score
+            return 3 - cv_score
         return cv_score


# mutant 6393
--- ethicml/metrics/cv.py
+++ ethicml/metrics/cv.py
@@ -44,6 +44,6 @@
         cv_score = super().score(prediction, actual)
         # the following is equivalent to 1 - abs(diff)
         if cv_score > 1:
-            return 2 - cv_score
+            return 2 + cv_score
         return cv_score
```","19","0.9959158180067594","Metrics operation","Validation"
"https://github.com/predictive-analytics-lab/EthicML","687065814","issue","https://github.com/Trusted-AI/AIF360/issues/380","Add EthicML to Conda it would maybe be useful if a user could install EthicML via conda as well as pip","21","0.7335997335997335","Installation and shell commands","Deployment"
"https://github.com/predictive-analytics-lab/EthicML","686612209","issue","https://github.com/Trusted-AI/AIF360/issues/379","Store data tuple internally as a single pd.DataFrame My dream is to store the datatuple internally as a single dataframe and the property functions like `data.s` do something like this:
```python
return self._internal_df[self.s_name]
```
i.e., the datatuple class returns columns from this dataframe on demand.

Storing everything as a single dataframe would save a lot of hassle. At the moment we have to constantly join and split the components.

Pseudo code:
```python
@dataclass
class DataTuple:
    all_data: pd.DataFrame
    x_columns: List[str]
    s_column: str
    y_column: str
		
    @property
    def x(self) -> pd.DataFrame:
        return self.all_data[self.x_columns]
		
    @property
    def s(self) -> pd.Series:
        return self.all_data[self.s_column]
	
    @property
    def y(self) -> pd.Series:
        return self.all_data[self.y_column]
```","17","0.7161924299408629","Troubleshooting","Maintenance"
"https://github.com/predictive-analytics-lab/EthicML","684766599","issue","https://github.com/Trusted-AI/AIF360/issues/371","Add GitHub Action which rebuilds documentation on every push to `main` as was done in https://github.com/predictive-analytics-lab/ranzen","32","0.6708724253208236","Dependency and Release","Deployment"
"https://github.com/predictive-analytics-lab/EthicML","681737329","issue","https://github.com/Trusted-AI/AIF360/issues/362","Change how synthetic 1 data is generated from 
```
             +----------------+
             |                |
             |                v
    +-----+  |   +-----+    +---+
    | X_1 |--+-->| X_2 | -->| Y |
    +-----+      +-----+    +---+

                            +---+
                            | S |
                            +---+
```

to this

```
+-----+     +-----+
|     |     |     |
| Y_1 +<----+ X_1 +--+
|     |     |     |  |   +-----+   +-----+
+-----+     +-----+  |   |     |   |     |
                     +-->+ Y_3 |   |  S  |
+-----+     +-----+  |   |     |   |     |
|     |     |     |  |   +-----+   +-----+
| Y_2 +<----+ X_2 +--+
|     |     |     |
+-----+     +-----+
```

Similar, but now we have 3 potential labels, so we can demonstrate transfer learning","14","0.2896690245393414","Documentation","Development"
"https://github.com/predictive-analytics-lab/EthicML","679167102","issue","https://github.com/Trusted-AI/AIF360/issues/354","Should be able to use tabular dataloading without torch at the moment if you have a new environment and do `pip install EthicML`, then `from ethicml.data import adult` an error will be thrown saying torch isn't installed.

If I only want to train an sklearn LR model, then it seems a bit overkill to require torch.","29","0.3610590440487346","Troubleshooting","Maintenance"
"https://github.com/predictive-analytics-lab/EthicML","678539449","issue","https://github.com/Trusted-AI/AIF360/issues/353","Proposal: Multiple S ### Abstract
We currently only really support one group of binary S values. FairML literature has a large gap in terms of analysis of a more real-world setup: multiple groups of S. This proposal is for multiple S groups, which in turn are exclusive, to be supported.

For example, currently we have `{gender=0, gender=1}`. This has recently been extended to include more than one sensitive group, `{race0-gender0=0, race1-gender0=1, race2-gender0=2, race0-gender1=3, race1-gender1=4, race2-gender1=5}`. 

This proposal is to formalise this process a little.

### Motivation 
Fairness and Equality laws around the world consider multiple sensitive groups. For example you should be fair with regard to race, gender AND religion (amongst others). But how to best represent this? 

### Specification 

The recent approach is to combine all potential groups giving N sensitive values. During training we can potentially OHE the sensitive value, and during evaluation we can then treat the result as a (1v1/1vA) binary problem. Throughout we can use a lookup table to map back from 3 -> race=1, gender=1. This lookup table would exist as part of the created Dataset object.

#### Lookup Table

The lookup table should map between 2 tables. One which is held in DataTupleInstance.s, and another, more readable one

_DataTupleInstance.s_
| Index | S |
|:-----:|:-:|
|     0 | 0 |
|     1 | 4 |
|     2 | 3 |
|     3 | 1 |
|     4 | 3 |
|     5 | 5 |
|     6 | 2 |
| ...| ...|

There should be a mapping between these 2 tables

_readable_
|       | Gender  |         | Race  |       | |
|-------|---------|---------|-------|-------|-----|
| Index | Gender0 | Gender1 | Race0 | Race1 | Race2 |
|     0 |    1    | 0       | 1     | 0     | 0 |
|     1 |    0    | 1       | 0     | 1     | 0 |
|     2 |    0    | 1       | 1     | 0     | 0 |
|     3 |    1    | 0       | 0     | 1     | 0 |
|     4 |    0    | 1       | 1     | 0     | 0 |
|     5 |    0    | 1       | 0     | 0     | 1 |
|     6 |    1    | 0       | 0     | 0     | 1 |
|     ... |   ...    | ...       | ...     | ...     | ... |

Why multi-hierarchy? This will make it easier to look at just Gender, or just Race

#### Evaluation Metrics
If you're woking on intersectional data groups, then we will need this mapping to produce metrics that are readable. The `run_metrics` helper should optionally accept a Dataset object. If it receives this, then the data should be unpacked prior to reporting the values. If a dataset is not provided, then the evaluations should not try to map from a number `15` to a text description `sex=value0,gender=value0,nationality=value15`.

### Rejected Ideas 

Not collapsing the intersectional groups and leaving just the hierarchical 

### Outstanding Problems

How to extend to continuous S? - We don't have any datasets where this is the case, so not a primary concern.

What if, as a user, I don't want to create a lookup mechanism? I just want to get some metrics for the data I put in? - These lookup tables will be provided for all datasets in EthicML. Unless you're loading in your own data directly into a DataTuple, in which case if no Dataset is provided to one of the evaluation helpers, then the values are assumed to belong to one S group.

What if I have an S group which has no value, or a missing value? This will need to be remedied prior to using EthicML. We assume that each sensitive group has exactly one value per row.","30","0.2904990121281796","Troubleshooting","Maintenance"
"https://github.com/predictive-analytics-lab/EthicML","659524631","issue","https://github.com/Trusted-AI/AIF360/issues/347","change package name on pip `EthicML` is a strange naming strategy. `ethicml` would be more consistent with other packages","21","0.4620942521298391","Installation and shell commands","Deployment"
"https://github.com/predictive-analytics-lab/EthicML","641271420","issue","https://github.com/Trusted-AI/AIF360/issues/346","Add MEPS datatset https://meps.ahrq.gov/mepsweb/","30","0.4531218222493391","Troubleshooting","Maintenance"
"https://github.com/predictive-analytics-lab/EthicML","639708997","issue","https://github.com/Trusted-AI/AIF360/issues/344","run_metrics should return a df instead of a dict There's already a suggestion in teh code for this, just making an issue for it

https://github.com/predictive-analytics-lab/EthicML/blob/master/ethicml/evaluators/evaluate_models.py#L87

One potential problem with dicts is that really the values should be sorted when for example writing to file, which is really the most likely thing to do with some metrics. ","15","0.3651134685108096","Metrics operation","Validation"
"https://github.com/predictive-analytics-lab/EthicML","635356718","issue","https://github.com/Trusted-AI/AIF360/issues/338","Add Random Classifier This will be a bit similar to the Majority classifier, but instead of just returning the majority label, it draws from a Categorical distribution with uniform probability of each label in the training set.","23","0.4974310579846912","Bias mitigation methodology","Design"
"https://github.com/predictive-analytics-lab/EthicML","634654482","issue","https://github.com/Trusted-AI/AIF360/issues/336","SVM Kernel name Clearly [this](https://github.com/predictive-analytics-lab/EthicML/blob/f7fcf435b5807ef9931f3ff3b259fc7cc4b38da8/ethicml/algorithms/inprocess/svm.py#L20) is not right ","30","0.2070368110636567","Troubleshooting","Maintenance"
"https://github.com/predictive-analytics-lab/EthicML","612448400","issue","https://github.com/Trusted-AI/AIF360/issues/328","Movielens dataset This paper [https://arxiv.org/pdf/1906.10673.pdf](https://arxiv.org/pdf/1906.10673.pdf) uses this dataset [https://grouplens.org/datasets/movielens/100k/](https://grouplens.org/datasets/movielens/100k/) using the gender of the reviewer as a sensitive attribute and predicting the review score as the task","30","0.47561553030303","Troubleshooting","Maintenance"
"https://github.com/predictive-analytics-lab/EthicML","612447948","issue","https://github.com/Trusted-AI/AIF360/issues/327","More options with dataset creation This paper [https://arxiv.org/pdf/1905.12728.pdf](https://arxiv.org/pdf/1905.12728.pdf) landed on arxiv this week. Their main point is that unfair behaviour can occur during data cleaning, in particular how we deal with missing values. They explicitly talk about how every fairness framework out there (they don't mention EthicML) doesn't take this into account.... but we kind of can.

This user story is to put a bit more oomph around it. Maybe we should have the strategies that they suggest as part of the dataset class, i.e.
`Adult(missing_data='drop_row')`, `Adult(missing_data='drop_column')` or `Adult(missing_data='something_else')`, with `drop_row` being the default (as that's what happens already - I think)","30","0.3525400476132833","Troubleshooting","Maintenance"
"https://github.com/predictive-analytics-lab/EthicML","612447202","issue","https://github.com/Trusted-AI/AIF360/issues/326","Report function The Aequitas tool is really cool. It would be great to have something similar","25","0.3018084066471164","Research","Requirement Analysis"
"https://github.com/predictive-analytics-lab/EthicML","612446691","issue","https://github.com/Trusted-AI/AIF360/issues/325","Ability to skew dataset with label bias (label flipping) This one would involve flipping some labels","22","0.7276714513556618","Bias mitigation methodology","Design"
"https://github.com/predictive-analytics-lab/EthicML","612446222","issue","https://github.com/Trusted-AI/AIF360/issues/324","Ability to skew dataset with sample bias I think this has mostly been done as one of the splitting functions, but I'd need to double check","23","0.2259170653907497","Bias mitigation methodology","Design"
"https://github.com/predictive-analytics-lab/EthicML","612445188","issue","https://github.com/Trusted-AI/AIF360/issues/323","Make a webapp interface So this is a little abstract at the moment, but conceptually this is what I'm thinking.... Novi wants something he can show off as what we're doing in fairness, we've go this package, so really we just need to show it off. I'm thinking we have a web app called something like Fairness Auditing (or something similar) which aims to do similar things to Aequitas or XAI. You will be able to upload your data and get a report (analysis) of your data from a fairness perspective, and you will also be able to upload your predictions and get a report (analysis) of how fair/unfair your predictions are. Conceptually we have a front end that explains a bit about what's going on on the landing page, then has an audit page which consists of a data uploader which gets the data then calls a Flask API which uses EthicML to do the analysis, the information gets sent back to the front end to display.","25","0.3796009722149364","Research","Requirement Analysis"
"https://github.com/predictive-analytics-lab/EthicML","611979826","issue","https://github.com/Trusted-AI/AIF360/issues/322","FairERM GitHub repo https://github.com/jmikko/fair_ERM with fair empirical risk minimization model. I've only given it a quick glance, but I don't think we can do a quick lift and port, it'll take a little time to work out what they're trying to do.","24","0.3326171612729957","UI","Requirement Analysis"
"https://github.com/predictive-analytics-lab/EthicML","580625384","issue","https://github.com/Trusted-AI/AIF360/issues/304","add nhanes dataset https://wwwn.cdc.gov/nchs/nhanes/ContinuousNhanes/Default.aspx?BeginYear=2015","9","0.2562538133007933","Feature engineering methodology","Design"
"https://github.com/predictive-analytics-lab/EthicML","580624938","issue","https://github.com/Trusted-AI/AIF360/issues/303","add wage data https://rdrr.io/github/gschofl/coursedata/man/Wage.html","18","0.582027168234065","Troubleshooting","Maintenance"
"https://github.com/predictive-analytics-lab/EthicML","571972100","issue","https://github.com/Trusted-AI/AIF360/issues/297","Standard model interpretation methods Maybe use

https://pypi.org/project/trelawney/

to let us do more well used model interpretation analysis","22","0.3784983593900793","Bias mitigation methodology","Design"
"https://github.com/predictive-analytics-lab/EthicML","568634805","issue","https://github.com/Trusted-AI/AIF360/issues/291","Zhang implementation Oliver:

> I feel as though implementing zhang’s mitigating unwanted bias by Adversarial learning algorithm is possible to implement in a way that doesn’t suck. In his original paper he hand writes a den layer network and in aif 360 they do the same. If there’s not a way in tensorflow or pytorch to easily the project the gradients with respect to one loss onto the gradients with respect to another loss perhaps we’ll have to go a bit lower level. It might be a good opportunity to play around with JAX https://colab.research.google.com/github/google/jax/blob/master/notebooks/neural_network_and_data_loading.ipynb ","28","0.419810922596438","Bias mitigation methodology","Design"
"https://github.com/predictive-analytics-lab/EthicML","568633989","issue","https://github.com/Trusted-AI/AIF360/issues/290","Re-discuss public API I think importing stuff from EthicML is too verbose. This is the current state of the API:

```
algorithms
algorithms.inprocess
algorithms.postprocess
algorithms.preprocess
data
evaluators
metrics
preprocessing
utility
vision
visualisation
```

I think it would be nice to shorten everything a bit. For example like this:

```
algo
algo.intra
algo.post
algo.pre
data
eval
metrics
preprocess
utils
vision
plot
```

@MylesBartlett @olliethomas ","30","0.2797342020891509","Troubleshooting","Maintenance"
"https://github.com/predictive-analytics-lab/EthicML","552550512","issue","https://github.com/Trusted-AI/AIF360/issues/254","add run_black.py script so that @MylesBartlett doesn't always run the wrong command.","4","0.6708724253208235","Installation and shell commands","Deployment"
"https://github.com/predictive-analytics-lab/EthicML","537933456","issue","https://github.com/Trusted-AI/AIF360/issues/229","Idea for simplifying algorithms: have each algorithm either run in its own process or not. Don't allow both With this change, lots of code can be removed and it is then way more straight-forward to add new algorithms.

Now we only have to decide which algorithms should run in their own process and which shouldn't.

- Algorithms using PyTorch: I think they should be in their own process. If you run PyTorch and then keep the process alive, it never really cleans up after itself and often still hogs the GPU.
- Algorithms using Sklearn: I don't see much reason for them to be their own process. Let's run those in the main process.
- ""Installed algorithms"" that are cloned from GitHub: these just have to run in their own process because they might use a different version of Python","30","0.3945366069453661","Troubleshooting","Maintenance"
"https://github.com/predictive-analytics-lab/EthicML","517129836","issue","https://github.com/Trusted-AI/AIF360/issues/225","Agarwal doesn't work anymore with the newest version It gives this error:

```
    """"""Implementation of logistic regression (actually just a wrapper around sklearn)""""""
    import pandas as pd

>   from fairlearn.classred import expgrad
E   ModuleNotFoundError: No module named 'fairlearn.classred'

ethicml/implementations/agarwal.py:4: ModuleNotFoundError
```","28","0.3721603654308903","Bias mitigation methodology","Design"
"https://github.com/predictive-analytics-lab/EthicML","515284400","issue","https://github.com/Trusted-AI/AIF360/issues/223","add diabetes dataset as used in edwards and storkey Form the censoring representations with an adversary paper
""The Diabetes dataset consists of hospital data from the US and the task is to predict whether a patient will be readmitted to hospital. The sensitive attribute we chose was Race, we changed this to a binary variable by creating a new attribute isCaucasian. The data has around 100 thousand instances and 235 attributes. We used 80 thousand instances for the training set and approximately 10 thousand instances each for the validation and test sets.""","26","0.2821278478212783","Bias mitigation methodology","Design"
"https://github.com/predictive-analytics-lab/EthicML","487101938","issue","https://github.com/Trusted-AI/AIF360/issues/170","The implementations are currently type checked, but maybe they shouldn't be They're always causing a lot of trouble.","32","0.479227761485826","Dependency and Release","Deployment"
"https://github.com/predictive-analytics-lab/EthicML","485755828","issue","https://github.com/Trusted-AI/AIF360/issues/166","make the query string go through the same substitutions as the column names when query is called in domain adaptation file","7","0.2685354990830494","Opinion","Requirement Analysis"
"https://github.com/predictive-analytics-lab/EthicML","483548179","issue","https://github.com/Trusted-AI/AIF360/issues/162","plot_mean_std_box() doesn't work when the result contains multiple datasets with different s names The plots are just empty","22","0.3434665122563213","Bias mitigation methodology","Design"
"https://github.com/predictive-analytics-lab/EthicML","478386884","issue","https://github.com/Trusted-AI/AIF360/issues/146","Consider letting metrics define how their values are compared See how algofairness does it:

```python
class DIBinary(Metric):
    def __init__(self):
        # ...

    def calc(self, actual, predicted, dict_of_sensitive_lists, single_sensitive_name,
             unprotected_vals, positive_pred):
        # ...
        return DI

    def is_better_than(self, val1, val2):
        dist1 = math.fabs(1.0 - val1)
        dist2 = math.fabs(1.0 - val2)
        return dist1 <= dist2
```

Each metric has a function `is_better_than()` that compares two of its results and tells you which one is the better result.","18","0.370443715576459","Troubleshooting","Maintenance"
"https://github.com/predictive-analytics-lab/EthicML","475688747","issue","https://github.com/Trusted-AI/AIF360/issues/129","Binary by deafult -> Binomial by Default At the moment we try to handle binary sensitive attributes and class labels and non-binary sensitive attributes and class labels as two distinct approaches.

We should move to make binary attributes a pair, so for example

```
[‘sex_Male’][0, 1, 1, 0, 1, 0]
```
becomes
```
[‘sex_Male’, ‘sex_Female’][[0, 1], [1, 0], [1, 0], [0, 1], [1, 0], [0, 1]]
```","26","0.44200734241802","Bias mitigation methodology","Design"
"https://github.com/predictive-analytics-lab/EthicML","463753400","issue","https://github.com/Trusted-AI/AIF360/issues/125","Limit API depth Can we agree that this is the public API:

```python
algorithms
algorithms.inprocess
algorithms.postprocess
algorithms.preprocess
data
evaluators
metrics
preprocessing
utility
visualisation
```

And that that you never have to go deeper in order to import something?

Examples:

```python
# NOT okay
from ethicml.preprocessing.train_test_split import train_test_split

# okay
from ethicml.preprocessing import train_test_split
```","27","0.3329076542421582","Troubleshooting","Maintenance"
"https://github.com/predictive-analytics-lab/EthicML","463224288","issue","https://github.com/Trusted-AI/AIF360/issues/123","Generate documentation with Sphinx Sphinx is the standard documentation generator for Python. Our code is using Google-style documentation which [is supported by Sphinx](https://www.sphinx-doc.org/en/1.8/usage/extensions/napoleon.html)!

The best tutorial I've found is this: https://sphinxcontrib-napoleon.readthedocs.io/en/latest/#getting-started","14","0.4022753486421306","Documentation","Development"
"https://github.com/predictive-analytics-lab/EthicML","461497015","issue","https://github.com/Trusted-AI/AIF360/issues/116","Consider storing a datatuple as a single numpy file ~~This would work by using hierarchical columns. See here: https://gist.github.com/thomkeh/e3cb2af55e5d6e7a3442e1ae9c6091dd~~

The column names can be stored as a numpy array with strings. (Feather also had the requirement that column names must be strings.)","17","0.7226107226107229","Troubleshooting","Maintenance"
"https://github.com/predictive-analytics-lab/EthicML","432576534","issue","https://github.com/Trusted-AI/AIF360/issues/68","Calibration score is missing This library REALLY needs a calibration score.","30","0.7897116324535679","Troubleshooting","Maintenance"
"https://github.com/predictive-analytics-lab/EthicML","432574378","issue","https://github.com/Trusted-AI/AIF360/issues/67","No Theil index This library really needs the Theil index as a measure of inequality.
https://en.wikipedia.org/wiki/Theil_index","3","0.6595057200244511","Bias detection metrics validation","Validation"
"https://github.com/predictive-analytics-lab/EthicML","412559816","issue","https://github.com/Trusted-AI/AIF360/issues/16","Proposal: replace the dataframe dictionary with a NamedTuple Dictionaries always have the problem that you can store anything in there and you can also store nothing in there. `dictionary['x']` can always fail.

NamedTuple on the other hand have a fixed number of fields and all fields are mandatory:

```python
from typing import NamedTuple
import pandas as pd

class DataTuple(NamedTuple):
    x: pd.DataFrame
    y: pd.DataFrame
    s: pd.DataFrame
```

They are instantiated as follows:

```python
data = DataTuple(x=df1, y=df2, s=df3)
```

The values are accessed like this:
```python
data.x
```

Finally, NamedTuple are *immutable*. This: `data.x = df4` throws an error.","17","0.7165275200989488","Troubleshooting","Maintenance"
"https://github.com/predictive-analytics-lab/EthicML","410807170","issue","https://github.com/Trusted-AI/AIF360/issues/9","Proposal: algorithm in separate processes I imagine running algorithms in separate processes to work something like this:

At the end of the data preparation, the data is stored on the hard drive:

```python
with TemporaryDirectory() as tmpdir:
    tmp_path = Path(tmpdir)
    data_path = tmp_path / Path(""data.parquet"")
    df = pd.concat([data_dict['x'], data_dict['s'], data_dict['y']], axis='columns')
    df.to_parquet(data_path, compression=None)
```

Then a python script for the algorithm is started as a separate process. The path to the data file is passed as a commandline argument.

```python
from subprocess import call
PYTHON_EXE = ""/home/ubuntu/anaconda3/envs/pytorch_p36/bin/python -u""

...

cmd = f""{PYTHON_EXE} {script_path} {data_path}""
call(cmd, shell=True)
```

Then the data file is read by the called script.

```python
import sys
import pandas as pd

...

data_path = sys.argv[1]
with open(data_path, 'rb') as f:
df = pd.read_parquet(data_path)
```

(`parquet` has to be install with `pip install fastparquet`.)","4","0.6631875093958728","Installation and shell commands","Deployment"
"https://github.com/Nathanlauga/transparentai","669901442","issue","https://github.com/Trusted-AI/AIF360/issues/8","XGBoostClassifier - ValueError: feature_names mismatch: ```
File ""C:\ProgramData\Anaconda3\envs\transparentai-ui\lib\site-packages\transparentai\models\explainers\model_explainer.py"", line 213, in __init__
self.explainer = self.init_explainer(X)
File ""C:\ProgramData\Anaconda3\envs\transparentai-ui\lib\site-packages\transparentai\models\explainers\model_explainer.py"", line 238, in init_explainer
explainer = shap.KernelExplainer(self.predict, X)
File ""C:\ProgramData\Anaconda3\envs\transparentai-ui\lib\site-packages\shap\explainers\kernel.py"", line 97, in __init__
model_null = match_model_to_data(self.model, self.data)
File ""C:\ProgramData\Anaconda3\envs\transparentai-ui\lib\site-packages\shap\common.py"", line 89, in match_model_to_data
out_val = model.f(data.data)
File ""C:\ProgramData\Anaconda3\envs\transparentai-ui\lib\site-packages\transparentai\models\explainers\model_explainer.py"", line 188, in <lambda>
f = lambda x: model.predict_proba(x)[:,1]
File ""C:\ProgramData\Anaconda3\envs\transparentai-ui\lib\site-packages\xgboost\sklearn.py"", line 834, in predict_proba
validate_features=validate_features)
File ""C:\ProgramData\Anaconda3\envs\transparentai-ui\lib\site-packages\xgboost\core.py"", line 1284, in predict
self._validate_features(data)
File ""C:\ProgramData\Anaconda3\envs\transparentai-ui\lib\site-packages\xgboost\core.py"", line 1690, in _validate_features
data.feature_names))
```

Solution : handle XGBoostClassifier as a particular case.","16","0.7215265118490923","Testing","Maintenance"
"https://github.com/Nathanlauga/transparentai","649984052","issue","https://github.com/Trusted-AI/AIF360/issues/7","Error: fairness.plot_bias privileged_group with numerical values Errors with numerical attributes in **fairness.plot_bias**.

Also check fairness.compute_metrics to handle numerical values.

```python
privileged_group = {'SEX': [1]}
fairness.plot_bias(y_test, y_pred_a, X_test, privileged_group, pos_label=0)
```

```
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-73-ecba7ab9a910> in <module>
----> 1 fairness.plot_bias(y_test, y_pred_a, X_test, privileged_group, pos_label=0)

/opt/conda/lib/python3.7/site-packages/transparentai/fairness/fairness_plots.py in plot_bias(y_true, y_pred, df, privileged_group, pos_label, regr_split, with_text, **kwargs)
    315     for attr, bias_scores in scores.items():
    316         ax = fig.add_subplot(gs[row, :])
--> 317         plot_attr_title(ax, attr, df, privileged_group)
    318 
    319         axes = [fig.add_subplot(gs[row+1+j, i])

/opt/conda/lib/python3.7/site-packages/transparentai/fairness/fairness_plots.py in plot_attr_title(ax, attr, df, privileged_group)
    137         attr, df, privileged_group, privileged=False)
    138 
--> 139     priv_text = format_priv_text(priv_values, max_char=30)
    140     unpriv_text = format_priv_text(unpriv_values, max_char=30)
    141 

/opt/conda/lib/python3.7/site-packages/transparentai/fairness/fairness_plots.py in format_priv_text(values, max_char)
     88 
     89     for val in values:
---> 90         if (len(val) + len(priv_text) > max_char) & (priv_text != ''):
     91             priv_text = priv_text[:-2] + ' and others  '
     92             break

TypeError: object of type 'int' has no len()

```","26","0.5638667783829073","Bias mitigation methodology","Design"
"https://github.com/Nathanlauga/transparentai","623181598","issue","https://github.com/Trusted-AI/AIF360/issues/6","plot_local_explain_interact with columns name length too big. The function `plot_local_explain_interact` in the `models.explainers.ModelExplainer` class is not well designed for columns with a length above 40 characters.","17","0.3041125541125541","Troubleshooting","Maintenance"
"https://github.com/Nathanlauga/transparentai","621076389","issue","https://github.com/Trusted-AI/AIF360/issues/5","classification.plot_performance y_pred is n,1 dimensionnal ```python
>>> print(y_pred)

0       0.00
1       0.04
2       0.02
3       0.06
4       0.04
        ... 
7682    0.70
7683    0.82
7684    0.42
7685    0.10
7686    0.12

>>> classification.plot_performance(y_true, y_pred)

---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-25-ee64c9c30a95> in <module>
      4     y_pred = p.iloc[:,3]
      5     print(y_pred)
----> 6     classification.plot_performance(y_true, y_pred)

~\Anaconda3\lib\site-packages\transparentai-0.2.1-py3.7.egg\transparentai\models\classification\classification_plots.py in plot_performance(y_true, y_pred, y_true_valid, y_pred_valid, metrics, **kwargs)
    265 
    266     # 1. Compute scores
--> 267     perf_prob = compute_prob_performance(y_true, y_prob, metrics)
    268     if validation:
    269         perf_prob_valid = compute_prob_performance(

~\Anaconda3\lib\site-packages\transparentai-0.2.1-py3.7.egg\transparentai\models\classification\classification_plots.py in compute_prob_performance(y_true, y_prob, metrics)
    160         return None
    161 
--> 162     if len(y_prob.shape) > 1:
    163         n_classes = y_prob.shape[1]
    164         if n_classes == 2:

AttributeError: 'NoneType' object has no attribute 'shape'
```

Todo : 
- `models.classification.preprocess_scores` if y_pred.shape == 1, check that is not probabilities
-  handle y_prob n,1 dimensional for `models.classification.compute_prob_performance`","18","0.5367132867132867","Troubleshooting","Maintenance"
"https://github.com/Nathanlauga/transparentai","603315186","issue","https://github.com/Trusted-AI/AIF360/issues/4","plot_class_distribution : KDE bandwidth is 0. Cannot estiamte density. ```
~\Anaconda3\lib\site-packages\transparentai\models\models_plots.py in plot_classification_scores(scores, y_true, y_proba)
     44     ax = plt.subplot(int(f'212'))
     45     plot_class_distribution(
---> 46         y_true=y_true, y_proba=y_proba, n_classes=n_classes)
     47 
     48     plots.plot_or_save(fname='classification_scores_plot.png')

~\Anaconda3\lib\site-packages\transparentai\models\models_plots.py in plot_class_distribution(y_true, y_proba, n_classes)
    138     for v in df['y_true'].unique():
    139         kde = df[df['y_true'] == v][v].nunique() > 1
--> 140         sns.distplot(df[df['y_true'] == v][v], label=v, kde=kde)
    141         plt.legend(loc=0)
    142 

~\Anaconda3\lib\site-packages\seaborn\distributions.py in distplot(a, bins, hist, kde, rug, fit, hist_kws, kde_kws, rug_kws, fit_kws, color, vertical, norm_hist, axlabel, label, ax)
    231     if kde:
    232         kde_color = kde_kws.pop(""color"", color)
--> 233         kdeplot(a, vertical=vertical, ax=ax, color=kde_color, **kde_kws)
    234         if kde_color != color:
    235             kde_kws[""color""] = kde_color

~\Anaconda3\lib\site-packages\seaborn\distributions.py in kdeplot(data, data2, shade, vertical, kernel, bw, gridsize, cut, clip, legend, cumulative, shade_lowest, cbar, cbar_ax, cbar_kws, ax, **kwargs)
    703         ax = _univariate_kdeplot(data, shade, vertical, kernel, bw,
    704                                  gridsize, cut, clip, legend, ax,
--> 705                                  cumulative=cumulative, **kwargs)
    706 
    707     return ax

~\Anaconda3\lib\site-packages\seaborn\distributions.py in _univariate_kdeplot(data, shade, vertical, kernel, bw, gridsize, cut, clip, legend, ax, cumulative, **kwargs)
    293         x, y = _statsmodels_univariate_kde(data, kernel, bw,
    294                                            gridsize, cut, clip,
--> 295                                            cumulative=cumulative)
    296     else:
    297         # Fall back to scipy if missing statsmodels

~\Anaconda3\lib\site-packages\seaborn\distributions.py in _statsmodels_univariate_kde(data, kernel, bw, gridsize, cut, clip, cumulative)
    365     fft = kernel == ""gau""
    366     kde = smnp.KDEUnivariate(data)
--> 367     kde.fit(kernel, bw, fft, gridsize=gridsize, cut=cut, clip=clip)
    368     if cumulative:
    369         grid, y = kde.support, kde.cdf

~\Anaconda3\lib\site-packages\statsmodels\nonparametric\kde.py in fit(self, kernel, bw, fft, weights, gridsize, adjust, cut, clip)
    138             density, grid, bw = kdensityfft(endog, kernel=kernel, bw=bw,
    139                     adjust=adjust, weights=weights, gridsize=gridsize,
--> 140                     clip=clip, cut=cut)
    141         else:
    142             density, grid, bw = kdensity(endog, kernel=kernel, bw=bw,

~\Anaconda3\lib\site-packages\statsmodels\nonparametric\kde.py in kdensityfft(X, kernel, bw, weights, gridsize, adjust, clip, cut, retgrid)
    451         bw = float(bw)
    452     except:
--> 453         bw = bandwidths.select_bandwidth(X, bw, kern) # will cross-val fit this pattern?
    454     bw *= adjust
    455 

~\Anaconda3\lib\site-packages\statsmodels\nonparametric\bandwidths.py in select_bandwidth(x, bw, kernel)
    172         # eventually this can fall back on another selection criterion.
    173         err = ""Selected KDE bandwidth is 0. Cannot estiamte density.""
--> 174         raise RuntimeError(err)
    175     else:
    176         return bandwidth

RuntimeError: Selected KDE bandwidth is 0. Cannot estiamte density.
```

Solution : remove kde argument from function.","9","0.8694687313459047","Feature engineering methodology","Design"
"https://github.com/Nathanlauga/transparentai","599553968","issue","https://github.com/Trusted-AI/AIF360/issues/3","model_explainer : multi_label not defined ## Todo :
solution : add `multi_label = False` with class variable","12","0.5295556461475742","Metrics operation","Validation"
"https://github.com/Nathanlauga/transparentai","587092527","issue","https://github.com/Trusted-AI/AIF360/issues/2","datasets plots : plot_numerical_var boxplot With this dataset : https://www.kaggle.com/c/ieee-fraud-detection/data
For `D1` to `D15` columns output : 
```
/usr/local/lib/python3.7/site-packages/transparentai/datasets/structured_dataset.py in plot_one_numeric_variable(self, var, nrows)
    180             plots.plot_numerical_var(df=df, var=self.target)
    181         else:
--> 182             plots.plot_numerical_var(df=df, var=var, target=self.target)
    183 
    184     def plot_one_categorical_variable(self, var, nrows=None):

/usr/local/lib/python3.7/site-packages/transparentai/datasets/datasets_plots.py in plot_numerical_var(df, var, target)
     76     x = df[target] if target != None else None
     77 
---> 78     sns.boxplot(x=x, y=df[var].dropna())
     79 
     80     if target != None:

/usr/local/lib/python3.7/site-packages/seaborn/categorical.py in boxplot(x, y, hue, data, order, hue_order, orient, color, palette, saturation, width, dodge, fliersize, linewidth, whis, ax, **kwargs)
   2239     plotter = _BoxPlotter(x, y, hue, data, order, hue_order,
   2240                           orient, color, palette, saturation,
-> 2241                           width, dodge, fliersize, linewidth)
   2242 
   2243     if ax is None:

/usr/local/lib/python3.7/site-packages/seaborn/categorical.py in __init__(self, x, y, hue, data, order, hue_order, orient, color, palette, saturation, width, dodge, fliersize, linewidth)
    441                  width, dodge, fliersize, linewidth):
    442 
--> 443         self.establish_variables(x, y, hue, data, orient, order, hue_order)
    444         self.establish_colors(color, palette, saturation)
    445 

/usr/local/lib/python3.7/site-packages/seaborn/categorical.py in establish_variables(self, x, y, hue, data, orient, order, hue_order, units)
    202                 # Group the numeric data
    203                 plot_data, value_label = self._group_longform(vals, groups,
--> 204                                                               group_names)
    205 
    206                 # Now handle the hue levels for nested ordering

/usr/local/lib/python3.7/site-packages/seaborn/categorical.py in _group_longform(self, vals, grouper, order)
    248 
    249         # Group the val data
--> 250         grouped_vals = vals.groupby(grouper)
    251         out_data = []
    252         for g in order:

/usr/local/lib/python3.7/site-packages/pandas/core/series.py in groupby(self, by, axis, level, as_index, sort, group_keys, squeeze, observed)
   1685             group_keys=group_keys,
   1686             squeeze=squeeze,
-> 1687             observed=observed,
   1688         )
   1689 

/usr/local/lib/python3.7/site-packages/pandas/core/groupby/groupby.py in __init__(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, squeeze, observed, mutated)
    407                 sort=sort,
    408                 observed=observed,
--> 409                 mutated=self.mutated,
    410             )
    411 

/usr/local/lib/python3.7/site-packages/pandas/core/groupby/grouper.py in get_grouper(obj, key, axis, level, sort, observed, mutated, validate)
    606         if is_categorical_dtype(gpr) and len(gpr) != obj.shape[axis]:
    607             raise ValueError(
--> 608                 f""Length of grouper ({len(gpr)}) and axis ({obj.shape[axis]}) ""
    609                 ""must be same length""
    610             )

ValueError: Length of grouper (590540) and axis (589271) must be same length
```

","18","0.8297589771067176","Troubleshooting","Maintenance"
"https://github.com/Nathanlauga/transparentai","587001735","issue","https://github.com/Trusted-AI/AIF360/issues/1","datasets plot correlation : plot_correlation_matrix with a series Using a Series as argument : 
`plot_correlation_matrix(some_series)`

Output : 
```
/usr/local/lib/python3.7/site-packages/transparentai/datasets/datasets_plots.py in plot_correlation_matrix(corr_df, fname)
    309         vmin=-1, vmax=1, center=0,
    310         cmap=sns.color_palette(""RdBu_r"", 100),
--> 311         square=(corr_df.shape[0] == corr_df.shape[1]),
    312         annot=annot,
    313         fmt='.2f'

IndexError: tuple index out of range
```

## Fix to do
```python
If type(corr_df) == pd.Series:
    corr_df = corr_df.to_frame()
```","17","0.3330241187384046","Troubleshooting","Maintenance"
"https://github.com/Trusted-AI/AIF360","1013644319","issue_comment","https://github.com/Trusted-AI/AIF360/issues/286#issuecomment-1013644319","try to use the Sklearn version instead of the TF version and you will have no problem with pickle dump after the fit. if you want to use the TF version try to save it as a TF model.","16","0.477763016385146","Testing","Maintenance"
"https://github.com/Trusted-AI/AIF360","1003763761","issue_comment","https://github.com/Trusted-AI/AIF360/issues/283#issuecomment-1003763761","I guess one reason for this error might be due to the default labels. German datasets has default labels values [1,2] which is different from all other datasets [0,1].  I tried to hack this issue as following:

`dataset = GermanDataset()`
`dataset.labels = (dataset.labels-1.0).astype('float64')`
`dataset.favorable_label = 1.0`
`dataset.unfavorable_label = 0.0`
`inp = GerryFairClassifier()`
`inp.fit(dataset)`

The above code produces no error. Not sure if this is the proper solution.","26","0.4608300665795533","Bias mitigation methodology","Design"
"https://github.com/Trusted-AI/AIF360","958582320","issue_comment","https://github.com/Trusted-AI/AIF360/issues/276#issuecomment-958582320","@pronics2004 ^^^. If we agree that this is a genuine issue, may be @giandos200 - you can open a PR.
","20","0.5235826001955036","Opinion","Requirement Analysis"
"https://github.com/Trusted-AI/AIF360","958936601","issue_comment","https://github.com/Trusted-AI/AIF360/issues/276#issuecomment-958936601","okay, another issue is that the classifier and the adversary network are trained before the nested gradient training in the paper. I'll open a PR and implement a pretraining cycle if the debias=True.","22","0.4670002780094522","Bias mitigation methodology","Design"
"https://github.com/Trusted-AI/AIF360","959239488","issue_comment","https://github.com/Trusted-AI/AIF360/issues/276#issuecomment-959239488","@giandos200 @nrkarthikeyan If I remember correctly, we were not seeing big differences with these enhancements across datasets. So we decided to keep the basic version of approach. 

@giandos200 Please free to open a PR with these enhancements if that leads to consistent improvements across a couple of datasets. Thanks! ","15","0.4602858481724462","Metrics operation","Validation"
"https://github.com/Trusted-AI/AIF360","989822187","issue_comment","https://github.com/Trusted-AI/AIF360/issues/276#issuecomment-989822187","@pronics2004 @nrkarthikeyan i'm trying to pass the continuous integration test for the Pull Request but did not pass the test Continuous Integration / build-py (3.6/3.7/3.8) since the adversary test is done with TF1 but the code i've pushed is upgraded to TF2.","31","0.4461845915412537","Testing","Maintenance"
"https://github.com/Trusted-AI/AIF360","989933420","issue_comment","https://github.com/Trusted-AI/AIF360/issues/276#issuecomment-989933420","> @pronics2004 @nrkarthikeyan i'm trying to pass the continuous integration test for the Pull Request but did not pass the test Continuous Integration / build-py (3.6/3.7/3.8) since the adversary test is done with TF1 but the code i've pushed is upgraded to TF2.

@giandos200 You would have to update the tests as well I believe in this case. ","31","0.4375411255411255","Testing","Maintenance"
"https://github.com/Trusted-AI/AIF360","954875088","issue_comment","https://github.com/Trusted-AI/AIF360/issues/275#issuecomment-954875088","The various metrics are listed in the following places in the API documentation:
- https://aif360.readthedocs.io/en/latest/modules/generated/aif360.metrics.BinaryLabelDatasetMetric.html
- https://aif360.readthedocs.io/en/latest/modules/generated/aif360.metrics.ClassificationMetric.html
- https://aif360.readthedocs.io/en/latest/modules/generated/aif360.metrics.SampleDistortionMetric.html
","30","0.5186257479251108","Troubleshooting","Maintenance"
"https://github.com/Trusted-AI/AIF360","958582514","issue_comment","https://github.com/Trusted-AI/AIF360/issues/275#issuecomment-958582514","Closing in lieu of @krvarshney 's response","31","0.7335997335997337","Testing","Maintenance"
"https://github.com/Trusted-AI/AIF360","950723641","issue_comment","https://github.com/Trusted-AI/AIF360/issues/274#issuecomment-950723641","https://github.com/Trusted-AI/AIF360/blob/master/examples/demo_adversarial_debiasing.ipynb ?","22","0.5023388244864756","Bias mitigation methodology","Design"
"https://github.com/Trusted-AI/AIF360","953155045","issue_comment","https://github.com/Trusted-AI/AIF360/issues/274#issuecomment-953155045","@Drew2019 I have a complete script from end to end. Will update them this weekend. Thanks for your interest.","24","0.4602393360355144","UI","Requirement Analysis"
"https://github.com/Trusted-AI/AIF360","923957467","issue_comment","https://github.com/Trusted-AI/AIF360/issues/273#issuecomment-923957467","The Theil Index is the Generalized Entropy Index with alpha = 1. The Generalized Entropy Index is implemented in the standard way using the equation https://wikimedia.org/api/rest_v1/media/math/render/svg/844bcf9d016e032d7e03f0de29b7733e36f0b8a9.","3","0.525745738636364","Bias detection metrics validation","Validation"
"https://github.com/Trusted-AI/AIF360","925628975","issue_comment","https://github.com/Trusted-AI/AIF360/issues/273#issuecomment-925628975","Ok, thanks.

But the interface forcus only on the binary classification.
I would like to extend Theil Index for multiclass or regression.
Actually, I woud like to use probabilities instead of binary predictions.

""In this paper, we will focus on binary classification, but our work extends to multiclass classification and regression, as well.""
[A Unified Approach to Quantifying Algorithmic Unfairness: Measuring Individual & Group Unfairness via Inequality Indices](https://arxiv.org/pdf/1807.00787.pdf)

You can close the issue if this does not fit the package, or it is not considered, thanks for the reply","6","0.4887347113940346","API expansion","Development"
"https://github.com/Trusted-AI/AIF360","925837248","issue_comment","https://github.com/Trusted-AI/AIF360/issues/273#issuecomment-925837248","Extending for multiclass and/or regression will be great. Please go ahead. We can discuss your solution through a pull request once you've made it.","20","0.3854281311908429","Opinion","Requirement Analysis"
"https://github.com/Trusted-AI/AIF360","907266867","issue_comment","https://github.com/Trusted-AI/AIF360/issues/266#issuecomment-907266867","The AIF360 website is https://aif360.mybluemix.net/","1","0.2070368110636568","Fix warnings","Maintenance"
"https://github.com/Trusted-AI/AIF360","909263011","issue_comment","https://github.com/Trusted-AI/AIF360/issues/263#issuecomment-909263011","Hello, can you please help me get this PR merged?","32","0.3923719958202717","Dependency and Release","Deployment"
"https://github.com/Trusted-AI/AIF360","914549997","issue_comment","https://github.com/Trusted-AI/AIF360/issues/263#issuecomment-914549997","Hi @asm582 - what is in the zip file you have uploaded in the dataset?
","0","0.6039464411557435","Dataset usage","Requirement Analysis"
"https://github.com/Trusted-AI/AIF360","917769087","issue_comment","https://github.com/Trusted-AI/AIF360/issues/263#issuecomment-917769087","Hi @nrkarthikeyan the zip file is processed MEP dataset using Rscript ran on python2.7","7","0.4270369400052393","Opinion","Requirement Analysis"
"https://github.com/Trusted-AI/AIF360","917789505","issue_comment","https://github.com/Trusted-AI/AIF360/issues/263#issuecomment-917789505","Ok. Can we provide instructions to create the zip file rather than actual
data? we do not want to include actual data.

On Sun, Sep 12, 2021 at 9:41 PM Abhishek Malvankar ***@***.***>
wrote:

> Hi @nrkarthikeyan
> <https://urldefense.com/v3/__https://github.com/nrkarthikeyan__;!!IKRxdwAv5BmarQ!Ipuax5GR3QJcbyoarqExfak3hiunZ3cJtHE-xt7MD42qDzCIrV_ta2FkAglTs44$>
> the zip file is processed MEP dataset using Rscript ran on python2.7
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://urldefense.com/v3/__https://github.com/Trusted-AI/AIF360/issues/263*issuecomment-917769087__;Iw!!IKRxdwAv5BmarQ!Ipuax5GR3QJcbyoarqExfak3hiunZ3cJtHE-xt7MD42qDzCIrV_ta2Fku1NMUJg$>,
> or unsubscribe
> <https://urldefense.com/v3/__https://github.com/notifications/unsubscribe-auth/ABNX22XXJ2FYXLBOGUHNFTTUBVJETANCNFSM5CVDHA5Q__;!!IKRxdwAv5BmarQ!Ipuax5GR3QJcbyoarqExfak3hiunZ3cJtHE-xt7MD42qDzCIrV_ta2FklrQo2FM$>
> .
> Triage notifications on the go with GitHub Mobile for iOS
> <https://urldefense.com/v3/__https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675__;!!IKRxdwAv5BmarQ!Ipuax5GR3QJcbyoarqExfak3hiunZ3cJtHE-xt7MD42qDzCIrV_ta2Fkru9gPn8$>
> or Android
> <https://urldefense.com/v3/__https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign*3Dnotification-email*26utm_medium*3Demail*26utm_source*3Dgithub__;JSUlJSU!!IKRxdwAv5BmarQ!Ipuax5GR3QJcbyoarqExfak3hiunZ3cJtHE-xt7MD42qDzCIrV_ta2FkdCmrFs4$>.
>
>
","7","0.6557742203945752","Opinion","Requirement Analysis"
"https://github.com/Trusted-AI/AIF360","950727014","issue_comment","https://github.com/Trusted-AI/AIF360/issues/261#issuecomment-950727014","I am experiencing trouble with the `read.xport()` function from the `SASxport` library. 

RStudio tells me:
`Error in read.xport(ssp_file) : could not find function ""read.xport""`

This happens while running the `convert` function in `generate_data.R` file to convert h181.ssp to h181.csv in the Medical Expenditure Tutorial. Any suggestions how to fix this?","21","0.4377920170871713","Installation and shell commands","Deployment"
"https://github.com/Trusted-AI/AIF360","950869345","issue_comment","https://github.com/Trusted-AI/AIF360/issues/261#issuecomment-950869345","Never mind – Loading the library manually: `library(""SASxport"")` solved the issue.","30","0.4940127077223851","Troubleshooting","Maintenance"
"https://github.com/Trusted-AI/AIF360","972460240","issue_comment","https://github.com/Trusted-AI/AIF360/issues/261#issuecomment-972460240","Thanks and closing this issue","24","0.1507849580138736","UI","Requirement Analysis"
"https://github.com/Trusted-AI/AIF360","984367363","issue_comment","https://github.com/Trusted-AI/AIF360/issues/258#issuecomment-984367363","I am getting the same issue and I have followed the steps given in the notebook https://github.com/Trusted-AI/AIF360/blob/master/examples/sklearn/demo_grid_search_reduction_regression_sklearn.ipynb. 
![errir](https://user-images.githubusercontent.com/40518134/144378493-cc5fa78f-4e3c-4e4a-90fd-d4869a37214b.png)

","28","0.765173347305912","Bias mitigation methodology","Design"
"https://github.com/Trusted-AI/AIF360","987934369","issue_comment","https://github.com/Trusted-AI/AIF360/issues/258#issuecomment-987934369","Did anyone solve this issue?
Working with the following versions, the error still exists.
Python 3.8.x
fairlearn==0.7.0
aif360==0.4.0
Followed the steps derived from this notebook: https://github.com/Trusted-AI/AIF360/blob/746e763191ef46ba3ab5c601b96ce3f6dcb772fd/examples/sklearn/demo_grid_search_reduction_classification_sklearn.ipynb","28","0.3832442067736185","Bias mitigation methodology","Design"
"https://github.com/Trusted-AI/AIF360","997995817","issue_comment","https://github.com/Trusted-AI/AIF360/issues/258#issuecomment-997995817","Hi,
Take a look at the requirements.txt : https://github.com/Trusted-AI/AIF360/blob/master/requirements.txt
fairlearn's version should be 0.4.6. It worked for me.","21","0.5357293868921775","Installation and shell commands","Deployment"
"https://github.com/Trusted-AI/AIF360","887713297","issue_comment","https://github.com/Trusted-AI/AIF360/issues/256#issuecomment-887713297","cc @hoffmansc @nrkarthikeyan ","31","0.428513321130771","Testing","Maintenance"
"https://github.com/Trusted-AI/AIF360","900419633","issue_comment","https://github.com/Trusted-AI/AIF360/issues/256#issuecomment-900419633","PR created #260 ","20","0.3923719958202717","Opinion","Requirement Analysis"
"https://github.com/Trusted-AI/AIF360","896627274","issue_comment","https://github.com/Trusted-AI/AIF360/issues/253#issuecomment-896627274","Got the same problem too. Following closely...","11","0.360762800417973","Troubleshooting","Maintenance"
"https://github.com/Trusted-AI/AIF360","874814897","issue_comment","https://github.com/Trusted-AI/AIF360/issues/245#issuecomment-874814897","@monindersingh , can you take a look and see what if any can be done and provide your thoughts? @psortos you are also welcome to provide your thoughts on how to fix this.","25","0.2663647147632912","Research","Requirement Analysis"
"https://github.com/Trusted-AI/AIF360","982092288","issue_comment","https://github.com/Trusted-AI/AIF360/issues/241#issuecomment-982092288","Was a solution found for this? I am seeing the same thing now","21","0.3792963188936343","Installation and shell commands","Deployment"
"https://github.com/Trusted-AI/AIF360","817181963","issue_comment","https://github.com/Trusted-AI/AIF360/issues/236#issuecomment-817181963","You have a few options. You can simply drop the records associated with South and East, and only compute the group fairness metrics between North and West. Or if you have reason to believe that for some subject matter reason that South and/or East are privileged or unprivileged, you can put them together with North and/or West.","26","0.4920173160173161","Bias mitigation methodology","Design"
"https://github.com/Trusted-AI/AIF360","814805033","issue_comment","https://github.com/Trusted-AI/AIF360/issues/235#issuecomment-814805033","You need protected attributes for estimating the amount of unwanted bias with respect to outcomes. So this is not possible.","22","0.7171395483497395","Bias mitigation methodology","Design"
"https://github.com/Trusted-AI/AIF360","817004903","issue_comment","https://github.com/Trusted-AI/AIF360/issues/235#issuecomment-817004903","Thank You!","0","0.0303030303030302","Dataset usage","Requirement Analysis"
"https://github.com/Trusted-AI/AIF360","814805855","issue_comment","https://github.com/Trusted-AI/AIF360/issues/233#issuecomment-814805855","If you have tabular meta data along with the image datasets, you can use AIF360 with the tabular meta data. Directly using image data is not possible.","27","0.3530696009082175","Troubleshooting","Maintenance"
"https://github.com/Trusted-AI/AIF360","795774415","issue_comment","https://github.com/Trusted-AI/AIF360/issues/231#issuecomment-795774415","This appears to be specific to scikit-learn==0.24. Using scikit-learn<0.24 might resolve this.

CC: @nrkarthikeyan @hoffmansc ","32","0.4490456163054028","Dependency and Release","Deployment"
"https://github.com/Trusted-AI/AIF360","796002432","issue_comment","https://github.com/Trusted-AI/AIF360/issues/231#issuecomment-796002432","I couldn't replicate with AIF360 v0.4.0. Does that fix it?","26","0.4039048200122027","Bias mitigation methodology","Design"
"https://github.com/Trusted-AI/AIF360","811752451","issue_comment","https://github.com/Trusted-AI/AIF360/issues/231#issuecomment-811752451","Yes, it does work well now.","0","0.0303030303030302","Dataset usage","Requirement Analysis"
"https://github.com/Trusted-AI/AIF360","778336779","issue_comment","https://github.com/Trusted-AI/AIF360/issues/228#issuecomment-778336779","related to #74 and #85?","14","0.1507849580138735","Documentation","Development"
"https://github.com/Trusted-AI/AIF360","778924481","issue_comment","https://github.com/Trusted-AI/AIF360/issues/228#issuecomment-778924481","@nrkarthikeyan  Really appreciate your help!","31","0.2833150784958013","Testing","Maintenance"
"https://github.com/Trusted-AI/AIF360","779082310","issue_comment","https://github.com/Trusted-AI/AIF360/issues/228#issuecomment-779082310","@nrkarthikeyan I adopted the method you recommended but still resulting in large memory consumption in the repeated running. I found that the problem caused by function ' ClassificationMetric()', has possible solution schemes? Looking forward to your helpful reply.","7","0.4571863390179489","Opinion","Requirement Analysis"
"https://github.com/Trusted-AI/AIF360","779147795","issue_comment","https://github.com/Trusted-AI/AIF360/issues/228#issuecomment-779147795","I've solved the problem. The problem was caused by the code, 
`self.dataset = dataset
self.classified_dataset = classified_dataset`
Just use these codes to replace the original initial codes:
`    def __init__(self, X_train,Y_train,pred ,cloums_name,protected_value,
                 unprivileged_groups=None, privileged_groups=None):
     
        X_train =X_train.reset_index(drop=True)
        Y_train = Y_train.reset_index(drop=True)
        pred = pred.reset_index(drop=True)
        
        x=pd.concat([X_train,Y_train], axis=1)
        dataset=BinaryLabelDataset(df = x,label_names = list(Y_train.columns) ,
                    protected_attribute_names=[list(X_train.columns)[cloums_name]],
                    unprivileged_protected_attributes=protected_value,
                    privileged_protected_attributes=abs(1-protected_value))
        del x
        x=pd.concat([X_train,pred], axis=1)
        
        
        classified_dataset =BinaryLabelDataset(df = x,label_names = list(Y_train.columns) ,
                    protected_attribute_names=[list(X_train.columns)[cloums_name]],
                    unprivileged_protected_attributes=protected_value,
                    privileged_protected_attributes=abs(1-protected_value))
        del x

        self.classified_dataset = classified_dataset
        self.dataset = dataset
        self.unprivileged_groups=None
        self.privileged_groups=None

        if isinstance(classified_dataset, BinaryLabelDataset) or isinstance(classified_dataset, MulticlassLabelDataset) :
            self.classified_dataset = classified_dataset
        else:
            raise TypeError(""'classified_dataset' should be a ""
                            ""BinaryLabelDataset or a MulticlassLabelDataset."")

        with self.dataset.temporarily_ignore('labels', 'scores'):
            if self.dataset != self.classified_dataset:
                raise ValueError(""The two datasets are expected to differ only ""
                                 ""in 'labels' or 'scores'."")`


","27","0.6151045964179973","Troubleshooting","Maintenance"
"https://github.com/Trusted-AI/AIF360","772628292","issue_comment","https://github.com/Trusted-AI/AIF360/issues/224#issuecomment-772628292","Can you explain a little more what you mean by that? What kind of fairness metric are you interested in?

Satisfying group fairness requires sensitive attributes to partition the samples into groups. Individual fairness would not require this but we don't currently have any algorithms in the toolbox that only target individual fairness.","6","0.5547875663855047","API expansion","Development"
"https://github.com/Trusted-AI/AIF360","767017624","issue_comment","https://github.com/Trusted-AI/AIF360/issues/222#issuecomment-767017624","Please see the [instructions](https://github.com/Trusted-AI/AIF360/tree/master/aif360/data/raw/meps) here to download the dataset. Due to licensing issues, we do not distribute the datasets with the AIF360 package, however, there is a script in that folder which automates the process if you accept the terms.","13","0.3578693509489357","Artifact generation and benchmarking","Deployment"
"https://github.com/Trusted-AI/AIF360","778415721","issue_comment","https://github.com/Trusted-AI/AIF360/issues/222#issuecomment-778415721","This is expected, so closing.
","24","0.2659352142110764","UI","Requirement Analysis"
"https://github.com/Trusted-AI/AIF360","767022333","issue_comment","https://github.com/Trusted-AI/AIF360/issues/221#issuecomment-767022333","You can use any of the functions of the form: `metric_fun(privileged)`, e.g., [`disparate_impact()`](https://github.com/Trusted-AI/AIF360/blob/master/aif360/metrics/binary_label_dataset_metric.py#L114) is just `self.ratio(self.base_rate)`. Many of these combinations already exist and are usually named like `[metric_fun]_difference()` and `[metric_fun]_ratio()` but you can also come up with novel ones if they make sense.","15","0.4411011523687577","Metrics operation","Validation"
"https://github.com/Trusted-AI/AIF360","788878161","issue_comment","https://github.com/Trusted-AI/AIF360/issues/221#issuecomment-788878161","Hi @hoffmansc I got it now. Thanks for the clarification!","31","0.2975444096133753","Testing","Maintenance"
"https://github.com/Trusted-AI/AIF360","724980256","issue_comment","https://github.com/Trusted-AI/AIF360/issues/213#issuecomment-724980256","The part of the code that needs attention is not present in this library. In fact, the code with pip install has Numba decorators in it, which is not the case in the code provided in this Github. Hence I am closing this issue for now. In either, case if anyone faces a similar issue as above, I have a solution for it and I will be happy to have a pull request to help resolve it. ","21","0.3277458297646722","Installation and shell commands","Deployment"
"https://github.com/Trusted-AI/AIF360","697134874","issue_comment","https://github.com/Trusted-AI/AIF360/issues/205#issuecomment-697134874","When I install from CRAN, I get the same error.","21","0.5515558267236121","Installation and shell commands","Deployment"
"https://github.com/Trusted-AI/AIF360","697750331","issue_comment","https://github.com/Trusted-AI/AIF360/issues/205#issuecomment-697750331","@FrieseWoudloper please provide more details. @gdequeiroz ^^^","31","0.7134532134532136","Testing","Maintenance"
"https://github.com/Trusted-AI/AIF360","698822048","issue_comment","https://github.com/Trusted-AI/AIF360/issues/205#issuecomment-698822048","Information about my configuration. Which additional information do you need?

R.Version()

$platform
[1] ""x86_64-w64-mingw32""
$arch
[1] ""x86_64""
$os
[1] ""mingw32""
$system
[1] ""x86_64, mingw32""
$status
[1] """"
$major
[1] ""3""
$minor
[1] ""6.3""
$year
[1] ""2020""
$month
[1] ""02""
$day
[1] ""29""
$`svn rev`
[1] ""77875""
$language
[1] ""R""
$version.string
[1] ""R version 3.6.3 (2020-02-29)""
$nickname
[1] ""Holding the Windsock""

packageVersion(""aif360"")
[1] ‘0.1.0’","21","0.7185779151772774","Installation and shell commands","Deployment"
"https://github.com/Trusted-AI/AIF360","698869678","issue_comment","https://github.com/Trusted-AI/AIF360/issues/205#issuecomment-698869678","Because I ran the install on a Windows 8.1 laptop with R version 3.6.3, I thought I'd give it a try on a Windows 10 laptop with the latest stable R version. I also decided to run R with administrative privileges.  I executed the following code:
```
install.packages('aif360')
library(aif360)
install_aif360()
```
When the system asked me to install Miniconda, I said 'Yes'.
I didn't get an error this time, only a warning telling me that I should add `%USERPROFILE%\AppData\Local\R-MINI~1\envs\R-RETI~1\Scripts` to my `PATH` variable, so I did.

Subsequently I executed `load_aif360()`. This also generated an error:
```
2020-09-25 12:57:11.356581: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudart64_100.dll'; dlerror: cudart64_100.dll not found
2020-09-25 12:57:11.357097: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
WARNING:root:No module named 'numba.decorators': LFR will be unavailable. To install, run:
pip install 'aif360[LFR]'
```
I ignored the warning and executed this line of code:
```
ad <- adult_dataset()
```
An error was returned:
```
Error in py_call_impl(callable, dots$args, dots$keywords) : SystemExit: 1 
3.
stop(structure(list(message = ""SystemExit: 1"", call = py_call_impl(callable, 
    dots$args, dots$keywords), cppstack = NULL), class = c(""Rcpp::exception"", 
""C++Error"", ""error"", ""condition""))) 
2.
datasets$AdultDataset() 
1.
adult_dataset() 
```
Please inform me on any addition information you need or solutions I should try.
","21","0.5652284034373587","Installation and shell commands","Deployment"
"https://github.com/Trusted-AI/AIF360","698875552","issue_comment","https://github.com/Trusted-AI/AIF360/issues/205#issuecomment-698875552","Executing `R.Version()` gave me some interesting information
```
$platform
[1] ""x86_64-w64-mingw32""

$arch
[1] ""x86_64""

$os
[1] ""mingw32""

$system
[1] ""x86_64, mingw32""

$status
[1] """"

$major
[1] ""4""

$minor
[1] ""0.2""

$year
[1] ""2020""

$month
[1] ""06""

$day
[1] ""22""

$`svn rev`
[1] ""78730""

$language
[1] ""R""

$version.string
[1] ""R version 4.0.2 (2020-06-22)""

$nickname
[1] ""Taking Off Again""

IOError: [Errno 2] No such file or directory: '%USERPROFILE%\\AppData\\Local\\r-miniconda\\envs\\r-reticulate\\lib\\site-packages\\aif360\\datasets\\..\\data\\raw\\adult\\adult.data'
To use this class, please download the following files:

	https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data
	https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test
	https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.names

and place them, as-is, in the folder:

	%USERPROFILE%\AppData\Local\r-miniconda\envs\r-reticulate\lib\site-packages\aif360\data\raw\adult

```

I downloaded the `adult.*` files and put them in the directory mentioned above. I executed the code again:
```
ad <- adult_dataset()
```
This time I only got this warning: 
```
WARNING:root:Missing Data: 3620 rows removed from AdultDataset.
```
So I guess it is working fine now. Could you confirm that?

Upgrading R on my Windows 7 laptop, and installing + running R with administrative privileges didn't solve the problem on my Windows 8.1 laptop. When I execute the following code:
```
library(aif360)
install_aif360()
```
I still get this error:
```
Error: Installing Python packages into a virtualenv is not supported on Windows
```
So maybe I should just accept that it is not going to work on my old laptop?","21","0.6969696969696971","Installation and shell commands","Deployment"
"https://github.com/Trusted-AI/AIF360","765157170","issue_comment","https://github.com/Trusted-AI/AIF360/issues/203#issuecomment-765157170","So far no news on this... I have been waiting for a while now.","18","0.2975444096133752","Troubleshooting","Maintenance"
"https://github.com/Trusted-AI/AIF360","767014345","issue_comment","https://github.com/Trusted-AI/AIF360/issues/203#issuecomment-767014345","If either of you would like to give it a shot, swapping the imports with `import tensorflow.compat.v1 as tf` will probably work for now if you're willing to test it.","27","0.4018529241459178","Troubleshooting","Maintenance"
"https://github.com/Trusted-AI/AIF360","767872061","issue_comment","https://github.com/Trusted-AI/AIF360/issues/203#issuecomment-767872061","Some of the functions, such as .fit(), won't work wit the compat version of TF. Is there any resolution for this?","32","0.3707538013587835","Dependency and Release","Deployment"
"https://github.com/Trusted-AI/AIF360","883375134","issue_comment","https://github.com/Trusted-AI/AIF360/issues/203#issuecomment-883375134","Hi, I can't run aif360 because of the incompatibility with tensorflow 2, is there any solution for that?
","21","0.7233262861169838","Installation and shell commands","Deployment"
"https://github.com/Trusted-AI/AIF360","687332708","issue_comment","https://github.com/Trusted-AI/AIF360/issues/200#issuecomment-687332708","Hi @Aloumora, could you please provide the steps you are following and the error in you are getting. ","0","0.2702297702297701","Dataset usage","Requirement Analysis"
"https://github.com/Trusted-AI/AIF360","699462880","issue_comment","https://github.com/Trusted-AI/AIF360/issues/200#issuecomment-699462880","I may have had the same problem as @Aloumora. This is the solution that worked for me:

Download these three files:
	https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data
	https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test
	https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.names

and place them in the folder:

	%USERPROFILE%\AppData\Local\r-miniconda\envs\r-reticulate\lib\site-packages\aif360\data\raw\adult","21","0.8419622231281423","Installation and shell commands","Deployment"
"https://github.com/Trusted-AI/AIF360","758148852","issue_comment","https://github.com/Trusted-AI/AIF360/issues/199#issuecomment-758148852","I have the same problem after installing aif360 using [all] command. Do you find any solution for this?","21","0.7233262861169838","Installation and shell commands","Deployment"
"https://github.com/Trusted-AI/AIF360","758194607","issue_comment","https://github.com/Trusted-AI/AIF360/issues/199#issuecomment-758194607","Did you clone the full repo from GitHub or just download the single demo notebook file? In the readme we recommend cloning and installing with `pip install -e '.[all]'` so the examples are included with all the necessary files. The examples aren't included in the package on PyPI.","21","0.6587394564198691","Installation and shell commands","Deployment"
"https://github.com/Trusted-AI/AIF360","758228872","issue_comment","https://github.com/Trusted-AI/AIF360/issues/199#issuecomment-758228872","I installed it using same command (pip install 'aif360[all]'). I also downloaded the repo folder for aif360 and put it in the same directory but the error ""No module named ""common_utils"" is still there. 

Thanks, ","21","0.8101195440644979","Installation and shell commands","Deployment"
"https://github.com/Trusted-AI/AIF360","760530930","issue_comment","https://github.com/Trusted-AI/AIF360/issues/199#issuecomment-760530930","I also have the same problem although I did the setup as you suggested in the README.  ","4","0.2299367299367299","Installation and shell commands","Deployment"
"https://github.com/Trusted-AI/AIF360","809603640","issue_comment","https://github.com/Trusted-AI/AIF360/issues/199#issuecomment-809603640","Has anyone found a solution to this? I'm still getting the same error.","11","0.1292041292041292","Troubleshooting","Maintenance"
"https://github.com/Trusted-AI/AIF360","883412605","issue_comment","https://github.com/Trusted-AI/AIF360/issues/199#issuecomment-883412605","I have the same issue, I've done %pip install common_utils in the notebook, but I still get the error :
 ModuleNotFoundError: No module named 'common_utils'
Is there any solution? ","21","0.8584046745481723","Installation and shell commands","Deployment"
"https://github.com/Trusted-AI/AIF360","917407290","issue_comment","https://github.com/Trusted-AI/AIF360/issues/199#issuecomment-917407290","I have the same issue, I've done %pip install common_utils in the notebook, but I still get the error :
ModuleNotFoundError: No module named 'common_utils'
Is there any solution?","21","0.8584046745481723","Installation and shell commands","Deployment"
"https://github.com/Trusted-AI/AIF360","665967863","issue_comment","https://github.com/Trusted-AI/AIF360/issues/194#issuecomment-665967863","Hi @jakwisn, thank you for the message. The R version is very new, less than 2 months old. We appreciate external contributions so if this is something you are interested in, we can coordinate the contribution. Let us know. Cheers!","20","0.4381977671451357","Opinion","Requirement Analysis"
"https://github.com/Trusted-AI/AIF360","644433545","issue_comment","https://github.com/Trusted-AI/AIF360/issues/185#issuecomment-644433545","@hoffmansc I faced the same issue.

Either we need to pin this to 0.48.0 or update code to `numba.core.decorators`","21","0.4163495419309373","Installation and shell commands","Deployment"
"https://github.com/Trusted-AI/AIF360","817240188","issue_comment","https://github.com/Trusted-AI/AIF360/issues/185#issuecomment-817240188","@hoffmansc Getting the same error. 

!pip install numba==0.48
from numba.decorators import jit
import aif360.algorithms.preprocessing
from aif360.algorithms.preprocessing import Reweighing

---------------------------------------------------------------------------
ModuleNotFoundError                       Traceback (most recent call last)
<ipython-input-3-3b650a37b035> in <module>
      1 #!pip install numba==0.48
----> 2 from numba.decorators import jit
      3 import aif360.algorithms.preprocessing
      4 from aif360.algorithms.preprocessing import Reweighing

ModuleNotFoundError: No module named 'numba.decorators'

**got the below error when removing from numba.decorators import jit**

---------------------------------------------------------------------------
ModuleNotFoundError                       Traceback (most recent call last)
<ipython-input-13-f9b7a2091049> in <module>
      1 get_ipython().system('pip install numba==0.48')
----> 2 import aif360.algorithms.preprocessing
      3 from aif360.algorithms.preprocessing import Reweighing

~\anaconda3\lib\site-packages\aif360\algorithms\preprocessing\__init__.py in <module>
      1 from aif360.algorithms.preprocessing.disparate_impact_remover import DisparateImpactRemover
----> 2 from aif360.algorithms.preprocessing.lfr import LFR
      3 from aif360.algorithms.preprocessing.optim_preproc import OptimPreproc
      4 from aif360.algorithms.preprocessing.reweighing import Reweighing

~\anaconda3\lib\site-packages\aif360\algorithms\preprocessing\lfr.py in <module>
      3 
      4 from aif360.algorithms import Transformer
----> 5 from aif360.algorithms.preprocessing.lfr_helpers import helpers as lfr_helpers
      6 
      7 

~\anaconda3\lib\site-packages\aif360\algorithms\preprocessing\lfr_helpers\helpers.py in <module>
      1 # Based on code from https://github.com/zjelveh/learning-fair-representations
----> 2 from numba.decorators import jit
      3 import numpy as np
      4 
      5 @jit

ModuleNotFoundError: No module named 'numba.decorators'","27","0.8818038201924457","Troubleshooting","Maintenance"
"https://github.com/Trusted-AI/AIF360","817839959","issue_comment","https://github.com/Trusted-AI/AIF360/issues/185#issuecomment-817839959","@dvd-king what version of AIF360 are you using? Are you able to upgrade to the latest version, v0.4.0? That removes the numba dependency entirely.","21","0.7546065845777657","Installation and shell commands","Deployment"
"https://github.com/Trusted-AI/AIF360","637755566","issue_comment","https://github.com/Trusted-AI/AIF360/issues/177#issuecomment-637755566","Could you make a PR to the master branch with this change? Thanks!","32","0.7335997335997336","Dependency and Release","Deployment"
"https://github.com/Trusted-AI/AIF360","637789532","issue_comment","https://github.com/Trusted-AI/AIF360/issues/177#issuecomment-637789532","Sure","0","0.0303030303030302","Dataset usage","Requirement Analysis"
"https://github.com/Trusted-AI/AIF360","624696208","issue_comment","https://github.com/Trusted-AI/AIF360/issues/173#issuecomment-624696208","In the LFR demo notebook is demonstrating LFR as a preprocessing algorithm and geared towards improving `consistency` a measure of individual fairness. 

The following default parameters can be tuned to improve other metrics.
```
k (int, optional): Number of prototypes.
Ax (float, optional): Input reconstruction quality term weight.
Az (float, optional): Fairness constraint term weight.
Ay (float, optional): Output prediction error
```

For usage questions, please reach out to the slack channel.

Feel free to reopen the issue or create a new issue if you find any bugs in the implementation. 
","6","0.3432546701029192","API expansion","Development"
"https://github.com/Trusted-AI/AIF360","623208450","issue_comment","https://github.com/Trusted-AI/AIF360/issues/172#issuecomment-623208450","Hi, The code for Prejudice Remover is the original (Kamishima's) code with a wrapper around it. However, I will take a look to make sure nothing untoward is happening. Can you please provide your code using the Adult dataset?","0","0.5250398724082936","Dataset usage","Requirement Analysis"
"https://github.com/Trusted-AI/AIF360","623228763","issue_comment","https://github.com/Trusted-AI/AIF360/issues/172#issuecomment-623228763","Thank you for following up. Here's my code:

```python
from aif360.datasets import AdultDataset
from aif360.algorithms.preprocessing.optim_preproc_helpers.data_preproc_functions import load_preproc_data_adult
from aif360.metrics import BinaryLabelDatasetMetric, ClassificationMetric
from aif360.algorithms.inprocessing.prejudice_remover import PrejudiceRemover

from sklearn.metrics import accuracy_score

dataset_orig = load_preproc_data_adult()

privileged_groups = [{'race': 1}]
unprivileged_groups = [{'race': 0}]

dataset_orig_train, dataset_orig_test = dataset_orig.split([0.7], shuffle=True)

etas = [1,15,50,100]

for eta in etas:
    debiased_model = PrejudiceRemover(eta=eta, sensitive_attr='race', class_attr=dataset_orig_train.label_names[0])        
    model = debiased_model.fit(dataset_orig_train)
    pred = model.predict(dataset_orig_test)

    metric = ClassificationMetric(dataset_orig_test, pred, unprivileged_groups=unprivileged_groups, privileged_groups=privileged_groups)

    print('Accuracy:', accuracy_score(pred.labels, dataset_orig_test.labels))
    print('Disparity impact:', metric.disparate_impact())
    print(""Statistical Parity Difference = %f"" % metric.statistical_parity_difference())
    print(""Equal opportunity difference = %f"" % metric.equal_opportunity_difference())
    print(""Average odds difference = %f"" % metric.average_odds_difference())
    print(""Theil_index = %f"" % metric.theil_index())
    print()
```

and here's my results when running it:
```
Accuracy: 0.7908960622398143
Disparity impact: 0.3745778126422821
Statistical Parity Difference = -0.124567
Equal opportunity difference = -0.198108
Average odds difference = -0.133181
Theil_index = 0.172030

Accuracy: 0.7908960622398143
Disparity impact: 0.3745778126422821
Statistical Parity Difference = -0.124567
Equal opportunity difference = -0.198108
Average odds difference = -0.133181
Theil_index = 0.172030

Accuracy: 0.7908960622398143
Disparity impact: 0.3745778126422821
Statistical Parity Difference = -0.124567
Equal opportunity difference = -0.198108
Average odds difference = -0.133181
Theil_index = 0.172030

Accuracy: 0.7908960622398143
Disparity impact: 0.3745778126422821
Statistical Parity Difference = -0.124567
Equal opportunity difference = -0.198108
Average odds difference = -0.133181
Theil_index = 0.172030
```","27","0.4381209304341231","Troubleshooting","Maintenance"
"https://github.com/Trusted-AI/AIF360","625285491","issue_comment","https://github.com/Trusted-AI/AIF360/issues/172#issuecomment-625285491","Hi
I think the results you are getting are an artifact of the dataset itself. The Adult dataset that is produced by load_preproc_data_adult() has only two discretized features (plus the sensitive features). I suspect eta has a minimal impact on the model and so PR is probably not a good model for it.
If you use the full Adult dataset (using dataset_orig = AdultDataset()), you will see that eta has an effect on fairness pretty quickly. Be sure to scale the features since PR uses regularized LR.
For example,
scaler = StandardScaler()
dataset_orig_train.features = scaler.fit_transform(dataset_orig_train.features)
dataset_orig_test.features = scaler.transform(dataset_orig_test.features)
...","27","0.4410240571055973","Troubleshooting","Maintenance"
"https://github.com/Trusted-AI/AIF360","627282713","issue_comment","https://github.com/Trusted-AI/AIF360/issues/172#issuecomment-627282713","I see. Thank you very much for the explanation! I'm closing this issue.","8","0.1578198088265202","Understanding Fairness Definition","Requirement Analysis"
"https://github.com/Trusted-AI/AIF360","621252789","issue_comment","https://github.com/Trusted-AI/AIF360/issues/171#issuecomment-621252789","Could you please provide a code snippet of the issues or some more details about your use case.  Are you creating multiple instances of the AdversarialDebiasing in the same session? Did the change you are suggesting fix the issue?","31","0.2021889036502816","Testing","Maintenance"
"https://github.com/Trusted-AI/AIF360","621552855","issue_comment","https://github.com/Trusted-AI/AIF360/issues/171#issuecomment-621552855","Hi,

Thanks for looking into this, Prasanna. I hope you can help me out if I have missed some steps.

This was the code I used: 

```
privileged_groups, unprivileged_groups = get_attributes(data_train, selected_attr=[priv_category])
sess = tf.Session()

debiased_model = AdversarialDebiasing(privileged_groups = privileged_groups,
                          unprivileged_groups = unprivileged_groups,
                          scope_name='debiased_classifier',
                          num_epochs=10,
                          debias=True, sess=sess)

debiased_model.fit(data_train)

data_pred = debiased_model.predict(data_test)
```

Running above code once returns fine but running it a second time, say for example just to change parameters or doing a `.fit()` on a different dataset returns:

```
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-20-69a62ef0fae3> in <module>
      8                           debias=True, sess=sess)
      9 
---> 10 debiased_model.fit(data_orig_sex_train)
     11 
     12 # fair = get_fair_metrics_and_plot(data_orig_sex_test, debiased_model, plot=False, model_aif=True)

~/anaconda3/lib/python3.7/site-packages/aif360/algorithms/transformer.py in wrapper(self, *args, **kwargs)
     25     @wraps(func)
     26     def wrapper(self, *args, **kwargs):
---> 27         new_dataset = func(self, *args, **kwargs)
     28         if isinstance(new_dataset, Dataset):
     29             new_dataset.metadata = new_dataset.metadata.copy()

~/anaconda3/lib/python3.7/site-packages/aif360/algorithms/inprocessing/adversarial_debiasing.py in fit(self, dataset)
    141 
    142             # Obtain classifier predictions and classifier loss
--> 143             self.pred_labels, pred_logits = self._classifier_model(self.features_ph, self.features_dim, self.keep_prob)
    144             pred_labels_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=self.true_labels_ph, logits=pred_logits))
    145 

~/anaconda3/lib/python3.7/site-packages/aif360/algorithms/inprocessing/adversarial_debiasing.py in _classifier_model(self, features, features_dim, keep_prob)
     81         with tf.variable_scope(""classifier_model""):
     82             W1 = tf.get_variable('W1', [features_dim, self.classifier_num_hidden_units],
---> 83                                   initializer=tf.contrib.layers.xavier_initializer())
     84             b1 = tf.Variable(tf.zeros(shape=[self.classifier_num_hidden_units]), name='b1')
     85 

~/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/variable_scope.py in get_variable(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)
   1494       constraint=constraint,
   1495       synchronization=synchronization,
-> 1496       aggregation=aggregation)
   1497 
   1498 

~/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/variable_scope.py in get_variable(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)
   1237           constraint=constraint,
   1238           synchronization=synchronization,
-> 1239           aggregation=aggregation)
   1240 
   1241   def _get_partitioned_variable(self,

~/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/variable_scope.py in get_variable(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)
    560           constraint=constraint,
    561           synchronization=synchronization,
--> 562           aggregation=aggregation)
    563 
    564   def _get_partitioned_variable(self,

~/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/variable_scope.py in _true_getter(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, constraint, synchronization, aggregation)
    512           constraint=constraint,
    513           synchronization=synchronization,
--> 514           aggregation=aggregation)
    515 
    516     synchronization, aggregation, trainable = (

~/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/variable_scope.py in _get_single_variable(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource, constraint, synchronization, aggregation)
    862         tb = [x for x in tb if ""tensorflow/python"" not in x[0]][:5]
    863         raise ValueError(""%s Originally defined at:\n\n%s"" %
--> 864                          (err_msg, """".join(traceback.format_list(tb))))
    865       found_var = self._vars[name]
    866       if not shape.is_compatible_with(found_var.get_shape()):

ValueError: Variable debiased_classifier/classifier_model/W1 already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:

  File ""/Users/xmpuspus/anaconda3/lib/python3.7/site-packages/aif360/algorithms/inprocessing/adversarial_debiasing.py"", line 83, in _classifier_model
    initializer=tf.contrib.layers.xavier_initializer())
  File ""/Users/xmpuspus/anaconda3/lib/python3.7/site-packages/aif360/algorithms/inprocessing/adversarial_debiasing.py"", line 143, in fit
    self.pred_labels, pred_logits = self._classifier_model(self.features_ph, self.features_dim, self.keep_prob)
  File ""/Users/xmpuspus/anaconda3/lib/python3.7/site-packages/aif360/algorithms/transformer.py"", line 27, in wrapper
    new_dataset = func(self, *args, **kwargs)
  File ""<ipython-input-19-69a62ef0fae3>"", line 10, in <module>
    debiased_model.fit(data_orig_sex_train)
  File ""/Users/xmpuspus/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py"", line 3326, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
```

Is there anything wrong with how I run things? I haven't tried a proper PR and fix on the code but quick inspection tells me my suggestion in the first thread might fix things but I defer to your recommmendations.","22","0.8360398437886202","Bias mitigation methodology","Design"
"https://github.com/Trusted-AI/AIF360","624704225","issue_comment","https://github.com/Trusted-AI/AIF360/issues/171#issuecomment-624704225","Yes, this is a valid point. `fit()` here was designed to be called once on the whole dataset.

Changing the `scope_name` for each instance of `AdversarialDebiasing ` should fix this.

Your use case is to call `fit()` on different chunks of data separately? And perhaps update the model with additional datasets?

The fix you suggested should work. Feel free to submit a PR. Thanks!","22","0.3034458874458876","Bias mitigation methodology","Design"
"https://github.com/Trusted-AI/AIF360","910804439","issue_comment","https://github.com/Trusted-AI/AIF360/issues/171#issuecomment-910804439","Hi Xavier, 

I've got the same problem as you. Tried your fix but it didn't work for me - my version of aif360 seems different to yours (v0.4.0) because the initializer parameter settings are different from yours. I tried setting reuse = True and reuse=tf.AUTO_REUSE (see bolded) in the scope but still getting the same error. Have you got any ideas for me please?

Thanks a lot



`with tf.variable_scope(""classifier_model"", reuse = True):`

            W1 = tf.get_variable('W1', [features_dim, self.classifier_num_hidden_units],
                                  initializer=tf.initializers.glorot_uniform(seed=self.seed1))

            b1 = tf.Variable(tf.zeros(shape=[self.classifier_num_hidden_units]), name='b1')

            h1 = tf.nn.relu(tf.matmul(features, W1) + b1)

            h1 = tf.nn.dropout(h1, keep_prob=keep_prob, seed=self.seed2)

            W2 = tf.get_variable('W2', [self.classifier_num_hidden_units, 1],
                                 initializer=tf.initializers.glorot_uniform(seed=self.seed3))
            b2 = tf.Variable(tf.zeros(shape=[1]), name='b2')

            pred_logit = tf.matmul(h1, W2) + b2
            pred_label = tf.sigmoid(pred_logit)
","22","0.7845444263282365","Bias mitigation methodology","Design"
"https://github.com/Trusted-AI/AIF360","620836199","issue_comment","https://github.com/Trusted-AI/AIF360/issues/169#issuecomment-620836199","Yes, this is intentional. A couple dependencies (mainly cvxpy) can be very tricky to install, especially on Windows. To lower the barrier to entry, we made these ""extra"" dependencies optional.

With the latest release, you can install everything in one line:
```sh
pip install 'aif360[all]>=0.3.0rc0'
```","21","0.4508356763511404","Installation and shell commands","Deployment"
"https://github.com/Trusted-AI/AIF360","613448235","issue_comment","https://github.com/Trusted-AI/AIF360/issues/167#issuecomment-613448235","Duplicate of #162 and #109. The fix is in master as well as the latest rc version, v0.3.0rc0. You can download this from PyPI like: `pip install aif360==0.3.0rc0`","32","0.5377768718575894","Dependency and Release","Deployment"
"https://github.com/Trusted-AI/AIF360","616845817","issue_comment","https://github.com/Trusted-AI/AIF360/issues/164#issuecomment-616845817","You're correct that there is no inverse_transform function available. However, the original data (`train` or `test` in the demo) is unchanged by the `fit_transform()` function so you can still use that. Because of how the algorithm works, I don't think this would even be possible since there may be a many-to-one mapping in `transform()`.","27","0.3685463472697514","Troubleshooting","Maintenance"
"https://github.com/Trusted-AI/AIF360","616838772","issue_comment","https://github.com/Trusted-AI/AIF360/issues/163#issuecomment-616838772","GerryFairClassifier is only available in release v0.3.0rc0 currently or on the master branch","32","0.6210007047216348","Dependency and Release","Deployment"
"https://github.com/Trusted-AI/AIF360","604021801","issue_comment","https://github.com/Trusted-AI/AIF360/issues/162#issuecomment-604021801","Hi @srnghn , the most recent master version (https://github.com/IBM/AIF360/blob/b3f589d3e87afc6ceb19646f426f09e695f81ea6/aif360/datasets/standard_dataset.py#L121)  seems to have the fix for this. Can you please use the latest master and retry?","32","0.6022328548644339","Dependency and Release","Deployment"
"https://github.com/Trusted-AI/AIF360","604033585","issue_comment","https://github.com/Trusted-AI/AIF360/issues/162#issuecomment-604033585","Thanks @nrkarthikeyan, you're right. For some reason my build wasn't being recognised which is why I referred to aif360 version `0.2.3`. I'm now using `0.2.2` from build and it runs as expected. When will this version be available with pip?","32","0.4012146517365723","Dependency and Release","Deployment"
"https://github.com/Trusted-AI/AIF360","604045464","issue_comment","https://github.com/Trusted-AI/AIF360/issues/162#issuecomment-604045464","ok thanks @srnghn, I will close this issue now, and if you can reproduce the build error, please open a new one.","32","0.272238514173998","Dependency and Release","Deployment"
"https://github.com/Trusted-AI/AIF360","604056497","issue_comment","https://github.com/Trusted-AI/AIF360/issues/162#issuecomment-604056497","> When will this version be available with pip?

Soon! This really should have been included in 0.2.3 but that was an oversight, I guess. This will be in 0.3.0 which we're still finalizing, hence the delay. Sorry.","21","0.4359969804809661","Installation and shell commands","Deployment"
"https://github.com/Trusted-AI/AIF360","604494511","issue_comment","https://github.com/Trusted-AI/AIF360/issues/162#issuecomment-604494511","Thanks @srnghn, perhaps temporarily use the patch if the error exists. ","21","0.3992952783650458","Installation and shell commands","Deployment"
"https://github.com/Trusted-AI/AIF360","616838076","issue_comment","https://github.com/Trusted-AI/AIF360/issues/160#issuecomment-616838076","Regarding the first question, we only have an in-processing algorithm available for rich subgroup fairness.

As for the second question, there are a few comparison papers. For example, this one from [fairness-comparison](https://arxiv.org/pdf/1802.04422.pdf). [Our paper](https://arxiv.org/pdf/1810.01943.pdf) also contains some results comparing the algorithms we have in AIF360.

You might have more luck with general informational questions by posting in our [Slack](https://aif360.slack.com/).","6","0.2734613972837392","API expansion","Development"
"https://github.com/Trusted-AI/AIF360","595815101","issue_comment","https://github.com/Trusted-AI/AIF360/issues/156#issuecomment-595815101","Thanks for letting us know.   As a security measure, the link expires periodically.  We will update the link on the website shortly.  In the meantime, here is the new link to subscribe

https://join.slack.com/t/aif360/shared_invite/zt-5hfvuafo-X0~g6tgJQ~7tIAT~S294TQ","14","0.2822561553030304","Documentation","Development"
"https://github.com/Trusted-AI/AIF360","595853330","issue_comment","https://github.com/Trusted-AI/AIF360/issues/156#issuecomment-595853330","This is now fixed.  Thanks!","9","0.1507849580138737","Feature engineering methodology","Design"
"https://github.com/Trusted-AI/AIF360","596005883","issue_comment","https://github.com/Trusted-AI/AIF360/issues/154#issuecomment-596005883","Will this make it easier/unnecessary to convert back and forth between Pandas DataFrames and Binary Label Datasets, for example? I've been having issues with Reweighing, as AIF360 tends to only work with numerical data but does not provide instructions for dummification while storing the metadata mappings to de-dummify later on.","17","0.4307725138711052","Troubleshooting","Maintenance"
"https://github.com/Trusted-AI/AIF360","596104854","issue_comment","https://github.com/Trusted-AI/AIF360/issues/154#issuecomment-596104854","Yes, this will allow DataFrames to be used directly with the algorithms. Reweighing is already implemented so you can try it out if you're comfortable using the master branch from GitHub. It should be released in the latest stable version soon as well.

Do you mind explaining exactly what issues you were facing? Was it with `convert_to_dataframe()`?","32","0.3488653148919026","Dependency and Release","Deployment"
"https://github.com/Trusted-AI/AIF360","596618673","issue_comment","https://github.com/Trusted-AI/AIF360/issues/154#issuecomment-596618673","Hi @hoffmansc, yes, I've been having trouble understanding how to use convert_to_dataframe() after creating my own BinaryLabelDataset. Perhaps it's my own fault, but I can't find the documentation that describes how to dummify the data in a way that retains the mappings so it can be reversed after using a PreProcessing tool such as Reweighing. ","31","0.2414831545266327","Testing","Maintenance"
"https://github.com/Trusted-AI/AIF360","596740349","issue_comment","https://github.com/Trusted-AI/AIF360/issues/154#issuecomment-596740349","@jimbudarz, if you encode your categorical data with `pd.get_dummies()`, or use `StandardDataset`, you will end up with `feature_names` that look like, e.g., `[..., native-country=United-States, native-country=Vietnam, native-country=Yugoslavia, ...]`. Then, if you do `convert_to_dataframe(de_dummy_code=True)`, you will get a DataFrame that looks something like:
```
  ... native-country
0 ... United-States
1 ... United-States
2 ... Vietnam
...
```
with the columns magically mapped from one-hot to categories.

You can also include maps for the labels and protected attributes manually (since these are encoded differently) by supplying them when creating the BinaryLabelDataset (note: `protected_attribute_maps` should be in the same order as `protected_attribute_names`):
```
metadata = {
    'label_maps': [{1.0: '>50K', 0.0: '<=50K'}],
    'protected_attribute_maps': [{1.0: 'White', 0.0: 'Non-white'},
                                 {1.0: 'Male', 0.0: 'Female'}]
}
BinaryLabelDataset(..., metadata=metadata)
```
otherwise they will just be 0/1 which is probably also fine.","17","0.2905770029057701","Troubleshooting","Maintenance"
"https://github.com/Trusted-AI/AIF360","596784733","issue_comment","https://github.com/Trusted-AI/AIF360/issues/154#issuecomment-596784733","Thanks for the help- this led me to a resolution: Pandas' get_dummies() uses the separator prefix_sep=""_"" by default, and convert_to_dataframe() uses sep=""="" by default. 

It might be helpful to explain what sep attribute does in the https://aif360.readthedocs.io/en/latest/modules/datasets.html documentation.","7","0.4225561672370183","Opinion","Requirement Analysis"
"https://github.com/Trusted-AI/AIF360","596817990","issue_comment","https://github.com/Trusted-AI/AIF360/issues/154#issuecomment-596817990","> It might be helpful to explain what sep attribute does in the https://aif360.readthedocs.io/en/latest/modules/datasets.html documentation.

That's a good point. Would you be willing to write a quick PR to that effect?","7","0.2966468559688899","Opinion","Requirement Analysis"
"https://github.com/Trusted-AI/AIF360","596830720","issue_comment","https://github.com/Trusted-AI/AIF360/issues/154#issuecomment-596830720","I've gladly submitted a PR. 

It looks like reversing dummy-encoding could soon become a part of pandas itself, which AIF360 may be able to leverage for scikit-learn compatibility: https://github.com/pandas-dev/pandas/pull/31795","32","0.4012146517365722","Dependency and Release","Deployment"
"https://github.com/Trusted-AI/AIF360","886219495","issue_comment","https://github.com/Trusted-AI/AIF360/issues/154#issuecomment-886219495","Is `convert_to_dataframe()` supposed to to return the original DataFrame? I am using `Reweighing` and get back a `BinaryLabelDataset` which I would like to convert back to a DataFrame(with the weights applied).","17","0.4111624053030304","Troubleshooting","Maintenance"
"https://github.com/Trusted-AI/AIF360","592177201","issue_comment","https://github.com/Trusted-AI/AIF360/issues/150#issuecomment-592177201","@hoffmansc Do you skeleton for this conversion? ","31","0.4531218222493391","Testing","Maintenance"
"https://github.com/Trusted-AI/AIF360","596105675","issue_comment","https://github.com/Trusted-AI/AIF360/issues/150#issuecomment-596105675","I don't really have skeletons for these but I thought examples would be enough:

For the MEPS dataset, a good place to start might be the [COMPAS port](https://github.com/IBM/AIF360/blob/master/aif360/sklearn/datasets/compas_dataset.py).

For differential fairness, you could look at any number of the [already ported metrics](https://github.com/IBM/AIF360/blob/master/aif360/sklearn/metrics/metrics.py).","0","0.4064498192938557","Dataset usage","Requirement Analysis"
"https://github.com/Trusted-AI/AIF360","597786870","issue_comment","https://github.com/Trusted-AI/AIF360/issues/150#issuecomment-597786870","@hoffmansc I will take a look at it this week.","31","0.2659352142110763","Testing","Maintenance"
"https://github.com/Trusted-AI/AIF360","594005460","issue_comment","https://github.com/Trusted-AI/AIF360/issues/149#issuecomment-594005460","It is even worse. Disparate Impact Remover does not even work with a preprocessed dataset, so you can not compare the different algorithms on the same data.","6","0.3434665122563214","API expansion","Development"
"https://github.com/Trusted-AI/AIF360","594201677","issue_comment","https://github.com/Trusted-AI/AIF360/issues/149#issuecomment-594201677","Well, I got disparate impact remover working with German credit dataset (no pre-processing). My only concern is that if some techniques require/or are given pre-processed data, then we can't compare across them which is bad.","22","0.3610321969696969","Bias mitigation methodology","Design"
"https://github.com/Trusted-AI/AIF360","594228673","issue_comment","https://github.com/Trusted-AI/AIF360/issues/149#issuecomment-594228673","Literally in the boat haha. My issue is that some others algos do not like the unprocessed, whole datasets (likely cause it is one hot encoded and too large). I had issues on the Learning Fair Representations, where I tried the complete unprocessed datasets. Instead of giving me actual results it either made the dataset consits solely of 0s or every row was the same value.","6","0.4372705679118981","API expansion","Development"
"https://github.com/Trusted-AI/AIF360","594767800","issue_comment","https://github.com/Trusted-AI/AIF360/issues/149#issuecomment-594767800","I believe the first comment by @vsahil is based on the notebooks in the examples folder. The reason why we choose to this custom preprocessing for optimized preprocessing is because we want small number of features and small number of categories per feature. It just gives more compact datasets (in terms of features). This is mainly for computational/statistical reasons since the probability estimates used in optimized pre-processing will not be good otherwise. For the other notebooks, sometimes we choose to use smaller datasets to limit runtime, else travis will not be able to execute them fast enough during testing.

Also, generally speaking, the notebooks are just an example of how we can implement various bias mitigation methods, and are by no means the only way to do things. Other approaches of loading data can be definitely tried out. However, like in all ML/data science approaches, choosing the right kind of data pre-processing for the algorithm is question may take a few trials of experimentation.

Hope this helps. If you have specific problems, please provide concrete reproducible examples and raise issues.","0","0.4071831334401165","Dataset usage","Requirement Analysis"
"https://github.com/Trusted-AI/AIF360","636553121","issue_comment","https://github.com/Trusted-AI/AIF360/issues/149#issuecomment-636553121","Hi I have a simple question I am using GermanDataset
GermanDataSet is a class?? Where can I see the parameteres that it receive?
Can i get the dataset German in a dataframe using this package and extract values of 'age' column
I am trying it but it didnt return nothing
thank you","22","0.351525636992765","Bias mitigation methodology","Design"
"https://github.com/Trusted-AI/AIF360","583065103","issue_comment","https://github.com/Trusted-AI/AIF360/issues/144#issuecomment-583065103","The StructuredDataset class has a _scores_ field which gets populated by many models. See https://aif360.readthedocs.io/en/latest/modules/datasets.html#structured-dataset for more details.","27","0.4018529241459178","Troubleshooting","Maintenance"
"https://github.com/Trusted-AI/AIF360","589808638","issue_comment","https://github.com/Trusted-AI/AIF360/issues/144#issuecomment-589808638","I'm still not seeing a way to do this for adversarial debiasing specifically, and I'd be quite thankful for guidance. For example, if I run the following, all I get is a 0/1 labels instead of probabilities.

dataset_nodebiasing_test = plain_model.predict (dataset_orig_test)
scores = dataset_nodebiasing_test.scores
scores = pd.DataFrame (scores)
scores","22","0.3704935064935065","Bias mitigation methodology","Design"
"https://github.com/Trusted-AI/AIF360","589849723","issue_comment","https://github.com/Trusted-AI/AIF360/issues/144#issuecomment-589849723","It is possible that the scores are saturated to 0 or 1 during training, in general, `dataset_nodebiasing_test.scores` should return non-thresholded scores.","18","0.2934609250398725","Troubleshooting","Maintenance"
"https://github.com/Trusted-AI/AIF360","569467001","issue_comment","https://github.com/Trusted-AI/AIF360/issues/136#issuecomment-569467001","Hi @bhavyaghai , there is no method currently to do this. But it should be easy to implement. Basically, we have to create a new dataset by subsetting the fields features, labels, scores, protected_attributes, instance_names and instance_weights from the original dataset.  The rest of the fields can be just copied over. We may have to change the metadata also to denote the subsetting.","30","0.4459220779220782","Troubleshooting","Maintenance"
"https://github.com/Trusted-AI/AIF360","569739251","issue_comment","https://github.com/Trusted-AI/AIF360/issues/136#issuecomment-569739251","Can you give me any timeframe on when can I expect this feature?
My current research is dependent on this feature.
","11","0.1683072765875314","Troubleshooting","Maintenance"
"https://github.com/Trusted-AI/AIF360","569773256","issue_comment","https://github.com/Trusted-AI/AIF360/issues/136#issuecomment-569773256","@bhavyaghai, please feel free to implement it yourself and contribute it to the community.  I am sure many people will benefit from it.","20","0.5770121598147078","Opinion","Requirement Analysis"
"https://github.com/Trusted-AI/AIF360","578393753","issue_comment","https://github.com/Trusted-AI/AIF360/issues/136#issuecomment-578393753","@krvarshney @nrkarthikeyan Thanks for your support!
I have added a pull request where I have implemented the subset feature.
It seems to work fine on my end.
Hope it helps!","0","0.42267971191738","Dataset usage","Requirement Analysis"
"https://github.com/Trusted-AI/AIF360","565473767","issue_comment","https://github.com/Trusted-AI/AIF360/issues/132#issuecomment-565473767","What kind of machine are you trying to install on? Windows?

Can you post the full error message?","29","0.7288168307276586","Troubleshooting","Maintenance"
"https://github.com/Trusted-AI/AIF360","565476120","issue_comment","https://github.com/Trusted-AI/AIF360/issues/132#issuecomment-565476120","You can also take a look at these more detailed installation instructions (""Installing CVXPY"" section):
https://github.com/IBM/AIF360/blob/master/docs/oreilly-tutorial-2019/Installation_Guide.pdf","21","0.6937849835939008","Installation and shell commands","Deployment"
"https://github.com/Trusted-AI/AIF360","585911958","issue_comment","https://github.com/Trusted-AI/AIF360/issues/132#issuecomment-585911958","Closing due to lack of activity.","24","0.6328671328671329","UI","Requirement Analysis"
"https://github.com/Trusted-AI/AIF360","561778752","issue_comment","https://github.com/Trusted-AI/AIF360/issues/131#issuecomment-561778752","Fixed it. Copied examples folder inside the cloned aif360 and then imported as,

from aif360.examples.common_utils import compute_metrics","21","0.4241791611311444","Installation and shell commands","Deployment"
"https://github.com/Trusted-AI/AIF360","561271439","issue_comment","https://github.com/Trusted-AI/AIF360/issues/130#issuecomment-561271439","Can you post the full sample code you used to create the dataset, initialize the metric, etc.?","29","0.2130987292277615","Troubleshooting","Maintenance"
"https://github.com/Trusted-AI/AIF360","585912864","issue_comment","https://github.com/Trusted-AI/AIF360/issues/130#issuecomment-585912864","Closing due to lack of activity.","24","0.5724275724275725","UI","Requirement Analysis"
"https://github.com/Trusted-AI/AIF360","558477658","issue_comment","https://github.com/Trusted-AI/AIF360/issues/128#issuecomment-558477658","really interesting","20","0.2391383716684923","Opinion","Requirement Analysis"
"https://github.com/Trusted-AI/AIF360","554526342","issue_comment","https://github.com/Trusted-AI/AIF360/issues/126#issuecomment-554526342","It looks like the docstring is fixed in the code but readthedocs is showing an older version for some reason. I'll investigate.","32","0.4798707536459698","Dependency and Release","Deployment"
"https://github.com/Trusted-AI/AIF360","554570932","issue_comment","https://github.com/Trusted-AI/AIF360/issues/126#issuecomment-554570932","Should be fixed on readthedocs now. Thanks for pointing it out!","30","0.526947325605044","Troubleshooting","Maintenance"
"https://github.com/Trusted-AI/AIF360","546722175","issue_comment","https://github.com/Trusted-AI/AIF360/issues/119#issuecomment-546722175","Hi! Is this after installing the requirements.txt?","21","0.4555903866248696","Installation and shell commands","Deployment"
"https://github.com/Trusted-AI/AIF360","546733524","issue_comment","https://github.com/Trusted-AI/AIF360/issues/119#issuecomment-546733524","Hi :) Sorry, there was a fine print: 

> Some algorithms require additional dependencies not included in the minimal installation. To use these, we recommend a full installation.
> 

You might find it useful from a user perspective to have all libraries, required by the core code, included within the pip install setup. ","21","0.6373452838241569","Installation and shell commands","Deployment"
"https://github.com/Trusted-AI/AIF360","605634380","issue_comment","https://github.com/Trusted-AI/AIF360/issues/119#issuecomment-605634380","---------------------------------------------------------------------------
ModuleNotFoundError                       Traceback (most recent call last)
<ipython-input-4-b9e5bc932b87> in <module>
      8 from aif360.datasets import GermanDataset
      9 from aif360.metrics import BinaryLabelDatasetMetric
---> 10 from aif360.algorithms.preprocessing import Reweighing
     11 
     12 from IPython.display import Markdown, display

~\AppData\Local\Programs\Python\Python37\lib\site-packages\aif360\algorithms\preprocessing\__init__.py in <module>
      1 from aif360.algorithms.preprocessing.disparate_impact_remover import DisparateImpactRemover
----> 2 from aif360.algorithms.preprocessing.lfr import LFR
      3 from aif360.algorithms.preprocessing.optim_preproc import OptimPreproc
      4 from aif360.algorithms.preprocessing.reweighing import Reweighing

~\AppData\Local\Programs\Python\Python37\lib\site-packages\aif360\algorithms\preprocessing\lfr.py in <module>
      3 
      4 from aif360.algorithms import Transformer
----> 5 from aif360.algorithms.preprocessing.lfr_helpers import helpers as lfr_helpers
      6 
      7 

~\AppData\Local\Programs\Python\Python37\lib\site-packages\aif360\algorithms\preprocessing\lfr_helpers\helpers.py in <module>
      1 # Based on code from https://github.com/zjelveh/learning-fair-representations
----> 2 from numba.decorators import jit
      3 import numpy as np
      4 
      5 @jit

ModuleNotFoundError: No module named 'numba'

Facing the same problem... anyway to fix it??","27","0.9747205452489022","Troubleshooting","Maintenance"
"https://github.com/Trusted-AI/AIF360","609683754","issue_comment","https://github.com/Trusted-AI/AIF360/issues/119#issuecomment-609683754","I ran into the same ""issue"" today. Although it can be easily fixed by running ```pip install numba```, it feels like running the first block of code from the introduction tutorial should not require installing additional libraries. Like @bottydim, I would recommend putting numba in the setup.","21","0.5169868791002814","Installation and shell commands","Deployment"
"https://github.com/Trusted-AI/AIF360","538021765","issue_comment","https://github.com/Trusted-AI/AIF360/issues/112#issuecomment-538021765","Orange3, scs, and networkx were added to the requirements file because they're dependencies for other packages used by the toolbox. We previously noticed installation issues with different versions of these so we specified them here. It looks like Orange3 is no longer a required dependency for BlackBoxAuditing though so we can probably remove that.

As for the notebook dependencies, I'd like to keep them in the same file. This makes it easier to install everything in one go and make use of everything in the repository.","21","0.5768108091841956","Installation and shell commands","Deployment"
"https://github.com/Trusted-AI/AIF360","538067331","issue_comment","https://github.com/Trusted-AI/AIF360/issues/109#issuecomment-538067331","Hi @nrkarthikeyan, can you share how to reproduce the warring message? ","29","0.4845665961945031","Troubleshooting","Maintenance"
"https://github.com/Trusted-AI/AIF360","538483296","issue_comment","https://github.com/Trusted-AI/AIF360/issues/109#issuecomment-538483296","Hi @autoih, the following steps work in my machine

conda create -n myenv python=3.7
conda activate myenv
pip install aif360

and from inside python:

import aif360
from aif360.datasets import AdultDataset
data = AdultDataset()

This is the message I get

WARNING:root:Missing Data: 3620 rows removed from AdultDataset.
/Users/knatesa/anaconda3/envs/myenv/lib/python3.7/site-packages/aif360/datasets/standard_dataset.py:121: FutureWarning: outer method for ufunc <ufunc 'equal'> is not implemented on pandas objects. Returning an ndarray, but in the future this will raise a 'NotImplementedError'. Consider explicitly converting the Series to an array with '.array' first.
  priv = np.logical_or.reduce(np.equal.outer(vals, df[attr]))
/Users/knatesa/anaconda3/envs/myenv/lib/python3.7/site-packages/aif360/datasets/standard_dataset.py:142: FutureWarning: outer method for ufunc <ufunc 'equal'> is not implemented on pandas objects. Returning an ndarray, but in the future this will raise a 'NotImplementedError'. Consider explicitly converting the Series to an array with '.array' first.
  df[label_name]))
","26","0.5808697260310163","Bias mitigation methodology","Design"
"https://github.com/Trusted-AI/AIF360","619519242","issue_comment","https://github.com/Trusted-AI/AIF360/issues/109#issuecomment-619519242","Custom datasets cannot be used - NotImplementedError being raised.","26","0.5724275724275725","Bias mitigation methodology","Design"
"https://github.com/Trusted-AI/AIF360","619523421","issue_comment","https://github.com/Trusted-AI/AIF360/issues/109#issuecomment-619523421","Nvm - packages on PyPi are just a little outdated.","13","0.4716949716949716","Artifact generation and benchmarking","Deployment"
"https://github.com/Trusted-AI/AIF360","533105864","issue_comment","https://github.com/Trusted-AI/AIF360/issues/108#issuecomment-533105864","Aah oke I missed that the metrics always run on the labels..

If I may suggest: I find it very strange to update/change the label feature with the actual prediction.. You see this also when doing: fit_predict with a postprocessing algorithm where you have to pass two datasets.. which are completely the same besides the label attribute.. Would be ideal if a prediction attribute exists which is used instead of updating the label attrbute. 

In this case one can raise a flag whanever a fairness metric is ran on a dataset without prediction and only 1 dataset has to be passed to the fit_predict methods.. ","23","0.3679476865317574","Bias mitigation methodology","Design"
"https://github.com/Trusted-AI/AIF360","538016741","issue_comment","https://github.com/Trusted-AI/AIF360/issues/108#issuecomment-538016741","Yes, there were many design choices we considered for this. The reason we chose to separate input datasets and output datasets is one may wish to compare metrics at different stages of the pipeline which would not be possible if the dataset were modified in-place (e.g. in preprocessing, the features may change). In most cases, the underlying data should not be copied unless it differs between input and output.","15","0.288379388599653","Metrics operation","Validation"
"https://github.com/Trusted-AI/AIF360","538028980","issue_comment","https://github.com/Trusted-AI/AIF360/issues/105#issuecomment-538028980","@srnghn would you mind putting in a PR that fixes these descriptions?","32","0.3609481915933528","Dependency and Release","Deployment"
"https://github.com/Trusted-AI/AIF360","538111117","issue_comment","https://github.com/Trusted-AI/AIF360/issues/105#issuecomment-538111117","I'd like to submit a PR to fix the issue. ","32","0.391108891108891","Dependency and Release","Deployment"
"https://github.com/Trusted-AI/AIF360","527787651","issue_comment","https://github.com/Trusted-AI/AIF360/issues/101#issuecomment-527787651","Dear Mx. Keyes,

Thank you for raising this important concern.  As you rightly point out, the task itself is fundamentally problematic. Even if there may not have been maleficent intentions when the tutorial was composed, I agree that it should be taken down going forward.  We will do so posthaste.","20","0.2870318087709392","Opinion","Requirement Analysis"
"https://github.com/Trusted-AI/AIF360","528516574","issue_comment","https://github.com/Trusted-AI/AIF360/issues/101#issuecomment-528516574","Thanks!","0","0.0303030303030302","Dataset usage","Requirement Analysis"
"https://github.com/Trusted-AI/AIF360","538050717","issue_comment","https://github.com/Trusted-AI/AIF360/issues/101#issuecomment-538050717","Hi @pronics2004, should this issue be closed?","31","0.4871995820271681","Testing","Maintenance"
"https://github.com/Trusted-AI/AIF360","538052963","issue_comment","https://github.com/Trusted-AI/AIF360/issues/101#issuecomment-538052963","yes. thanks!","0","0.0303030303030302","Dataset usage","Requirement Analysis"
"https://github.com/Trusted-AI/AIF360","526618884","issue_comment","https://github.com/Trusted-AI/AIF360/issues/97#issuecomment-526618884","Thanks for sharing your thoughts.  ""Bias"" has many meanings and it is the technical term that the AI fairness community is using (see for example: https://fatconference.org/)","25","0.7440398218496199","Research","Requirement Analysis"
"https://github.com/Trusted-AI/AIF360","512552214","issue_comment","https://github.com/Trusted-AI/AIF360/issues/94#issuecomment-512552214","Closed by #93 ","11","0.1949616648411831","Troubleshooting","Maintenance"
"https://github.com/Trusted-AI/AIF360","506437869","issue_comment","https://github.com/Trusted-AI/AIF360/issues/92#issuecomment-506437869","This seems to be related to the numba version. Currently numba==0.42.0 is supported.","27","0.2799154334038055","Troubleshooting","Maintenance"
"https://github.com/Trusted-AI/AIF360","506667630","issue_comment","https://github.com/Trusted-AI/AIF360/issues/92#issuecomment-506667630","Yes, thanks that (numba==0.42.0) fixed it.","21","0.2343260188087775","Installation and shell commands","Deployment"
"https://github.com/Trusted-AI/AIF360","610966270","issue_comment","https://github.com/Trusted-AI/AIF360/issues/87#issuecomment-610966270","Is the issue resolved? Not sure, if Kibnelson's code is the final solution. It implements the fairness constraint, but when using it on some test data the overall accuracy turns out to be very low.

Any help would be much appreciated. Thanks!","10","0.2911401005571409","Model development","Development"
"https://github.com/Trusted-AI/AIF360","491414149","issue_comment","https://github.com/Trusted-AI/AIF360/issues/83#issuecomment-491414149","That sound right and thanks for pointing this out. 

The final results should not be affected as M_nk loops over k.","15","0.1750242639922354","Metrics operation","Validation"
"https://github.com/Trusted-AI/AIF360","491424439","issue_comment","https://github.com/Trusted-AI/AIF360/issues/83#issuecomment-491424439","Yes, as long as k < P (else you have a bound error). ","6","0.2070368110636568","API expansion","Development"
"https://github.com/Trusted-AI/AIF360","491436661","issue_comment","https://github.com/Trusted-AI/AIF360/issues/83#issuecomment-491436661","Yes, you are right. Thanks. Should be fixed in PR #84.","32","0.3607628004179729","Dependency and Release","Deployment"
"https://github.com/Trusted-AI/AIF360","490108171","issue_comment","https://github.com/Trusted-AI/AIF360/issues/80#issuecomment-490108171","Good catch! Looks like you're right, it's a typo. Thanks for pointing it out!","15","0.3105228105228104","Metrics operation","Validation"
"https://github.com/Trusted-AI/AIF360","491360627","issue_comment","https://github.com/Trusted-AI/AIF360/issues/80#issuecomment-491360627","Thank You!!","0","0.0303030303030302","Dataset usage","Requirement Analysis"
"https://github.com/Trusted-AI/AIF360","471573509","issue_comment","https://github.com/Trusted-AI/AIF360/issues/74#issuecomment-471573509","referenced here --> https://github.com/aif360-learn/aif360-learn/issues/44","1","0.2659352142110764","Fix warnings","Maintenance"
"https://github.com/Trusted-AI/AIF360","471572462","issue_comment","https://github.com/Trusted-AI/AIF360/issues/73#issuecomment-471572462","The issue is also being tracked --> https://github.com/aif360-learn/aif360-learn/issues/43","20","0.2027168234064786","Opinion","Requirement Analysis"
"https://github.com/Trusted-AI/AIF360","472009980","issue_comment","https://github.com/Trusted-AI/AIF360/issues/73#issuecomment-472009980","The issue was you initialized the BLD with `favorable_label=0` but sklearn does `pos_label=1` by default. To solve this you could add `pos_label=0` to `recall_score` and `precision_score` or change to `favorable_label=1`, depending on your application","26","0.404950917626974","Bias mitigation methodology","Design"
"https://github.com/Trusted-AI/AIF360","459672152","issue_comment","https://github.com/Trusted-AI/AIF360/issues/71#issuecomment-459672152","could you maybe share the piece of code which triggers the issue @hoffmansc ?","31","0.3508158508158508","Testing","Maintenance"
"https://github.com/Trusted-AI/AIF360","478597321","issue_comment","https://github.com/Trusted-AI/AIF360/issues/58#issuecomment-478597321","Hi @adrinjalali. Apologies for poor communication about this. I’m reopening this issue because, after feedback from many other users, it sounds like stronger scikit-learn support (and probably pandas to a further degree) would be greatly appreciated. There are still a bunch of challenging technical issues that we would need to resolve which has stalled progress in this direction though. I and the rest of the team would love to have a chat with you and anyone else with ideas about this to see if we can move forward with integrating scikit-learn better with AIF360. This discussion is probably best served in a call of some sort which we can summarize afterward but this is up to you.","20","0.3549103601459623","Opinion","Requirement Analysis"
"https://github.com/Trusted-AI/AIF360","478699655","issue_comment","https://github.com/Trusted-AI/AIF360/issues/58#issuecomment-478699655","Happy to see this going forward! I've been working on it on the fork I've made from the project, but as you say it's not an easy task, and there are a lot of things which are just done very differently here compared to scikit-learn.

However, for some of the challenges, we talked about them during the scikit-learn sprint in February in Paris, and some of the related issues are going forward.

I'd be happy to have a call, we can coordinate on your slack channel, where I'm always present.","20","0.5550001412070378","Opinion","Requirement Analysis"
"https://github.com/Trusted-AI/AIF360","480309966","issue_comment","https://github.com/Trusted-AI/AIF360/issues/58#issuecomment-480309966","We had a good conversation about this today. Here’s a summary of what we discussed and next steps:

Challenges/workarounds:
- @adrinjalali mentioned some relevant feature additions under review on scikit-learn:
    - resampling API: https://github.com/scikit-learn/enhancement_proposals/pull/15
    - feature names: https://github.com/scikit-learn/scikit-learn/pull/13307
    - sample properties: https://github.com/scikit-learn/enhancement_proposals/pull/16
- Until these are added to a stable release, there are some workarounds for some of these issues:
    - Using Pandas DataFrames to store sample properties in the index of y values

Next steps:
- Move changes to a single branch in AIF360
- Start by converting metrics to scorers using workarounds, if needed
- Convert as many algorithms to Estimators as possible
- Push for sklearn PRs mentioned above and any additional necessary functionality
- Swap out code in large chunks (e.g. all metrics at once) in the master branch to ensure a useable state at each release

Future considerations:
- Will backward-compatibility be an issue for users?
- Add ""nightly"" release with features as we go?
- Get sklearn-contrib status?

Feel free to add anything I've missed.","32","0.3529583757295067","Dependency and Release","Deployment"
"https://github.com/Trusted-AI/AIF360","484691096","issue_comment","https://github.com/Trusted-AI/AIF360/issues/58#issuecomment-484691096","@hoffmansc would it make more sense to do this in branch on sklearn? Or at-least create a major feature request there, which drives most of the conversations and points back here? With this we would be able to get more from sklearn community be able to look at what`s happening around fairness and bias, and be able to contribute ideas, code etc... ? @krvarshney ","25","0.2913358941527955","Research","Requirement Analysis"
"https://github.com/Trusted-AI/AIF360","485354962","issue_comment","https://github.com/Trusted-AI/AIF360/issues/58#issuecomment-485354962","@animeshsingh I'm not sure what you mean by a branch on sklearn. I don't think this would get in the main sklearn repo. For these projects, we use the scikit-learn-contrib collection or projects to give more visibility to those projects, and aif360, or a variation of it, would be a perfect match there.

There are some constraints that we need to satisfy before that. For instance, I don't think we should have a hard dependency on tensorflow, specially since it doesn't even support python3.7 yet, and by the time it does, python3.8 is probably out. Also, there seem to be some focus on python2.7 support here which is ditched already in many parts of the ecosystem (pandas and sklearn included). We're planning a release in the coming week and it supports python3.5+.

But in general, I like the idea of getting this closer to the scikit-learn-contrib realm.","32","0.3442565186751234","Dependency and Release","Deployment"
"https://github.com/Trusted-AI/AIF360","486395321","issue_comment","https://github.com/Trusted-AI/AIF360/issues/58#issuecomment-486395321","Thanks @adrinjalali for the response. The intention was not to move the core AIF360 there - but essentially opening up a feature request in sklearn community around having a default bias checker and mitigator, and then pointing back to the branch you or @hoffmansc has created from AIF360 here. This way the community there can be made aware of the effot","20","0.2803030303030303","Opinion","Requirement Analysis"
"https://github.com/Trusted-AI/AIF360","499131806","issue_comment","https://github.com/Trusted-AI/AIF360/issues/58#issuecomment-499131806","I finally started some work on this in the `sklearn-compat` branch.

In the [README](https://github.com/IBM/AIF360/tree/sklearn-compat/aif360/sklearn) for `aif360.sklearn` I put together a rough to-do list/roadmap. A lot of it is straightforward and I think almost all of it can be done with tricks/workarounds but explicit scikit-learn support for some of these things would be better.

Feedback and contributions are needed and we should probably advertise this in some way in case other people want to contribute.","20","0.6635951605049257","Opinion","Requirement Analysis"
"https://github.com/Trusted-AI/AIF360","499182172","issue_comment","https://github.com/Trusted-AI/AIF360/issues/58#issuecomment-499182172","Thanks @hoffmansc. 
@adrinjalali it would be great to sync up once on this ","20","0.2500832500832499","Opinion","Requirement Analysis"
"https://github.com/Trusted-AI/AIF360","445309140","issue_comment","https://github.com/Trusted-AI/AIF360/issues/56#issuecomment-445309140","This seems reasonable to me. Would you be interested in putting in a PR for this?

We'll probably need to update the contributing guidelines at some point as well to reflect the style requirements.","14","0.385428131190843","Documentation","Development"
"https://github.com/Trusted-AI/AIF360","445752556","issue_comment","https://github.com/Trusted-AI/AIF360/issues/56#issuecomment-445752556","In the mean time, we moved the flake8 check to circle-ci (which also generates the documentations), since it's gives the feedback on flake8 issues much faster than the travis, and the developer doesn't have to wait until the whole travis job is finished. It's this one:

```
  lint:
    docker:
      - image: circleci/python:3.6.1
    steps:
      - checkout
      - run: ./build_tools/circle/checkout_merge_commit.sh
      - run:
          name: dependencies
          command: sudo pip install flake8
      - run:
          name: flake8
command: ./build_tools/circle/flake8_diff.sh
```
Should we start configuring a circle-ci since it's also related to #57? The flake8 check would be an easy start.

I can send a PR in either case, but I think starting it on the circle-ci would be a better option.
","32","0.4687301974165247","Dependency and Release","Deployment"
"https://github.com/Trusted-AI/AIF360","447964291","issue_comment","https://github.com/Trusted-AI/AIF360/issues/56#issuecomment-447964291","I think it's better if we stick to just one CI tool. I'm more familiar with Travis so I'd prefer if we keep that. We should be able to do the same thing still, right?

Since you can view the Travis log as it's running, it should be more or less the same time to feedback.

Can you elaborate on why circle-ci is needed for #57?","32","0.6625998115359816","Dependency and Release","Deployment"
"https://github.com/Trusted-AI/AIF360","448575620","issue_comment","https://github.com/Trusted-AI/AIF360/issues/56#issuecomment-448575620","It's not that it's needed, you can see the background at least for scikit-learn to move linting to Circle-CI here: https://github.com/scikit-learn/scikit-learn/pull/12606

But I'll put together a PR for travis, at least for now I guess.","32","0.7050407219898744","Dependency and Release","Deployment"
"https://github.com/Trusted-AI/AIF360","439464495","issue_comment","https://github.com/Trusted-AI/AIF360/issues/51#issuecomment-439464495","I [almost] concur. I get 0.11 vs 0.21. Is the input data changing? What's the source of these different results?

It may be a good idea to have some of these as tests or doctests in the docstrings (as examples).","14","0.3854281311908429","Documentation","Development"
"https://github.com/Trusted-AI/AIF360","441519526","issue_comment","https://github.com/Trusted-AI/AIF360/issues/51#issuecomment-441519526","@nrkarthikeyan Can you advise on this?  Just sent an email cc'ing the AIOS team.  Need this fixed so we can show integration.","7","0.401852924145918","Opinion","Requirement Analysis"
"https://github.com/Trusted-AI/AIF360","441539050","issue_comment","https://github.com/Trusted-AI/AIF360/issues/51#issuecomment-441539050","I know the German credit risk dataset is small, but that doesn't explain the odd behavior of AIF360's notebook.  E.g. I looped through a hundred different splits of that dataset looking for one that would de-bias properly.  

In the process I found that for EVERY split of test/train, AIF360 actually generates a more biased dataset than the initial one.  Something else is wrong here.

Here's my notebook showing this bug:  [tutorial_credit_scoring_merged.ipynb.zip](https://github.com/IBM/AIF360/files/2614414/tutorial_credit_scoring_merged.ipynb.zip)","22","0.3714446565819745","Bias mitigation methodology","Design"
"https://github.com/Trusted-AI/AIF360","441575746","issue_comment","https://github.com/Trusted-AI/AIF360/issues/51#issuecomment-441575746","The warning shown in the notebook might also be significant. The preprocessing sets the privileged and unprivileged groups, (gender and race I guess), and then in the notebook when the user tries to set the privileged/unprivileged group, it's ignored as a result. That would quite change the results.","17","0.2699510634293243","Troubleshooting","Maintenance"
"https://github.com/Trusted-AI/AIF360","441645207","issue_comment","https://github.com/Trusted-AI/AIF360/issues/51#issuecomment-441645207","Hi all, the optimized pre-processing (used in the original credit scoring tutorial) has a lot of randomness built into it, which will create issues with small datasets. I suggest that we use re-weighing pre-processing to circumvent this issue. Please see the attached notebook: 
[tutorial_credit_scoring_reweighing.ipynb.zip](https://github.com/IBM/AIF360/files/2615783/tutorial_credit_scoring_reweighing.ipynb.zip)

The key thing to keep in mind is that reweighing pre-processing works by changing the instance level weights (this is available in dataset.instance_weights). So, the classifier trained on the debiased data should be capable of handling instance level weights. Let me know what you folks think.
","23","0.3423778602350031","Bias mitigation methodology","Design"
"https://github.com/Trusted-AI/AIF360","444550003","issue_comment","https://github.com/Trusted-AI/AIF360/issues/51#issuecomment-444550003","Alternatively, we could run it on the Adult dataset which seems to be much more stable and effective.
[tutorial_credit_scoring_adult.ipynb.zip](https://github.com/IBM/AIF360/files/2649622/tutorial_credit_scoring_adult.ipynb.zip)

","0","0.3782501423420005","Dataset usage","Requirement Analysis"
"https://github.com/Trusted-AI/AIF360","446305168","issue_comment","https://github.com/Trusted-AI/AIF360/issues/51#issuecomment-446305168","Any update on fixing this? We've a [code pattern](https://github.com/IBM/ensure-loan-fairness-aif360) on developer.ibm.com that uses this notebook, and it shows that using AIF360 makes fairness worse.","24","0.5452344265903587","UI","Requirement Analysis"
"https://github.com/Trusted-AI/AIF360","455905490","issue_comment","https://github.com/Trusted-AI/AIF360/issues/51#issuecomment-455905490","We have modified the tutorial and this issue is fixed.","22","0.3546878177750662","Bias mitigation methodology","Design"
"https://github.com/Trusted-AI/AIF360","438292944","issue_comment","https://github.com/Trusted-AI/AIF360/issues/47#issuecomment-438292944","Hi Danizu,  it is best to ask this type of question in slack.  Instructions on how to join are on http://aif360.mybluemix.net/","11","0.3905180840664713","Troubleshooting","Maintenance"
"https://github.com/Trusted-AI/AIF360","437479036","issue_comment","https://github.com/Trusted-AI/AIF360/issues/45#issuecomment-437479036","Fixed with pull request #46","31","0.4531218222493391","Testing","Maintenance"
"https://github.com/Trusted-AI/AIF360","435469635","issue_comment","https://github.com/Trusted-AI/AIF360/issues/44#issuecomment-435469635","Getting closer.  Found first part of a solution here: https://www.cvxpy.org/install/index.html
Switched from ""pip install cvxgrp"" to this:
```
conda install -c conda-forge lapack
conda install -c cvxgrp cvxpy
```

So now cvxgrp is installed but getting this error:
```
/anaconda/envs/dlaas_cos/lib/python3.6/site-packages/aif360/algorithms/preprocessing/optim_preproc_helpers/opt_tools.py in <module>()
     15 import numpy as np
     16 import pandas as pd
---> 17 from cvxpy import Problem, Minimize, Variable, sum_entries,\
     18                   mul_elemwise, norm
     19 from logging import info, debug, warn

ImportError: cannot import name 'sum_entries'
```","21","0.6220677361853835","Installation and shell commands","Deployment"
"https://github.com/Trusted-AI/AIF360","435472226","issue_comment","https://github.com/Trusted-AI/AIF360/issues/44#issuecomment-435472226","What version of CVXPY is used by AIF360?  According to the current v0.4.8 release of CVXPY:
```
Several CVXPY atoms have been renamed: ...sum_entries is now sum...Due to the name changes, 
we now strongly recommend against importing CVXPY using the syntax from cvxpy import *.
```
https://media.readthedocs.org/pdf/cvxpy/stable/cvxpy.pdf","21","0.8232607767819035","Installation and shell commands","Deployment"
"https://github.com/Trusted-AI/AIF360","435472616","issue_comment","https://github.com/Trusted-AI/AIF360/issues/44#issuecomment-435472616","Hi! It looks like your most recent error is due to a newer version of cvxpy. We require 0.4.11. Are you using a virtual python environment to install packages?","21","0.7844136431580377","Installation and shell commands","Deployment"
"https://github.com/Trusted-AI/AIF360","435473674","issue_comment","https://github.com/Trusted-AI/AIF360/issues/44#issuecomment-435473674","Yes.  I'm using conda and used this command. I expect that should have installed the most recent release.
```
conda install -c cvxgrp cvxpy
```
As above though, are you sure you're using v0.4.11?  The PDF above says that `sum_entries` was rename to `sum` in v0.4.8 and also recommends not using `from cvxpy import *`.","21","0.7334049505393669","Installation and shell commands","Deployment"
"https://github.com/Trusted-AI/AIF360","435473980","issue_comment","https://github.com/Trusted-AI/AIF360/issues/44#issuecomment-435473980","And I'm guessing that since v0.4.8 was installed by `conda install -c cvxpy` then v0.4.11 isn't yet available via conda install?

Just tried and got this error:
![image](https://user-images.githubusercontent.com/814855/47934760-805ef400-de95-11e8-8291-5cae86a92ac6.png)
","21","0.7083732057416269","Installation and shell commands","Deployment"
"https://github.com/Trusted-AI/AIF360","435475274","issue_comment","https://github.com/Trusted-AI/AIF360/issues/44#issuecomment-435475274","Sorry, I wasn't clear. I meant did you create a new environment like `conda create --name aif360 python=3.6`.

The latest version of cvxpy is incompatible with our code as you pointed out. It appears that 0.4.11 is not available on conda though. You could try a lower version but we've only tested with 0.4.11. It should be available from pip, however: `pip install cvxpy==0.4.11`

If you still encounter the gcc error you can try `conda install -c cvxgrp libgcc`","21","0.825457215325057","Installation and shell commands","Deployment"
"https://github.com/Trusted-AI/AIF360","435477621","issue_comment","https://github.com/Trusted-AI/AIF360/issues/44#issuecomment-435477621","Actually, the original error may have occurred if you haven't installed the command line tools on that machine before. You can do this by `xcode-select --install`. This should give you things like gcc, etc.","21","0.6767887644714369","Installation and shell commands","Deployment"
"https://github.com/Trusted-AI/AIF360","435484039","issue_comment","https://github.com/Trusted-AI/AIF360/issues/44#issuecomment-435484039","Also, regarding the confusion around versioning: the latest version of cvxpy is 1.0.10. I'm not sure why 0.4.8 is designated as the ""stable"" version according to that pdf but it seems that the passage about `sum_entries` is in the section ""What’s New in CVXPY 1.0"" which is, admittedly, a confusing section to include in the 0.4.8 documentation","14","0.4049509176269739","Documentation","Development"
"https://github.com/Trusted-AI/AIF360","435492918","issue_comment","https://github.com/Trusted-AI/AIF360/issues/44#issuecomment-435492918","Starting over again in a new console window with these commands and not yet in a conda environment:
```
$xcode-select --install
$conda create --name aios python=3.6
$source activate aios
$conda install -c cvxgrp libgcc
$pip install --upgrade pip
$pip install cvxpy==0.4.11
```
This seems to have worked slightly better.  Now getting this error:
```
...
In file included from /usr/local/include/stdint.h:59:
    In file included from /usr/local/include/stdint.h:59:
    In file included from /usr/local/include/stdint.h:59:
    In file included from /usr/local/include/stdint.h:59:
    In file included from /usr/local/include/stdint.h:59:
    /usr/local/include/stdint.h:2:10: error: #include nested too deeply
    #include <stddef.h>
             ^
    /usr/local/include/stdint.h:59:11: error: #include nested too deeply
    # include <stdint.h>
              ^
    /usr/local/include/stdint.h:72:11: error: #include nested too deeply
    # include <sys/types.h>
              ^
    /usr/local/include/stdint.h:76:10: error: #include nested too deeply
    #include <limits.h>
             ^
    /usr/local/include/stdint.h:82:11: error: #include nested too deeply
    # include <inttypes.h>
              ^
    5 errors generated.
    error: command 'gcc' failed with exit status 
```
Any ideas?  Can't find any guidance/workarounds in Google.","21","0.9142518769625256","Installation and shell commands","Deployment"
"https://github.com/Trusted-AI/AIF360","435495094","issue_comment","https://github.com/Trusted-AI/AIF360/issues/44#issuecomment-435495094","Also looks like v0.4.* is 2.5 years old.  Seems easiest for all users if AIF360 is updated to the latest version as my guess is most people will hit the same issues as me.
https://pypi.org/project/cvxpy/#history
","32","0.414458486207365","Dependency and Release","Deployment"
"https://github.com/Trusted-AI/AIF360","435500306","issue_comment","https://github.com/Trusted-AI/AIF360/issues/44#issuecomment-435500306","That's odd. Which command caused that error? I don't have that header file on my machine so it might have been installed by something else. According to https://github.com/SOHU-Co/kafka-node/issues/881, you might have luck just renaming it.

Also, when I installed on a mac, I was able to do `pip install cvxpy==0.4.11` with just the xcode gcc installed. I didn't need to install libgcc from cvxgrp.","21","0.8288425492033742","Installation and shell commands","Deployment"
"https://github.com/Trusted-AI/AIF360","435505079","issue_comment","https://github.com/Trusted-AI/AIF360/issues/44#issuecomment-435505079","OK.  Starting over again and just editing the AIF360 file to use the latest version of cvxpy.  Just ran this:
```
$conda create --name aios python=3.6
$source activate aios
$conda install -c cvxgrp cvxpy
```
Then I edited the opt_tools.py from this:
```
from cvxpy import Problem, Minimize, Variable, sum_entries, mul_elemwise, norm
```
to this
```
from cvxpy import Problem, Minimize, Variable, norm
import cvxpy as cvx
```
and replaced `sum_entries` to `cvx.sum` and `mul_elemwise` to `cvx.multiply`","21","0.9376526034864574","Installation and shell commands","Deployment"
"https://github.com/Trusted-AI/AIF360","435517511","issue_comment","https://github.com/Trusted-AI/AIF360/issues/44#issuecomment-435517511","OK, finally found the solution.  Conda had the older v0.4.9 version of cvxpy so installed it as below and the notebook now works:
```
$conda create --name aios python=3.6
$source activate aios
$conda install -c cvxgrp cvxpy=0.4.9
```","21","0.9162185232607766","Installation and shell commands","Deployment"
"https://github.com/Trusted-AI/AIF360","445262200","issue_comment","https://github.com/Trusted-AI/AIF360/issues/43#issuecomment-445262200","Hi
Can you please provide some more detail? The code you used, the line where the error occurred, etc...best will be if you attach the code. I suspect this is an issue with the datafile but need to verify.","31","0.1776714513556619","Testing","Maintenance"
"https://github.com/Trusted-AI/AIF360","445697975","issue_comment","https://github.com/Trusted-AI/AIF360/issues/43#issuecomment-445697975","Identified slight bug in code. Fixed and pull request created.","31","0.4229483446565296","Testing","Maintenance"
"https://github.com/Trusted-AI/AIF360","438761789","issue_comment","https://github.com/Trusted-AI/AIF360/issues/42#issuecomment-438761789","fixed in #50 ","24","0.1949616648411828","UI","Requirement Analysis"
"https://github.com/Trusted-AI/AIF360","429917629","issue_comment","https://github.com/Trusted-AI/AIF360/issues/35#issuecomment-429917629","Seems like it passed after i restarted the build.","24","0.3054708155379297","UI","Requirement Analysis"
"https://github.com/Trusted-AI/AIF360","431148517","issue_comment","https://github.com/Trusted-AI/AIF360/issues/35#issuecomment-431148517","#37 fixes this","32","0.1507849580138736","Dependency and Release","Deployment"
"https://github.com/Trusted-AI/AIF360","431379008","issue_comment","https://github.com/Trusted-AI/AIF360/issues/35#issuecomment-431379008","#37 did not fix this...","24","0.1507849580138736","UI","Requirement Analysis"
"https://github.com/Trusted-AI/AIF360","437054374","issue_comment","https://github.com/Trusted-AI/AIF360/issues/35#issuecomment-437054374","Looks like merge of issue #40 fixed this.","24","0.3607628004179729","UI","Requirement Analysis"
"https://github.com/Trusted-AI/AIF360","428697416","issue_comment","https://github.com/Trusted-AI/AIF360/issues/34#issuecomment-428697416","Fixed for now (#33). Travis will now look at the https URL for the datasets first and fall back to ftp if necessary.","9","0.4835939007913534","Feature engineering methodology","Design"
"https://github.com/Trusted-AI/AIF360","421421126","issue_comment","https://github.com/Trusted-AI/AIF360/issues/16#issuecomment-421421126","Closed in https://github.com/IBM/AIF360/pull/15/commits/dc682ef4aa3fb9a165e0f8f7188ce9b1d6f67691","2","0.1711076280041798","Performance measurement","Validation"
"https://github.com/fairlearn/fairlearn","1003462896","issue_comment","https://github.com/Trusted-AI/AIF360/issues/1004#issuecomment-1003462896","Hi LengMei,

Thanks for opening this issue and finding this bug! `sample_params` is expecting array-like parameters with length matching that of `y_true` and `y_pred`. Unfortunately, passing in `pos_label` as an array (e.g. `""pos_label"" = [pos_label]*len(y_true)`) won't fix the problem since the `false_positive_rate` function is expecting `pos_label` to be a scalar value.

It should be a simple fix on our end to update the `MetricFrame` to accept scalar and array-like values for `sample_params`. 

","12","0.4777729644986284","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","985874273","issue_comment","https://github.com/Trusted-AI/AIF360/issues/999#issuecomment-985874273","Thank you for opening this issue!

My immediate thought is that bootstrapping isn't really anything to do with `MetricFrame` - couldn't the approach you're describing be made into a wrapper around any metric? Then that metric could be placed in a `MetricFrame` like any other.

**However**, that would force the 'DataFrame of tuples' return type, rather than the 'dictionary of DataFrames' you propose. If people feel that the latter return type would be more useful, then the approach described here seems reasonable. It is possible that my instinct to compartmentalise is operating excessively.

At the risk of displaying my ignorance of statistics, I would like to know a bit more about the significance testing part. My default assumption is that if two numbers have overlapping error bars, then they're not significantly different. It sounds like that's not the case?","7","0.506334987693177","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","986605129","issue_comment","https://github.com/Trusted-AI/AIF360/issues/999#issuecomment-986605129","Thank you for opening this issue and the thorough and clear description @Harsha-Nori! Let me start by saying I'm super happy that you're picking this up and I think the plan looks really good.
 
> My immediate thought is that bootstrapping isn't really anything to do with MetricFrame - couldn't the approach you're describing be made into a wrapper around any metric? Then that metric could be placed in a MetricFrame like any other.

An advantage of adding bootstrapping directly to `MetricFrame` is that it creates a lot less overhead for the user, especially if you have a bunch of them. And, not unimportantly, it is much easier to locate compared to a separate function. It also opens up some opportunities to automatically ignore the CI's in e.g., `'overall'` or `difference()`, which could avoid some hard-to-interpret results (see also below).

> At the risk of displaying my ignorance of statistics, I would like to know a bit more about the significance testing part. My default assumption is that if two numbers have overlapping error bars, then they're not significantly different. It sounds like that's not the case?

As far as I understand, the ""problem"" is that bootstrapping gives a confidence interval around the estimates for each group separately (e.g., *p_a* and *p_b*). Because you do not make any assumptions about the distributions (e.g., that they're normally distributed), the overlap of these estimates does not correspond directly to the null hypothesis of comparing means. To use bootstrapping as a hypothesis test I believe you'd need to run the procedure to get an estimation of the *difference* between the groups  (*p_a* - *p_b*) directly. But perhaps somebody who understand this on a deeper level can explain it better :)

This does correspond to a point briefly discussed in last week's community call: our documentation needs to be *super* clear about what you can and cannot conclude from these confidence intervals. Given that the entire goal of `MetricFrame` is to compare groups, it is very natural that people *will* (visually) compare the confidence intervals as an informal hypothesis test, even if we add a warning that they shouldn't. 

Tagging @fairlearn/fairlearn-maintainers 

[A final note not directly relevant to this discussion but so we don't forget:] @MiroDudik also remarked that even if we would add functionality for formal hypothesis testing, statistical significance is not necessarily equal to practical relevance (if your sample is big enough, you'll always find a statistically significant difference). So if we go that route we might want to go for e.g., equivalence testing, where you need to specify beforehand what you'd consider a relevant difference.","7","0.613719150575896","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","988339920","issue_comment","https://github.com/Trusted-AI/AIF360/issues/999#issuecomment-988339920","Hi @riedgar-ms and @hildeweerts, 

Thanks for taking the time to provide such thoughtful feedback! Here's my take on a few of the questions raised: 

> My immediate thought is that bootstrapping isn't really anything to do with MetricFrame - couldn't the approach you're describing be made into a wrapper around any metric? Then that metric could be placed in a MetricFrame like any other.""

I agree with @hildeweerts' response here -- my feeling is that direct integration means less work for the user in both setting up the analysis and in post-processing results afterwards. A dataframe of tuples is particularly nasty to work with, and will break many existing pandas/numpy workflows users may depend on. In addition, it makes plotting with error bars significantly more tedious as well. 

> > At the risk of displaying my ignorance of statistics, I would like to know a bit more about the significance testing part. My default assumption is that if two numbers have overlapping error bars, then they're not significantly different. It sounds like that's not the case?

> As far as I understand, the ""problem"" is that bootstrapping gives a confidence interval around the estimates for each group separately (e.g., p_a and p_b). Because you do not make any assumptions about the distributions (e.g., that they're normally distributed), the overlap of these estimates does not correspond directly to the null hypothesis of comparing means. To use bootstrapping as a hypothesis test I believe you'd need to run the procedure to get an estimation of the difference between the groups (p_a - p_b) directly. But perhaps somebody who understand this on a deeper level can explain it better :)

Completely agree with @hildeweerts here as well! Hypothesis testing frameworks also often take into account the number of samples present in each group, which the simple bootstrap based error bars don't explicitly account for. In addition, things get even more complex when doing multiple comparisons (for example, a sensitive feature like 'Race' may have 5-10+ different levels to it). Checking for error bar overlap doesn't intuitively translate to probabilities in these settings. 

> This does correspond to a point briefly discussed in last week's community call: our documentation needs to be super clear about what you can and cannot conclude from these confidence intervals. Given that the entire goal of MetricFrame is to compare groups, it is very natural that people will (visually) compare the confidence intervals as an informal hypothesis test, even if we add a warning that they shouldn't.

This is a really important point. On top of documentation on the website, are there any other mechanisms that FairLearn uses to alert users about these types of pitfalls (e.g. warnings or visualization hints)? 
","7","0.604608834140475","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","988341396","issue_comment","https://github.com/Trusted-AI/AIF360/issues/999#issuecomment-988341396","Also want to call out that @fredcam has expressed interest in working on this too, so we'll be handling this together 🙂","23","0.296969696969697","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","988661198","issue_comment","https://github.com/Trusted-AI/AIF360/issues/999#issuecomment-988661198","> This is a really important point. On top of documentation on the website, are there any other mechanisms that FairLearn uses to alert users about these types of pitfalls (e.g. warnings or visualization hints)?

We have discussed this in our learning resources working group, but we currently don't have any. Warnings, hints and documentation on this sounds like a great idea!","25","0.6550061181890161","Research","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","990182438","issue_comment","https://github.com/Trusted-AI/AIF360/issues/999#issuecomment-990182438","@Harsha-Nori I agree that if a DataFrame-of-tuples is considered undesirable, then building this into `MetricFrame` is the proper approach.

I do think that the statistics is going to need careful explanation. I can sort of see that 'different underlying distributions' etc. etc. might mean that the error bars on a particular metric for subgroups A, B and C aren't _strictly_ comparable. But if seeing the overlap means _nothing_ then the usefulness of the error bars is somewhat lessened.","7","0.4543030303030305","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","990352716","issue_comment","https://github.com/Trusted-AI/AIF360/issues/999#issuecomment-990352716","Some quick thoughts:

* I'm in favor of this being part of `MetricFrame`. First, because of convenience, but more importantly, because we really need to be able to obtain error bars on `difference()`, `ratio()`, `group_max()`, and `group_min()`. The error bars on those are particularly important for the ability to quantify fairness.

* Also, I agree that we should document how we suggest that the data scientist view these error bars. For example, when comparing, say, false positive rate (FPR) on two segments of population, the data scientist shouldn't be necessarily looking for the overlap, but instead should be looking at what's the **worst case** difference between those two groups (i.e., the opposite of the overlap).

* Basically, in the context of fairness, we are not worried that the point estimates are too pessimistic, we are actually worried that the point estimates might be too optimistic, and so with error bars on subpopulations, we are looking at their extreme values. If the error bars are too big, this means that we have too little data--and with the data that we have, it might be the case that the two subpopulations have indeed starkly different FPRs even if their error bars overlap (if their error bars are also big).

* Another usage of the `MetricFrame` with error bars is to examine the error bars of `difference()` (or some of the other aggregates), and again the focus should be on the worst case. This can be even turned into a statistical test. If we get a 95%-confidence interval [*a*,*b*] for the value of the difference, we can reject the hypothesis that `difference()`>*b* at *p*=0.05 (in fact we might get a better *p*, because the test is one-sided, whereas the confidence interval is two-sided).","7","0.7912641637512396","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","990377086","issue_comment","https://github.com/Trusted-AI/AIF360/issues/999#issuecomment-990377086","Having had a meeting with @MiroDudik  and @Harsha-Nori , my problem was thinking about the issue backwards. I'm more used to the case of seeing people claim that two points with massively overlapping error bars are actually different. However, for fairness (at least as practised by `MetricFrame`) the 'good' case is where the numbers are the same (because you then have zero disparity), so the incentives for stretching the statistics are reversed.

Hence point above that making bootstrapping a builtin of `MetricFrame`, we can add errors to the `difference()` and `ratio()` methods, which is where the errors are really useful.","7","0.4850748145353957","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","990763148","issue_comment","https://github.com/Trusted-AI/AIF360/issues/999#issuecomment-990763148","> I'm in favor of this being part of MetricFrame. First, because of convenience, but more importantly, because we really need to be able to obtain error bars on difference(), ratio(), group_max(), and group_min(). The error bars on those are particularly important for the ability to quantify fairness.

Just to make sure I understand this correctly, the error bars on `difference()` would be computed by bootstrapping the difference, right?

> Another usage of the MetricFrame with error bars is to examine the error bars of difference() (or some of the other aggregates), and again the focus should be on the worst case. This can be even turned into a statistical test. If we get a 95%-confidence interval [a,b] for the value of the difference, we can reject the hypothesis that difference()>b at p=0.05 (in fact we might get a better p, because the test is one-sided, whereas the confidence interval is two-sided).

As a note for future documentation, this would basically constitute a non-inferiority test (the one-sided version of an equivalence test): https://en.wikipedia.org/wiki/Equivalence_test","7","0.8163199794555719","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","991053413","issue_comment","https://github.com/Trusted-AI/AIF360/issues/999#issuecomment-991053413","> Just to make sure I understand this correctly, the error bars on `difference()` would be computed by bootstrapping the difference, right?

Yes! That's exactly what I had in mind -- it should work reasonably well even if we have many groups, because we don't have to worry about multiple hypothesis testing. However, we'll need to ensure that we deal correctly (that means conservatively) with some bad cases. The main bad case is when the number of data points in one of the subgroups, say *G* is too small. This can demonstrate in a variety of ways. Two extreme examples that come to mind:
* Some of the bootstrap replicates have no data points in *G*. (This could be prevented by stratification.)
* The sample variance (in the overall dataset) within group *G* is artificially small. This can happen for example when the value of the metric is 0/1 and we have too few data points and they are all 0s or all 1s. Another extreme case is when we have only one data point from some group (its sample variance is then obviously 0).

I am not aware of how to address the latter point in the fully distribution-free manner. The most appealing approach to me would be to allow the user specify the range of values that the metric can take (e.g., [0,1] in case of the binary metrics), which seems like a minimal information that would then allow us to construct conservative confidence intervals. Plus we would issue warnings if we detect sample sizes too small and the metric ranges are not specified.  (Another alternative would be to go Bayesian and construct credible intervals, but perhaps we can keep that for future.)

> > Another usage of the MetricFrame with error bars is to examine the error bars of difference() (or some of the other aggregates), and again the focus should be on the worst case. This can be even turned into a statistical test. If we get a 95%-confidence interval [a,b] for the value of the difference, we can reject the hypothesis that difference()>b at p=0.05 (in fact we might get a better p, because the test is one-sided, whereas the confidence interval is two-sided).
> 
> As a note for future documentation, this would basically constitute a non-inferiority test (the one-sided version of an equivalence test): https://en.wikipedia.org/wiki/Equivalence_test

Nice. I wasn't aware that that's how these are called!","7","0.7699516643096861","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","991054242","issue_comment","https://github.com/Trusted-AI/AIF360/issues/999#issuecomment-991054242","Thank you @MiroDudik for the thoughtful comments!

> Another usage of the MetricFrame with error bars is to examine the error bars of difference() (or some of the other aggregates), and again the focus should be on the worst case. This can be even turned into a statistical test. If we get a 95%-confidence interval [a,b] for the value of the difference, we can reject the hypothesis that difference()>b at p=0.05 (in fact we might get a better p, because the test is one-sided, whereas the confidence interval is two-sided).

@hildeweerts, I'm expanding on your question so quoting it here:

> Just to make sure I understand this correctly, the error bars on `difference()` would be computed by bootstrapping the difference, right?

I think this is a natural plan for the 2 groups setting, but I have a few questions about the case when there are >2 groups. I believe the current `.difference()` implementation adaptively chooses the maximum disparity between any two observed groups. For group/metric combinations with high variance and similar point estimates, the two groups chosen for the absolute difference calculation may change between bootstrap iterations if we just use the built-in fairlearn utility as a subroutine.

A natural solution here might be to fix the two groups chosen for the difference before doing bootstrap. We can use the entire dataset to estimate the groups with the largest differences in point estimates and fix those as the chosen ""differencing"" groups.

But in terms of the hypothesis testing translation, I think it's important to note that this would only be valid for the comparison between the two groups chosen. Take the case of the ""2nd largest"" group, for example, which may have a smaller mean but maybe a significantly larger variance than the ""largest"" group. This ""2nd largest"" group may very well have a significant disparity with another group, even if the two groups chosen for differencing don't have one. 

It could very well be that this is a pathological case and not worth considering too carefully, but I believe it gets increasingly more likely as the number of groups grows. 

One other option is to explicitly run the bootstrap difference on every pairwise combination of groups, and simply log all results. This increases the runtime cost, but we could then adaptively choose at the end of the bootstrap which groups we want to publish a .difference() summary for, instead of choosing before we know the metric variances on each group. Or even allow users to access all pairwise difference distributions. Would appreciate any thoughts on this!    ","7","0.6803091330736883","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","991066734","issue_comment","https://github.com/Trusted-AI/AIF360/issues/999#issuecomment-991066734","@Harsha-Nori :

> I think this is a natural plan for the 2 groups setting, but I have a few questions about the case when there are >2 groups. I believethe current `.difference()` implementation adaptively chooses the maximum disparity between any two observed groups. For group/metric combinations with high variance and similar point estimates, the two groups chosen for the absolute difference calculation may change between bootstrap iterations if we just use the built-in fairlearn utility as a subroutine.
> 
> A natural solution here might be to fix the two groups chosen for the difference before doing bootstrap. We can use the entire dataset to estimate the groups with the largest differences in point estimates and fix those as the chosen ""differencing"" groups.

I don't think we should fix the two groups in advance. We should basically calculate the `difference()` value separately for each bootstrap replicate and then report the confidence intervals of that statistic, and be clear in our documentation that we do this and what it means.

For a comparison of two specific groups, we could consider creating a new aggregator, maybe called ""deviation()"" or ""pairwise_difference()"" that would return all-pairs **signed** differences in the form of a skew-symmetric matrix, and we could then possibly construct separate confidence intervals for each of the matrix entries. However, I'm thinking that this should be proposed and discussed in a separate issue. (Thoughts on this?) Practically speaking, since our groups are non-overlapping, we can construct conservative confidence intervals for the signed differences just using confidence intervals for individual groups... of course, multiple hypothesis issues arise when interpreting such all-pairs difference matrix.","7","0.6038691150910608","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","991120203","issue_comment","https://github.com/Trusted-AI/AIF360/issues/999#issuecomment-991120203","I agree that we should not fix the groups in advance - the points raised by @Harsha-Nori are very valid and I think we have the responsibility to avoid possibly misleading results as much as possible.

> Yes! That's exactly what I had in mind -- it should work reasonably well even if we have many groups, because we don't have to worry about multiple hypothesis testing. 

Could you expand a bit more on this @MiroDudik? It is not entirely clear to me why this approach would alleviate issues with multiple hypothesis testing. It seems like you'd still need to repeat the bootstrapping procedure for all pairs of groups and adjust the significance levels accordingly. Or do you mean that you would compute one confidence interval for the difference between *any* two groups? I am a bit worried that such a result wouldn't be very informative/actionable - if there *is* a difference, you'd probably want to investigate more fine grained results anyways.","7","0.7519802759271887","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","967321160","issue_comment","https://github.com/Trusted-AI/AIF360/issues/994#issuecomment-967321160","Hi, I would like to help with this.
","0","0.0303030303030302","Dataset usage","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","967862386","issue_comment","https://github.com/Trusted-AI/AIF360/issues/994#issuecomment-967862386","Perfect! I will assign this issue to you as well, @Carlosbogo.","16","0.4531218222493391","Testing","Maintenance"
"https://github.com/fairlearn/fairlearn","974839963","issue_comment","https://github.com/Trusted-AI/AIF360/issues/994#issuecomment-974839963","Hey, i want to solve this issue! Is this issue opened to solve? ","24","0.6933066933066933","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","975258332","issue_comment","https://github.com/Trusted-AI/AIF360/issues/994#issuecomment-975258332","Hi @Vibhaa5, thank you for your interest! Currently @Carlosbogo is already working on this issue.","20","0.3306693306693306","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","979881349","issue_comment","https://github.com/Trusted-AI/AIF360/issues/994#issuecomment-979881349","Fixed by #995 ","24","0.1507849580138736","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","967324069","issue_comment","https://github.com/Trusted-AI/AIF360/issues/993#issuecomment-967324069","Hi, I would like to work on this. Is `['difference', 'group_min', 'group_max', 'ratio']` the complete list of options for the arguments?
","3","0.5644048554711372","Bias detection metrics validation","Validation"
"https://github.com/fairlearn/fairlearn","967860080","issue_comment","https://github.com/Trusted-AI/AIF360/issues/993#issuecomment-967860080","Yup, that's the whole list. Thanks @Carlosbogo, I will assign the issue to you.","16","0.6499898311978852","Testing","Maintenance"
"https://github.com/fairlearn/fairlearn","966560506","issue_comment","https://github.com/Trusted-AI/AIF360/issues/992#issuecomment-966560506","I think that this is a great idea. Two thoughts:
* Prioritization -- we should reach out to our intended audience to get the sense how to prioritize this.
* Maintenance -- I imagine that we would snapshot a specific version of the website and work on translating that version. What would the process look like for translating future versions with some updates etc.?","25","0.5500624804748517","Research","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","1010279604","issue_comment","https://github.com/Trusted-AI/AIF360/issues/992#issuecomment-1010279604","I have still not had time to read through it in detail, but it does seem like it would require people to manually translate pages. I'm unsure what will happen when we reorganize the structure of the user guide, for example, and what that does to translations. I think we should try to understand this better first, since the translations themselves will be a massive effort as far as I can tell.","25","0.6196678752070184","Research","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","965548606","issue_comment","https://github.com/Trusted-AI/AIF360/issues/991#issuecomment-965548606","#614 does this.","0","0.0303030303030302","Dataset usage","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","966434280","issue_comment","https://github.com/Trusted-AI/AIF360/issues/988#issuecomment-966434280","The PR I started a long time ago about this:
https://github.com/fairlearn/fairlearn/pull/480
This was more about explaining to myself what was going on.","20","0.6056292461986411","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","968338695","issue_comment","https://github.com/Trusted-AI/AIF360/issues/988#issuecomment-968338695","We may want to reevaluate the level of detail and math, too. Perhaps yours is more of a deep dive than a ""user guide"" @riedgar-ms ? I feel like there should be a place for that, too, just perhaps not as the user guide. These are just some thoughts I had, and I'm happy to be convinced otherwise 🙂 ","20","0.6284681679177091","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","982069152","issue_comment","https://github.com/Trusted-AI/AIF360/issues/988#issuecomment-982069152","One key concept that we should include (that doesn't appear to be in @riedgar-ms's PR) is how to select a good *epsilon* (or set of *epsilons*) for your `Moment` of choice. I have a couple of thoughts on this, based on my experience using `ExponentiedGradient` for the SciPy tutorial. 

Regarding the ""mathiness"", I think we should motivate the quantitative intuition behind the optimization process of the algorithm first (using visualizations and possibly mathematical animations) and then formalize the details mathematically in a subsequent subsection of the user guide.  ","8","0.4272601534012324","Understanding Fairness Definition","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","953187016","issue_comment","https://github.com/Trusted-AI/AIF360/issues/986#issuecomment-953187016","Note that this proposed structure of the ""Learn"" section may not be what we'll want for the learning resources we've been discussing in the working group (@hildeweerts @romanlutz @LeJit, Ayodele, Lisa), so we may want to propose an alternative structure. 

For instance, ""Sociotechnical Examples"" and ""Case Studies"" may be quite similar. 

And we may want to integrate the existing User Guide with new resources into a different structure. One way that we're discussing (and will discuss more in community calls) is to model it a bit after Google's [People and AI Guidebook](https://pair.withgoogle.com/guidebook/), which structures the material around both chapters (which may be more akin to an ML development process-oriented structure like the existing user guide) and ""patterns"", which we're discussing as an additional structuring heuristic, around particular concepts or sets of questions. 

Please comment with other thoughts below!","25","0.7900731452455589","Research","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","953188020","issue_comment","https://github.com/Trusted-AI/AIF360/issues/986#issuecomment-953188020","For the Contribute menu, we may want to add a page about ""Contributing resources"" (which might include examples, documentation, blog posts, or other learning resources). Or, replace the ""Contribute examples"" with ""Contribute resources"".

What do folks think?","14","0.5774044795783928","Documentation","Development"
"https://github.com/fairlearn/fairlearn","953244589","issue_comment","https://github.com/Trusted-AI/AIF360/issues/986#issuecomment-953244589","I like this idea of consolidating contribution types under one umbrella. 

""Contribute resources"" sounds good. I wonder if ""resources"" is immediately clear to our audience?  If so another option could be ""Contribute to Fairlearn"".","25","0.8841256602724492","Research","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","953253648","issue_comment","https://github.com/Trusted-AI/AIF360/issues/986#issuecomment-953253648","> I like this idea of consolidating contribution types under one umbrella.
> 
> ""Contribute resources"" sounds good. I wonder if ""resources"" is immediately clear to our audience? If so another option could be ""Contribute to Fairlearn"".

Good point, although I think the whole menu should be about contributing to Fairlearn. 

Maybe: ""Contribute learning resources""?","25","0.8760934083099033","Research","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","953676824","issue_comment","https://github.com/Trusted-AI/AIF360/issues/986#issuecomment-953676824","I think ""Contribute Learning Resources"" would work for now, although I'm a bit worried that it will include many different types of content, which all have different requirements (location in the repo, file extension etc.). So eventually we might want to expand the list again to make it easier for folks to find what they're looking for. But I think we can revisit this once the ""Learn"" structure becomes clearer!","20","0.5548188935086339","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","947764891","issue_comment","https://github.com/Trusted-AI/AIF360/issues/984#issuecomment-947764891","I gave a summary of the paper a few weeks ago during one of our community calls, so I have some experience using the `folktables` library with Fairlearn, and I would be happy to help out with this.

When I used folktables, it downloaded the census zip files locally and unzipped them. Would it be possible to download only the necessary CSV file, and not the entire zip file.

","21","0.4633988339451523","Installation and shell commands","Deployment"
"https://github.com/fairlearn/fairlearn","947769761","issue_comment","https://github.com/Trusted-AI/AIF360/issues/984#issuecomment-947769761","All the datasets in our dataset module rely on OpenML, so if we would want to incorporate them in Fairlearn the first step would be to publish (the versions of the dataset that we would like to include) on there.","0","0.4298187688018196","Dataset usage","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","947770836","issue_comment","https://github.com/Trusted-AI/AIF360/issues/984#issuecomment-947770836","I just came to say what @hildeweerts said.","20","0.239138371668492","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","948241737","issue_comment","https://github.com/Trusted-AI/AIF360/issues/984#issuecomment-948241737","@hildeweerts that sounds like a great first step. I'll put the datasets we're interested in on OpenML and get back to you when that's finished.

@LeJit that's great to hear! You should be able to download the datasets automatically, as in the quickstart example [here](https://github.com/zykls/folktables#quick-start-examples). If you were running into issues doing that, or if the documentation was unclear, could you open on issue?  I'll update/take care of it asap. Thank you!","20","0.6038827702072056","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","948999025","issue_comment","https://github.com/Trusted-AI/AIF360/issues/984#issuecomment-948999025","Thanks for submitting this issue @millerjohnp! 

I'm very much in favor of this project since we're not doing a good job at providing the information on why this dataset may not be ideal to use as a benchmark. Of course, your paper (and repo) add several alternative datasets which could be added to `fairlearn.datasets`. More on that below.

I'd also encourage breaking it down into as many pieces as possible. In my own experience (see many PRs of mine) it's a lot easier to add a small change/addition rather than a big chunk since all reviewers need to be satisfied and the more content there is the more there is to disagree on.

I see several pieces here:
- documenting the existing Adult dataset - all we have right now is https://fairlearn.org/main/api_reference/fairlearn.datasets.html#fairlearn.datasets.fetch_adult . A more elaborate exploration would probably go into the user guide. 
- Uploading the new datasets to OpenML
- Adding functions to the `datasets` module to import the OpenML datasets with a basic docstring
- Adding context on why these new datasets are in some ways preferable to the existing adult dataset either in the user guide or through an example.

Note: #961 is currently adding a datasets section in the user guide to which some of this could be added.

I would be a bit reluctant to replace the adult dataset in the ""get started"" guide because we were planning to use a synthetic dataset there, based on #907. The rationale in short is that when you don't have the time to explain the context, don't use the data. In user guide or examples we do have the space to elaborate, so it's different there.","0","0.3509986449066162","Dataset usage","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","1005397199","issue_comment","https://github.com/Trusted-AI/AIF360/issues/984#issuecomment-1005397199","@millerjohnp did you have any success with OpenML?

We may have people willing to capture the most important information from the paper regarding the original Adult dataset and the new datasets in a section of the user guide as mentioned in my previous post here. If the datasets are on OpenML we can directly use them from there, otherwise we'll perhaps try to upload them ourselves. Let me know 🙂 ","0","0.4116399526897509","Dataset usage","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","1006174711","issue_comment","https://github.com/Trusted-AI/AIF360/issues/984#issuecomment-1006174711","Hey @romanlutz apologies for the delay here. I've been distracted by other projects the last few months. I have some time right after the ICML deadline to update the documentation for the Adult dataset and also finish uploading our datasets to OpenML so they're usable in fairlearn. Thanks for your patience :) ","20","0.3468837863167759","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","1006243080","issue_comment","https://github.com/Trusted-AI/AIF360/issues/984#issuecomment-1006243080","No worries! @kspieks and @britneyting will probably try to come up with a first version of a documentation through the user guide. This should be based on the findings in your paper, of course, but may also include other information.

I believe the ICML deadline is end of January, right? It would be great if you could provide feedback or make additions at that point. Of course, you're more than welcome to join our community call (every Thursday, 8am PT) on [Discord](https://discord.gg/R22yCfgsRn) to discuss further. Good luck with your deadline and I'm looking forward to getting this effort off the ground 🙂 ","20","0.7932529864048737","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","943610881","issue_comment","https://github.com/Trusted-AI/AIF360/issues/977#issuecomment-943610881","At first, I found it slightly confusing that every subsection of Metrics (except for the first) is about grouped metrics, while the header of the second subsection is called 'Metrics with Grouping'. Really no big deal though :)","15","0.5758759469696971","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","966565356","issue_comment","https://github.com/Trusted-AI/AIF360/issues/977#issuecomment-966565356","We should really update the material here to be more similar to how we talk about the assessment, e.g., in our [SciPy tutorial](https://github.com/fairlearn/talks/blob/main/2021_scipy_tutorial/fairness-in-AI-systems-instructors.ipynb).","25","0.5221378045585536","Research","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","934458648","issue_comment","https://github.com/Trusted-AI/AIF360/issues/972#issuecomment-934458648","I can reproduce this using this minimal example:
```
_create_group_metric_set(y_true=[0,1], predictions={'model1': [1,0]}, sensitive_features={""a"": [0, 1], ""b"": [1, 3]}, prediction_type='binary_classification')
```

However, it does not happen with
```
_create_group_metric_set(y_true=[0,1,1,0], predictions={'model1': [1,0,1,0]}, sensitive_features={""a"": [1, 1, 1, 1]}, prediction_type='binary_classification')
{'schemaType': 'dashboardDictionary', 'schemaVersion': 0, 'predictionType': 'binaryClassification', 'trueY': [0, 1, 1, 0], 'modelNames': ['model1'], 'predictedY': [[1, 0, 1, 0]], 'precomputedFeatureBins': [{'featureBinName': 'a', 'binVector': [0, 0, 0, 0], 'binLabels': ['1']}], 'precomputedMetrics': [[{'accuracy_score': {'global': 0.5, 'bins': [0.5]}, 'count': {'global': 4, 'bins': [4]}, 'fallout_rate': {'global': 0.5, 'bins': [0.5]}, 'f1_score': {'global': 0.5, 'bins': [0.5]}, 'overprediction': {'global': 0.25, 'bins': [0.25]}, 'underprediction': {'global': 0.25, 'bins': [0.25]}, 'miss_rate': {'global': 0.5, 'bins': [0.5]}, 'precision_score': {'global': 0.5, 'bins': [0.5]}, 'recall_score': {'global': 0.5, 'bins': [0.5]}, 'balanced_accuracy_score': {'global': 0.5, 'bins': [0.5]}, 'selection_rate': {'global': 0.5, 'bins': [0.5]}, 'specificity_score': {'global': 0.5, 'bins': [0.5]}}]]}
```

@ameyn21 can you check whether every group (as defined through your sensitive features) has at least some 0s and some 1s in their `y_true`? I suspect there's one group with only positives or only negatives and that doesn't work with `sklearn.metrics.roc_auc_score`. That said, we should have proper error handling for such a case. I think @riedgar-ms thought about this in the past already (?)","12","0.4931250443545525","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","934476429","issue_comment","https://github.com/Trusted-AI/AIF360/issues/972#issuecomment-934476429","What do you mean by 'proper error handling' @romanlutz ? The error is coming from `sklearn` not `MetricFrame`. We could have `MetricFrame` catch any exceptions from the metric functions, and wrap them in its own error, but I'm not sure what the advantage of that would be.","15","0.5948290241868222","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","935615905","issue_comment","https://github.com/Trusted-AI/AIF360/issues/972#issuecomment-935615905","Thanks @romanlutz for the hint ! One of my sensitive_features were not encoded. It had values ['Male', 'Female'] which is why the '_create_group_metric_set' was complaining. But it threw a generic error. It worked now.","15","0.2113427856547121","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","935666617","issue_comment","https://github.com/Trusted-AI/AIF360/issues/972#issuecomment-935666617","Good to hear that @ameyn21!

@riedgar-ms evidently people are unsure what to do with these errors so we should at least surface which metric failed to process the input data. Additionally, this is an entirely legitimate scenario (only 1s in one group) that should work. The ROC AUC score should perhaps be skipped in that case (?)

I'll reopen so that we can address these issues.","17","0.2358830596716059","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","936054660","issue_comment","https://github.com/Trusted-AI/AIF360/issues/972#issuecomment-936054660","How would `MetricFrame` know to skip any particular metric? As I said, we could certainly catch and wrap the exception, but 'which metric failed to process the input data' is already shown in the exception thrown.","15","0.5718688091569446","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","936325576","issue_comment","https://github.com/Trusted-AI/AIF360/issues/972#issuecomment-936325576","> @riedgar-ms evidently people are unsure what to do with these errors so we should at least surface which metric failed to process the input data. Additionally, this is an entirely legitimate scenario (only 1s in one group) that should work. The ROC AUC score should perhaps be skipped in that case (?)

Would it make sense to add and argument for error handling behavior to `MetricFrame`, similar to what we have for e.g., `difference()`: `error` with options `'raise'` (raise error, possibly with a more meaningful error message compared to the current situation), and `'coerce'` (return `NaN` for the group for which AUC is undefined).","17","0.270911065047873","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","936590593","issue_comment","https://github.com/Trusted-AI/AIF360/issues/972#issuecomment-936590593","The problem is that at this stage, `MetricFrame` is only requiring the inputs to be 1D arrays. The contents of the arrays is the preserve of the metric functions themselves. For all `MetricFrame` knows, the two arrays being passed in are lists of images. We have the error handling on `difference()` etc. because there we know that we can only give meaning to those operations when the metric function _results_ are scalars.

We could perhaps improve the error from `_create_group_metric_set()` (well, to be precise, we can yank that from Fairlearn, since it doesn't really belong there any more), but I don't see how it can be done in `MetricFrame`. And even then, I'm not convinced of the value - the error is simply going to say ""One of the metric functions had a problem; read the inner exception to see what it was.""","12","0.5734249119851402","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","937025542","issue_comment","https://github.com/Trusted-AI/AIF360/issues/972#issuecomment-937025542","The fact that I can't run this function when one group only has 1s or only 0s is sufficient proof that we're not handling error cases that definitely should be handled. Either it should skip metric functions that aren't working for the given data, or it should work as @hildeweerts suggested using `coerce` vs. `raise`. Not being able to work around this problem is a sad state. It just so happened that that wasn't the issue for @ameyn21, but it just as well could have been.

If we catch the resulting errors and print them as warnings in the `coerce` case the user can still decide if it's worth looking into the issue, but they also have the choice of moving on since it's by design.

Crucially, are you planning to move this functionality elsewhere (`raiwidgets`?)? Because if you do that then we don't necessarily have to worry about it from a Fairlearn perspective...
It does make me wonder whether Fairlearn should still have a ""dump all metrics"" functionality. I feel like that could be useful.","15","0.600988018851854","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","937325890","issue_comment","https://github.com/Trusted-AI/AIF360/issues/972#issuecomment-937325890","Re-reading @hildeweerts comment, that does make sense, so long as we're handling the 'coerce' case by calling the metric function itself, and not trying to figure out in advance what it's going to do (which is what I misunderstood it to mean). In the 'raise' case, we can just leave the behaviour unchanged. My only concern is making sure that returning NaNs doesn't mess up `group_min()` etc.

In terms of `_create_group_metric_set()` it probably does belong in `raiwidgets`, and moving it has been on my ToDo list for a while.

As for 'dump all metrics' ... we could do that by predefining metrics dictionaries. e.g.
```python
classification_metrics = {
    'accuracy' : skm.accuracy_score,
    'recall' : skm.recall_score,
    # etc.
}
```
Is that what you meant?","12","0.3559328450021767","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","937533434","issue_comment","https://github.com/Trusted-AI/AIF360/issues/972#issuecomment-937533434","By dumping I meant exporting as a pre-step to serialization. Just as a dict basically. Maybe we have that already on the MetricFrame and I'm not aware of it...","15","0.3213693127237794","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","937611348","issue_comment","https://github.com/Trusted-AI/AIF360/issues/972#issuecomment-937611348","> My only concern is making sure that returning NaNs doesn't mess up `group_min()` etc. 

Ah yes good point. I think we should be able to figure something out, but it might mean that we need to add some more complex error handling behavior to `group_min()` etc. as well. I would say it's worth looking into, because I expect this type of error to be quite common, especially if you're using `control_features` or multiple `sensitive_features`.","15","0.5266319583500202","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","934454528","issue_comment","https://github.com/Trusted-AI/AIF360/issues/971#issuecomment-934454528",">  For CDD, I am a bit puzzled as to how that's supposed to work since it's somewhat more complex and needs the number of positive and negative samples for both the sensitive feature group in question as well as overall.

I've only had time for a very quick skim through, but from what you've written it sounds like CDD should be implemented using a `MetricFrame` internally. Basically, use `MetricFrame` as an easy way of getting the metric applied to all the splits, and then process them according to the requirements of CDD. Since the denominators are based on 'whole group' numbers, I don't think that CDD could be usefully implemented as a metric function *passed into* `MetricFrame`.

Or did you mean that CDD could be a method on `MetricFrame` similar to `difference()` and `ratio()` ?","3","0.3913801416861392","Bias detection metrics validation","Validation"
"https://github.com/fairlearn/fairlearn","934472166","issue_comment","https://github.com/Trusted-AI/AIF360/issues/971#issuecomment-934472166","I suggested creating a function to pass into `MetricFrame`, but that has its own issues (see the part with `functools.partial`). If we pass in the full `y_pred` that should be possible, though. It's certainly not pretty, though.

At first, I actually rather liked your idea with implementing it as a method on the `MetricFrame`! I hadn't thought of that. However, it wouldn't work unless one of the metrics passed into the `MetricFrame` is `count`.
","15","0.7716175842936406","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","934474059","issue_comment","https://github.com/Trusted-AI/AIF360/issues/971#issuecomment-934474059","I was wondering if 'smuggle the full data to every call' was what you meant....

*Edit since I didn't read carefully enough*
Good point about the 'method on `MetricFrame` approach needing to include `count()`. I suppose we could add that 'behind the scenes' - or add `np.unique(s_f, return_counts=true)` as appropriate.

Or we just go with the separate function idea, and then the internal `MetricFrame` objects can be called with whatever is needed.","15","0.84661340243288","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","934482158","issue_comment","https://github.com/Trusted-AI/AIF360/issues/971#issuecomment-934482158","I hadn't really thought of this as a separate function much, because I dislike special cases... Perhaps this is a good one, though.

I suppose you're right! We can just calculate the `count()` internally whether or not the user asked for it (but still only show it if they asked). That would make it very easy to calculate in the CCD-as-a-method case. I certainly like the simplicity of this approach (in the way it's shown to the user), but I dislike that it's not the same pattern that we'd use for other metrics. But then I feel like  I'm asking to have the cake and eat it, too. 🤔 ","15","0.7853506493506497","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","936156824","issue_comment","https://github.com/Trusted-AI/AIF360/issues/971#issuecomment-936156824","I think we've seen several use cases now for which having the option to somehow retrieve counts from `MetricFrame` would be useful. I think one way to achieve this would be to have a `compute_counts=True` argument in `MetricFrame` that controls the property `mf.counts` that stores the counts per group (or whatever we want to call these). I would personally prefer such a construction to an `mf.count()` function that ""pretends"" to compute things that have already been computed.

From the paper: *""Conditional demographic disparity differs from demographic disparity only in the sense that one or more additional conditions are added. For example, if we condition on grade point average (GPA) and we admit 70% of white applicants to graduate school that had a 4.0 GPA, conditional demographic parity would be satisfied only if 70% of black applicants with a 4.0 GPA were also admitted.""*

I haven't had the time to dive into the actual formulation (and I find the description from AWS/in the paper a bit confusing), but this specific example suggests to me that conditional demographic disparity is the weighted average of `demographic_parity_difference` across some 'explaining' feature. This is closely related to the `control_features` argument in `MetricFrame`. If this would be true, the only thing we would still require is the option to compute the weighted average over the `control_features` in `mf.by_group`.","3","0.4913859406245193","Bias detection metrics validation","Validation"
"https://github.com/fairlearn/fairlearn","937003279","issue_comment","https://github.com/Trusted-AI/AIF360/issues/971#issuecomment-937003279","The calculation from AWS is as follows:

![](https://render.githubusercontent.com/render/math?math=\frac{n^0_d}{n^0}-\frac{n^1_d}{n^1})

where the first term is the fraction of all 0 predictions that belong to a group `d`, and the second term is the fraction of all 1 predictions that belong to the same group `d`. So in that formula you get 0 if you have demographic parity in the sense that we've implemented. They add the individual values up for all the groups weighted by group size.

@hildeweerts conditioning isn't mentioned like that in the AWS documentation. I agree with your assessment, though, especially since the example you cite clearly uses conditioning on GPA. Definitely sounds like a case for `control_features`.

In terms of performing the actual aggregation, I'm not so sure whether/how that should be done. We can certainly define a `MetricFrame` that computes the counts for all groups, but would we aggregate them separately per control feature value, or even across control groups? Note that the example you cited only talks about the applicants with 4.0 GPA, but not any others. That is actually consistent with our control features because we aggregate only within the control group, but not across (see https://fairlearn.org/v0.7.0/user_guide/assessment.html#control-features-for-grouped-metrics). The `weighted_average` would just be another way of aggregating in addition to `group_min`, `group_max`, `ratio`, and `difference`, wouldn't it? ","3","0.7250025039228117","Bias detection metrics validation","Validation"
"https://github.com/fairlearn/fairlearn","937601673","issue_comment","https://github.com/Trusted-AI/AIF360/issues/971#issuecomment-937601673","> @hildeweerts conditioning isn't mentioned like that in the AWS documentation. I agree with your assessment, though, especially since the example you cite clearly uses conditioning on GPA. Definitely sounds like a case for control_features.

I think the conditioning aspect is hidden in the part where they talk about subgroups:
* *""A conditional demographic disparity (CDD) metric that **conditions DD on attributes that define a strata of subgroups on the dataset** is needed to rule out Simpson's paradox.*
* *""The CDD metric gives a single measure for all of the disparities found in the subgroups defined by an attribute of a dataset by averaging them. ""*

So when they talk about subgroups here, they do **not** mean sensitive subgroups (I think they use ""facet"" for that).

> The `weighted_average` would just be another way of aggregating in addition to group_min, group_max, ratio, and difference, wouldn't it?

It's a slightly different aggregation because you compute it *after* one of the other aggregations you mention, so you'd need to have something like  `mf.weighted_average(method='difference')`. But it actually might more sense to add an argument to existing methods that specifies how multiple values are aggregated when `control_features` are used, e.g.: `mf.difference(method='between_groups', agg='weighted_average')`","3","0.5886000791661168","Bias detection metrics validation","Validation"
"https://github.com/fairlearn/fairlearn","948969679","issue_comment","https://github.com/Trusted-AI/AIF360/issues/971#issuecomment-948969679","Let me start a bit broader with my response...

I looked at the paper. Let's ignore control feature for a moment. As you've probably noticed, there is a key difference in how they operationalize standard demographic parity (but it's equivalent to ours!):

* Our definition of demographic parity:
  * **P[Y=1 | A=a] = P[Y=1]** for all **a**.
* Their (equivalent) definition:
  * **P[A=a | Y=1] = P[A=a | Y=0]** for all **a**.
* An equivalent rewrite of their definition:
  * **P[A=a | Y=1] = P[A=a]** for all **a**.

With this definition in mind, they focus on measuring the amount of ""disadvantage"" as experienced by a specific ""protected group"" **a**. We could do likewise. This gives rise to the following ""disadvantage"" metrics:

* Our metric for disadvantage of **a**:
  * **P[Y=0 | A=a] - P[Y=0]**, which is equal to **P[Y=1] - P[Y=1 | A=a]**
* Their approach gives rise to three possible disadvantage metrics (they seem to focus on the first one):
  * **P[A=a | Y=0] - P[A=a | Y=1]**
  * **P[A=a | Y=0] - P[A=a]**
  * **P[A=a] - P[A=a | Y=1]**

Our disadvantage is a disaggregated metric (because we condition on ""A=a""), but their disadvantage is NOT a disaggregated metric, so I don't think that there should be a natural way how to use `MetricFrame` with just a single metric to get a `pandas.Series` that would list their disadvantage for each group.

In terms of next steps re. implementing this, I see three possibilities:
1. Create an example where we create a suitable `MetricFrame` and then show how to use that metric frame to calculate the disadvantage of each group. I expect that such a metric frame would be using these metrics:
```
metrics = {'n' : fairlearn.metrics.count,
           'n_Y0': lambda y_true, y_pred: np.sum(1-y_true),
           'n_Y1': lambda y_true, y_pred: np.sum(y_true)}
```
2. Create a new class that would calculate the disadvantages of the groups, using a similar API as `MetricFrame`.
3. Create a new function, e.g., instead of `demographic_parity_difference`, it could be called something like `demgraphic_parity_disadvantage` or something like that...

Once we settle on one of the above three, the question of aggregation across control features should be much more straightforward.","3","0.6175640782095392","Bias detection metrics validation","Validation"
"https://github.com/fairlearn/fairlearn","949398055","issue_comment","https://github.com/Trusted-AI/AIF360/issues/971#issuecomment-949398055","Thanks a lot for setting this out so clearly, @MiroDudik! In the spirit of considering things from a broader perspective: given that our definition of demographic parity difference is equivalent to what they propose, should we even implement this metric? 

I might be biased because I'm so used to demographic parity difference by now, but I find it much more difficult to interpret their metric compared to ours. If we disregard the part about GPA in [the example](https://github.com/fairlearn/fairlearn/issues/971#issuecomment-936156824) I mentioned earlier, it seems like even the authors themselves also prefer the, in my view, more natural comparison of selection rates across groups. I've tried to rewrite their example to the *P[A=a | Y=1] = P[A=a | Y=0]* definition but I find it really hard to do that without ending up with an unreadable sentence.","3","0.4681543732137113","Bias detection metrics validation","Validation"
"https://github.com/fairlearn/fairlearn","966666584","issue_comment","https://github.com/Trusted-AI/AIF360/issues/971#issuecomment-966666584","I think that there might be valid reasons to consider it. Here are some of the top of my head:
* The metrics of the form **P[A=a | Y=1]** and **P[A=a | Y=0]** seem to be related to the EU law (see ""negative dominance"" on pages 50-51 of the paper)
* If you have access to population level marginals **P[A=a]** as well as access to data data with Y=1 (say only know who was admitted to some program / hired / received a loan), but not Y=0 (who was rejected), then you can evaluate **P[A=a]-P[A=a|Y=1]** but you cannot evaluate **P[Y=1|A]**. Basically, you are checking how the relative proportions (of different groups) among accepted applicants compare with the relative proportions among the population at large.","3","0.4453245869175075","Bias detection metrics validation","Validation"
"https://github.com/fairlearn/fairlearn","966923894","issue_comment","https://github.com/Trusted-AI/AIF360/issues/971#issuecomment-966923894","Good points, @MiroDudik! I especially find the second one convincing, because it shows how this metric may provide functionality we currently do not offer.

Re. possible implementations. I personally really believe that showing the actual comparisons that are being made in something like `MetricFrame` encourages more thoughtful processing of information than compressing everything in a single number, so I would personally prefer to also have (1) or (2). I'm slightly hesitant re. (2) because it might add additional confusion on which frame to use in what scenario. On the other hand, going through hoops to achieve (1) might not be ideal either. 

WDYT?","15","0.6618267648952147","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","932632094","issue_comment","https://github.com/Trusted-AI/AIF360/issues/970#issuecomment-932632094","""Black"" and ""White"" definitely should be capitalized. A quick search retrieved a [style guide](https://www.usca.edu/diversity-initiatives/training-resources/guide-to-inclusive-language/inclusive-language-guide/file) that recommends using ""people"" when referring to groups of individuals. ","3","0.5313468716391475","Bias detection metrics validation","Validation"
"https://github.com/fairlearn/fairlearn","933475981","issue_comment","https://github.com/Trusted-AI/AIF360/issues/970#issuecomment-933475981","I 100% agree with referring to anybody as people/person in any case.

My two cents re. female/women: as far as I know male/female/intersex person is used when we talk about somebody's sex, whereas women/men/nonbinary person/etc. refers to somebody's gender identity. As most scenarios in Fairlearn refer to social issues, I would suggest to go with gender-related terminology, unless we specifically refer to somebody's sex (which I expect to be only relevant in a very small set of scenarios in the medical domain).","8","0.6144896059571829","Understanding Fairness Definition","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","933811252","issue_comment","https://github.com/Trusted-AI/AIF360/issues/970#issuecomment-933811252","When it comes to *writing* about different groups of people, my suggestion would be to use an existing style guide like the one that @alliesaizan mentioned. The one that I've been using as a go-to is here:
* [APA style guide](https://apastyle.apa.org/style-grammar-guidelines/bias-free-language)

It is reasonably broad / well annotated plus is freely available online.

While ""Black"" is always capitalized (and used as an adjective), some standard press manuals go with lower-case for ""white"" and ""brown"" (but still only used as adjectives):
* https://news.jrn.msu.edu/culturalcompetence/2020/07/20/associated-press-explains-its-capitalization-of-black-and-white/
* https://www.nytimes.com/2020/07/05/insider/capitalized-black.html

When we are talking about existing data sets which might include features with values like 'male', 'female', 'Caucasian' etc., I'd be in favor of using the language like:
* individuals labeled as 'female'
* individuals labeled as 'Caucasian'

It's a bit of a delicate dance, but I think that it's important to draw the distinction between how people identify vs. how they are labeled in the data (unless we know that the data was based on self-identification).","25","0.3055492329403196","Research","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","931330941","issue_comment","https://github.com/Trusted-AI/AIF360/issues/966#issuecomment-931330941","Basically, if we know the next release is gonna be after December, then the dependency is updated already in the main branch. The release branch will still support the older python, and minor releases to that release will also include builds for the older python. If there are specific bugs to that python version, they may be included in a minor release even though the dependency is removed from the main branch. But that rarely happens.","32","0.9263247301221984","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","931157213","issue_comment","https://github.com/Trusted-AI/AIF360/issues/965#issuecomment-931157213","I'm all for it! Should be simple to do by just copying our nightly test pipeline and adjusting the installations. Thanks for the suggestion, I love it!","32","0.534688995215311","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","931245159","issue_comment","https://github.com/Trusted-AI/AIF360/issues/965#issuecomment-931245159","Agreed - this sounds like a great idea.","25","0.5522810522810523","Research","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","931124300","issue_comment","https://github.com/Trusted-AI/AIF360/issues/964#issuecomment-931124300","The test doesn't check if `fit` accepts `sample_weight`, it's [here](https://github.com/scikit-learn/scikit-learn/commit/cbfbd863804da1005eda2fe8ee34ebe6380f9b3a#diff-49f814863816a0e8d199ba647472450ecc59a4375171db23cb1ca36063d6dc25R84) that the test is not run if `sample_weight` is not in the signature. The idea is that third party developers (like us here), shouldn't run individual tests. But we still haven't managed to fix the issues with extra tests on the scikit-learn side. The test runs because we run it individually as a part of this list: https://github.com/fairlearn/fairlearn/blob/62fc80c77bcd3bef6a3d7bc44e54827ec9fb8d09/test/unit/preprocessing/linear_dep_remover/test_sklearn_compat.py#L31

We could do what's done in sklearn, and use `has_fit_parameter` to skip the tests, or just remove them from this list:


``` python
from sklearn.utils.validation import has_fit_parameter
from fairlearn.preprocessing import CorrelationRemover
has_fit_parameter(CorrelationRemover(sensitive_feature_ids=0),  'sample_weight')  # returns False
```

@glemaitre what would you recommend here? Related to the common tests and API tests issue.","23","0.2822302581784544","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","931157840","issue_comment","https://github.com/Trusted-AI/AIF360/issues/964#issuecomment-931157840","I think that we expect our user to call either `check_estimator` or `parametrize_with_checks`. Those should be in charge of selecting whether or not the tests apply to the tested estimator.

This is something that we do in `imbalanced-learn` for instance: https://github.com/scikit-learn-contrib/imbalanced-learn/blob/master/imblearn/tests/test_common.py#L47-L50

Then, if a test is known to fail, the mechanism of the estimator tags can be used with the `xfail` tag.

What is the reason for running the individual tests here?","32","0.5266319583500202","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","931303389","issue_comment","https://github.com/Trusted-AI/AIF360/issues/964#issuecomment-931303389","Somehow I had missed the `xfail` tag, I'll send a PR fixing this.","32","0.7100183327941335","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","931089366","issue_comment","https://github.com/Trusted-AI/AIF360/issues/963#issuecomment-931089366","To be fair, comparing `None` and `int` makes no sense. What was the result up till now anyway? The error is coming from `numpy`, it refuses to sort an array which has `None` I guess, which to me makes sense.","6","0.5275507367250484","API expansion","Development"
"https://github.com/fairlearn/fairlearn","931132548","issue_comment","https://github.com/Trusted-AI/AIF360/issues/963#issuecomment-931132548","I agree. Up until now it ""just worked"", i.e., the `confusion_matrix` returned a result that made sense. So if you passed in an array with just a single value, say `[-1, -1, -1, -1]` (for both `y_true` and `y_pred`) and `pos_label=-1` then it returned the correct FPR, TPR, FNR, TNR while assuming this is indeed binary classification and it just so happened to have only one of the two values. Now it seems to have stopped accepting just one value and we need to specify another due to this new comparison. 

We could adjust the code to specify a value other than the specified `pos_label`, so that it would provide `[0, -1]` to `confusion_matrix`. That should solve the problem.

Surprisingly (to me at least), this only shows up in the Python 3.7+ builds. 3.6 seems to still use the older <1.0 version of scikit-learn. After checking I just realized that scikit-learn dropped 3.6 support. Should we also drop that? I thought until the end of the year 3.6 is still supported (?) Perhaps this should be a separate issue...","32","0.2740921277114307","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","931136738","issue_comment","https://github.com/Trusted-AI/AIF360/issues/963#issuecomment-931136738","re: py36 support, that one's long gone, and even py37 support is already removed from the main branches of numpy and scipy AFAIK, and we'll be removing it too for the next release.","32","0.8605048396280129","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","931248889","issue_comment","https://github.com/Trusted-AI/AIF360/issues/963#issuecomment-931248889","I wrote them, but it's been a while....

The tests themselves look sensible to me, so something like @romanlutz 's solution sounds reasonable - if `len(np.unique(y_true+y_pred))==1` then pad the labels array with some other value so that `confusion_matrix()` doesn't get confused.","12","0.4670002780094521","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","931339784","issue_comment","https://github.com/Trusted-AI/AIF360/issues/963#issuecomment-931339784","I'll check if I can send a PR now","32","0.6731601731601731","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","925672169","issue_comment","https://github.com/Trusted-AI/AIF360/issues/959#issuecomment-925672169","Thank you for opening this issue @bram49!

IMO the deterministic version of exposure makes sense, but I'd love to hear other people's thoughts on this as well @fairlearn/fairlearn-maintainers. Could you give an example of how utility would be defined in scenario (2)?

As a side note, I think we need to be careful with overloading terminology. Demographic parity/disparate treatment* are typically use I n the context of classification and regression problems. To avoid confusion, I think it would make sense to instead focus on the *types of harm* that are being measured. E.g., differences in exposure can be seen as a measure of allocation harm in a ranking scenario, whereas differences in exposure/utility is an indicator of quality-of-service harm.

[*In the Fairlearn community we generally try to avoid the term ""disparate treatment"" which originates from US laws on employment discrimination. Using that term may suggest compliance with US law even when it's not the case. First of all, most applications will fall outside of the employment domain. Moreover, considering only the output of the model (i.e., disregarding how it's used, by whom, etc.) is a very narrow frame.]","25","0.3596235225448708","Research","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","925714624","issue_comment","https://github.com/Trusted-AI/AIF360/issues/959#issuecomment-925714624","Thank you for the reply @hildeweerts
Indeed it makes more sense to define the methods by the types of harms they try to measure. I will change the headings in the titles to the corresponding harms. 
An example of a utility score would be the SAT score to rank the top-k students who will be admitted into a numerus fixus program. Or any type of relevance score, which is calculated to rank a query.","3","0.4967048839938613","Bias detection metrics validation","Validation"
"https://github.com/fairlearn/fairlearn","922718012","issue_comment","https://github.com/Trusted-AI/AIF360/issues/958#issuecomment-922718012","For some reason my @fairlearn/fairlearn-maintainers tag did not work, so I'll do it like this:  @adrinjalali  @hildeweerts  @MiroDudik @mmadaio @riedgar-ms @romanlutz ","20","0.8048644338118024","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","922718148","issue_comment","https://github.com/Trusted-AI/AIF360/issues/958#issuecomment-922718148","I would like to work on this issue :)
","0","0.0303030303030302","Dataset usage","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","922940464","issue_comment","https://github.com/Trusted-AI/AIF360/issues/958#issuecomment-922940464","Thank you for opening this issue @bramreinders97! I personally think this technique would be a good addition to Fairlearn (but I am 100% biased because I would like to get rid of AIF360 in my teaching). I think it would be sufficient to stick to Reject Option based Classification for now.

@bramreinders97: could you perhaps write a short summary of the technique such that @fairlearn/fairlearn-maintainers can get an idea of how the method works?","23","0.3537316017316018","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","922989507","issue_comment","https://github.com/Trusted-AI/AIF360/issues/958#issuecomment-922989507","I have no objection. But let's not use ROC as an acronym here. It will 100% get confused with the more commonly used receiver operating characteristic 🙂 ","7","0.4135302065238374","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","922996456","issue_comment","https://github.com/Trusted-AI/AIF360/issues/958#issuecomment-922996456","100% agree re. naming, @romanlutz!","20","0.3291536050156741","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","923742571","issue_comment","https://github.com/Trusted-AI/AIF360/issues/958#issuecomment-923742571","> @bramreinders97: could you perhaps write a short summary of the technique such that @fairlearn/fairlearn-maintainers can get an idea of how the method works?
>

Reject Option Classification is a technique based on the intuitive hypothesis that discriminatory decisions are often made close to the decision boundary because of the decision maker’s bias. 

To tackle this problem, the authors focus on a critical region around the decision boundary. This critical region contains all instances for which: 

![image](https://user-images.githubusercontent.com/50204600/134129829-d0ca7019-e61a-4134-877c-fa34bb403d64.png)

Outside of the critical region labeling is done in the normal manner, that is: 

![image](https://user-images.githubusercontent.com/50204600/134130562-56dffca6-a931-4be6-91ab-9d88cb0c143f.png)

Inside of the critical region, the labels are assigned as follows:

![image](https://user-images.githubusercontent.com/50204600/134131447-d89a20d8-7e88-41cf-a177-1bc5d56a0970.png)


The algorithm requires neither modification of the learning algorithm nor preprocessing of historical data. Pretrained classifiers can be made discrimination-aware at prediction time. Furthermore, it is not restricted to a particular classifier, it works with any probabilistic classifier. 

","23","0.6623242598474797","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","923835088","issue_comment","https://github.com/Trusted-AI/AIF360/issues/958#issuecomment-923835088","Thanks @bramreinders97 ! This sounds like postprocessing with special treatment in this particular region. Many unfairness mitigation algorithms have this ""advantaged""/""disadvantaged"" or ""deprived"" terminology which is binary AFAICT and doesn't match how we've approached things so far. For example, if you have 3 groups (or more), and on some metric (say, accuracy) they are at 0.4, 0.5, and 0.6, then would the first two groups be ""deprived""? Or just the first one? Perhaps different thresholds (theta) per group? 

Taking a step back, this somewhat reminds me of the `ThresholdOptimizer` in terms of the result. There, we end up with per-group thresholds (not exactly single thresholds, but rather two between which we interpolate using probabilities) to equalize certain target metrics.

So I'm curious how this applies to more than 2 groups w.r.t. creating thresholds and - based on that - what you're thinking in terms of API.","23","0.4825491609183568","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","924713634","issue_comment","https://github.com/Trusted-AI/AIF360/issues/958#issuecomment-924713634","In the paper, the authors propose the following API:

![image](https://user-images.githubusercontent.com/50204600/134195673-fa863599-8c53-4b61-823b-68eb1be0bd48.png)

, where the big difference with `ThresholdOptimizer` is that the user is responsible for choosing which group(s) is/are in the deprived group(s) and providing the threshold(s) <img src=""https://render.githubusercontent.com/render/math?math=\theta"">. If we would be happy with this, then I think the amount of groups does not really matter. I can see the following logic apply for any number of groups:

![logic_scheme_v2](https://user-images.githubusercontent.com/50204600/134207425-69f3b8cc-f812-4762-bf3f-ecdee15183f4.png)

If we follow this logic, then as a starting point I would suggest the following API:

![image](https://user-images.githubusercontent.com/50204600/134310501-e677d37f-6a39-4acb-a38c-77814e4f1825.png)

with method:

![image](https://user-images.githubusercontent.com/50204600/134311223-f6e16707-0a55-499a-af47-1a0e6449c6c4.png)
","23","0.4602976452734124","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","925812290","issue_comment","https://github.com/Trusted-AI/AIF360/issues/958#issuecomment-925812290","Thanks for the APIs! I'd imagine the theta would be set in the constructor or learned during `fit`, rather than provided in `predict`. Is there a `fit` method?
I'm also a bit confused about `classifiers`. What are we doing with multiple ones here? I had (perhaps mistakenly) assumed we're dealing with a single one.

Perhaps I'm misunderstanding, so let me draw up a simple example and you tell me if this matches your expectation:

Let's say we have 2 groups, A and B. A is in your terminology privileged and B is unprivileged. So let's say we manually choose theta as 0.6 after some consideration. Then our classifier would output some probabilities for every sample and we postprocess them as

- 1, if sample is from group A and probability is >0.6 or sample is from group B and probability is > 0.4
- 0 otherwise.

I can even extend that to more groups, but to me this sounds like a set of manually configured thresholds. I don't quite see the need to call any group privileged or unprivileged, since that would get messy pretty quickly. As I mentioned before, what do you call the ""in between"" groups, ""somewhat privileged""? It would be simpler to just list all the thresholds without the ""privilege"" associations (unless we're discussing something in written text where I don't see such issues).

Beyond that, is there a particular reason why the critical region needs to be symmetric? I don't particularly see why it shouldn't be 0.4 and 0.55, for example. If we have more than two groups that's bound to happen anyway, isn't it?

Finally, this seems to assume that we know the sensitive features at predict time, correct? The same applies to the `ThresholdOptimizer`, but it's not a nice property since it requires you to have these features at predict time. For example, `ExponentiatedGradient` doesn't need them at predict time. It's not a problem for the implementation, I just want to make sure I understand.

Maybe I've also misunderstood something here, so feel free to correct my example! Thanks for you patience 🙂 ","23","0.6117396250414283","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","926034448","issue_comment","https://github.com/Trusted-AI/AIF360/issues/958#issuecomment-926034448","> Thanks for the APIs! I'd imagine the theta would be set in the constructor or learned during `fit`, rather than provided in `predict`. Is there a `fit` method?

Theta should indeed be moved out of `predict`.  If it would be set in the constructor, we would not need a `fit` method I think. I had a meeting with @hildeweerts this morning, and she mentioned having a `fit` method is standard practice, but we should also think about how users might use our algorithm in a pipeline, and base our choice on that. Conclusion: I'm not sure yet where to put it (except that it should go out of `predict`). 

> I'm also a bit confused about classifiers. What are we doing with multiple ones here? I had (perhaps mistakenly) assumed we're dealing with a single one.

The algorithm also works when being fed an ensemble of probabilistic classifiers, in which case we look at the (weighted) average of the predicted probabilities. This average is then treated in the same manner as the probability that would be outputted by a single classifier. In the same meeting as mentioned above, Hilde suggested we change it to `classifier`, where if the user wants to input an ensemble, a single `sklearn.ensemble` estimator should be provided as input, which our algorithm would then treat in the same way as a single classifier (by calling `classifier.predict_proba`).

> is there a particular reason why the critical region needs to be symmetric? I don't particularly see why it shouldn't be 0.4 and 0.55, for example.

This symmetry makes sense in a binary classification setting, since if there are only 2 labels to choose from, the probability of the chosen label will always be >= 0.5. If we would also allow for multi-class classification, then this symmetry doesn't make sense. 

> Perhaps I'm misunderstanding, so let me draw up a simple example and you tell me if this matches your expectation:
> 
> Let's say we have 2 groups, A and B. A is in your terminology privileged and B is unprivileged. So let's say we manually choose theta as 0.6 after some consideration. Then our classifier would output some probabilities for every sample and we postprocess them as

For clarity's sake, let's first look at what the labels would have been without our postprocessing. Without that, the label of any sample will be based on the probability calculated by `classifier`. Let's call this label <img src=""https://render.githubusercontent.com/render/math?math=l"">.

Now, switching to your example, the postprocessing would output each sample as (here I assumed 1 is the desirable label) :

- 1 if sample is from B and probability is <= 0.6
- 0 if sample is from A and probability is <= 0.6
- <img src=""https://render.githubusercontent.com/render/math?math=l""> if sample is from A or B and probability is > 0.6

>I don't quite see the need to call any group privileged or unprivileged, since that would get messy pretty quickly. As I mentioned before, what do you call the ""in between"" groups, ""somewhat privileged""? It would be simpler to just list all the thresholds without the ""privilege"" associations (unless we're discussing something in written text where I don't see such issues).

The only moment I intend to use this terminology is for written text. For specifying the thresholds it is indeed the plan to just list all the thresholds (or maybe provide them in a dict, which would look like {'group' : <img src=""https://render.githubusercontent.com/render/math?math=\theta"">}  ).

> Finally, this seems to assume that we know the sensitive features at predict time, correct? 

Yes, unfortunately we do need the sensitive features at prediction time. ","23","0.6626014419472828","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","926595672","issue_comment","https://github.com/Trusted-AI/AIF360/issues/958#issuecomment-926595672","I suppose `fit` could be a no-op, no harm done...

I agree with @hildeweerts on ensembles being handled externally so that we can just assume we have a classifier.

> Now, switching to your example, the postprocessing would output each sample as (here I assumed 1 is the desirable label) :
> 1 if sample is from B and probability is <= 0.6
>  0 if sample is from A and probability is <= 0.6
>  if sample is from A or B and probability is > 0.6

If these are the probabilities for getting 1 then with probability 0 it should always be a 0 and with 1 always a 1 and in between it becomes more likely to be a 1 the closer we get to 1. Given that, I'm not sure how it makes sense to assign ""1 if sample is from B and probability is <= 0.6."" The way I understood before was that we want to adjust close to the decision boundary (0.5), so for one group we lower it and for another we increase it, e.g., by 0.1 to have 0.4 and 0.6. Then in that area between 0.4 and 0.6 we'd get different results for the groups.
Perhaps my intuition is completely off here and I should stay out of the discussion until I've had the chance to thoroughly review the paper. Others, please feel free to chime in, too 🙂 

Everything else makes perfect sense.","23","0.5405684284001758","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","927317340","issue_comment","https://github.com/Trusted-AI/AIF360/issues/958#issuecomment-927317340",">The way I understood before was that we want to adjust close to the decision boundary (0.5), so for one group we lower it and for another we increase it, e.g., by 0.1 to have 0.4 and 0.6. Then in that area between 0.4 and 0.6 we'd get different results for the groups.

You're right yes, this is indeed the way it is. So, to clarify:

- 1 if sample is from B and probability (to get 1) is between 0.4 and 0.6 
- 0 if sample is from A and probability (to get 1) is between 0.4 and 0.6
- <img src=""https://render.githubusercontent.com/render/math?math=l""> if sample is from A or B and probability (to get 1) is < 0.4 or > 0.6
","23","0.4842665950494604","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","930001298","issue_comment","https://github.com/Trusted-AI/AIF360/issues/958#issuecomment-930001298","So taking the above into consideration, the following would be the API I'd suggest at this point:

```python

fairlearn.postprocessing.RejectOptionClassifier( estimator , thresholds )

```

With `estimator` = A sklearn estimator which supports the `predict_proba(X)` method, where `X` is the matrix of features
`thresholds` = An array or dict specifying the selected threshold(s) 

(I changed the name of `classifier` to `estimator` to stay consistent with the naming of parameters of other Algorithms.)

With methods

```python

.fit()

.predict( X , sensitive_features )

```

With `X` = the test data
`sensitive_features` = sensitive features to identify groups by




>I suppose `fit` could be a no-op, no harm done...

I wasn't entirely sure what you meant with this, but found this definition online: _A NOOP operation is short for no operation, and is a line of code that has no effect when executed._ So I thought I'd include the `fit` method without it doing anything. Is this the correct interpretation? 

If there are no more comments about the API at this point, what would be the next step? Should I start creating code locally, or is there something else that should happen first? 
","23","0.5040318438623523","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","930955589","issue_comment","https://github.com/Trusted-AI/AIF360/issues/958#issuecomment-930955589","Could we theoretically accept regressors and then threshold on the predictions? Because for ThresholdOptimizer we're doing that rather than expecting probabilities from predict_proba. I guess the same handling as we're doing there would make sense. Check the inputs and you'll find a predict_method argument.

However, if it's really that simple then the helper class there might already do that. I think you should check that one out before starting an implementation. 

Your interpretation of what I meant by no-op is correct. Sorry, should have explained.","23","0.8711654533009769","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","931102207","issue_comment","https://github.com/Trusted-AI/AIF360/issues/958#issuecomment-931102207","I don't think accepting regressors makes sense. The intuition behind RejectOptionClassifier is that we focus on the decisions that are made close to the decision boundary (i.e. with low confidence). As far as I'm aware, regressors don't mention how certain they are of a predicted value? 

Furthermore, if, for example, we predict income with regression, what would be done with uncertain decisions? Would an instance from a deprived group get a ""bonus"" of x$?","23","0.5812955012133657","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","931154542","issue_comment","https://github.com/Trusted-AI/AIF360/issues/958#issuecomment-931154542","That's not quite what I meant. Let me explain a bit better: 
Right now you're saying you'll accept values from 0 to 1 and then you'll put a threshold on and everything below is 0 and everything above is 1. That's not so different from saying you'll accept any value and you'll put a threshold somewhere with the same outcome, is it? In fact, it's a generalization of what you're proposing. The reason I'm asking is that this is identical to `InterpolatedThresholder` ([see this code](https://github.com/fairlearn/fairlearn/blob/main/fairlearn/postprocessing/_interpolated_thresholder.py)) where you pass
```
interpolation_dict[sensitive_feature_value] = Bunch(p0=1, # probability 1, so always choose operation0
                operation0=ThresholdOperation('>', t), # this is the actual threshold
                p1=0, # probability 0 so basically ignore this part
                operation1=ThresholdOperation('>', t)) # this one doesn't matter
```

Obviously one could simplify this a lot by removing the interpolation which isn't needed for your case, but technically this is all already possible today. Given the similarities I think the entire API should basically be identical to `InterpolatedThresholder` with the exception of the `interpolation_dict` which should just be a `threshold_dict` that maps sensitive features to their threshold (called `t` above). You'll notice that `fit` is not a no-op, but it trains the estimator unless `prefit=True` is set. That's probably a good pattern to keep for this as well (?).

Now the reason I'm saying that it can be beneficial to accept any real values rather than just those within 0-1 is that you can run any sort of model to approximate scores (your dataset may be about scores, not 0/1 binary values) and then threshold them. It's just a somewhat more general formulation. Nothing stops you from providing just values within 0-1, of course.","23","0.5268049486058966","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","932217357","issue_comment","https://github.com/Trusted-AI/AIF360/issues/958#issuecomment-932217357","From a practical perspective, I have no objections, as it is indeed a more general formulation. The only ""worry"" I have, is that this does not really capture the original intuition of the authors when they introduced RejectOptionClassifier, which is based on looking at the decisions close to the decision boundary. So maybe we should take this into consideration when naming the algorithm, and not name it RejectOptionClassifier, as it deviates from the form in which it was originally proposed? 

> You'll notice that fit is not a no-op, but it trains the estimator unless prefit=True is set. That's probably a good pattern to keep for this as well (?).

For the algorithm is doesn't really matter if we fit ourselves or take in a pre-fitted estimator, so I think it is fine to have the same pattern here. 

> Given the similarities I think the entire API should basically be identical to `InterpolatedThresholder`

I have a question about the `predict_method` parameter. If I understand correctly, we would want to use `predict` for regressors, and `predict_proba` for classifiers? If this is the case, is it really necessary to let the user input a `predict_method`? Wouldn't it be easier if we just make sure on our side that the correct method gets used? ","23","0.789501401995531","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","932527834","issue_comment","https://github.com/Trusted-AI/AIF360/issues/958#issuecomment-932527834","I'm open to other names. `Thresholder` comes to mind, but I'm sure there are better names.
 
> > Given the similarities I think the entire API should basically be identical to `InterpolatedThresholder`
> 
> I have a question about the `predict_method` parameter. If I understand correctly, we would want to use `predict` for regressors, and `predict_proba` for classifiers? If this is the case, is it really necessary to let the user input a `predict_method`? Wouldn't it be easier if we just make sure on our side that the correct method gets used?

As long as scikit-learn-conforming binary classifiers with `predict_proba` are used this is good and nice. Beyond that you need customizability. Sadly, not every ML package respects scikit-learn conventions, so this is a rather useful additions that I'd love to see here as well. @MiroDudik @adrinjalali @hildeweerts @riedgar-ms do you have thoughts on this or other parts of the API?","23","0.6004549516613862","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","933892945","issue_comment","https://github.com/Trusted-AI/AIF360/issues/958#issuecomment-933892945","One question: Is the current proposal just to implement a deterministic version of `InterpolatedThresholder` or is it to implement something closer to the Reject Option Classifier in the paper?

The issue is that the Reject Option Classifier in the paper only works with binary / binarized sensitive features, and uses symmetric thresholds of the form 1/2-delta and 1/2+delta. That structure has some advantages and disadvantages:
* the rule is based on easy-to-explain intuition, it's easy to interpret, and it is easy to construct a trade-off curve
* while the rule can be explained easily, the choice of symmetric thresholds is a bit ad-hoc even in binary settings and it's not clear how to generalize the rule beyond binarized sensitive features

My sense is that `ThresholdOptimizer` addresses the disadvantages in the second bullet, but the nice features of the first bullet are lost. A deterministic version of `InterpolatedThresholder` helps with interpretability, but it's still not clear how we should fit those thresholds and/or explore the trade-offs, so there is something to be said for just implementing the binarized version from the paper.

In the end, I'm fine with either of the two approaches.

Re. naming, if we go for deterministic version of `InterpolatedThresholder`, then I like @romanlutz's suggestion `Thresholder`, which would be basically just a specialized constructor for `InterpolatedThresholder`. For the original algorithm, `RejectOptionThresholder` could be basically just a specialized constructor for `Thresholder`.","23","0.5652284034373587","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","933900250","issue_comment","https://github.com/Trusted-AI/AIF360/issues/958#issuecomment-933900250","The more I think about it I feel like we should do the generic one I mentioned, and then have the Reject Option Classifier additionally as a special case of the Thresholder. It can be its own class and all to make it easy to explain with as few args as possible. Wdyt?","23","0.4670002780094523","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","933901641","issue_comment","https://github.com/Trusted-AI/AIF360/issues/958#issuecomment-933901641","In principle, this works for me. Should we be worried about proliferation of what are basically just different constructors for `InterpolatedThresholder`? Right now I'm counting `InterpolatedThresholder`, `Thresholder`, `RejectOptionThresholder`.","15","0.4720070661774698","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","943142037","issue_comment","https://github.com/Trusted-AI/AIF360/issues/958#issuecomment-943142037","I agree with @romanlutz!

I personally don't worry too much about the proliferation. 

I think there is something to be said for implementing the Reject Option Classifier from the paper, even if it's just for benchmarking/teaching purposes (and because people who move from AIF360 to Fairlearn might be looking for it).

Just to see if I understand correctly: the idea of the deterministic `Thresholder` is to set a different (but deterministic) threshold for each sensitive group, right? I think this is a strategy that is commonly discussed (and scrutinized from legal perspective, lol) but not really implemented anywhere, so it would make sense to me to include in Fairlearn. I am probably missing something here, but I don't really see why exploring the trade-offs would be more difficult here compared to other algorithms; you can just treat the set of thresholds as hyperparameters, right?

@MiroDudik @romanlutz WDYT?","23","0.4218846629560916","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","943171019","issue_comment","https://github.com/Trusted-AI/AIF360/issues/958#issuecomment-943171019","I agree with everything you wrote @hildeweerts. I think @MiroDudik just meant that you don't have an algorithm that computes the thresholds for you, so you may end up trying lots and lots of combinations until you're finally (somewhat) happy. I personally like the interactive visualization with sliders that Google provided (can't find the link at the moment) for such a purpose, but that's a bit out of scope for this. I'm still a fan of providing the generic version and making the Reject Option Classifier a special case of it with its own class name so that people know what it is.","15","0.6616977529740046","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","943185877","issue_comment","https://github.com/Trusted-AI/AIF360/issues/958#issuecomment-943185877","> so you may end up trying lots and lots of combinations until you're finally (somewhat) happy.

Sounds like the typical ML process to me!

![](https://koaning.io/posts/mean-squared-terror/ivory.png)
@koaning ","20","0.6364315803777687","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","945113521","issue_comment","https://github.com/Trusted-AI/AIF360/issues/958#issuecomment-945113521","Am I correct in my interpretation that we will now have two different classes (RejectOptionClassifier and Thresholder) with the same API, but with the difference that RejectOptionClassifier accepts binary classifiers and Thresholder accepts regressors? Or does Thresholder accept anything? But if that's the case, what can RejectOptionClassifier do that can't be done by Thresholder?

```python 

fairlearn.postprocessing.RejectOptionClassifier/Thresholder( estimator, threshold_dict, prefit, predict_method)

.fit( X , y)

.predict( X , sensitive_features )

```","23","0.9172244344658138","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","948418494","issue_comment","https://github.com/Trusted-AI/AIF360/issues/958#issuecomment-948418494",">Am I correct in my interpretation that we will now have two different classes (RejectOptionClassifier and Thresholder) with the same API, but with the difference that RejectOptionClassifier accepts binary classifiers and Thresholder accepts regressors?

I realize now that the above is not the case, however the two API's will not differ greatly I suspect:

 ```python
fairlearn.postprocessing.Thresholder( estimator, threshold_dict, prefit, predict_method)

.fit( X , y)

.predict( X , sensitive_features )

```

and

 ```python
fairlearn.postprocessing.RejectOptionClassifier( estimator, theta , prefit, predict_method)

.fit( X , y)

.predict( X , sensitive_features )

```

Where the only difference in API lies in multiple thresholds for `Thresholder` and a single one for `RejectOptionClassifier`. 

I plan to start working on `Thresholder` now.
","23","0.8815988301242996","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","948424982","issue_comment","https://github.com/Trusted-AI/AIF360/issues/958#issuecomment-948424982","I think you will need `sensitive_features` in `.fit` as well, but other than that this looks like a good place to start from!

(Tagging @romanlutz @MiroDudik)","20","0.5115710418304076","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","950917249","issue_comment","https://github.com/Trusted-AI/AIF360/issues/958#issuecomment-950917249","For `Thresholder` I would suggest the following logic for the class. Please let me know if this makes sense :)

```python

def __init__(self, estimator, threshold_dict, prefit=False, predict_method='deprecated'):
        self.estimator = estimator
        self.threshold_dict = threshold_dict
        self.prefit = prefit
        self.predict_method = predict_method

def fit(self, X, y, sensitive_features?, **kwargs):
        *

def predict(self, X, *, sensitive_features):
        check_is_fitted(self)
        #get soft predictions
        #validate and reformat input
        **
        #for the groups mentioned in threshold_dict -> predict by comparing the soft predictions to the mentioned thresholds
```

*It makes sense to me to use the exact same fit method as implemented in `InterpolatedThresholder`, where `sensitive_features` isn't used in the `fit` method, am I missing something why we should include it in `fit` here?

**I'm not sure what to do with the 'normal' groups. How would we decide on the standard threshold? Is that an attribute we can get from `estimator`, or will we have to provide thresholds in `threshold_dict` for all groups?


","23","0.7529796102286809","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","950944035","issue_comment","https://github.com/Trusted-AI/AIF360/issues/958#issuecomment-950944035","

```python
check_is_fitted(self)
base_predictions = np.array(
  _get_soft_predictions(self.estimator_, X, self._predict_method)
)
_, base_predictions_vector, sensitive_feature_vector, _ = _validate_and_reformat_input(
  X, y=base_predictions, sensitive_features=sensitive_features, expect_y=True,
  enforce_binary_labels=False)

positive_probs = 0.0*base_predictions_vector
for a, interpolation in self.interpolation_dict.items():
  interpolated_predictions = \
      interpolation.p0 * interpolation.operation0(base_predictions_vector) + \
      interpolation.p1 * interpolation.operation1(base_predictions_vector)
  if 'p_ignore' in interpolation:
      interpolated_predictions = \
          interpolation.p_ignore * interpolation.prediction_constant + \
          (1 - interpolation.p_ignore) * interpolated_predictions
  positive_probs[sensitive_feature_vector == a] = \
      interpolated_predictions[sensitive_feature_vector == a]
return np.array([1.0 - positive_probs, positive_probs]).transpose()
```

I have a question about the logic in `InterpolatedThresholder`. Is my understanding correct that groups that are not mentioned in `interpolation_dict`, will always get label 0? I think this because of the line `positive_probs = 0.0*base_predictions_vector`, and the indices of groups not in `InterpolatedThresholder` are not updated in the for loop afterwards, so all will have a `positive_prob` equal to 0, which would mean they only receive label 1 if the random threshold = 0 in the final line of the `predict` method.

```python
return positive_probs >= random_state.rand(len(positive_probs))) * 1
```

","23","0.4927518646151854","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","953679813","issue_comment","https://github.com/Trusted-AI/AIF360/issues/958#issuecomment-953679813","Summary from offline discussion with @bramreinders97:
* @bramreinders97 pointed out that including `sensitive_features` in `.fit()` doesn't make a lot of sense here, because `fit` doesn't really do anything apart from fitting `X` and `y` to the estimator.
* For now, we assume that `threshold_dict` includes all sensitive groups and an error is thrown if it doesn't.","23","0.8766153500393302","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","974849013","issue_comment","https://github.com/Trusted-AI/AIF360/issues/958#issuecomment-974849013","For `Thresholder`, I am wondering what we plan to do if there are multiple sensitive features? Consider the following example.

![image](https://user-images.githubusercontent.com/50204600/142769767-fa2317fb-98c8-4b01-9f31-cae13803cb6b.png)

Say, for example, that the threshold for 'A' is set at .6, and the threshold for 'C' is set at .5, and the score of the sample at index 0 is .55, will we output 1 or 0? I guess it makes most sense to let the user input thresholds for the combinations of groups, i.e. a threshold for the combination 'A,C' in my example, but then that is basically the same as expecting the user to combine the two sensitive_feature columns and providing this combined column as input for `sensitive_features` (so only accepting a single column as input). I can imagine that the latter option is less prone to confusion/mistakes when specifying the `threshold_dict`. What do you think? ","23","0.4428640600411574","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","975256278","issue_comment","https://github.com/Trusted-AI/AIF360/issues/958#issuecomment-975256278","How do we deal with multiple sensitive features in e.g., `ThresholdOptimizer`? 

In my view setting thresholds for the subgroups makes sense. I agree that defining the `threshold_dict` for combinations might be a bit tricky, but in my view we should still provide the functionality. We could provide examples of how to generate the dictionary in some specific format (I suppose tuples would make sense?) semi-automatically using e.g., `itertools.product`.

Tagging @romanlutz @MiroDudik

","15","0.6128756444109145","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","975383438","issue_comment","https://github.com/Trusted-AI/AIF360/issues/958#issuecomment-975383438","> I have a question about the predict_method parameter. If I understand correctly, we would want to use predict for regressors, and predict_proba for classifiers? If this is the case, is it really necessary to let the user input a predict_method? Wouldn't it be easier if we just make sure on our side that the correct method gets used?

That would only be true if all classifiers had `predict_proba`, but not all of them do, and sometimes we use `decision_function` instead, as a soft value instead of the final prediction.

Other than that, I think at this point it makes sense for us to have PR to discuss the details there, instead of looking at small pieces of code in the issue to make decisions.","23","0.7512576592041579","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","981216456","issue_comment","https://github.com/Trusted-AI/AIF360/issues/958#issuecomment-981216456","Multiple sensitive features are not handled for each dimension (thresholds for all groups in SF1, thresholds for all groups in SF2, etc.) but we rather generate all possible overlap groups (AC, AD, BC, BD in your example) and collapse them into a single sensitive feature internally before working on it further. That way it’s no different from a single sensitive feature column. [This code](https://github.com/fairlearn/fairlearn/blob/a991aeccc88c8838d39f531f45aef7430125f13a/fairlearn/utils/_input_validation.py#L88) may be of interest to you. It’s packaged within `validate_and_reformat_input`.
You may have noticed that it may happen that there's AC, AD, BC, but no member of BD. In that case that group doesn't exist as far as the thresholder is concerned. Which directly leads me to the next point...

Groups that aren’t part of sensitive_features during training should not be part of sensitive_features at predict time. We should throw a sensible error in that case, because we have never seen anyone from that group before and shouldn’t make predictions. 
[Sidenote: Arguably, we should even detect and notify the model builder if a group is disproportionately small (which is often a contributing factor in ML systems causing harms). But we shouldn't club this into this issue and leave it for a future issue if it's of interest. I could imagine that we want to tackle this at a different point in the ML lifecycle, potentially significantly before one considers building a model.]

I concur 100% with @adrinjalali’s assessment of `predict_method`. He introduced that for the particular reason he mentioned. In your PR you seem to have used the existing `predict_method` functionality which is what I’d expect to see. If you want us to elaborate more please don’t hesitate to ask!","15","0.282305165914349","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","981733689","issue_comment","https://github.com/Trusted-AI/AIF360/issues/958#issuecomment-981733689","> Groups that aren’t part of sensitive_features during training should not be part of sensitive_features at predict time. We should throw a sensible error in that case, because we have never seen anyone from that group before and shouldn’t make predictions.

I was working under the assumption that we do not provide sensitive features in the fit method:
>Summary from offline discussion with @bramreinders97:
> 
> @bramreinders97 pointed out that including sensitive_features in .fit() doesn't make a lot of sense here, because fit doesn't really do anything apart from fitting X and y to the estimator.

Do you think we should change this in order to perform the comparison you mentioned? ","23","0.7389845530514055","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","981969745","issue_comment","https://github.com/Trusted-AI/AIF360/issues/958#issuecomment-981969745","Sorry, perhaps I phrased it badly. What I meant to say is that the groups for which we have thresholds are pre-defined, either in constructor or fit method. I don't particularly care which one. I believe you're setting them in the constructor. There just shouldn't be new ones in the predict method for which we don't have a threshold.","23","0.7620738636363638","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","982791954","issue_comment","https://github.com/Trusted-AI/AIF360/issues/958#issuecomment-982791954","Ah makes sense, I get what you mean now :)","25","0.3792963188936344","Research","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","921955165","issue_comment","https://github.com/Trusted-AI/AIF360/issues/957#issuecomment-921955165","Would the proposed change be to just install the package, and no code changes?","21","0.25008325008325","Installation and shell commands","Deployment"
"https://github.com/fairlearn/fairlearn","921982881","issue_comment","https://github.com/Trusted-AI/AIF360/issues/957#issuecomment-921982881","Remove the `#noqa` tags within `test_othermlpackages` as well","13","0.3992952783650457","Artifact generation and benchmarking","Deployment"
"https://github.com/fairlearn/fairlearn","922935004","issue_comment","https://github.com/Trusted-AI/AIF360/issues/957#issuecomment-922935004","but that means an extra dependency. I'm happy with what we have now.","32","0.4112554112554111","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","922936196","issue_comment","https://github.com/Trusted-AI/AIF360/issues/957#issuecomment-922936196","Fair enough. I can see the argument that a few `# noqa` tags are better than bringing in another package.","13","0.3905180840664713","Artifact generation and benchmarking","Deployment"
"https://github.com/fairlearn/fairlearn","919316549","issue_comment","https://github.com/Trusted-AI/AIF360/issues/956#issuecomment-919316549","This is awesome, thanks for the summary!

First of all, I would create separate issues for the various algorithms once you've decided which to implement. Starting from the metrics sounds like a very reasonable approach to me. For example, you could already evaluate standard (fairness-unaware) PCA that way!

#### Dependencies
If there are already implementations that exist for these algorithms I'd suggest checking their dependencies. We wouldn't want to introduce something heavyweight to Fairlearn. However, it might be fine as a soft dependency. I'm particularly thinking of packages to solve these optimization problems. Just something to keep in mind while you're working on this.

#### Metrics
In terms of measures I can think of the following (definitely not exhaustive):
- overall reconstruction error
- overall reconstruction loss of fair PCA compared to fairness-unaware PCA
- sensitive feature group-specific reconstruction error
- sensitive feature group-specific reconstruction loss of fair PCA compared to fairness-unaware PCA (for this latter one we'd have to decide whether to compare with PCA on just that group or PCA on the whole dataset, or alternatively do both)
- The most important metrics IMO are outcome based. It still surprises me that these papers usually don't run e2e analyses where they evaluate how using the method (or not) affects the outcomes per group as measured by typical performance metrics (grouped and ungrouped) as well as other potentially useful metrics (e.g., runtime). Perhaps that has something to do with paper length restrictions. In any case, these are all already available! We'd only have to set up an example to evaluate this end-to-end.
- any others mentioned in these papers?

#### Naming
I actually dislike the term ""Fair PCA"" a bit since it sort of implies that it is (or the results are) fair. As we've described in many places fairness is a sociotechnical concept, so the outcomes may be anything but fair. I can see why the considerations for a paper are somewhat different than for a project like Fairlearn, so perhaps it's time to rethink this before we add it to the package.
Some ideas:
- `FairnessAwarePCA`
- `GroupFairnessAwarePCA`
- `ReconstructionLossMinimizingPCA` (that's a mouthful...)
- something specific to the approach
- using author names (I personally dislike this since we'd be doing this for some techniques and not others; plus, we already cite the papers in the API reference and user guide, of course)","6","0.5815716400744705","API expansion","Development"
"https://github.com/fairlearn/fairlearn","920779667","issue_comment","https://github.com/Trusted-AI/AIF360/issues/956#issuecomment-920779667","Apologies for the amount of text below. I want to give as much information about the fairness measures to allow for a better discussion about them. 

All three papers below try to solve multi objective optimization problems. They try to minimize the overall reconstruction error while simultaneously maximizing a fairness metric.

For the fairness measures of the optimization problem, I was thinking about the ones proposed in the papers. They are the following:

The goal of the first two papers is to have the additional error compared to the optimal group-specific solution be equal for each group. For example, the additional error your dimensionality reduction algorithm imposes on females compared to the optimal solution for females should be equal to the additional error for males. Note that since the additional error compared to the optimal solution is used, it does not necessarily mean that females and males are equally well represented in the embedded space. The optimal solution for females could be much worse than for males.

**[Fair PCA](https://arxiv.org/abs/1811.00103)**:
Image below describes the fairness measure very well.
![image](https://user-images.githubusercontent.com/38749496/133588432-a32b9350-a0e4-47f7-97f9-9406879da756.png)

**[Efficient Fair PCA](https://arxiv.org/abs/1911.04931)**:
This method uses almost the same fairness measure, however, since optimal solutions do not always assign the same loss to each group, which is especially a problem when sensitive features have multiple groups, they add additional objectives to the multi objective loss function. In particular, they add the difference between the previously mentioned additional group-specific errors for each combination of groups (called pairwise disparity error). This ensures that the solution tries to minimize all pairwise disparity errors at the same time and addresses the aforementioned issue.

 **[Multi Objective approach](https://arxiv.org/abs/2006.06137)**:
This method simply tries to minimize the difference in reconstruction error of each group compared to each other group. This thus does not take into account that the optimal solution for one group might be very different than another group.

Currently, I am debating which of the three makes the most sense. 

Compared to the first fairness measure (paper 1) the second fairness measure (paper 2) is, in my opinion, better than the first one since it is designed to alleviate the issue of the first paper. 

The fairness measure of the third paper does not take into account that the optimal solution for each group might have a vastly different reconstruction error. This can be ""problematic"". Let's say the optimal reconstruction error for females is ten times as high as males. If your desired solution measures fairness in the way paper 3 does it might not improve the reconstruction error for males since it only cares about the difference between the two. Improving the reconstruction error for males even though the female reconstruction error is higher, yet optimal for that group, would decrease the fairness. This results in an overall reconstruction error which is worse. 

The ""problem"" with the fairness metric in paper 3 is thus that the way fairness is measured might cause the algorithm to not improve on overall reconstruction error because the fairness measure gets worse, even though the optimal reconstruction error for a certain group is already met. The fairness metric in paper 2 does not have this ""problem"" since it takes into account the additional reconstruction error compared to the optimal group-specific reconstruction error. I am, however, doubting how fair a solution is which has a very low reconstruction error for males compared to females even though the optimal reconstruction error for females is much lower than males. Paper 2 would call it a fair solution since the additional errors might be very low, however, the reconstruction errors between groups still differ greatly. I doubt whether this would result in a fair decomposition. It seems to me that in this case, it is impossible to do PCA in a fair way since males will be much better represented than females regardless.

What are your thoughts on the advantages/disadvantages between these fairness measures? Do you agree with the problems sketched in the above paragraph or do you think I am missing something?

Maybe a nice alternative would be to implement both fairness measures and allow the user to choose. Of course, I would evaluate both of them and add my findings in the documentation. Do you agree this is the way to go? ","6","0.8142837845661031","API expansion","Development"
"https://github.com/fairlearn/fairlearn","920941946","issue_comment","https://github.com/Trusted-AI/AIF360/issues/956#issuecomment-920941946","@Thomasq99: Can you spell out more precisely different objectives / fairness measures? It would be good if you presented them in the form that makes it clear how to evaluate them with more than two groups (unless the generalization to multiple groups is obvious). Feel free to just copy-paste the relevant snippets from the papers.

As things stand, fairlearn supports both worst-case performance metrics called `*_group_min()` and `*_group_max()` for the smallest/largest metric value across slices of data corresponding to different group, as well as the disparity metrics called `*_difference` for the largest (pairwise) difference and `*_ratio` for the smallest (pairwise) ratio (see [[here]](https://fairlearn.org/v0.7.0/user_guide/assessment.html#scalar-results-from-metricframe)).

In the absence of more information (what are the downstream harms caused by PCA?), the goals of generic preprocessing seem to fall into the [quality-of-service setting](https://fairlearn.org/v0.7.0/user_guide/fairness_in_machine_learning.html#types-of-harms), for which worst-case performance fairness metrics are more appropriate, so my hunch would be to start with the ""Fair PCA"" algorithm, but like @romanlutz said, we may want to rename it to be more descriptive of the kind of fairness it seeks to enforce. The idea is quite close to what we call [bounded group loss](https://fairlearn.org/v0.7.0/user_guide/mitigation.html#bounded-group-loss), but in ""Fair PCA"" it is presented as an objective, whereas in our reductions it appears as a constraint. We could possibly call it ""MinMaxPCA"" since it minimizes the worst-case reconstruction error among low-rank matrices?

By the way, we already have a preprocessing technique that is being applied to matrices and uses reconstruction error as the underlying metric, so you may want to use it for an inspiration for what the API might look like: [CorrelationRemover](https://fairlearn.org/v0.7.0/api_reference/fairlearn.preprocessing.html).


","6","0.4899917326465073","API expansion","Development"
"https://github.com/fairlearn/fairlearn","921043049","issue_comment","https://github.com/Trusted-AI/AIF360/issues/956#issuecomment-921043049","FWIW I am always in favor of providing as many metrics as possible, explaining their benefits and drawbacks (e.g., through a user guide or example), and then letting users choose which ones they care about. IMO the downstream harms are the most important, so we should really be looking at the fairness metrics for the whole system (not just the preprocessing step). Having a few specific ones for preprocessing doesn't hurt, of course, but we shouldn't get into the habit of thinking that it's sufficient to minimize the max reconstruction error of all groups (to mention one particular metric).

I'd definitely consider creating a very small example (you can check the existing ones in case one of those suits you) where you apply ""normal"" PCA, `CorrelationRemover`, and then subsequently the other techniques as you add them. Then you can measure these new metrics as well as the downstream effects on fairness metrics.

@hildeweerts do feel free to chime in! 🙂 ","6","0.3605979364692505","API expansion","Development"
"https://github.com/fairlearn/fairlearn","922994430","issue_comment","https://github.com/Trusted-AI/AIF360/issues/956#issuecomment-922994430","#### Fairness metrics for evaluating PCA
From the discussion above, it seems to me that a fairness metric tailored towards PCA could consider the reconstruction error across groups (but please correct me if I'm wrong). However, I fully agree with @romanlutz and @MiroDudik that downstream model performance should be leading in assessing fairness (and should be included in the user guide section for fairness-aware PCA). This also makes me think that the problems described with the ""optimal"" reconstruction error may not be *super* relevant, as it's the final model performance that we care about the most - but let me know if anybody disagrees!

#### Fairness-aware PCA algorithms
I like @MiroDudik 's suggestion to summarize and specific the objectives more precisely for each of the suggested fairness-aware PCA algorithms. I think that would make it easier to compare them.","6","0.5666399887010971","API expansion","Development"
"https://github.com/fairlearn/fairlearn","926645267","issue_comment","https://github.com/Trusted-AI/AIF360/issues/956#issuecomment-926645267","Update:

Currently I am implementing the methods and loss functions to evaluate them on downstream performance. I will get back to you soon!","6","0.5961051436555752","API expansion","Development"
"https://github.com/fairlearn/fairlearn","926682284","issue_comment","https://github.com/Trusted-AI/AIF360/issues/956#issuecomment-926682284","FWIW I didn't mean to make you run entire studies against benchmarks. I merely meant that the most meaningful metrics are those that capture downstream harms so we don't need to spend a ton of time implementing more metrics in addition to reconstruction error that only capture the results of preprocessing (since these don't necessarily imply harm). If you want to run extensive experiments I'm all for it, but I'd vouch for the inclusion of popular methods even based on face validity of the method (as @MiroDudik and others mentioned yesterday in the community call). Anyway, let us know if you need help in some way!","6","0.3251474113911748","API expansion","Development"
"https://github.com/fairlearn/fairlearn","927822142","issue_comment","https://github.com/Trusted-AI/AIF360/issues/956#issuecomment-927822142","Don't worry! I am interested in doing this study myself. It seemed like a nice internship topic to me! ","20","0.4053030303030304","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","920844528","issue_comment","https://github.com/Trusted-AI/AIF360/issues/948#issuecomment-920844528","Hi @romanlutz, after diving in to all the changes made in different versions, I am wondering what level of detail is expected in the versionchanged part of this issue.

To give an example of my doubts, consider the changed mentioned in the version guide related to ExponentGradient: 

v0.4.5:  
![image](https://user-images.githubusercontent.com/50204600/133609095-df9a7f0f-e7d1-4eb6-ba61-c49a2bb2215c.png)

v0.4.6:  
![image](https://user-images.githubusercontent.com/50204600/133609142-ad3b2a04-7852-4dd8-9a77-d753ae33950b.png)
v0.5.0:     
![image](https://user-images.githubusercontent.com/50204600/133609178-0bd9e991-73a5-434c-9ebe-747320404163.png)
![image](https://user-images.githubusercontent.com/50204600/133609187-2680bd01-f03b-4180-a2a2-701b5368031c.png)
![image](https://user-images.githubusercontent.com/50204600/133609201-300d95e8-b57a-4263-9daf-27ae3d152753.png)
![image](https://user-images.githubusercontent.com/50204600/133609211-d8bba913-7c36-4724-83da-28f18bec86cf.png)


So my question is, should all these changes be included, and if not, how do I know which to include?

","32","0.5589373915365106","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","921153790","issue_comment","https://github.com/Trusted-AI/AIF360/issues/948#issuecomment-921153790","That's a great question! I'm actually not entirely sure. @fairlearn/fairlearn-maintainers wdyt? Have you tried out whether it's technically possible to add more than one `versionchanged` tag? If it's not possible then I suppose it has to be the changes in the latest version in which it was changed.

Whenever it is an individual argument you can add it just for that one, e.g., `sample_weight_name` was added in v0.5.0. `max_iter`, `eta0`, and `run_linprog_step` were also added in v0.5.0.","32","0.4721082559562369","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","922474325","issue_comment","https://github.com/Trusted-AI/AIF360/issues/948#issuecomment-922474325","Turns out it is possible to include multiple `versionchanged` tags. So I'll try to add all mentioned changes. ","32","0.4098997088320931","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","922887415","issue_comment","https://github.com/Trusted-AI/AIF360/issues/948#issuecomment-922887415","I usually apply the tags only when a parameter changes behavior, or a major change in behavior is happening in a class. It's not supposed to mirror the changelog in any way IMO.","0","0.2940469412285695","Dataset usage","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","922987377","issue_comment","https://github.com/Trusted-AI/AIF360/issues/948#issuecomment-922987377","I agree. We've had some changes in the datasets module which @adrinjalali and I recommended omitting from `versionchanged` since only small changes or doc changes happened.

Example: from the above changes I'd definitely omit the one on pickle vs. clone (since it's internal). Several of the others are parameter renames or additions which should go directly on the parameter documentation.","14","0.2901827553889409","Documentation","Development"
"https://github.com/fairlearn/fairlearn","928049740","issue_comment","https://github.com/Trusted-AI/AIF360/issues/948#issuecomment-928049740","So I made some first changes and to test I tried committing to my local fork. I'm not sure if I accidentally pushed it to the main branch, as I do not have any experience using GitHub with multiple people on the same project?","32","0.6957464553794827","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","928315846","issue_comment","https://github.com/Trusted-AI/AIF360/issues/948#issuecomment-928315846","No worries!  You can always check your branch with `git status`, ideally before pushing 🙂 If you pushed to `main` on your fork you can always copy those changes to another branch (by creating another branch off of your `main`, for example) and then `git rebase -i HEAD~1` (or whatever number of commits you need to remove if larger than 1), then use `drop` to remove those commits. Pushing (perhaps with `--force`, not sure...) will erase the `main` changes.  Feel free to message me on Discord if you need more help! In any case, your fork is your playground and there's nobody working there but you. It only gets interesting when you pull in changes from `fairlearn/fairlearn` (""the main repo""). Some of the instructions I wrote for our last sprint might be useful https://github.com/romanlutz/fairlearn-scipy-sprint. Again, don't worry if your setup is slightly different. Just reach out and we'll be happy to help!

If you like quick and frequent feedback you can open a ""draft"" PR and get our comments right away while at the same time making clear that your PR requires more work. The nice thing about that is that we won't be as nitpicky, but we'll point out issues early on so you might save yourself some time.","32","0.4511137281718142","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","943583016","issue_comment","https://github.com/Trusted-AI/AIF360/issues/948#issuecomment-943583016","ExponentiatedGradient and GridSearch are done thanks to @bramreinders97! The other files under fairlearn.reductions remain, of course.","28","0.3461632453567938","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","914315699","issue_comment","https://github.com/Trusted-AI/AIF360/issues/945#issuecomment-914315699","Thank you for looking into this @bram49! 

What is your own view on these metrics, i.e., which one do you think would most useful for Fairlearn users?

I am not super familiar with ranking problems myself, but is logarithm base 2 standard for measures of exposure/discounting? I can imagine different ways of discounting could give quite different results.

Tagging @fairlearn/fairlearn-maintainers ","3","0.5438431583187858","Bias detection metrics validation","Validation"
"https://github.com/fairlearn/fairlearn","914357214","issue_comment","https://github.com/Trusted-AI/AIF360/issues/945#issuecomment-914357214","@hildeweerts 
I least like rRD, since it only works if the protected group is the minority group, and I'm not sure what the benefits are for this particular metric. rND and rKL, have quite similar results, but rKL is more smooth. So I would prefer rKL, but rND is simpler and easier to understand. I also like the exposure metric, since for me it is very intuitive (counting the position biases and compare the average position bias of each class)

Yes, I think that logarithmic base 2 discount is standard for measuring position bias. I have seen it in multiple papers and not seen any other discounts used, but using other discounts is definitely possible, and I think that the results can indeed be very different. ","3","0.6722139966439649","Bias detection metrics validation","Validation"
"https://github.com/fairlearn/fairlearn","914465769","issue_comment","https://github.com/Trusted-AI/AIF360/issues/945#issuecomment-914465769","I definitely think ranking scenarios are of interest and would be a great addition. Did you have a specific use case in mind that could be added along with the metrics?

Regarding the place of your additions: Assessment code should probably go into the `metrics` module and if there's anything else you were planning to add then perhaps a separate `ranking` module? [We've had similar thoughts on clustering as a new task.]

You may also be interested in the ranking notebook that has not (yet?) been migrated to the website: https://github.com/fairlearn/fairlearn/blob/main/notebooks/Mitigating%20Disparities%20in%20Ranking%20from%20Binary%20Data.ipynb
In particular, the visualization of the CDFs per group were quite insightful IMO.

Finally, this is a MASSIVE issue that you should absolutely feel free to break up into smaller issues and have a task list for in this issue: https://github.blog/2014-04-28-task-lists-in-all-markdown-documents/","20","0.3115190997927154","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","915254353","issue_comment","https://github.com/Trusted-AI/AIF360/issues/945#issuecomment-915254353","@romanlutz Thank you so much for the extended reply and the link to the notebook, I didn't spot that one yet. 

Yes I have a few use cases, I have added some datasets that are used in the literature to the post.

This issue can indeed easily become very massive, so I will definitely split this up into smaller issues. I thought about starting with implementing a few metrics and this can always be extended with algorithms or synthetic data creation.
This is why I am also not sure about where to place the code... With a separate `ranking` module, do you mean a completely separate folder in the Fairlearn folder? Or a ranking folder inside the `metrics` module?

I like the idea of making this issue a task list and creating a new issue for each smaller task.","30","0.4453448130328354","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","915265918","issue_comment","https://github.com/Trusted-AI/AIF360/issues/945#issuecomment-915265918","At this point I wouldn't worry too much about where to place the code. 

My suggestion would be to include any ranking metrics somewhere within the `metrics` module, e.g., a `_ranking.py` file or something even more specific. But this is something we can further think through in one of the future smaller issues.

Once we're at the stage of implementing a mitigation algorithm we can consider a `ranking` module or, depending on the algorithm, add it to e.g., the existing `postprocessing` module.","30","0.4297156587905487","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","914265055","issue_comment","https://github.com/Trusted-AI/AIF360/issues/944#issuecomment-914265055","Indeed! Would you like to do this @Thomasq99 ?","6","0.1949616648411827","API expansion","Development"
"https://github.com/fairlearn/fairlearn","914267716","issue_comment","https://github.com/Trusted-AI/AIF360/issues/944#issuecomment-914267716","@romanlutz Already on it!  @hildeweerts told me to make a new issue and she was going to assign me to the issue, but she has not gotten around to it yet.","20","0.5761643278421804","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","914279036","issue_comment","https://github.com/Trusted-AI/AIF360/issues/944#issuecomment-914279036","Fantastic! Let us know if you need any help/questions answered.","11","0.6745983323164534","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","914339756","issue_comment","https://github.com/Trusted-AI/AIF360/issues/944#issuecomment-914339756","Thank you for opening this issue @Thomasq99! Feel free to get started with a single class (e.g., `MetricFrame`) for your first PR.","20","0.4904375163741156","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","915894338","issue_comment","https://github.com/Trusted-AI/AIF360/issues/944#issuecomment-915894338","I wrote an example for the MetricFrame class utilizing all methods of the class.

![image](https://user-images.githubusercontent.com/38749496/132654884-e2c533f9-5f7b-47f7-981c-54b191fa632f.png)

I was thinking about making an example per separate function and class. For example, the above example for the MetricFrame class, an example for fairlearn.metrics.count, and so on. Is this indeed what you want? 
If not, please let me know!","15","0.7119436553030306","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","918877016","issue_comment","https://github.com/Trusted-AI/AIF360/issues/940#issuecomment-918877016","@riedgar-ms should we close this since #941 was merged?","24","0.6499898311978849","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","919025010","issue_comment","https://github.com/Trusted-AI/AIF360/issues/940#issuecomment-919025010","Yep, I forgot to add the ""Closes issue X"" on the MR","30","0.2097902097902097","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","898462199","issue_comment","https://github.com/Trusted-AI/AIF360/issues/937#issuecomment-898462199","Thanks for highlighting this.

Looking at the example code (e.g. [here](https://github.com/fairlearn/fairlearn/blob/c51e619e903a4652c79c02e375f145091408a5f3/examples/plot_quickstart.py#L54)), this isn't our plotting code per se - the `by_group` property is a `DataFrame`, so we're just delegating to `pandas` plotting routines.

Can you check which version of `pandas` you're using?","17","0.7832082290797884","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","898968885","issue_comment","https://github.com/Trusted-AI/AIF360/issues/937#issuecomment-898968885","This was also previously mentioned in #852 as a warning that shows up in our builds.","24","0.6380549682875264","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","906598455","issue_comment","https://github.com/Trusted-AI/AIF360/issues/937#issuecomment-906598455","So to check, is this file in the `docs` folder? I've taken care of the last error:
> MatplotlibDeprecationWarning: normalize=None does not normalize if the sum is less than 1 but this behavior is deprecated since 3.3 until two minor releases later. After the deprecation period the default value will be normalize=True. To prevent normalization pass normalize=False results = ax.pie(y, labels=blabels, **kwds)

I wasn't seeing the first error in `plot_quickstart.py` though.","26","0.6995337995337997","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","994250305","issue_comment","https://github.com/Trusted-AI/AIF360/issues/937#issuecomment-994250305","FWIW I can't reproduce this on my machine with the following versions:
```
System:
    python: 3.8.5 
   machine: macOS

Python dependencies:
    Cython: None
matplotlib: 3.4.3
     numpy: 1.19.5
    pandas: 1.3.4
       pip: 20.1.1
     scipy: 1.7.1
setuptools: 49.2.0
   sklearn: 1.0.1
```

Since we don't have version numbers from the original post I'm going to close the issue. @RWilsker feel free to reopen if you can post your versions (using `fairlearn`'s `show_versions()`). I suspect `pandas` is seeing those warnings in the corresponding version and hopefully fixed them along the way, so an upgrade to the latest version of pandas should do the trick. Without knowing your exact versions it's hard to confirm, though.

@alliesaizan the `plot_quickstart.py` file is under `examples`, but `sphinx-gallery` builds this into the documentation we see on the website, so it ends up in the build directory under `docs` even though the source file doesn't live there.

","29","0.4782992135091372","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","983046560","issue_comment","https://github.com/Trusted-AI/AIF360/issues/936#issuecomment-983046560","I can help with this.","0","0.0303030303030302","Dataset usage","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","983429452","issue_comment","https://github.com/Trusted-AI/AIF360/issues/936#issuecomment-983429452","Awesome, @iofall! I have assigned the issue to you. Please let us know if you have any questions.","25","0.3508158508158507","Research","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","897169147","issue_comment","https://github.com/Trusted-AI/AIF360/issues/935#issuecomment-897169147","@candalfigomoro You can actually compute this with `MetricFrame`!
```
>>> mf = MetricFrame(metrics=mean_squared_error, y_true=[1,0,1,0], y_pred=[0,1,1,1], sensitive_features=['a','a', 'b', 'b'])
>>> mf.by_group
sensitive_feature_0
a    1.0
b    0.5
Name: mean_squared_error, dtype: object
>>> mf.difference()
0.5
```

That table was set up based on recommendations by @MiroDudik I believe, so perhaps he can chime in. 🙂 ","12","0.5716363636363638","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","895706920","issue_comment","https://github.com/Trusted-AI/AIF360/issues/933#issuecomment-895706920","Unclear if this issue should be opened in RAI Widgets or here","24","0.5321345321345322","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","895717295","issue_comment","https://github.com/Trusted-AI/AIF360/issues/933#issuecomment-895717295","This is an issue in `raiwidgets` which depends on Fairlearn. I'll move it over. Thanks for reporting @michaelamoako !","24","0.5087976539589445","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","942646056","issue_comment","https://github.com/Trusted-AI/AIF360/issues/931#issuecomment-942646056","@nessamilan do you happen to have those available to download from Figma? I'd like to replace these two as well as the the organization image and our Twitter image.","14","0.6697767145135568","Documentation","Development"
"https://github.com/fairlearn/fairlearn","892533421","issue_comment","https://github.com/Trusted-AI/AIF360/issues/927#issuecomment-892533421","So it seems they've been nice to us on the sklearn side so far, we're using way more than the free credit, and we'd need to pay $180/month for our usage:

![Screenshot from 2021-08-04 12-03-13](https://user-images.githubusercontent.com/1663864/128162994-329ba8ca-9144-4262-a24f-c518bed0d7d0.png)

I agree with @romanlutz that if this is how it's gonna be, we should somehow move to another solution.","20","0.5005703516341816","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","892751347","issue_comment","https://github.com/Trusted-AI/AIF360/issues/927#issuecomment-892751347","Update: I got a response!

> Hi Roman,
> 
> Thank you for contacting CircleCI Support!
> 
> Sorry to hear you are having trouble getting your builds to trigger.
> 
> Would it be possible to get the webhook ID/payload for a PR from your project that was supposed to trigger a build? With this, it will give me more insight into what is blocking you.
> 
> https://support.circleci.com/hc/en-us/articles/360021511153-How-to-view-your-GitHub-webhook-deliveries
> 
> I do not think it is related to credits as there would be some indication of that on the dashboard (a workflow should still appear even if it is blocked due to no credits).
> 
> Please feel free to ask any additional questions as well!
> 
> Best Regards
> 
> [name omitted]
Support Engineer @ CircleCI

The webhooks tab turns out to be empty, so we'll see what they can find. The quick response is certainly encouraging. In parallel I'm trying a few other things that could solve this long-term...","32","0.319845121884714","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","893198698","issue_comment","https://github.com/Trusted-AI/AIF360/issues/927#issuecomment-893198698","I think I managed to resolve this with help from CircleCI! The problem turned out to be that my fork romanlutz/fairlearn got added (or technically: ""followed"", whatever that means) and it confused CircleCI. Lesson learned: don't add forks to CircleCI!

After ""unfollowing"" I pushed an empty commit to a PR of mine that was lacking builds (#925), and voila, they're back! I'll close this issue accordingly, but if this shows up again please reopen.

I will say this: CircleCI support was first rate, and extraordinarily quick in responding given that we're not a paying customer. ","32","0.4341885108756273","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","893248786","issue_comment","https://github.com/Trusted-AI/AIF360/issues/927#issuecomment-893248786","Final update on credits since CircleCI responded in direct reference to my assumption about credits:
> I took a look at the Github issue you linked in your tweet and thought I would give some clarification regarding credits for free plans.
> For open source linux builds, you can use 400,000 credits per month (macOS is 25,000 credits), but those totals won't appear on the plan usage page. I believe it currently shows it as a negative balance.

So my worries were entirely unjustified 🙂 That is probably the same thing you're seeing for scikit-learn @adrinjalali.","20","0.3358367308013255","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","893266717","issue_comment","https://github.com/Trusted-AI/AIF360/issues/927#issuecomment-893266717","Awesome, thanks for checking Roman","20","0.1578198088265202","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","891658275","issue_comment","https://github.com/Trusted-AI/AIF360/issues/924#issuecomment-891658275","I've been playing around a bit with this in a test repo: https://github.com/hildeweerts/testcitation/blob/main/CITATION.cff.

There was a minor issue with missing quotation marks in the article title. But the main issue with the current file seems to be that it is required to have authors and a version. It turns out that it doesn't actually allow you to directly cite the references; only the metadata provided in the 'main' part of the citation file:

<img width=""340"" alt=""image"" src=""https://user-images.githubusercontent.com/24417440/127983618-36afebc3-426a-4411-81b5-13689ae7df3c.png"">

Despite requiring the 'version', the rendering is a bit inconsistent; in APA you get the version, but in BibTex not. This is not entirely unsurprising as `misc` doesn't have a `version` field, but even manually adding a `note=""version 0.7.0""` field doesn't show in the BibTex on GitHub:

APA: 
````
Lisa M., Bot H. (2021). Fairlearn (version 0.7.0).
````

BibTex: 
```@misc{Lisa_Fairlearn_2021,
author = {Lisa, Mona and Bot, Hew},
month = {7},
title = {Fairlearn},
url = {https://github.com/fairlearn/fairlearn},
year = {2021}
}
```

Although I'm not a fan of the inconsistency, I do think it makes some sense to cite the package rather than the paper, given that the current version of Fairlearn is substantially different from the version for which the white paper was written. But I'm not sure who should be included as 'authors' of the package in that case.","25","0.2679663104191406","Research","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","892107009","issue_comment","https://github.com/Trusted-AI/AIF360/issues/924#issuecomment-892107009","Oooh that's a tough one! I definitely agree that that would be a preferable state, although I have never seen it done anywhere before. scikit-learn, for example, has two papers that they ask you to cite https://scikit-learn.org/stable/about.html#citing-scikit-learn. 

That said, we are a project that's focusing on educational materials, so I could see how we could make the case that our website/educational materials are the thing to cite. In that case I would perhaps suggest the following innovative option:
- each release of the package is a new version of the docs and thereby provides a nice snapshot (similar to ArXiv where you have v1, v2, etc.)
- Whenever we do a release we update the list of people who contributed to the docs (so if you cite v0.7.0 the authors are a subset of v0.8.0, for example). The problem is we'd need to get a starting set and I'm a little uncomfortable adding folks that don't consent explicitly (perhaps AUTHORS.md is a good starting point?). One could be more restrictive and say that only a certain set of changes counts, but then one would have to define where exactly the cutoff is and it's really not all that inclusive IMO. This will blow up over time, though, but then if anyone wants to cite this as anything other than <first author> et al. that's really their problem. Ordering is another question, but I suppose simply appending makes sense (?)

Alternatively, we can write a new paper. Who's got time? 🤣 

I think I made the mistake of using `references` rather than the ""first-level"" attributes. Thanks for playing around with this to show how to do it properly 🙂 ","25","0.53488791258308","Research","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","892482931","issue_comment","https://github.com/Trusted-AI/AIF360/issues/924#issuecomment-892482931","Although I really like your idea in terms of inclusivity, intuitively it doesn't feel right to me that something like a typo fix would get equal attribution to somebody who wrote an entire section in the user guide. But like you already said, it's very tricky to decide ""what's enough"". We could be very rigorous and only add maintainers plus maybe a few other people. On a more practical note, many people are very sloppy with citations and will simply cite whatever they find first, so I doubt that they will carefully select the one for the version they are using, resulting in messy attribution after all...

At some point we should definitely write a new paper, but I'd probably wait until we have decided upon what the educational materials will (roughly) look like. We could of course already get started with writing parts about e.g., the reasoning behind `MetricFrame` API (there will still be a similar ""problem"" of authorship though).

Regardless of authorship, if we're going to use the citation.cff file I think we need to figure out a way to render the version in BibTex (although the date might already give a hint on the version?)","25","0.2726685174576595","Research","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","889581923","issue_comment","https://github.com/Trusted-AI/AIF360/issues/923#issuecomment-889581923","@MiroDudik this sounds like a use case that's similar to your ideas on scalability.
@Dref360's streaming metrics may also be related. 
Just tagging both of you as FYI in case you'd like to comment.","15","0.6045217803030306","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","886220923","issue_comment","https://github.com/Trusted-AI/AIF360/issues/915#issuecomment-886220923","Min requirements are usually not the same for all versions of python. To me, min requirements are tested with the oldest python we support, not with the newest one. Although I'm usually on the side of having newer dependencies, but I don't think we need to update what `pip install fairlearn` _requires_ just because the situation under the latest python says so.  ","32","0.6398391127772569","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","888973068","issue_comment","https://github.com/Trusted-AI/AIF360/issues/915#issuecomment-888973068","I think I'd prefer a situation where having the minimum requirements installed will always work, which is not the case for the current requirements, right? I'm not sure whether that's realistic though... How does scikit-learn deal with this, @adrinjalali?","32","0.5329071969696969","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","889285804","issue_comment","https://github.com/Trusted-AI/AIF360/issues/915#issuecomment-889285804","@hildeweerts that's what I assumed as the expected situation before @adrinjalali's post. I do see the benefit in what @adrinjalali suggested, since it's not actually guaranteed that the latest working version of a package supports both Python 3.6 AND Python 3.9 (if they've already moved on to 3.7). We could have a python version-specific set of requirements, but that's not ideal either and doesn't gel well with PyPI which only takes one package upload (rather than n for n different Python versions).

So in conclusion I am in support of @adrinjalali's proposal of pinning the requirements-fixed build to the oldest python version we support (currently 3.6), even if the package versions used there may not even exist in the latest version we support (currently 3.9).","32","0.6697588126159556","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","889766236","issue_comment","https://github.com/Trusted-AI/AIF360/issues/915#issuecomment-889766236","Thank you for explaining it further @romanlutz. Sounds very reasonable. 

A related question: how do we decide to drop support for a particular version of Python?","32","0.4564531513684054","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","890255602","issue_comment","https://github.com/Trusted-AI/AIF360/issues/915#issuecomment-890255602","We don't 🙂 We just follow what NumPy does: https://numpy.org/neps/nep-0029-deprecation_policy.html
In particular:
```
On Dec 26, 2021 drop support for Python 3.7 (initially released on Jun 27, 2018)
...
On Apr 14, 2023 drop support for Python 3.8 (initially released on Oct 14, 2019)
```","32","0.6343402973837758","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","894075107","issue_comment","https://github.com/Trusted-AI/AIF360/issues/915#issuecomment-894075107","In conclusion, this would mean that the required change is to change the following line in `nightly-requirements-fixed.yml` from
```
    pyVersions: [3.6, 3.7, 3.8, 3.9]
```
to
```
    pyVersions: [3.6, 3.7, 3.8]
```

I'll tag this as an issue for new contributors, but would also love to hear if @riedgar-ms has any objections.","20","0.850486516541562","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","894079676","issue_comment","https://github.com/Trusted-AI/AIF360/issues/915#issuecomment-894079676","Für each Python version we should have the oldest dependencies we support, I would object dropping Python 3.9 here. Alternatively, we can do one ""min dependency"" check and only include Python 3.6","32","0.7159299416180149","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","894399838","issue_comment","https://github.com/Trusted-AI/AIF360/issues/915#issuecomment-894399838","I'm confused. I don't see scikit-learn testing this with per-version dependencies. https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/_min_dependencies.py

What you wrote sounds to me like we need multiple sets of dependency versions, because there's no way (that I'm aware of) to `pip install --min-version` or something like that. That's why @riedgar-ms 's script reads the version from the file and ensures we use the oldest one possible using `==` rather than `>=`. ","32","0.4614046951985315","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","895207934","issue_comment","https://github.com/Trusted-AI/AIF360/issues/915#issuecomment-895207934","@romanlutz I mean if we're going to test the min dependencies per python version. AFAIK in sklearn we only check for the oldest deps on the oldest python we support (i.e. 3.7): https://github.com/scikit-learn/scikit-learn/blob/3c732b9f6a77e95dfa6beb154ca2e1e7848b74f9/azure-pipelines.yml#L167-L174","32","0.7832082290797886","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","895224139","issue_comment","https://github.com/Trusted-AI/AIF360/issues/915#issuecomment-895224139","Sorry, should have commented before. I agree that maintaining separate minimum requirements per Python version is going to be a bit of a pain.

@adrinjalali  am I right in thinking that your suggestion is essentially to reduce the `nightly-fixed` set to `pyVersions: [3.6]` ? I can live with that.","20","0.4105539850220702","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","895264464","issue_comment","https://github.com/Trusted-AI/AIF360/issues/915#issuecomment-895264464","I'd vote for 3.7, but yes, having only one python version there is what I mean.","20","0.2299367299367299","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","895727935","issue_comment","https://github.com/Trusted-AI/AIF360/issues/915#issuecomment-895727935","Sounds good. So what I wrote [earlier](https://github.com/fairlearn/fairlearn/issues/915#issuecomment-894075107) wasn't all that far off, then! ","20","0.5321345321345321","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","895834647","issue_comment","https://github.com/Trusted-AI/AIF360/issues/915#issuecomment-895834647","> Sounds good. So what I wrote [earlier](https://github.com/fairlearn/fairlearn/issues/915#issuecomment-894075107) wasn't all that far off, then!

(Perhaps you can update your reply to what is expected now for the potential new contributor who wants to pick this up?)","20","0.602232854864434","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","895931742","issue_comment","https://github.com/Trusted-AI/AIF360/issues/915#issuecomment-895931742","Hello, I am a newbie to opensource. I followed your conversation above and am interested to contribute. Could you please tell me where exactly to start?  
Thanks","20","0.8044965786901274","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","896382706","issue_comment","https://github.com/Trusted-AI/AIF360/issues/915#issuecomment-896382706","@sushmitaS16 You can follow the steps outlined in [this post](https://github.com/fairlearn/fairlearn/issues/915#issuecomment-894075107). 

@hildeweerts The described lines there are fine. It does pass for all three listed versions, just 3.9 is failing. In the future we'll eventually remove 3.6 support at which point we can remove it from here, too. But there's no particular benefit in restricting to just 3.6 or 3.7. We do know it works for all three.","32","0.4264447979118235","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","896526863","issue_comment","https://github.com/Trusted-AI/AIF360/issues/915#issuecomment-896526863","Thanks @romanlutz 
Is the file mentioned [here](https://github.com/fairlearn/fairlearn/issues/915#issuecomment-894075107) the only one which needs to be edited, or do I need to modify any other .yml file?","16","0.3201119475004827","Testing","Maintenance"
"https://github.com/fairlearn/fairlearn","896544880","issue_comment","https://github.com/Trusted-AI/AIF360/issues/915#issuecomment-896544880","It's the only one 🙂 I will kick off a run of that pipeline once you have a pull request, just to be sure it works fine. Thanks again for picking this up!","32","0.2617255356108859","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","884326052","issue_comment","https://github.com/Trusted-AI/AIF360/issues/911#issuecomment-884326052","We may want to separate this into several issues (one per algorithm) if it turns out to be a lot of work. I do expect there to be a lot of common foundational work that can be shared by all of them since they already share the input validation code.","15","0.3859049788068578","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","884339288","issue_comment","https://github.com/Trusted-AI/AIF360/issues/911#issuecomment-884339288","This issue certainly doesn't have to be fixed in a single PR, we could update the description and list the algorithms which are relevant and go through them one by one, or we could have a separate issue for each, I'm easy either way.","20","0.2934609250398725","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","998252524","issue_comment","https://github.com/Trusted-AI/AIF360/issues/911#issuecomment-998252524","Hi @adrinjalali @romanlutz, I think scikit-learn's `LabelEncoder` can be used? I could give it a try if this issue is still relevant :)","20","0.76014173998045","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","999135536","issue_comment","https://github.com/Trusted-AI/AIF360/issues/911#issuecomment-999135536","@SeanMcCarren it's definitely still relevant! I would suggest starting with one algorithm only and trying it out with LabelEncoder. Thanks for volunteering!","20","0.5403860670764583","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","1011094259","issue_comment","https://github.com/Trusted-AI/AIF360/issues/911#issuecomment-1011094259","I've delved a bit deeper into the issue now. Starting with a list of algorithms to update (as @adrinjalali mentioned):
- [ ] threshold optimizer
- [ ] exponentiated gradient
- [ ] grid search

Then, my thoughts so far... My initial thought was that we could just translate strings to numbers in the shared input validation and be done with it. But I now think that this does not work, because we need to persist this encoding throughout calls to fit & predict. 
I assume we should follow [this sklearn guideline](https://scikit-learn.org/stable/developers/develop.html#specific-models) which tells us that we should store a list of classes in `self.classes_` on every estimator. This is kind of similar to what @adrinjalali suggests with `pos_label`, but different, so I guess we additionally need to add `pos_label` to the binary predictors.

Side notes: 
1) preferrably, I'd avoid writing the same piece of code to do this in every algorithm, but then I see that sklearn actually does repeat the initialization of `self.classes_` at times, instead of capturing this in some mixin or base class. I'd rather follow sklearn here then, but I wonder why sklearn made this decision (@adrinjalali maybe you know?)
2)  In `interpolated_thresholder.py` line 141 there is `enforce_binary_labels = False` while in `threshold_optimizer.py` line 288 there is `enforce_binary_labels = True`. I can not infer whether threshold optimizer only works on binary data (I think so after reading the docstring).
","23","0.411942444645234","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","1012172201","issue_comment","https://github.com/Trusted-AI/AIF360/issues/911#issuecomment-1012172201","@SeanMcCarren overusing Mixins makes the code less readable, and moves the codebase more towards a large framework kinda codebase, which is hard to maintain and read and change especially for new contributors. If the repeated code is just a single line of code, we don't try to abstract it away. There are exceptions to this, but that's partly historical. For instance, `MetaEstimatorMixin` used to be a lot larger than what it is now, which is a single line of code now.","30","0.3893037147657271","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","883392667","issue_comment","https://github.com/Trusted-AI/AIF360/issues/910#issuecomment-883392667","If you look at the rest of the stacktrace, the issue seems to be coming from our side in `fairlearn/reductions/_moments/utility_parity.py:251` : 


``` python
test_othermlpackages/package_test_common.py:88: in run_gridsearch_classification
    gs.fit(X_train, Y_train, sensitive_features=A_train)
fairlearn/reductions/_grid_search/grid_search.py:165: in fit
    weights = self.constraints.signed_weights(lambda_vec)
fairlearn/reductions/_moments/utility_parity.py:251: in signed_weights
    lambda_event = (lambda_vec[""+""] - self.ratio * lambda_vec[""-""]).sum(level=_EVENT) / \
/usr/share/miniconda/envs/test-conda-env/lib/python3.8/site-packages/pandas/core/generic.py:10699: in sum
    return NDFrame.sum(
/usr/share/miniconda/envs/test-conda-env/lib/python3.8/site-packages/pandas/core/generic.py:10437: in sum
    return self._min_count_stat_function(
```","10","0.3758653797564305","Model development","Development"
"https://github.com/fairlearn/fairlearn","884704258","issue_comment","https://github.com/Trusted-AI/AIF360/issues/910#issuecomment-884704258","In the release notes they say we should use groupby instead
https://pandas.pydata.org/pandas-docs/stable/whatsnew/v1.3.0.html","24","0.3105228105228104","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","880290689","issue_comment","https://github.com/Trusted-AI/AIF360/issues/900#issuecomment-880290689","Taking a step back: is that a useful page to have? [I created it so I don't feel bad questioning this 🤣 ]","24","0.2794133505877278","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","880473924","issue_comment","https://github.com/Trusted-AI/AIF360/issues/900#issuecomment-880473924","I can imagine that the repo can be a bit intimidating for new folks looking to contribute to the project, although right now there might not be enough information on this page for it to be really useful.

I'd be curious to hear what some of our newer contributors think @shimst3r @bram49 @alexquach @kurianbenoy ","25","0.4293713532843968","Research","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","881335096","issue_comment","https://github.com/Trusted-AI/AIF360/issues/900#issuecomment-881335096","I stumbled upon that page by chance, but didn't get anything out of it very much (sorry @romanlutz :sweat_smile:).

What *might* be a good idea in my opinion would be moving this section out of the documentation, into the README (or somewhere else closer to the ""raw"" source code). That's where I'd look for an explanation of the repository intuitively and have so for other projects in the past.

This could be also used to explain non-standard files like `requirements-customplots.txt` as well, or why there is a `pyproject.toml`, `setup.py`, **and** `setup.cfg`.

That's something I'd expect from a well-maintained project like Fairlearn! :nerd_face:","24","0.2538161699987427","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","883436796","issue_comment","https://github.com/Trusted-AI/AIF360/issues/900#issuecomment-883436796","My preference would be somewhere in the documentation, linked from the contribution guidelines. But I don't have a strong preference as long as people can find it while reading the contributing guidelines.","14","0.6446528060877836","Documentation","Development"
"https://github.com/fairlearn/fairlearn","905020790","issue_comment","https://github.com/Trusted-AI/AIF360/issues/900#issuecomment-905020790","Hi, I would like to help with a good first issue. Is this still an option for new contributors @hildeweerts? 
The layout I have in mind to start looks like the following:

Title
[Same short introductory paragraph as in [README](https://github.com/fairlearn/fairlearn#fairlearn)]

- Tests
[Add a short description of what tests are intended for. Unit tests, notebooks, etc]

- Visualization (delete)

- Automation
[Add a short description, goal]

- Documentation
   - API
    [Short description]
    - User guide
    [Short description]

- Examples
[List of examples with links]. We could potentially mention here that the API also includes examples when issue #894 is solved.

Thanks.
","14","0.6317422132185366","Documentation","Development"
"https://github.com/fairlearn/fairlearn","906673211","issue_comment","https://github.com/Trusted-AI/AIF360/issues/900#issuecomment-906673211","Maybe @romanlutz , @adrinjalali  or @shimst3r can confirm here?  :) ","20","0.511988011988012","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","908978543","issue_comment","https://github.com/Trusted-AI/AIF360/issues/900#issuecomment-908978543","I'm now wondering if this is a good first issue actually, since it requires a deep understanding of how the repo is structured. Hmm.

To me, the documentation should allow first time contributors and people who are not familiar with the repo to find their way around when they come here, and to know where to go for each type of contribution they'd like to have.","20","0.7638670441438606","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","910085908","issue_comment","https://github.com/Trusted-AI/AIF360/issues/900#issuecomment-910085908","For a first non-outdated version I'd be very happy with the layout suggested by @ojeda-e. WDYT @adrinjalali?","32","0.596105143655576","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","910089312","issue_comment","https://github.com/Trusted-AI/AIF360/issues/900#issuecomment-910089312","I would change it a little bit.

It doesn't include the modules and the code.

For examples, I don't think we need to have a list of examples here, it'd be very quickly outdated, it makes more sense to tell people how they can find the right example file if they want to contribute there.

And for the CI, it needs to explain where the scripts are and which CIs we use and for what.","14","0.3895162808206286","Documentation","Development"
"https://github.com/fairlearn/fairlearn","910095997","issue_comment","https://github.com/Trusted-AI/AIF360/issues/900#issuecomment-910095997","Ah yes, I agree.

@ojeda-e do you feel comfortable adding these suggestions? As indicated by @adrinjalali this might not be such a good first issue after all, so if you don't feel like digging through the repo please feel free to drop this issue!","20","0.6419238254100638","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","911154980","issue_comment","https://github.com/Trusted-AI/AIF360/issues/900#issuecomment-911154980","Thanks for your answers @hildeweerts @adrinjalali.
If you don't mind I would it give a try with the suggestions from @adrinjalali. If it's not good enough, you may not have an issue fixed, but it'll be still useful to me. 
If you think there is a better issue to help with, I would appreciate your suggestions as well.","20","0.6784063394232887","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","911444220","issue_comment","https://github.com/Trusted-AI/AIF360/issues/900#issuecomment-911444220","Awesome, @ojeda-e! Let's give it a try then :)","20","0.526947325605044","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","879712464","issue_comment","https://github.com/Trusted-AI/AIF360/issues/899#issuecomment-879712464","I have replaced `html_logo = ""_static/images/fairlearn_full_color.png""` in conf.py. Is there anything else I must look into? Pls guide me
","14","0.8602742118592255","Documentation","Development"
"https://github.com/fairlearn/fairlearn","879715893","issue_comment","https://github.com/Trusted-AI/AIF360/issues/899#issuecomment-879715893","Hi @svenkat19! Did you open a pull request for the change? You can find more information on how to contribute to fairlearn in this helpful guide by @romanlutz: https://github.com/romanlutz/fairlearn-scipy-sprint/blob/main/README.md

And specifics on how to build the documentation locally can be found here: https://fairlearn.org/main/contributor_guide/contributing_documentation.html","14","0.6115659702616226","Documentation","Development"
"https://github.com/fairlearn/fairlearn","879835897","issue_comment","https://github.com/Trusted-AI/AIF360/issues/898#issuecomment-879835897","I will see about creating a more compact reproducer. I don't believe this is related to AzureML/AutoML per se. It's that when `GridSearch` validates its inputs, it's too restrictive in the call to `check_X_y()`","17","0.3144031110132805","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","880891574","issue_comment","https://github.com/Trusted-AI/AIF360/issues/898#issuecomment-880891574","Updated the description to code which should work when run on anyone's machine.","29","0.1775898520084566","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","879703225","issue_comment","https://github.com/Trusted-AI/AIF360/issues/897#issuecomment-879703225","Hi @MG-Microsoft. Our `reductions` module offers support for regression tasks. You can find more information in our [user guide](https://fairlearn.org/v0.6.2/user_guide/mitigation.html#fairness-constraints-for-regression).

Please let me know if this answers your question.","28","0.5213344204375595","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","879829132","issue_comment","https://github.com/Trusted-AI/AIF360/issues/897#issuecomment-879829132","Thank you so much for your reply!
If I understood properly, this reduction method for regression is a post-processing method while in classification (using GridSearch), we can mitigate within the training process. So, we do not support a similar thing in regression? That was my feature request.
Also in bounded group loss example of your link, we have  :
bgl = BoundedGroupLoss(ZeroOneLoss(), upper_bound=0.1)

ZeroOneLoss is defining the min_val to be 0 and max_val to be 1. Then the question is what is upper bound ?

Regards,
Mohammad
From: Hilde Weerts ***@***.***>
Sent: Wednesday, July 14, 2021 4:31 AM
To: fairlearn/fairlearn ***@***.***>
Cc: Mohammad Ghodratigohar ***@***.***>; Mention ***@***.***>
Subject: Re: [fairlearn/fairlearn] Mitigation with Fairlean for REgression (#897)


Hi @MG-Microsoft<https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2FMG-Microsoft&data=04%7C01%7Cmohammadg%40microsoft.com%7C1c0f267f6ded441385ba08d946a1a584%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637618482375653103%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=cczP3FMfjU9BWOOQVWtmtNzHSHGtRFm76w0Mucny3Hc%3D&reserved=0>. Our reductions module offers support for regression tasks. You can find more information in our user guide<https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Ffairlearn.org%2Fv0.6.2%2Fuser_guide%2Fmitigation.html%23fairness-constraints-for-regression&data=04%7C01%7Cmohammadg%40microsoft.com%7C1c0f267f6ded441385ba08d946a1a584%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637618482375653103%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=mG8khG%2FO25VpJpiKmDK8fpZ5jsKbHT9TyVrsFPsgmPs%3D&reserved=0>.

Please let me know if this answers your question.

-
You are receiving this because you were mentioned.
Reply to this email directly, view it on GitHub<https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Ffairlearn%2Ffairlearn%2Fissues%2F897%23issuecomment-879703225&data=04%7C01%7Cmohammadg%40microsoft.com%7C1c0f267f6ded441385ba08d946a1a584%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637618482375663096%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=Kjtk3aI%2BpSKgRCizvlQvxBBEW0LY9G%2BQlhqRU6pJWbo%3D&reserved=0>, or unsubscribe<https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fnotifications%2Funsubscribe-auth%2FATAHVM4ORQQDPTFHAEZVVZLTXVDKRANCNFSM5AKQRZJA&data=04%7C01%7Cmohammadg%40microsoft.com%7C1c0f267f6ded441385ba08d946a1a584%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637618482375673089%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=tX63uV3CyKkfJTW7pzGLFCtzl3xbRUxqcSfk9ZC8SXw%3D&reserved=0>.
","28","0.3781843862352335","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","879837186","issue_comment","https://github.com/Trusted-AI/AIF360/issues/897#issuecomment-879837186","Sorry, we must have miscommunicated in our call. The `reductions` methods always work by retraining the models in various ways.","6","0.6039464411557434","API expansion","Development"
"https://github.com/fairlearn/fairlearn","879848322","issue_comment","https://github.com/Trusted-AI/AIF360/issues/897#issuecomment-879848322","Hi, @MG-Microsoft.

> If I understood properly, this reduction method for regression is a post-processing method while in classification (using GridSearch), we can mitigate within the training process. So, we do not support a similar thing in regression? That was my feature request.

The reductions approach does not work as a post-processing algorithm for an existing machine learning model but more like a 'wrapper' over a learning algorithm.
An example of how we can use `GridSearch` can be found [here](https://fairlearn.org/v0.7.0/auto_examples/plot_grid_search_census.html#mitigation-with-gridsearch). In the regression scenario, you can replace the `DemographicParity` constraint with `BoundedGroupLoss` (please correct me if I'm wrong about this @MiroDudik @romanlutz @riedgar-ms).


> Also in bounded group loss example of your link, we have  :
bgl = BoundedGroupLoss(ZeroOneLoss(), upper_bound=0.1)

I think the Note below the example answers your question regarding the upperbound: *""In the example above the BoundedGroupLoss object does not use the upper_bound argument. It is only used by reductions techniques during the unfairness mitigation. As a result the constraint violation detected by gamma is identical to the mean absolute error.""*

","28","0.5874676644493719","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","880064173","issue_comment","https://github.com/Trusted-AI/AIF360/issues/897#issuecomment-880064173","From a technical standpoint your answer is absolutely correct @hildeweerts. As mentioned elsewhere we always advocate for being mindful of your application context. `BoundedGroupLoss` is the only regression constraint we currently support, but that doesn't necessarily mean that it's appropriate for your context. 

I will close this issue since it's a misunderstanding/question that's been answered, but I think we need to redouble our efforts on documenting reductions.","28","0.3414669257899662","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","877465571","issue_comment","https://github.com/Trusted-AI/AIF360/issues/892#issuecomment-877465571","Hello, I'm new and would like to contribute by starting small and then making my way up.
Can this be assigned to me?

I believe the necessary change has to be made here?https://github.com/fairlearn/fairlearn/blob/main/docs/user_guide/assessment.rst","20","0.7952153110047848","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","877489202","issue_comment","https://github.com/Trusted-AI/AIF360/issues/892#issuecomment-877489202","That's correct @nishit-prasad ! It should say `control_features`. I'll assign it to you 🙂 ","16","0.4940127077223855","Testing","Maintenance"
"https://github.com/fairlearn/fairlearn","878543127","issue_comment","https://github.com/Trusted-AI/AIF360/issues/892#issuecomment-878543127","@romanlutz PR is up and ready! 😄 ","24","0.551555826723612","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","877501555","issue_comment","https://github.com/Trusted-AI/AIF360/issues/890#issuecomment-877501555","@ziqi-ma you may want to take a look at #460 which goes in depth about this topic. If there are additional questions feel free to reopen. Thanks!","20","0.6003667245705464","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","876662682","issue_comment","https://github.com/Trusted-AI/AIF360/issues/888#issuecomment-876662682","I'm happy with either pandas or sklearn style","15","0.3822410147991543","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","876784969","issue_comment","https://github.com/Trusted-AI/AIF360/issues/888#issuecomment-876784969","I prefer the scikit-learn style. So much nicer with logos and clear description.
Section within ""About Us"" works for me.
I think we also need to curate a list of what would be included so far. Perhaps I can get started with that.","14","0.7761924174480228","Documentation","Development"
"https://github.com/fairlearn/fairlearn","876651781","issue_comment","https://github.com/Trusted-AI/AIF360/issues/886#issuecomment-876651781","How do we get a permanent discord invite link?
","11","0.4918414918414917","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","876941988","issue_comment","https://github.com/Trusted-AI/AIF360/issues/886#issuecomment-876941988","@adrinjalali You can set the settings of the invite link under ‘link settings’. This link will allegedly never expire and has unlimited uses: https://discord.gg/R22yCfgsRn","11","0.2877687284466945","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","877001388","issue_comment","https://github.com/Trusted-AI/AIF360/issues/886#issuecomment-877001388","Following the instructions, `make doc` yields the following warning:
```shell
WARNING: 'pypandoc' not available. Using Sphinx-Gallery to convert rst text blocks to markdown for .ipynb files.
```
Is this expected or did I miss something in the setup process? :nerd_face: ","14","0.456054488355969","Documentation","Development"
"https://github.com/fairlearn/fairlearn","877025394","issue_comment","https://github.com/Trusted-AI/AIF360/issues/886#issuecomment-877025394","The latest version mentions the pandoc installation: https://fairlearn.org/main/contributor_guide/contributing_documentation.html
It's not related to the icons, though, so it's up to you whether you want to do that.

BTW this is the right icon https://fontawesome.com/v5.15/icons/discord?style=brands and the color should probably be sort of like this: https://en.wikipedia.org/wiki/Discord_%28software%29#/media/File:Discord_logo.svg","14","0.5887916431394693","Documentation","Development"
"https://github.com/fairlearn/fairlearn","875730036","issue_comment","https://github.com/Trusted-AI/AIF360/issues/884#issuecomment-875730036","I would love to pick this up.","20","0.4871995820271684","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","870734849","issue_comment","https://github.com/Trusted-AI/AIF360/issues/880#issuecomment-870734849","Jordan, can you provide location and instructions for where and how we can swap in illustrations once they are complete? As of now this is for the homepage only since other pages are postponed.

","14","0.5925837320574163","Documentation","Development"
"https://github.com/fairlearn/fairlearn","950987875","issue_comment","https://github.com/Trusted-AI/AIF360/issues/880#issuecomment-950987875","This is currently blocked, because we are waiting for the illustrations to be provided, so closing for now.","28","0.4675123326286117","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","876039680","issue_comment","https://github.com/Trusted-AI/AIF360/issues/874#issuecomment-876039680","Your assessment is correct @hildeweerts! @riedgar-ms filed an issue for this with sphinx-multiversion. I'm starting to wonder whether we'd be better off not using sphinx-multiversion and instead follow the approach scikit-learn uses. If I understand correctly that would involve building the doc pages separately and inserting a common nav part on each page with which one can move to the other versions (although only to the landing page of the other versions, not to the exact same page in a different version since that may not even exist). Is that an accurate description @adrinjalali ?","15","0.4598860358105045","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","876651008","issue_comment","https://github.com/Trusted-AI/AIF360/issues/874#issuecomment-876651008","Yes, that's correct. You independently would build the docs for different versions, which also makes a few things easier for us (like removing dependencies). As in, I think we should move away from sphinx-multiversion. It was a good quick fix, but I think now it's worth investing in a better solution.","32","0.4976943346508563","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","866190882","issue_comment","https://github.com/Trusted-AI/AIF360/issues/872#issuecomment-866190882","Hello, I have made the change and verified it a local build. I will go ahead and make a PR. ","32","0.7100183327941335","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","872088422","issue_comment","https://github.com/Trusted-AI/AIF360/issues/872#issuecomment-872088422","#873  was merged, so I'm closing this issue.","24","0.3792963188936343","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","861560038","issue_comment","https://github.com/Trusted-AI/AIF360/issues/866#issuecomment-861560038","Tagging @alexquach  and @michaelamoako too.","15","0.6745983323164533","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","862204818","issue_comment","https://github.com/Trusted-AI/AIF360/issues/866#issuecomment-862204818","It's a good point. It also depends on how we calculate those bars, we could do bootstrap on the end result of a metric, which would be a bit computationally intensive, but gives us somewhat reliable results.","7","0.5329071969696972","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","885089318","issue_comment","https://github.com/Trusted-AI/AIF360/issues/866#issuecomment-885089318","If they're calculated using standard error (symmetric bounds), how might we bootstrap an end result on a metric like Recall for example @adrinjalali ? Assume we want to understand the bounds around Recall difference (we have the bounds around Recall per subgroup)","3","0.5753734528382414","Bias detection metrics validation","Validation"
"https://github.com/fairlearn/fairlearn","885248415","issue_comment","https://github.com/Trusted-AI/AIF360/issues/866#issuecomment-885248415","@michaelamoako : the idea would be to generate, say 1000 bootstrap samples and calculate the quantiles of all the entries in `by_group` as well as the quantiles of `difference()`, `ratio()`, `group_min()` and `group_max()`. With 1000 bootstrap samples, we have enough data to calculate 95% confidence intervals. So with bootstrap, we would be reporting confidence intervals for everything.","7","0.5381125541125544","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","885528322","issue_comment","https://github.com/Trusted-AI/AIF360/issues/866#issuecomment-885528322","@MiroDudik my stat is kinda rusty, but do we need 1000 for the confidence interval to be kinda reliable? We could of course have that number as an input argument as well.","13","0.3410374935798664","Artifact generation and benchmarking","Deployment"
"https://github.com/fairlearn/fairlearn","863555044","issue_comment","https://github.com/Trusted-AI/AIF360/issues/858#issuecomment-863555044","### **New logo**: 
![Small](https://user-images.githubusercontent.com/6819397/125502725-dbf0413c-f6f6-4d87-9605-2bd3b38aa4d0.png)


**Export SVG logo here**: 
[Logo Asset](https://www.figma.com/file/wt76z0M87RBDDEoyAEpD2g/OPEN-SOURCE-Fairlearn-Redesign?node-id=2684%3A2790)


**To export SVG**:  
1. Click on Lockup / small version
2. Export menu appears on bottom right pane 
3. Select ""SVG"" if it's not already there as default
4. Click ""Export"" to save locally","24","0.4982365219894911","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","863829441","issue_comment","https://github.com/Trusted-AI/AIF360/issues/858#issuecomment-863829441","Looks good, @nessamilan!","14","0.4871995820271684","Documentation","Development"
"https://github.com/fairlearn/fairlearn","893046583","issue_comment","https://github.com/Trusted-AI/AIF360/issues/858#issuecomment-893046583","@nessamilan I tried the steps you mentioned above, but I can only export the individual pieces, so it ends up as 4 SVG files (3 rectangles for the bars and the Fairlearn text). Any chance they could be exported together?","24","0.4398082386363636","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","893280247","issue_comment","https://github.com/Trusted-AI/AIF360/issues/858#issuecomment-893280247","@romanlutz to expert as one image you have to select it in the pane on the lefthandside: 
<img width=""1272"" alt=""image"" src=""https://user-images.githubusercontent.com/24417440/128320443-5d47db24-1883-4cd3-898d-a8d2d3ddea96.png"">","24","0.3905180840664712","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","876477086","issue_comment","https://github.com/Trusted-AI/AIF360/issues/855#issuecomment-876477086","I'd be interested in picking this up.","20","0.5761643278421804","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","876501886","issue_comment","https://github.com/Trusted-AI/AIF360/issues/855#issuecomment-876501886","@matthew-so please tackle this issue one file/module at a time. You can figure out when things were added to the library with the help of `git blame` or gitlens on VSCode for instance, and checking the repository's history and changelog.","21","0.4105539850220701","Installation and shell commands","Deployment"
"https://github.com/fairlearn/fairlearn","876502924","issue_comment","https://github.com/Trusted-AI/AIF360/issues/855#issuecomment-876502924","Got it; thanks for the advice!","8","0.194961664841183","Understanding Fairness Definition","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","915748097","issue_comment","https://github.com/Trusted-AI/AIF360/issues/855#issuecomment-915748097","Since I do not have the time to fully commit to this, it would be preferable if others could tackle sections at a time. For example, I will now be working on fairlearn.metrics.","20","0.3636363636363635","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","915917847","issue_comment","https://github.com/Trusted-AI/AIF360/issues/855#issuecomment-915917847","Hi @matthew-so - could you perhaps open separate issues for the different sections? In that way other prospective contributors can easily identify that they can pick it up!","20","0.6794258373205743","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","917999471","issue_comment","https://github.com/Trusted-AI/AIF360/issues/855#issuecomment-917999471","Hi, I'd be interested in picking this up. I will start at fairlearn.reductions, considering that @matthew-so is doing metrics right now.","20","0.5664833387253315","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","918009678","issue_comment","https://github.com/Trusted-AI/AIF360/issues/855#issuecomment-918009678","Hi @bramreinders97: awesome! Could you open up a new issue in which you refer back to this issue? Thanks!","20","0.2799154334038055","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","949265814","issue_comment","https://github.com/Trusted-AI/AIF360/issues/855#issuecomment-949265814","I have been caught up with issues over these past months and have been unable to make progress. I will thus remove myself from this issue.","24","0.3707538013587835","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","949408231","issue_comment","https://github.com/Trusted-AI/AIF360/issues/855#issuecomment-949408231","Thanks a lot for letting us know, @matthew-so!","20","0.2659352142110763","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","855893299","issue_comment","https://github.com/Trusted-AI/AIF360/issues/853#issuecomment-855893299","Would this be something we add to the merge commit message in one way or another or would you suggest to add them manually when we create the changelog?

(I'm happy with anything as long as it's clear what's expected from me :D)","32","0.7316751045564605","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","855895493","issue_comment","https://github.com/Trusted-AI/AIF360/issues/853#issuecomment-855895493","We write the changelog manually, so it's something we need to make sure to do when we add an item there. Example: https://github.com/scikit-learn/scikit-learn/pull/20165/files#diff-c6c2db6c2fe3fd4325496252937b7a3c08ebdffc7c0546bce69e4508f2ba3a6bR126-R128","32","0.4504580690627202","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","855897768","issue_comment","https://github.com/Trusted-AI/AIF360/issues/853#issuecomment-855897768","Ah, I see! So basically you add a new item to the changelog each time new code is merged?","32","0.4496578690127077","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","855912610","issue_comment","https://github.com/Trusted-AI/AIF360/issues/853#issuecomment-855912610","Looks like it. I like it, but we'll all need to be quite disciplined to get this done. 

I think I also remember @adrinjalali saying that there's one file for ALL changes and another for just the ""highlights"" (as in bigger changes that will provide new features or breaking changes perhaps). Is that accurate?

Also, this is related to #738 in that our current CHANGES.md is just a markdown file in the repo root, while scikit-learn puts this on the webpage by having it formatted in restructured text.","14","0.704386260628535","Documentation","Development"
"https://github.com/fairlearn/fairlearn","855916408","issue_comment","https://github.com/Trusted-AI/AIF360/issues/853#issuecomment-855916408","Yep, there's the changelog, and then there's the highlights. For now we don't have much stuff in each release for it to need the highlights. But I think the changelog should be on the website. ","32","0.6120440069484655","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","857532712","issue_comment","https://github.com/Trusted-AI/AIF360/issues/853#issuecomment-857532712","I agree we should add the changelog on the website! I'm not sure what would be the best place for it given the navbar items in the current design: Get started, Learn, API docs, Contribute, Community, About us

Some examples of other projects:
* pandas has a specific navbar item called *Release notes* in their documentation: https://pandas.pydata.org/docs/whatsnew/index.html (I only just now realize they actually have a separate documentation website generated by sphinx and ""main"" website that's non-sphinx, which makes the navigation between those a bit weird)
* scikit-learn has a *What's new* subitem of the *more* item: https://scikit-learn.org/stable/whats_new/v0.24.html
* tensorflow does not put them on the website, only in the RELEASE.md on Github
* NumPy adds release notes to the user guide: https://numpy.org/doc/stable/release.html

Tagging @nessamilan @fairlearn/fairlearn-maintainers ","14","0.5291149787076468","Documentation","Development"
"https://github.com/fairlearn/fairlearn","857837776","issue_comment","https://github.com/Trusted-AI/AIF360/issues/853#issuecomment-857837776","Thanks for the overview of what other projects do! Looks like it's not very standardized 🤣 
Just one thing I want to add: right now we have the ""version migration"" guide as part of the user guide: https://fairlearn.org/main/user_guide/migrating_versions/index.html
I'm not opposed to moving it elsewhere, though, but I feel like it might make sense to have it in the same place as the changelog. 
[Background: @riedgar-ms initially created this I think for the 0.4.6 -> 0.5 entry, and recently @MeekaElla consolidated it with the existing guide for 0.2 -> 0.3 in #803 ]","32","0.3292106767777076","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","876115577","issue_comment","https://github.com/Trusted-AI/AIF360/issues/853#issuecomment-876115577","This is also highly related to #738 and basically builds upon it. Whoever sets up the changelog as an ReST file can probably tackle both at the same time. ","32","0.6215311004784688","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","876114703","issue_comment","https://github.com/Trusted-AI/AIF360/issues/852#issuecomment-876114703","I want to add that any contribution doesn't need to tackle all of these. Even just one is a step forward!","20","0.4716949716949716","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","895649862","issue_comment","https://github.com/Trusted-AI/AIF360/issues/852#issuecomment-895649862","Hi there, I joined the Discord recently and I'd like to address these warnings. @romanlutz is anyone currently working on this?","20","0.5069484655471918","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","895726453","issue_comment","https://github.com/Trusted-AI/AIF360/issues/852#issuecomment-895726453","Hi @alliesaizan ! Thanks for taking a look at this! Nobody else is currently investigating these.

You can try this out by following the steps in this guide: https://fairlearn.org/main/contributor_guide/contributing_documentation.html
The ""simple"" doc build using `python -m sphinx -v -b html -n -j auto docs docs/_build/html` or `make doc` should be sufficient, since the ""full"" doc build doesn't provide the same output verbosity.

The majority of remaining errors seem to be of the following kind:
```
\fairlearn\datasets\__init__.py:docstring of fairlearn.datasets._fetch_adult.fetch_adult:: WARNING: py:class reference target not found: default=True
\fairlearn\datasets\__init__.py:docstring of fairlearn.datasets._fetch_adult.fetch_adult:: WARNING: py:class reference target not found: default=None
\fairlearn\datasets\__init__.py:docstring of fairlearn.datasets._fetch_adult.fetch_adult:: WARNING: py:class reference target not found: default=False
\fairlearn\datasets\__init__.py:docstring of fairlearn.datasets._fetch_adult.fetch_adult:: WARNING: py:class reference target not found: default=False
```
meaning our docstrings have types that aren't recognized. This isn't entirely surprising since we sometimes use free text to describe the types rather than actual type names. The corresponding lines would be: 
- https://github.com/fairlearn/fairlearn/blob/3211828d6a89d0a243c996b23bf06d032446cba7/fairlearn/datasets/_fetch_adult.py#L30
- https://github.com/fairlearn/fairlearn/blob/3211828d6a89d0a243c996b23bf06d032446cba7/fairlearn/datasets/_fetch_adult.py#L33
- https://github.com/fairlearn/fairlearn/blob/3211828d6a89d0a243c996b23bf06d032446cba7/fairlearn/datasets/_fetch_adult.py#L38
- https://github.com/fairlearn/fairlearn/blob/3211828d6a89d0a243c996b23bf06d032446cba7/fairlearn/datasets/_fetch_adult.py#L46

... and a few dozen others. I don't think these are really worth fixing since there's no solution that I'm aware of ( @fairlearn/fairlearn-maintainers  please correct me if I'm wrong!). I haven't checked every single one, though, so there may very well be a few that are fixable.

Other errors that should show when you run the doc build command include:
- > Output from C:\Users\rolutz\git\fairlearn\examples\plot_quickstart.py          y
C:\Users\rolutz\Anaconda3\lib\site-packages\pandas\plotting\_matplotlib\core.py:1616: MatplotlibDeprecationWarning: normalize=None does not normalize if the sum is less than 1 but this behavior is deprecated since 3.3 until two minor releases later. After the deprecation period the default value will be normalize=True. To prevent normalization pass normalize=False
 
  I'm not entirely sure what this affects, but you can try passing it and see if it changes anything compared to the [existing quickstart](https://fairlearn.org/v0.7.0/quickstart.html). Ideally the plots should remain the same (unless you can improve them in some way, of course).
- > fairlearn\postprocessing\_interpolated_thresholder.py:115: UserWarning: The value of `prefit` is `True`, but `check_is_fitted` raised `NotFittedError` on the base estimator.
  
  This is actually a warning we're creating from Fairlearn and not yet resolvable (waiting on a fix in scikit-learn).
- > Anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1496: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  average, ""true nor predicted"", 'F-score is', len(true_sum)
  
  shows up a lot in [this example](https://fairlearn.org/main/auto_examples/plot_new_metrics.html#sphx-glr-auto-examples-plot-new-metrics-py). Would be good to figure out how we can fix that. It's just a contrived example to show the functionality of Fairlearn's metrics capabilities, so we can certainly change the data if it really has ""no true nor predicted samples"" in some cases, or change the `zero_division` param. Again, haven't looked into it so there may be multiple good options. Suggestions are welcome! ( @riedgar-ms FYI since you wrote that example originally)

@alliesaizan if you have issues, questions or would like to look into something different just reach out on Discord and we discuss! Again, welcome to the community.","1","0.3132456692659142","Fix warnings","Maintenance"
"https://github.com/fairlearn/fairlearn","899895327","issue_comment","https://github.com/Trusted-AI/AIF360/issues/852#issuecomment-899895327","@romanlutz I started with the last warning:

> Anaconda3\lib\site-packages\sklearn\metrics_classification.py:1496: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use zero_division parameter to control this behavior.
average, ""true nor predicted"", 'F-score is', len(true_sum)

 And the culprit appears to be line 234 in the *examples >> plot_new_metrics.py* file, `fbeta_06 = functools.partial(skm.fbeta_score, beta=0.6)`. Adding a `zero_division=1` parameter and re-running lines 234 & 236 appears to resolve the warning without changing the results as they appear on the blog post. Please let me know how you'd like to proceed!","1","0.499011419117944","Fix warnings","Maintenance"
"https://github.com/fairlearn/fairlearn","899899033","issue_comment","https://github.com/Trusted-AI/AIF360/issues/852#issuecomment-899899033","@alliesaizan that's fine by me. The particular function isn't important; it's about showing how to use metric functions in `MetricFrame`.","12","0.2500482532329667","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","902840877","issue_comment","https://github.com/Trusted-AI/AIF360/issues/852#issuecomment-902840877","@alliesaizan that's awesome! You can create a PR with this fix. The smaller the better and the faster it typically gets merged.","20","0.7171395483497397","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","904198572","issue_comment","https://github.com/Trusted-AI/AIF360/issues/852#issuecomment-904198572","@romanlutz I'm looking at this error now: 
> Output from C:\Users\rolutz\git\fairlearn\examples\plot_quickstart.py y
C:\Users\rolutz\Anaconda3\lib\site-packages\pandas\plotting_matplotlib\core.py:1616: MatplotlibDeprecationWarning: normalize=None does not normalize if the sum is less than 1 but this behavior is deprecated since 3.3 until two minor releases later. After the deprecation period the default value will be normalize=True. To prevent normalization pass normalize=False

It appears to be caused by the following code chunk:
> metric_frame.by_group.plot(
    kind=""pie"",
    subplots=True,
    layout=[3, 3],
    legend=False,
    figsize=[12, 8],
    title=""Show all metrics in pie"",
)

The pie chart in `plot_quickstart.py` looks like this: 
![output](https://user-images.githubusercontent.com/15962391/130530150-204fd820-c280-44d9-be29-25dc5722f501.png)

For a MetricFrame with the following values:
![temp](https://user-images.githubusercontent.com/15962391/130530466-f477171f-d901-4adf-a3af-aaaad1cf7ca5.png)

I get that the idea is to show users how to show how the metrics compare across subgroups, but I am not sure the pie chart achieves this goal. The values across Male and Female don't add up to 1 in most cases, so I don't think this is the correct use of a pie chart. Here's what the pie chart looks like when I pass a `normalize=True` parameter to the plot function:
![temp2](https://user-images.githubusercontent.com/15962391/130530683-4ee48bb6-c48e-497b-8248-6ad1fb2ef0b3.png)

In the charts above, each gender's metric is scaled so that the sum across equals 1 (if the metrics are less than zero), so it's showing the share of the metric distributed across genders. Personally, I'd recommend removing the pie charts because I don't think either explanation is accurate to what the quickstart is trying to show, but adding that parameter gets rid of the warning. I am not sure if there's a better fix. Please let me know what you think!
","26","0.4114647257504401","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","904202749","issue_comment","https://github.com/Trusted-AI/AIF360/issues/852#issuecomment-904202749","Thanks for the thorough assessment @alliesaizan !
The filename is a bit of a misnomer since it was originally built for the quickstart, but it's used in other places, too. If you check the webpage's quickstart, you'll find that the pie charts don't show up for the same reasons you describe. To my knowledge, they're only used in the user guide: https://fairlearn.org/main/user_guide/assessment.html#customize-plots-kind
The point there is to show that one can customize the `kind` parameter. I agree that these metrics don't lend themselves to this representation, perhaps with the exception of `count`. Perhaps it makes sense to show the pie chart only for `count`?","15","0.4607317993210111","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","904209214","issue_comment","https://github.com/Trusted-AI/AIF360/issues/852#issuecomment-904209214","@romanlutz I think it makes sense to only show the pie chart for `count`! Should I make the change in that `plot_quickstart.py` file, or somewhere else?","15","0.621531100478469","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","905831120","issue_comment","https://github.com/Trusted-AI/AIF360/issues/852#issuecomment-905831120","Yes, `plot_quickstart.py` would be the right file. I'd recommend running the local doc build afterwards and checking the user guide section, though, just to make sure it renders in the intended way. Thanks @alliesaizan !!!","14","0.44681679177092","Documentation","Development"
"https://github.com/fairlearn/fairlearn","913824707","issue_comment","https://github.com/Trusted-AI/AIF360/issues/852#issuecomment-913824707","I believe the only remaining warning that's fixable is the following:
```
fairlearn\postprocessing_interpolated_thresholder.py:115: UserWarning: The value of prefit is True, but check_is_fitted raised NotFittedError on the base estimator.
```
This should be fixed with the next scikit-learn release thanks to @adrinjalali's merged PR https://github.com/scikit-learn/scikit-learn/pull/20657 but we can validate once scikit-learn has that in a release and close this issue at that point.

Huge kudos to @alliesaizan for diligently working through these! I'd love to give you a shoutout in my next set of tweets. If you are okay with that please respond with some kind of emoji or DM me on Discord :-)","20","0.3815100869418419","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","855856909","issue_comment","https://github.com/Trusted-AI/AIF360/issues/850#issuecomment-855856909","top 5 on my machine:

```
1769.74s call     unit/reductions/test_smoke.py::test_smoke[10-LGBMClassifier-EqualizedOdds-ExponentiatedGradient]
1193.94s call     unit/reductions/test_smoke.py::test_smoke[10-LGBMClassifier-DemographicParity-ExponentiatedGradient]
942.77s call     unit/reductions/test_smoke.py::test_smoke[2-LGBMClassifier-EqualizedOdds-ExponentiatedGradient]
904.27s call     unit/reductions/test_smoke.py::test_smoke[2-LGBMClassifier-DemographicParity-ExponentiatedGradient]
752.96s call     unit/reductions/test_smoke.py::test_smoke[3-LGBMClassifier-DemographicParity-ExponentiatedGradient]
```","5","0.842632544760205","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","855968961","issue_comment","https://github.com/Trusted-AI/AIF360/issues/850#issuecomment-855968961","@MiroDudik anything you could add before we open this for contributions?","20","0.6745983323164535","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","865312208","issue_comment","https://github.com/Trusted-AI/AIF360/issues/850#issuecomment-865312208","In terms of alternatives I have a few thoughts:
- I think the `lightgbm` tests shouldn't exist in the `test` directory since we have `test_othermlpackages` for that. They're probably still in there as a remnant of the time before we had the other directory. I would still like to have the tests that run these scenarios, but perhaps with something ""cheaper"" (faster to train).
- We shouldn't use real datasets as much as we do at the moment. In part this stems from not wanting to build a smaller synthetic dataset that exhibit the same properties (because it takes time), but #793 is addressing that.","13","0.4423603905605662","Artifact generation and benchmarking","Deployment"
"https://github.com/fairlearn/fairlearn","873303428","issue_comment","https://github.com/Trusted-AI/AIF360/issues/850#issuecomment-873303428","I think there was a historic reason to include this, because we ran into some bugs. So this is a somewhat bloated regression test. Agreed that `test_othermlpackages` is a better place for this. I think we should keep some tests with `LGBMClassifier`, but we should run LGBMClassifier with hyperparameters that make training fast (e.g., limit the number of trees).","13","0.413034989065917","Artifact generation and benchmarking","Deployment"
"https://github.com/fairlearn/fairlearn","856002395","issue_comment","https://github.com/Trusted-AI/AIF360/issues/848#issuecomment-856002395","Is there any way to restrict this to our direct dependencies? For example, [in a recent build](https://dev.azure.com/responsibleai/fairlearn/_build/results?buildId=12007&view=logs&j=aa2fd561-a338-5537-45ee-49ae22aea958&t=7d222596-41fc-5765-4dd3-df02a80934d4&l=163) there's a deprecation warning on `ansiwrap` but that's being brought in by some other package.","21","0.4298187688018194","Installation and shell commands","Deployment"
"https://github.com/fairlearn/fairlearn","856011171","issue_comment","https://github.com/Trusted-AI/AIF360/issues/848#issuecomment-856011171","I think you can ignore certain warnings: 

https://github.com/scikit-learn/scikit-learn/blob/main/setup.cfg#L20","1","0.2070368110636568","Fix warnings","Maintenance"
"https://github.com/fairlearn/fairlearn","854681724","issue_comment","https://github.com/Trusted-AI/AIF360/issues/847#issuecomment-854681724","We do have a code coverage build:
https://dev.azure.com/responsibleai/fairlearn/_build?definitionId=24&_a=summary
Or is that not generally visible? We still haven't quite figured out permissions on that VSO tenant.","13","0.8737251449115856","Artifact generation and benchmarking","Deployment"
"https://github.com/fairlearn/fairlearn","854705701","issue_comment","https://github.com/Trusted-AI/AIF360/issues/847#issuecomment-854705701","Then why is that not failing on https://github.com/fairlearn/fairlearn/pull/822 @riedgar-ms ?","32","0.6007728289607486","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","854709035","issue_comment","https://github.com/Trusted-AI/AIF360/issues/847#issuecomment-854709035","It only runs on `main` (that can be changed). Why would it fail, though?","32","0.7134532134532136","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","854718841","issue_comment","https://github.com/Trusted-AI/AIF360/issues/847#issuecomment-854718841","Because for instance when #822 adds code which is not tested, we should see that it needs tests to improve the coverage. We shouldn't approve any PRs unless they also add the appropriate tests.","13","0.5221378045585533","Artifact generation and benchmarking","Deployment"
"https://github.com/fairlearn/fairlearn","854757367","issue_comment","https://github.com/Trusted-AI/AIF360/issues/847#issuecomment-854757367","So you'd like to gate on code coverage%? Would you like to include branch coverage as well?","32","0.7744890768146581","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","854775435","issue_comment","https://github.com/Trusted-AI/AIF360/issues/847#issuecomment-854775435","I don't think branches matter at this point, we can add this for the future. As long as we fail on newly written code, gradually things will get better.","32","0.5443381180223286","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","854788934","issue_comment","https://github.com/Trusted-AI/AIF360/issues/847#issuecomment-854788934","Just realised that 'branch' is ambiguous in this context. I was meaning in the `if/else` sense.... is that what you meant?

How would you want the threshold set? We're currently at >95%, which is well into ""increasing code coverage tells you nothing"" territory. I do see that the [codecov service you linked has a 'patch' setting](https://docs.codecov.io/docs/commit-status#patch-status) which requires code coverage on any changed lines. A threshold on that might be more useful?","32","0.301664532650448","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","855776902","issue_comment","https://github.com/Trusted-AI/AIF360/issues/847#issuecomment-855776902","Yes, I think we should set it to 100% for patch, and 98% overall as threshold for PRs.","32","0.4285133211307708","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","855891046","issue_comment","https://github.com/Trusted-AI/AIF360/issues/847#issuecomment-855891046","That's very high. I don't want us to slide from what we have, but as I said there comes a point where you have diminishing returns and there are better places to focus effort. After all,100% code coverage doesn't tell you that you've missed a complete scenario.","10","0.2171490691670063","Model development","Development"
"https://github.com/fairlearn/fairlearn","855894467","issue_comment","https://github.com/Trusted-AI/AIF360/issues/847#issuecomment-855894467","It doesn't, but anything less than 100% on patch, tells you that there's new untested code in the PR. It's really not hat hard to make sure the code is run at least once in at least one of the tests. Also, we can always merge ignoring the coverage if we think a test is not needed for a particular line of code. But in my experience, you definitely want tests for everything you write, at least one code.","32","0.4804559355018069","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","857872301","issue_comment","https://github.com/Trusted-AI/AIF360/issues/846#issuecomment-857872301","See examples of Umbuntu and Open Sans applied:
[Homepage](https://www.figma.com/file/wt76z0M87RBDDEoyAEpD2g/OPEN-SOURCE-Fairlearn-Redesign?node-id=2644%3A623)
[API Docs detail page](https://www.figma.com/file/wt76z0M87RBDDEoyAEpD2g/OPEN-SOURCE-Fairlearn-Redesign?node-id=2644%3A368)

Color theming example:

- Homepage: https://www.figma.com/file/wt76z0M87RBDDEoyAEpD2g/OPEN-SOURCE-Fairlearn-Redesign?node-id=2685%3A2712
- Doc page: in progress","14","0.7166096315032489","Documentation","Development"
"https://github.com/fairlearn/fairlearn","857926293","issue_comment","https://github.com/Trusted-AI/AIF360/issues/846#issuecomment-857926293","Nice! Open Sans looks really good and I generally like Ubuntu as well.

I'm not 100% sure about Ubuntu for H1 though. Would it be possible to increase the vertical spacing or does that mess up the mobile version?","14","0.2965790719696971","Documentation","Development"
"https://github.com/fairlearn/fairlearn","865367351","issue_comment","https://github.com/Trusted-AI/AIF360/issues/846#issuecomment-865367351","@hildeweerts Thanks for the feedback! I updated the H1 line-height, it does look better tightened up!  Also, I changed the weight to light (from Regular), I think this adds a nice weight balance with the tighter line-height and text below!","14","0.4086190625983013","Documentation","Development"
"https://github.com/fairlearn/fairlearn","951001959","issue_comment","https://github.com/Trusted-AI/AIF360/issues/846#issuecomment-951001959","Closing to be rescoped with Sphinx backend.","22","0.7335997335997335","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","951002532","issue_comment","https://github.com/Trusted-AI/AIF360/issues/845#issuecomment-951002532","Closing to be rescoped with Sphinx backend.","22","0.7335997335997335","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","951002788","issue_comment","https://github.com/Trusted-AI/AIF360/issues/844#issuecomment-951002788","Closing to be rescoped with Sphinx backend.","22","0.6731601731601733","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","951003292","issue_comment","https://github.com/Trusted-AI/AIF360/issues/843#issuecomment-951003292","Closing to be rescoped with Sphinx backend.","22","0.6933066933066933","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","857316714","issue_comment","https://github.com/Trusted-AI/AIF360/issues/842#issuecomment-857316714","Just a quick note: we may actually use Discord more actively now.

I recall worries about showing posts that go unfiltered (since anyone can really go on Gitter and post things) since showing them on the website looks like an endorsement. I would still vote against having this at all.","14","0.414962776554472","Documentation","Development"
"https://github.com/fairlearn/fairlearn","857473488","issue_comment","https://github.com/Trusted-AI/AIF360/issues/842#issuecomment-857473488","@romanlutz to be clear, are you specifically voting against featuring Gitter conversations, or also the other items (Github issues, stackoverflow questions, twitter feed).","11","0.361032196969697","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","857730760","issue_comment","https://github.com/Trusted-AI/AIF360/issues/842#issuecomment-857730760","@romanlutz @hildeweerts 

I think if we are moving away from Gitter to Discord, we should consider de-emphasizing Gitter throughout our experience or even replacing. Another option on the community page is to have the two chat platforms in a 2-column layout side by side instead of stacked so users can compare conversations, making a possible engagement decision easier.

For Gitter I recall us not being sure if we wanted to feature the most active conversations, but there was a worry about curation and how to prioritize since there could be many great conversations happening in tandem — similar may be true for Discord. It would probably be the fairest to have this be a ""most recent"" feed similar to Twitter.
","20","0.3540700371057514","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","857732832","issue_comment","https://github.com/Trusted-AI/AIF360/issues/842#issuecomment-857732832","I'm against unvetted posts being featured, which pretty much leaves only Twitter.

We're not moving away from Gitter for the time being.","14","0.3513556618819777","Documentation","Development"
"https://github.com/fairlearn/fairlearn","857735627","issue_comment","https://github.com/Trusted-AI/AIF360/issues/842#issuecomment-857735627","Gotcha, that makes sense.

IMHO having just Twitter is okay, this allows us to elevate some portion of social. We could even include a lead in paragraph that highlights meetings and events announced through the Twitter account. Alternatively this could be it’s own module.

From: Roman Lutz ***@***.***>
Date: Wednesday, June 9, 2021 at 7:16 AM
To: fairlearn/fairlearn ***@***.***>
Cc: Vanessa Milan ***@***.***>, Author ***@***.***>
Subject: Re: [fairlearn/fairlearn] [Website Redesign] Community Landing Page (#842)

I'm against unvetted posts being featured, which pretty much leaves only Twitter.

We're not moving away from Gitter for the time being.

—
You are receiving this because you authored the thread.
Reply to this email directly, view it on GitHub<https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Ffairlearn%2Ffairlearn%2Fissues%2F842%23issuecomment-857732832&data=04%7C01%7CVanessa.Milan%40microsoft.com%7C39ef8e2b05fb45c9951708d92b513849%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637588450113995217%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=xjOT9GZUXU979eh31cemcNAv3NbgQKYOr9JJHZp%2B4cs%3D&reserved=0>, or unsubscribe<https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fnotifications%2Funsubscribe-auth%2FABUA4RN7IWBVTD5SJOL7OKDTR5ZVBANCNFSM457SHOUQ&data=04%7C01%7CVanessa.Milan%40microsoft.com%7C39ef8e2b05fb45c9951708d92b513849%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637588450114005177%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=GxMNSITaopGSJgoTGfhdWQVMJdlAwMzPBT4%2FdXRFMw4%3D&reserved=0>.
","7","0.4667641563083925","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","857745236","issue_comment","https://github.com/Trusted-AI/AIF360/issues/842#issuecomment-857745236","I'd be happy with just Twitter as well!","20","0.551555826723612","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","951003626","issue_comment","https://github.com/Trusted-AI/AIF360/issues/842#issuecomment-951003626","Closing to be rescoped with Sphinx backend.","22","0.7335997335997335","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","951003948","issue_comment","https://github.com/Trusted-AI/AIF360/issues/841#issuecomment-951003948","Closing to be rescoped with Sphinx backend.","22","0.7134532134532136","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","951004128","issue_comment","https://github.com/Trusted-AI/AIF360/issues/840#issuecomment-951004128","Closing to be rescoped with Sphinx backend.","22","0.7335997335997335","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","857317376","issue_comment","https://github.com/Trusted-AI/AIF360/issues/839#issuecomment-857317376","Quick note: as of quite recently we now have the social links in a uniform color, but when you hover over them they turn into the color of the platform, e.g., light blue for Twitter, black for GitHub, etc.
The positioning is still according to pydata-sphinx-theme, of course.","14","0.6514210427253907","Documentation","Development"
"https://github.com/fairlearn/fairlearn","951004856","issue_comment","https://github.com/Trusted-AI/AIF360/issues/839#issuecomment-951004856","Closing to be rescoped with Sphinx backend.","22","0.6933066933066933","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","950989881","issue_comment","https://github.com/Trusted-AI/AIF360/issues/838#issuecomment-950989881","This needs better scoping. Closing for now.","22","0.4871995820271683","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","951005204","issue_comment","https://github.com/Trusted-AI/AIF360/issues/837#issuecomment-951005204","Closing to be rescoped with Sphinx backend.","22","0.7134532134532134","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","951006210","issue_comment","https://github.com/Trusted-AI/AIF360/issues/836#issuecomment-951006210","Closing to be rescoped with Sphinx backend.","22","0.7134532134532133","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","854320383","issue_comment","https://github.com/Trusted-AI/AIF360/issues/835#issuecomment-854320383","## Bugs

- [ ] Vertical spacing too tight between text in some modules:

  > <img width=""1204"" alt=""Screen Shot 2021-06-03 at 7 39 01 PM"" src=""https://user-images.githubusercontent.com/6819397/120738559-e88a7200-c4a4-11eb-9c71-47ef16fe2c7c.png"">
  > <img width=""586"" alt=""Screen Shot 2021-06-03 at 6 47 39 PM"" src=""https://user-images.githubusercontent.com/6819397/120738561-e88a7200-c4a4-11eb-8040-22a06653438f.png"">
  > <img width=""642"" alt=""Screen Shot 2021-06-03 at 6 47 29 PM"" src=""https://user-images.githubusercontent.com/6819397/120738563-e9230880-c4a4-11eb-92d6-ab9652587d37.png"">
  > <img width=""1183"" alt=""Screen Shot 2021-06-03 at 6 46 25 PM"" src=""https://user-images.githubusercontent.com/6819397/120738564-e9230880-c4a4-11eb-8c05-1df69dce12ff.png"">

- [ ] **Resources module**: 
Update font sizes and weights to match lighter weights in design

  > <img width=""1133"" alt=""Screen Shot 2021-06-03 at 6 47 34 PM"" src=""https://user-images.githubusercontent.com/6819397/120738842-6189c980-c4a5-11eb-9f9e-95d5c355ebc8.png"">

- [ ] Update footer title in column two to ""Fairlearn""

","14","0.5288945795988049","Documentation","Development"
"https://github.com/fairlearn/fairlearn","951006544","issue_comment","https://github.com/Trusted-AI/AIF360/issues/835#issuecomment-951006544","Closing to be rescoped with Sphinx backend.","22","0.6933066933066934","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","854322423","issue_comment","https://github.com/Trusted-AI/AIF360/issues/834#issuecomment-854322423","## Bugs 

- [ ]  Full grid width

  > Current: 1140px
  > <img width=""1326"" alt=""Screen Shot 2021-06-03 at 6 52 10 PM"" src=""https://user-images.githubusercontent.com/6819397/120739236-0d331980-c4a6-11eb-9323-e8b78650eb0a.png"">
  > 
  > [Expected](https://www.figma.com/file/wt76z0M87RBDDEoyAEpD2g/OPEN-SOURCE-Fairlearn-Redesign?node-id=2336%3A2019): 1180px
  > <img width=""806"" alt=""Screen Shot 2021-06-03 at 7 58 04 PM"" src=""https://user-images.githubusercontent.com/6819397/120739331-32278c80-c4a6-11eb-8b8b-c88418016048.png"">

- [ ] Navigation runs into logo. Please update so it swaps to mobile hamburger menu when the grid is approx 900px

  > <img width=""993"" alt=""Screen Shot 2021-06-03 at 7 29 54 PM"" src=""https://user-images.githubusercontent.com/6819397/120739486-6a2ecf80-c4a6-11eb-9191-c1bd053b0318.png"">
","14","0.5600913119376929","Documentation","Development"
"https://github.com/fairlearn/fairlearn","951006694","issue_comment","https://github.com/Trusted-AI/AIF360/issues/834#issuecomment-951006694","Closing to be rescoped with Sphinx backend.","22","0.7134532134532133","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","854327625","issue_comment","https://github.com/Trusted-AI/AIF360/issues/833#issuecomment-854327625","## Bugs

Header

- [ ] On homepage, no navigation is in active state. Currently, ""Getting Started"" is bolded

  > <img width=""294"" alt=""Screen Shot 2021-06-03 at 7 16 19 PM"" src=""https://user-images.githubusercontent.com/6819397/120736818-ca6f4280-c4a1-11eb-8680-837a5314e049.png"">
<br>

- [ ] Update select font weights to demibold so they are consistent (there was a regression in Figma that had a mix of regular and demibold):
  - [ ] Header navigation and Github button
  - [ ] Button labels throughout the page and footer
  - [ ] Small cap titles  
  > <img width=""609"" alt=""Screen Shot 2021-06-03 at 7 20 16 PM"" src=""https://user-images.githubusercontent.com/6819397/120736730-a4e23900-c4a1-11eb-9cb3-a440463c3e49.png"">
  > <img width=""288"" alt=""Screen Shot 2021-06-03 at 7 19 50 PM"" src=""https://user-images.githubusercontent.com/6819397/120736727-a449a280-c4a1-11eb-972c-e0dae322424a.png"">
  > <img width=""241"" alt=""Screen Shot 2021-06-03 at 7 19 58 PM"" src=""https://user-images.githubusercontent.com/6819397/120736728-a4e23900-c4a1-11eb-832a-a25e3d0e1a55.png"">
  > <img width=""167"" alt=""Screen Shot 2021-06-03 at 7 20 09 PM"" src=""https://user-images.githubusercontent.com/6819397/120736729-a4e23900-c4a1-11eb-8416-5add0636c1f3.png"">
  > <img width=""148"" alt=""Screen Shot 2021-06-03 at 7 46 01 PM"" src=""https://user-images.githubusercontent.com/6819397/120738322-86ca0800-c4a4-11eb-9115-124301ab0c13.png"">
  

","14","0.6854312354312357","Documentation","Development"
"https://github.com/fairlearn/fairlearn","870728052","issue_comment","https://github.com/Trusted-AI/AIF360/issues/833#issuecomment-870728052","Header: https://www.figma.com/file/wt76z0M87RBDDEoyAEpD2g/OPEN-SOURCE-Fairlearn-Redesign?node-id=2565%3A12134
- Links: Currently look too heavy and large. Expected: body 1 (open sans 13px semibold)
- Update link colors to Gray 2
- Links and Buttons: Added hover and active state colors to link above

Footer: https://www.figma.com/file/wt76z0M87RBDDEoyAEpD2g/OPEN-SOURCE-Fairlearn-Redesign?node-id=2565%3A22305 
- Social buttons: Please implement hover interaction specified for these here: https://www.figma.com/file/wt76z0M87RBDDEoyAEpD2g/OPEN-SOURCE-Fairlearn-Redesign?node-id=2336%3A2475
- Input field states: This was missing in my spec. Please see new addition in link above (in color at bottom).

Buttons: https://www.figma.com/file/wt76z0M87RBDDEoyAEpD2g/OPEN-SOURCE-Fairlearn-Redesign?node-id=2582%3A77
- fix white button font weight; blue buttons look correct
- fix height; currently 50px, should be 44px
- interaction: please update hover interactions as specified above","14","0.6710022835277417","Documentation","Development"
"https://github.com/fairlearn/fairlearn","951006890","issue_comment","https://github.com/Trusted-AI/AIF360/issues/833#issuecomment-951006890","Closing to be rescoped with Sphinx backend.","22","0.6933066933066933","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","853258211","issue_comment","https://github.com/Trusted-AI/AIF360/issues/831#issuecomment-853258211","The default is a rule `> threshold`, and `< threshold` is only considered if the `flip` argument is set to `True`. Does that resolve your doubt?","23","0.3224082934609251","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","853262742","issue_comment","https://github.com/Trusted-AI/AIF360/issues/831#issuecomment-853262742","> The default is a rule `> threshold`, and `< threshold` is only considered if the `flip` argument is set to `True`. Does that resolve your doubt?

I think the implementation of flip=True vs. flip=False is reversed? (for my previous comments I'm only talking about the > threshold case, which is the default)","15","0.342550674986745","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","853279541","issue_comment","https://github.com/Trusted-AI/AIF360/issues/831#issuecomment-853279541","Maybe the issue is that scores are sorted in descending order, so our comment is actually incorrect, but implementation is correct?
https://github.com/fairlearn/fairlearn/blob/fed04c6149ceb7c691fd496ee594cd08c7ba5cc5/fairlearn/postprocessing/_tradeoff_curve_utilities.py#L286","30","0.8090773202880825","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","853283674","issue_comment","https://github.com/Trusted-AI/AIF360/issues/831#issuecomment-853283674","Oops sorry never realized that... Closing this.","32","0.3546878177750662","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","853346589","issue_comment","https://github.com/Trusted-AI/AIF360/issues/831#issuecomment-853346589","@ziqi-ma : no worries--it's actually our fault, we should fix the comments!","24","0.6721634954193094","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","853403883","issue_comment","https://github.com/Trusted-AI/AIF360/issues/831#issuecomment-853403883","Then let's reopen to fix the comment! 🙂 
@MiroDudik you mean this line, right? https://github.com/fairlearn/fairlearn/blob/fed04c6149ceb7c691fd496ee594cd08c7ba5cc5/fairlearn/postprocessing/_tradeoff_curve_utilities.py#L278","30","0.5961051436555759","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","853419915","issue_comment","https://github.com/Trusted-AI/AIF360/issues/831#issuecomment-853419915","I saw an incorrect comment at two locations at least. Besides the one you mention, also here:
https://github.com/fairlearn/fairlearn/blob/fed04c6149ceb7c691fd496ee594cd08c7ba5cc5/fairlearn/postprocessing/_tradeoff_curve_utilities.py#L224","30","0.5886894421926268","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","876384406","issue_comment","https://github.com/Trusted-AI/AIF360/issues/831#issuecomment-876384406","Hello! I will be picking this up as a good first issue to get familiar with open-source development.
I will change the two comments and scan the document for more increase / ascending mistakes.","20","0.587104226117679","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","876409814","issue_comment","https://github.com/Trusted-AI/AIF360/issues/831#issuecomment-876409814","Send out a pull request.
Hope I did everything right, if not please let me know.
This will be my first contribution to fairlearn :) 
https://github.com/fairlearn/fairlearn/pull/889 ","31","0.2722385141739981","Testing","Maintenance"
"https://github.com/fairlearn/fairlearn","852841979","issue_comment","https://github.com/Trusted-AI/AIF360/issues/830#issuecomment-852841979","Could you please give some motivation as why we need to run this comparison in particular, as opposed to comparing with other solutions?","24","0.2533160789388548","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","853022034","issue_comment","https://github.com/Trusted-AI/AIF360/issues/830#issuecomment-853022034","@romanlutz and I had a conversation with the FairTorch team last week, and we were discussing the possibility of integrating their algorithm into our `Reductions` module. 

However, it was not clear how their algorithm compares with Fairlearn's, so we want to run some comparisons to understand the differences between the two methods.","28","0.4049509176269737","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","853049683","issue_comment","https://github.com/Trusted-AI/AIF360/issues/830#issuecomment-853049683","Right, I understood the issue as adding the comparison to the docs, not as a task. All good :+1: ","25","0.7157869012707725","Research","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","881936524","issue_comment","https://github.com/Trusted-AI/AIF360/issues/827#issuecomment-881936524","Hello! I will try and work on this issue.","0","0.0303030303030302","Dataset usage","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","849481138","issue_comment","https://github.com/Trusted-AI/AIF360/issues/826#issuecomment-849481138","hi @romanlutz 
i will like to solve this issue ","24","0.4555903866248695","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","849542231","issue_comment","https://github.com/Trusted-AI/AIF360/issues/826#issuecomment-849542231","Awesome, I have assigned the issue to you @Godwindaniel10. Let us know if you have any questions!","20","0.4112554112554112","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","849819993","issue_comment","https://github.com/Trusted-AI/AIF360/issues/826#issuecomment-849819993","ok , on it...
Thanks.

i was trying to clone the repo , but its not cloning.

![cmd error](https://user-images.githubusercontent.com/54848535/119872586-903ef780-bf1b-11eb-8177-c6c558ed30e6.JPG)
","4","0.5698379140239606","Installation and shell commands","Deployment"
"https://github.com/fairlearn/fairlearn","849868216","issue_comment","https://github.com/Trusted-AI/AIF360/issues/826#issuecomment-849868216","Looks like a network issue to me. I would delete the potentially created directory and retry.","14","0.3165933528836755","Documentation","Development"
"https://github.com/fairlearn/fairlearn","850623763","issue_comment","https://github.com/Trusted-AI/AIF360/issues/826#issuecomment-850623763","Closing since #828 fixes this!","24","0.5188087774294673","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","859607169","issue_comment","https://github.com/Trusted-AI/AIF360/issues/825#issuecomment-859607169","This issue is generating a lot of confusion. This might not be the best first issue. The solution is to:
* Reorder the items in the navbar of docs, so it's consistent with the ordering in the navbar on the landing page. The landing page is the one that has been vetted.
* It might also require re-naming some section titles to match the landing page.","14","0.9310034835374764","Documentation","Development"
"https://github.com/fairlearn/fairlearn","859617912","issue_comment","https://github.com/Trusted-AI/AIF360/issues/825#issuecomment-859617912","Reordering the items and renaming them in some cases seems like a very good first issue if you ask me. The only confusion arose from which part should be made consistent with which other part. Now it's just a fix in a single rst file.","14","0.6553794829024183","Documentation","Development"
"https://github.com/fairlearn/fairlearn","859668649","issue_comment","https://github.com/Trusted-AI/AIF360/issues/825#issuecomment-859668649","Ok--I've updated my comment above. Besides reordering, some section titles may need to be updated.","14","0.827456055214062","Documentation","Development"
"https://github.com/fairlearn/fairlearn","870849789","issue_comment","https://github.com/Trusted-AI/AIF360/issues/825#issuecomment-870849789","Let's keep this issue open since the associated PR(s) weren't merged as they preceded @MiroDudik's more specific instructions (on fixing the bar in all the other pages to make it consistent with the landing page, rather than the other way round). This is up for grabs for anyone who'd like to try! Just comment below.","14","0.4765659872042852","Documentation","Development"
"https://github.com/fairlearn/fairlearn","876112550","issue_comment","https://github.com/Trusted-AI/AIF360/issues/825#issuecomment-876112550","Some more information: the top navbar seems to be configurable as described in https://pydata-sphinx-theme.readthedocs.io/en/latest/user_guide/sections.html#the-navbar-items

The default templates are at https://github.com/pydata/pydata-sphinx-theme/tree/master/pydata_sphinx_theme/_templates","14","0.8090773202880827","Documentation","Development"
"https://github.com/fairlearn/fairlearn","942632417","issue_comment","https://github.com/Trusted-AI/AIF360/issues/825#issuecomment-942632417","I'm closing this since #976 was merged, and have opened #978 for the layout adjustments.","24","0.479227761485826","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","847398349","issue_comment","https://github.com/Trusted-AI/AIF360/issues/821#issuecomment-847398349","@riedgar-ms any workarounds?","32","0.3923719958202717","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","847472313","issue_comment","https://github.com/Trusted-AI/AIF360/issues/821#issuecomment-847472313","This sounds more like a job for the final printout, rather than `MetricFrame`. We could probably write a formatter to do something like this, though. Figuring out a useful set of heuristics for the descriptions might be non-trivial, though.","15","0.696162594467679","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","847626900","issue_comment","https://github.com/Trusted-AI/AIF360/issues/821#issuecomment-847626900","I agree with @riedgar-ms it might be tricky implement this generally. But it might make sense to do this for our predefined base metrics (e.g., `true_positive_rate`) and predefined fairness metrics (e.g., `equalized_odds_difference`). wdyt @michaelamoako?","15","0.4101152368758","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","847749997","issue_comment","https://github.com/Trusted-AI/AIF360/issues/821#issuecomment-847749997","@hildeweerts this isn't really about metrics per se. This is how, given a small number, to render it in a ""x times in y"" way which preserves some level of accuracy. but there are all sorts of questions to answer. These centre around specifying that accuracy and perception of the numbers. For example, is the distinction between ""5 times in 127"" and ""9 times in 253"" meaningful to people? Those are obviously pretty close, but you have to stop and match denominators to see that.","20","0.2938524458184393","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","848426005","issue_comment","https://github.com/Trusted-AI/AIF360/issues/821#issuecomment-848426005","I've encountered the same thing with the chest x-ray classification case study and my workaround was to always calculate:

- metrics of interest
- count (merged in today with #812 but can be manually implemented with 1 line as well)
- number of positives
- percentage of positives

You can do the same for error rate! 

- number of errors
- percentage of errors / error rate

They should appear next to each other in the `by_group` table and you can interpret it much easier.

Reducing it to the lowest common denominator may be more of a challenge, but I honestly don't quite see the point in doing that. For all numbers we should have a way of formatting the number output if someone doesn't like `5E-5`, for example. I can't quite say how one could do that but I'm sure pandas provides that as a configuration option somewhere.","3","0.271454799949101","Bias detection metrics validation","Validation"
"https://github.com/fairlearn/fairlearn","876046962","issue_comment","https://github.com/Trusted-AI/AIF360/issues/821#issuecomment-876046962","Can anyone see a reasonable way forward in this issue? 

The much more narrow issue of how to format metrics to avoid `5E-5` and instead have `.000` (restricted to some number of decimals) remains, of course. Or at least I'm not aware of any options we provide for people right now to configure that.","30","0.4339939184229842","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","876372620","issue_comment","https://github.com/Trusted-AI/AIF360/issues/821#issuecomment-876372620","I maintain that this is a text-formatter issue, not Fairlearn per se - and a potential nightmare of edge cases.","29","0.357705165534347","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","876378701","issue_comment","https://github.com/Trusted-AI/AIF360/issues/821#issuecomment-876378701","A quick google search revealed quite a few options to change the display format in pandas: https://re-thought.com/how-to-suppress-scientific-notation-in-pandas/

We could add this as an example to the user guide, but I agree that it is probably more related to pandas than Fairlearn.","29","0.301701777693427","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","876393958","issue_comment","https://github.com/Trusted-AI/AIF360/issues/821#issuecomment-876393958","The `%` format option could help as well, by lopping off some of the leading zeros.","30","0.3306693306693306","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","881694837","issue_comment","https://github.com/Trusted-AI/AIF360/issues/819#issuecomment-881694837","By default it seems to check for the `main` branch of the `fairlearn/fairlearn` repo rather than the PR branch, so we'd need to fix that.","32","0.6022328548644338","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","847156644","issue_comment","https://github.com/Trusted-AI/AIF360/issues/818#issuecomment-847156644","Can you elaborate on your question a bit? Fairlearn doesn't provide explanations - you may want to look at:
https://github.com/interpretml/interpret-community","15","0.5383675464320627","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","847178225","issue_comment","https://github.com/Trusted-AI/AIF360/issues/818#issuecomment-847178225"," we are using thresholdoptimizer to mitigate unfairness and we are tring to explain the model which we getting using thressholdoptimzer but this is not supported by lime/shap/eli5....however we tried gridsearch also provided by fairlearn but this is not improving the results.....","28","0.6844436009362942","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","847181860","issue_comment","https://github.com/Trusted-AI/AIF360/issues/818#issuecomment-847181860","I'm not surprised that `ThresholdOptimizer` doesn't work well with a lot of explainers. Its `fit()` method:

- Requires the sensitive attribute to be passed separately, meaning it doesn't quite match the `Estimator` pattern
- Is non-deterministic, which is likely to confuse any explainer

Can you elaborate what you mean  when you say that `GridSearch` is 'not improving the results' since that could mean different things?","15","0.4188485597632692","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","847289825","issue_comment","https://github.com/Trusted-AI/AIF360/issues/818#issuecomment-847289825","@shashankc28 can you elaborate on what errors you're facing, and which relevant packages and package versions you have installed?","21","0.6700724827525977","Installation and shell commands","Deployment"
"https://github.com/fairlearn/fairlearn","847653368","issue_comment","https://github.com/Trusted-AI/AIF360/issues/818#issuecomment-847653368","Model-agnostic post-hoc explanation methods (including LIME and model agnostic SHAP) perturb instances in a particular way and then see how this changes the predictions of the model. AFAIK most implementations require a `predict()` function that takes as input instances `X` as some array-like object. `ThresholdOptimizer` requires the user to pass `sensitive_features` at prediction time as well, which I expect the problems to arise from. I don't think the current implementations - at least of `shap` and `lime` - support passing additional arguments to the explainer. There might be a way to solve this through wrapping the functions etc. in some smart way, but it depends a bit on how the explainers try to access `X`. This could be quite tricky...

I'm a bit more surprised to hear that `GridSearch` didn't work either - if you could provide some more details that would be great, @shashankc28!","23","0.2945327081531387","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","847680882","issue_comment","https://github.com/Trusted-AI/AIF360/issues/818#issuecomment-847680882"," I am just sharing the sample implementation.
https://colab.research.google.com/drive/1sEn8pX7jTnf-eR2lxjHVlYgvsu8aYeQN?usp=sharing

if possible, request you to please update the notebook itself on how can we improve results and can get explanations.","15","0.2355661881977671","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","857296348","issue_comment","https://github.com/Trusted-AI/AIF360/issues/818#issuecomment-857296348","@romanlutz , @hildeweerts , @riedgar-ms Thanks team for your prompt responses...am also facing some kind of similar issues....please suggest how can we use XAI libraries with the model produced by Fairlearn threshold optimizer.","6","0.2725288895501661","API expansion","Development"
"https://github.com/fairlearn/fairlearn","857328005","issue_comment","https://github.com/Trusted-AI/AIF360/issues/818#issuecomment-857328005","As @hildeweerts explained above there's no way around providing sensitive features for `ThresholdOptimizer`. I would go a step further and say that you shouldn't use it for any real-world application since it's mostly just a benchmark of how much you have to give up in terms of performance to reach parity in terms of a chosen metric across groups. That comes with a few quirks such as applying randomness to outputs. 

What does explaining a random output even mean? Let's take a model with 10% random outputs, 40% depends on the rule `>0.3` and 50% on `>0.7`. Looking inside this model I can tell you that by sheer chance any sample regardless of features can get 0 or 1. If you try it 100 times you'll see how the results will vary and include both 0s and 1s (well, they probably will).
You don't need an explainer here, because you already know exactly how you get to the output. If you have a score less than 0.3 then you have a 10% chance of getting 1, if your score is >0.3 and <0.7 then you have a 50% chance, and if it's >0.7 then you are guaranteed to get a score of 1. Note that some groups will probably not have the baseline 10%. The PR #614 explains this quite nicely I think, but it requires a little more work to get merged, so you won't find that content in the docs yet.
I would always suggest derandomizing these thresholds as much as possible if you're actually considering using something like this. There's also the issue that it's quite obvious that you're using different thresholds per group, so depending on the laws governing your application domain this may be illegal (even though it may improve disparity metrics). I'll also note that fairness is sociotechnical, so there is no one-size-fits-all solution to fairness that can always be expressed through mathematical definitions.

Applying a post-hoc explainer will mislead here since it'll try and figure out how this solution was reached based on random outputs. Instead, the more interesting part would be to explain where your scores came from, since everything after that is quite interpretable as is.

Taking another step back, similar points apply to `fairlearn.reductions.ExponentiatedGradient` since it's also probabilistic. `GridSearch` should be just fine because it decides on a single predictor internally rather than probabilistically choosing one of many. Additionally, neither of the reductions techniques require `sensitive_features` at `predict` time.

I don't quite understand your concern about `GridSearch` in the notebook. This is your code:
```
sweep = GridSearch(RandomForestClassifier(n_estimators=30, random_state=0),
                   constraints=DemographicParity(),
                   grid_size=71)
sweep.fit(X_train, y_train,
          sensitive_features=x_gender)
```
Later you use existing code to calculate non-dominated models, but comment it out to just get all models generated from the grid, and you take the first one of those to create predictions:
```
y_pred_mitigated=non_dominated[0].predict(X_test)
```
which raises the question: why the first one? It's very likely not the best model you can choose. If you want the default choice just go with the `sweep` object and call `predict` on it. All this extra code looks like it's been copied from a notebook of ours and is mainly there to allow plotting to illustrate the fairness-accuracy trade-off, but since you're not doing that I don't see why you'd choose that model manually (as model 0). It's always recommended to factor in that trade-off (potentially in more than 2 dimensions) when choosing a model.

Later you write
> Now trying eli5 on model selected by gridsearch it works fine for explanation but model is not perfect as we have seen selection rate above.
That is probably related to choosing model 0 rather than one with a better trade-off. You can always plot the selection rate vs. another metric for all the predictors internal to grid search if you want to use that as a criterion for selecting a model.

All in all, the models resulting from `GridSearch` should work with post-hoc explainers.","23","0.3369072501913829","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","876111377","issue_comment","https://github.com/Trusted-AI/AIF360/issues/818#issuecomment-876111377","Closing this issue for now. If there are further questions please reopen. We can also consider explicitly supporting compatibility with some explainers, although I would discourage using blackbox explainers for some of our mitigation techniques as explained above.","24","0.3093566141582842","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","845981244","issue_comment","https://github.com/Trusted-AI/AIF360/issues/814#issuecomment-845981244","I'd worry that if we start adding tensorflow, pytorch etc. to our `requirements-dev.txt` (the only way to get code run in the documentation) we're going to discover that they have mutually incompatible dependencies (and possibly require different Python versions). We could point people at the `test_othermlpackages` directory, but that code isn't written as a teaching example :-(","28","0.315338659756712","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","846187032","issue_comment","https://github.com/Trusted-AI/AIF360/issues/814#issuecomment-846187032","Indeed. I would recommend adding a `code-block` to start with. It wouldn't get run during the doc build, so we'd avoid the issue you mention (which I mentioned in the description as well). As long as it's consistent with the code in the unit tests we can be confident it works.","13","0.4806135893092416","Artifact generation and benchmarking","Deployment"
"https://github.com/fairlearn/fairlearn","847118826","issue_comment","https://github.com/Trusted-AI/AIF360/issues/814#issuecomment-847118826","@romanlutz : I'm a bit confused, why should the example be named `plot_tensorflow.py`? If I understand the issue correctly, it's not just about plotting, but possibly other ways of linking the assessment and mitigation with tensorflow?","15","0.6260061553030305","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","847131961","issue_comment","https://github.com/Trusted-AI/AIF360/issues/814#issuecomment-847131961","All example files need to be named something that starts with plot, it's required by Sphinx gallery.","21","0.3165933528836757","Installation and shell commands","Deployment"
"https://github.com/fairlearn/fairlearn","848068324","issue_comment","https://github.com/Trusted-AI/AIF360/issues/814#issuecomment-848068324","Ouch... that seems like an anti-pattern (but I see it in sklearn too). But we can deal with that another day...","30","0.5186751233262862","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","848068590","issue_comment","https://github.com/Trusted-AI/AIF360/issues/814#issuecomment-848068590","tagging @hannanabdul55 ","11","0.5820271682340651","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","848070741","issue_comment","https://github.com/Trusted-AI/AIF360/issues/814#issuecomment-848070741","@MiroDudik It's like test files for `pytest` having to be named `test_<something>.py`. I'm not a huge fan, but know when I'm swimming against the tide.","32","0.3980160347873353","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","848180213","issue_comment","https://github.com/Trusted-AI/AIF360/issues/814#issuecomment-848180213","@riedgar-ms is right, this isn't necessarily an anti-pattern in the way we've adopted it. It's just the naming convention adopted by sphinx-gallery because they store plots/pictures generated from these python files in the gallery and you can subsequently reference them. That's the purpose of sphinx-gallery (unlike the example notebooks that we've been using it for). I don't think they even allow us to configure that any other way.","21","0.3846844736019995","Installation and shell commands","Deployment"
"https://github.com/fairlearn/fairlearn","864427371","issue_comment","https://github.com/Trusted-AI/AIF360/issues/814#issuecomment-864427371","Hi @romanlutz, I would like to contribute a notebook for this issue. I have used the dataset shap.datasets.adult() to train a neural network model, using TensorFlow and Keras. After this, I used the fairlearn library that displays a dashboard for testing the fairness of the model built. This could serve as the perfect example to use TensorFlow, along with the fairlearn library. I will create a pull request, once you approve.

Thank You,
Priti","13","0.3287611800827661","Artifact generation and benchmarking","Deployment"
"https://github.com/fairlearn/fairlearn","864542467","issue_comment","https://github.com/Trusted-AI/AIF360/issues/814#issuecomment-864542467","@pritishaw1 please note that it should be in the form of an example python file, and not a notebook. Check https://github.com/fairlearn/fairlearn/tree/main/examples for other examples.

I'm not sure what you mean by dashboard, but the dashboard is deprecated and you can use the alternative plotting functionality we have now.

I'm also not sure how you have use fairlearn with TF, but you probably should use scikeras for this example, since it can be used nicely by others as well.","15","0.3584720443875373","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","864555549","issue_comment","https://github.com/Trusted-AI/AIF360/issues/814#issuecomment-864555549","@adrinjalali thanks for the response.

After reading through your comment, I am in a dilemma that whether the solution that I have would do justice to the issue.

Could you please help me a little more, by briefly explaining what exactly is expected as a solution to this issue?","7","0.3410374935798664","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","864556374","issue_comment","https://github.com/Trusted-AI/AIF360/issues/814#issuecomment-864556374","Your scenario sounds good, but on the implementation side, fairleanr expects a scikit-learn compatible estimator, and the recommended way for people to wrap their TF models into a scikit-learn compatible estimator is by using https://github.com/adriangb/scikeras

And when it comes to visualization, please look at the recent updates such as https://github.com/fairlearn/fairlearn/pull/806 , https://github.com/fairlearn/fairlearn/pull/770 , and https://github.com/fairlearn/fairlearn/pull/767 to see how you can visualize your results. Does this help?","32","0.4419218755031719","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","864568256","issue_comment","https://github.com/Trusted-AI/AIF360/issues/814#issuecomment-864568256","@adrinjalali can you please verify if this is correct? 

https://colab.research.google.com/drive/1olUuihBk_9JVH6lwA2AijKZFT_p61zU_?usp=sharing

I have implemented this in the jupyter notebook, where the dashboard is getting displayed, but as it is not supported with colab, it does not show up here. 

Instead, if this is not what's needed for the example using TensorFlow, then please do let me know how can I make a successful contribution to this issue.","28","0.5125725338491296","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","864569567","issue_comment","https://github.com/Trusted-AI/AIF360/issues/814#issuecomment-864569567","In that notebook, you're using the dashboard, and as I mentioned, you should instead use the matplotlib based plotting functionalities, and you can see examples of it in the three PRs I linked above. Your example should also not depend on SHAP, and should instead use our datasets module, or directly use `fetch_openlm` provided by scikit-learn.

Would it be maybe easier for you to start with one of the issues listed here? https://github.com/fairlearn/fairlearn/labels/good%20first%20issue","14","0.328402792773339","Documentation","Development"
"https://github.com/fairlearn/fairlearn","864575293","issue_comment","https://github.com/Trusted-AI/AIF360/issues/814#issuecomment-864575293","Okay @adrinjalali, now I got it. I'll consider all the points that you have mentioned above, this time, and look out for its implementation.

I will also consider the other issues that you have listed above.

Thank you!","20","0.798880524995175","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","867726036","issue_comment","https://github.com/Trusted-AI/AIF360/issues/814#issuecomment-867726036","Hello @adrinjalali,
I have implemented a notebook for this issue, following the points that you've mentioned above.

I have used the fetch_openlm from sklearn.datasets and as you said, that fairlearn uses scikit-learn compatible estimator, so I've used scikeras for wrapping the TensorFlow model.

Besides, I also went for matplotlib based plotting this time, and have used fairlearn.metric for that. 

Can you please verify, if this is what's needed for this issue?
Here's the link to the notebook.
https://github.com/pritishaw1/Fairlearn_Tensorflow/blob/main/Fairlearn_plot_tensorflow.ipynb

This is the link to the python file named plot_tensorflow.py.
https://github.com/pritishaw1/Fairlearn_Tensorflow/blob/main/plot_tensorflow.py

Thank you!

","28","0.3595975914231164","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","868475172","issue_comment","https://github.com/Trusted-AI/AIF360/issues/814#issuecomment-868475172","Yes, it looks much better, now you can add a story and comments around your code as you see in our `examples` folder. @MiroDudik and @hildeweerts would be probably better judges of the content and the story of the file.","20","0.5636363636363637","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","868582804","issue_comment","https://github.com/Trusted-AI/AIF360/issues/814#issuecomment-868582804","@adrinjalali Thanks for the review.
I will start with the story and the comments around the code and will reach out to the team soon, for their approval.","20","0.658753136460143","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","868790211","issue_comment","https://github.com/Trusted-AI/AIF360/issues/814#issuecomment-868790211","The part I don't quite understand yet is how this will work with the dependencies (as mentioned in the issue description).","25","0.4675123326286117","Research","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","868961161","issue_comment","https://github.com/Trusted-AI/AIF360/issues/814#issuecomment-868961161","@romanlutz For this, we can go by adding a code-block for the issue, so that people know that they can use TensorFlow with fairlearn. 
And as you have recommended before that adopting this would ensure that the file does not get run during document build, we can overcome the issues of incompatible dependencies.","21","0.3705567788612771","Installation and shell commands","Deployment"
"https://github.com/fairlearn/fairlearn","869092434","issue_comment","https://github.com/Trusted-AI/AIF360/issues/814#issuecomment-869092434","The code-block is a directive, though, so that would perhaps work in the user guide, but not in the `examples`. In the `examples` we have code that's actually executed, so that won't work without taking a (dev) dependency on tensorflow/keras. So I'd suggest having the 3-5 specific lines of code needed for plugging tensorflow/keras classifiers into our mitigation techniques as the `estimator` argument and put those into a `code-block` in the user guide. I suppose a subsection under `mitigation.rst` would make sense (I'm thinking 3.1, before the current 3.1 section). https://fairlearn.org/main/user_guide/mitigation.html ","28","0.4754507096279248","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","869103831","issue_comment","https://github.com/Trusted-AI/AIF360/issues/814#issuecomment-869103831","Got it. So, as per your suggestions, I will go by adding some specific code lines for this, as a code-block in the user guide as subsection 3.1 under mitigation.rst, so that the dependencies are not disturbed. Here ""fairlearn/docs/user_guide/mitigation.rst""","28","0.4966707433075145","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","869112836","issue_comment","https://github.com/Trusted-AI/AIF360/issues/814#issuecomment-869112836","@fairlearn/fairlearn-maintainers does that sound right to you?
In any case, the code-block shouldn't include all the code that would go into an example, but rather the important lines only (should be no more than a handful or so). I believe your current example only uses Fairlearn in assessment, but not yet for mitigation. Assessment isn't really different for tensorflow models since it's just about the predictions/output, so I don't think we'd need to mention tensorflow in any way there. For mitigation we (might) need to plug that model into one of Fairlearn's mitigation techniques, so that's somewhat different. 

Out of curiosity I check the pytorch tutorials and it turns out they have an entirely separate repository for the tutorial build. I haven't quite figured out how they merge the generated docs with the generated docs from the basic doc build (for API reference etc.), but that would be another option (although much more involved, of course). I think we can keep this in mind for the future, although it's not really viable for this issue right now.","28","0.3066477613568194","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","869147262","issue_comment","https://github.com/Trusted-AI/AIF360/issues/814#issuecomment-869147262","yeah I'd be happy not to include tensorflow as a dependency. Missed that about this issue.","21","0.4098997088320931","Installation and shell commands","Deployment"
"https://github.com/fairlearn/fairlearn","869163422","issue_comment","https://github.com/Trusted-AI/AIF360/issues/814#issuecomment-869163422","@adrinjalali @romanlutz So, shall I proceed, and add some specific lines of the code, addressing this issue, in the form of a code-block, under the user guide section?
And from some specific lines, I mean lines between 37 to 70, along with some important imports from this. https://github.com/pritishaw1/Fairlearn_Tensorflow/blob/main/plot_tensorflow.py","20","0.2813382269904009","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","869972318","issue_comment","https://github.com/Trusted-AI/AIF360/issues/814#issuecomment-869972318","@pritishaw1 your example doesn't actually use our mitigation techniques yet. The classifier should be plugged into `ExponentiatedGradient` or `GridSearch` or `ThresholdOptimizer` (the last one only if we can get it to produce scores rather than binary output).","28","0.595325451827694","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","870075825","issue_comment","https://github.com/Trusted-AI/AIF360/issues/814#issuecomment-870075825","@romanlutz Thanks for the details. I'll try using the mitigation techniques to it. ","20","0.3481324876673713","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","847119355","issue_comment","https://github.com/Trusted-AI/AIF360/issues/813#issuecomment-847119355","@romanlutz : I'm a bit confused, why should the example be named `plot_pytorch.py`? If I understand the issue correctly, it's not just about plotting, but possibly other ways of linking the assessment and mitigation with pytorch?

[tagging @Dref360 since I think he's been using pytorch with fairlearn]","15","0.6025889004612411","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","847132101","issue_comment","https://github.com/Trusted-AI/AIF360/issues/813#issuecomment-847132101","All example files need to be named something that starts with plot, it's required by Sphinx gallery.","21","0.4053030303030304","Installation and shell commands","Deployment"
"https://github.com/fairlearn/fairlearn","847915677","issue_comment","https://github.com/Trusted-AI/AIF360/issues/813#issuecomment-847915677","I only used the metrics for my [project](https://github.com/ElementAI/active-fairness), hence why I started working on streaming metrics. ","15","0.7897116324535685","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","843107688","issue_comment","https://github.com/Trusted-AI/AIF360/issues/810#issuecomment-843107688","One clue: those pieces which aren't rendering are defined as custom LaTeX macros:
https://github.com/fairlearn/fairlearn/blob/389159c5c6d30479742ad6ab268964fe50d2fa2f/docs/conf.py#L212
I don't know what's going on, but it's obviously to do with the macro support.","14","0.5636363636363637","Documentation","Development"
"https://github.com/fairlearn/fairlearn","843387081","issue_comment","https://github.com/Trusted-AI/AIF360/issues/810#issuecomment-843387081","One way to fix it would be to replace the occurrences of the macro throughout the files, but I really like the macros :-(","14","0.7601417399804501","Documentation","Development"
"https://github.com/fairlearn/fairlearn","845292615","issue_comment","https://github.com/Trusted-AI/AIF360/issues/810#issuecomment-845292615","I can help with this","0","0.0303030303030302","Dataset usage","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","845307733","issue_comment","https://github.com/Trusted-AI/AIF360/issues/810#issuecomment-845307733","Great! This looks like it'll require some investigation. Are you familiar with the documentation build? https://fairlearn.org/main/contributor_guide/contributing_documentation.html","14","0.3610047846889953","Documentation","Development"
"https://github.com/fairlearn/fairlearn","842923655","issue_comment","https://github.com/Trusted-AI/AIF360/issues/805#issuecomment-842923655","Fixed by #807 ","24","0.1507849580138736","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","841894550","issue_comment","https://github.com/Trusted-AI/AIF360/issues/804#issuecomment-841894550","@fairlearn/fairlearn-maintainers : This would be nice to have for the scipy-tutorial. I have this already implemented and will create a PR shortly, but I'd like to get a sense what you think the API should be.","20","0.5644048554711375","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","842005123","issue_comment","https://github.com/Trusted-AI/AIF360/issues/804#issuecomment-842005123","Perhaps I'm misremembering but isn't cost represented as a Moment (ErrorRate?) as well? In that case just make it optional with default ErrorRate and this feature would be accommodated through arguments to that Moment. For regression it would be some other Moment presumably (?)

Specifying the objective in the API makes sense to me since we do that in postprocessing. We only use strings there, but that has its own limitations. For example, I can't plug in F1 score even though there's no particularly good reason why I shouldn't. If it's a Moment I can write one custom for my case. Definitely not the primary concern, though.","15","0.5357171362439139","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","842447000","issue_comment","https://github.com/Trusted-AI/AIF360/issues/804#issuecomment-842447000","@romanlutz -- yep, this was also my sentiment. Let's go with providing the objective as a moment object. Then the question is whether we should be existing the existing `ErrorRate` moment or creating a new moment with this functionality and if so what should be the name of the new moment and the mechanism of providing different costs. Some possibilities:

```python
# should we extend ErrorRate?
objective = ErrorRate(costs={'fn': 1.0, 'fp': 0.1}) 

# or create a new moment?
objective = CostSensitiveLoss(costs={'fn': 1.0, 'fp': 0.1})
```
I like the name cost sensitive loss, but I'm not sure whether it's obvious it's a classification loss.","28","0.4483465655770929","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","847998608","issue_comment","https://github.com/Trusted-AI/AIF360/issues/804#issuecomment-847998608","Ping @romanlutz and @riedgar-ms: `ErrorRate` or `CostSensitiveLoss`? (I started the PR with the former, but it would be easy to tweak to the latter.)","20","0.6700724827525977","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","848081441","issue_comment","https://github.com/Trusted-AI/AIF360/issues/804#issuecomment-848081441","What's wrong with ErrorRate? Indeed I wasn't sure cost sensitive loss is for classification.","28","0.5087976539589443","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","848706122","issue_comment","https://github.com/Trusted-AI/AIF360/issues/804#issuecomment-848706122","Not sure if this is relevant, but in scikit-learn the `class_weight` argument is typically used to define different costs per class, in the form of a dictionary with different weight for each class (e.g., `class_weight = {0 : 1.0, 1 : 0.1}`. If we ever wanted to extend `ExponentiatedGradient` to a multi class classification scenario, the current proposal would be more difficult to extend without breaking existing code.","15","0.4068528000515248","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","842863380","issue_comment","https://github.com/Trusted-AI/AIF360/issues/800#issuecomment-842863380","@vamsidesu5 would this be interesting for you?","20","0.5188087774294673","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","843754845","issue_comment","https://github.com/Trusted-AI/AIF360/issues/800#issuecomment-843754845","@romanlutz Yup I can pick this up! Any suggestions on what to read up on so I can understand the issue better and contribute faster? I'm new so not too familiar with all the vocabulary and functions.","20","0.6446528060877837","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","843848874","issue_comment","https://github.com/Trusted-AI/AIF360/issues/800#issuecomment-843848874","Hi @vamsidesu5 - I think [section 2.1 of our user guide](https://fairlearn.org/v0.6.2/user_guide/assessment.html#metrics) and the [API Reference of MetricFrame](https://fairlearn.org/v0.6.2/api_reference/fairlearn.metrics.html) would be good places to get started.

Here's a small code example to show what the issue is:

```python
from fairlearn.metrics import MetricFrame
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix

y_true = [0, 0, 0, 0, 1, 1, 1, 1]
y_pred = [0, 0, 0, 1, 1, 1, 1, 0]
sensitive_features = [1, 1, 0, 0, 1, 1, 0, 0]

mf = MetricFrame(metric= { 'accuracy_score' : accuracy_score, 'confusion matrix' : confusion_matrix},
            y_true = y_true,
            y_pred = y_pred,
            sensitive_features = sensitive_features)
```

Now, if we print the results in `mf.by_group` that works perfectly fine:

```python
                    accuracy_score  confusion matrix
sensitive_feature_0                                 
0                              0.5  [[1, 1], [1, 1]]
1                                1  [[2, 0], [0, 2]]
```

However, if we do `mf.difference()`, which computes the difference of each metric between the two groups, pandas raises `ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()`. The reason is that the 'confusion matrix' column of `mf.by_group` contains arrays instead of floats. This error is expected, but not a good user experience. In particular, it would have been possible to compute difference for `accuracy_score` if we hadn't put in the confusion matrix as well:

```python
mf = MetricFrame(metric= { 'accuracy_score' : accuracy_score},
            y_true = y_true,
            y_pred = y_pred,
            sensitive_features = sensitive_features)

mf.difference()
```

which simply outputs:

```
accuracy_score    0.5
dtype: float64
```

I hope this makes things a bit clearer. If you have any questions, feel free to reach out. Also, if you feel like the user guide is unclear in any way, please raise an issue as well - all feedback is super valuable :)","12","0.6032125508457543","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","843932258","issue_comment","https://github.com/Trusted-AI/AIF360/issues/800#issuecomment-843932258","Thanks for adding that repro code @hildeweerts ","24","0.2808623144193614","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","850146980","issue_comment","https://github.com/Trusted-AI/AIF360/issues/800#issuecomment-850146980","@hildeweerts Thank you so much for the explanation - getting to work on it!","15","0.360762800417973","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","850154250","issue_comment","https://github.com/Trusted-AI/AIF360/issues/800#issuecomment-850154250","So the solution here is to throw an exception whenever the output from a function run on a MetricFrame such as mf.groupby() whenever the the output does not consist of all floats? ","9","0.3224082934609251","Feature engineering methodology","Design"
"https://github.com/fairlearn/fairlearn","850180042","issue_comment","https://github.com/Trusted-AI/AIF360/issues/800#issuecomment-850180042","That sounds correct to me. A slight correction perhaps: scalar can be `int`, too, and maybe other types. I'd suggest going with something like https://numpy.org/doc/stable/reference/generated/numpy.isscalar.html to check.

@hildeweerts @riedgar-ms you wouldn't expect it to still work for the other metrics and ignore the ones that don't return scalars, right?","15","0.3895162808206286","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","850234877","issue_comment","https://github.com/Trusted-AI/AIF360/issues/800#issuecomment-850234877","If I read @riedgar-ms's issue correctly I actually think the idea was to ignore the ones that don't return scalars. Right now there is already a `ValueError` so there's not really a need to throw a new exception, apart from providing the user with a more meaningful message. 

I think it would make sense to return the value for the ones that are scalar and `NaN` for those that aren't, potentially with a warning message that says something like ""Some of the metrics are not scalar and `NaN` is returned"". Or we could even add an argument which controls how to handle such errors, I've seen a similar `errors` argument in pandas (e.g., [`pd.to_numeric`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.to_numeric.html)), where you can choose to `'raise'`, `'coerce'`,  or`'ignore'` invalid values. Though that might be a bit overkill for our scenario, it can be quite useful I think.","15","0.3012750675866524","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","850323933","issue_comment","https://github.com/Trusted-AI/AIF360/issues/800#issuecomment-850323933","@hildeweerts is correct; the issue is that if one has mixed scalar and non-scalar metrics in a single `MetricFrame`, it is no longer possible to call `difference()` etc. on that `MetricFrame`. That may be a problem for some users (in fact, we already know it is).

I do like @hildeweerts 's suggestion of a `errors` argument (defaulting to `ignore` in this case).","15","0.8064358252783562","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","851923404","issue_comment","https://github.com/Trusted-AI/AIF360/issues/800#issuecomment-851923404","Assuming the thumbs up from @romanlutz means that he agrees, I think the `errors` argument is the way to go.

@vamsidesu5 is this enough information for you to get started with this issue? Please feel free to ask questions if anything is unclear! Perhaps it makes sense to start with some examples of what you think the API should look like and return in a few scenarios, just to make sure that we're all on the same page before you spend a lot of time implementing it.","20","0.5171601731601733","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","854364172","issue_comment","https://github.com/Trusted-AI/AIF360/issues/800#issuecomment-854364172","@hildeweerts Thanks - that helps - I'll keep you guys updated here","20","0.3709623709623708","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","876624793","issue_comment","https://github.com/Trusted-AI/AIF360/issues/800#issuecomment-876624793","This issue is up for grabs again 🙂 ","0","0.3716684921504202","Dataset usage","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","878351761","issue_comment","https://github.com/Trusted-AI/AIF360/issues/800#issuecomment-878351761","I would like to pick this up","0","0.1949616648411828","Dataset usage","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","841851503","issue_comment","https://github.com/Trusted-AI/AIF360/issues/798#issuecomment-841851503","Hi! @romanlutz I would like to work on this as part of the mentored sprint. I am still a very beginner in Python so I'm trying to find some issues that are doable at my level. My understanding of this issue is just moving the docs from document process to a new file under the migrating versions. Is that right?","20","0.5033879164313947","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","841855270","issue_comment","https://github.com/Trusted-AI/AIF360/issues/798#issuecomment-841855270","That's correct! It's meant to get a first issue in and to familiarize yourself with the repository/doc build. Let me know if you need any help! I'm available as part of the mentored sprints for another ~2 hours, but also asynchronously beyond that, of course.","20","0.5443381180223286","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","841857304","issue_comment","https://github.com/Trusted-AI/AIF360/issues/798#issuecomment-841857304","Thank you! I will be working on it right now. Just another clarification as per the release process instructions, I will be making a new branch called release/v0.6.3 right? once I made a PR on it should the branch contains the all of the changes that I made? or is it a separate PR?","20","0.5073672504865163","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","841858058","issue_comment","https://github.com/Trusted-AI/AIF360/issues/798#issuecomment-841858058","@MeekaElla you don't need to follow the release instructions. They're just for maintainers when pushing out a new package release. I linked them above only to indicate that the release instructions should get an additional point. The branch names starting with `release` are actually protected, so I'd suggest something different, e.g., `update_release_instructions`. You can also put all your changes in the same branch, no need to have multiple PRs, unless you prefer that.","32","0.6452927633625786","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","841858856","issue_comment","https://github.com/Trusted-AI/AIF360/issues/798#issuecomment-841858856","Ohh noted. Thank you! I'm still familiarizing myself with Git and the branches are kinda confusing me xD This sprint is a really great learning opportunity for me.","25","0.4318339975960018","Research","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","841859234","issue_comment","https://github.com/Trusted-AI/AIF360/issues/798#issuecomment-841859234","No problem! I've written about this a little in the instructions steps 6-8 https://github.com/romanlutz/fairlearn-pycon-sprint/blob/main/README.md
But I'm also around for a little while longer in case you want to look at it together.","25","0.6066719063837217","Research","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","841860193","issue_comment","https://github.com/Trusted-AI/AIF360/issues/798#issuecomment-841860193","Thank you so much!","0","0.0303030303030302","Dataset usage","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","837047244","issue_comment","https://github.com/Trusted-AI/AIF360/issues/796#issuecomment-837047244","This is very useful. A first step (that's not nearly as sophisticated, of course) towards understanding whether we have enough data is #737 , but I definitely agree that we should go beyond that.","11","0.2702297702297702","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","838024506","issue_comment","https://github.com/Trusted-AI/AIF360/issues/796#issuecomment-838024506","I like the idea of having a tool that helps people think about sample sizes. However, I am always a bit cautious with recommending statistical tests for other purposes than statistical testing in a scientific study. The results can become misleading if the tools aren't used properly. So I'd like to make sure that we write proper documentation for this function.

For example, if you compute the sample sizes for multiple sensitive features, you'd need to correct alpha to account for multiple testing (i.e., in addition to the correction that's inherent in the post-hoc ANOVA itself). I also believe that a power analysis is supposed to be computed based on the theoretical effect size of interest, rather than the observed effect size (see e.g., [this blog](http://daniellakens.blogspot.com/2014/12/observed-power-and-what-to-do-if-your.html)). In our scenario, I *think* that means that you need to define the difference in proportions you would deem relevant (which is a hard context-dependent question, but one you need to answer anyways).

Anyways, I wouldn't call myself a statistics expert, so it would be nice if we can involve more experienced folks to ensure we have the proper documentation to guide people who don't know much about stats at all.","7","0.2547254725472548","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","838477671","issue_comment","https://github.com/Trusted-AI/AIF360/issues/796#issuecomment-838477671","Agreed @hildeweerts . I think the same can be said of Error Bars too (tagged you in that issue) ","20","0.2799154334038055","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","841218701","issue_comment","https://github.com/Trusted-AI/AIF360/issues/796#issuecomment-841218701","Like with the error bars, I think that this is a case where we shouldn't be trying to do this within Fairlearn (for the reasons @hildeweerts  outlines), but we should be making sure that we support users applying the appropriate statistical tests themselves. Part of that would be some worked examples with _different_ statistical tests (along with a note as to why the particular statistical test is appropriate, without a digression into a full course on statistics).","15","0.427913248679627","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","876109484","issue_comment","https://github.com/Trusted-AI/AIF360/issues/796#issuecomment-876109484","What's a good way forward here?
@riedgar-ms for the error bars you presumably use another package to calculate the bounds, right? Can't we do the same for these tests?","7","0.3742031263645093","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","840628350","issue_comment","https://github.com/Trusted-AI/AIF360/issues/795#issuecomment-840628350","I know some folks in Microsoft are thinking about this - I can follow up with them to see which of their resources are publicly available!","25","0.3992952783650457","Research","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","842142102","issue_comment","https://github.com/Trusted-AI/AIF360/issues/795#issuecomment-842142102","That would be awesome, @mmadaio!

@hannawallach Do you happen to know (some of) the authors of the CRM in fairness paper? At the risk of sounding like a stalker I believe I've seen some twitter interactions. I'd love to get them involved in case they're interested! :)","20","0.6941527955612461","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","842309721","issue_comment","https://github.com/Trusted-AI/AIF360/issues/795#issuecomment-842309721","I do! I know several of them. Would you like an intro?","9","0.2833150784958017","Feature engineering methodology","Design"
"https://github.com/fairlearn/fairlearn","842311061","issue_comment","https://github.com/Trusted-AI/AIF360/issues/795#issuecomment-842311061","Yes, that would be amazing!","8","0.2833150784958017","Understanding Fairness Definition","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","842321125","issue_comment","https://github.com/Trusted-AI/AIF360/issues/795#issuecomment-842321125","Can you shoot me an email at my MSFT address so that I don't forget? :-)","13","0.4334038054968288","Artifact generation and benchmarking","Deployment"
"https://github.com/fairlearn/fairlearn","857464462","issue_comment","https://github.com/Trusted-AI/AIF360/issues/795#issuecomment-857464462","@mmadaio did you manage to get some pointers on this from the folks at Microsoft?

Update from my side: It looks like I will be able to chat with the authors of the CRM in fairness paper somewhere in July!","20","0.5772758904282914","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","883261982","issue_comment","https://github.com/Trusted-AI/AIF360/issues/795#issuecomment-883261982","Some interesting resources
* ""How We Collect Data Determines Whose Voice Is Heard"" - guide towards DEI in data collection https://www.schusterman.org/blogs/rella-kaplowitz/how-we-collect-data-determines-whose-voice-is-heard
* ""STRAIGHTWASHING THE CENSUS"" https://lawdigitalcommons.bc.edu/cgi/viewcontent.cgi?article=3814&context=bclr
* ""What you need to know (at a minimum)  about your social identity data"" by https://weallcount.com
   * Name of variable,
   * Source of Variable (Data Source) 
   * What were the categories?
   * How many could be selected?
   * Who is constructing the categories?
   * Whose definitions are we using?
   * Who collected the data?
   * Who supplied the data?
* https://weallcount.com/2021/02/19/getting-past-identity-to-what-you-really-want/?mc_cid=90b8b08e12&mc_eid=3ec8760ca5
* https://arxiv.org/pdf/2106.11410.pdf","8","0.704202970025755","Understanding Fairness Definition","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","883270702","issue_comment","https://github.com/Trusted-AI/AIF360/issues/795#issuecomment-883270702","Some thoughts on the question ""what to do with small groups"":
* What are strategies to group smaller categories in a meaningful way (i.e., without lumping them together in some 'other' category), e.g., based on the type of experience.
* How to balance the privacy of small groups and explicit representation?
* What are ways to ensure the categories are *contextual*? e.g., pose as an open question, get in touch with local communities, etc.
* How can we best handle social identity data of people who identify as belonging to multiple groups (e.g., multiethnic)?","8","0.2435209689340295","Understanding Fairness Definition","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","834869545","issue_comment","https://github.com/Trusted-AI/AIF360/issues/793#issuecomment-834869545","I'd prefer to go with 4 categories: Women / Men / WriteIn / PreferNotSay... (see #792). This becomes slightly subtle, because ""PreferNotSay"" should be treated as ""NaN"" for the purposes of fairness evaluation (and I'm frankly not sure whether we deal with this properly everywhere... ehmm... perhaps we should have an issue to check that?).

We could also pick a different sensitive feature like age.","0","0.3914458874458877","Dataset usage","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","835160535","issue_comment","https://github.com/Trusted-AI/AIF360/issues/793#issuecomment-835160535","We could certainly give quite a few knobs for users to tune generating a synthetic dataset. Like how many categorical sensitive features, how many continues ones, etc. On the plus side, we could use these in our tests instead of downloading from openML.","0","0.541006394217403","Dataset usage","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","839763849","issue_comment","https://github.com/Trusted-AI/AIF360/issues/793#issuecomment-839763849","Oh. I like the idea of a synthetic fully reproducible generator with a couple of knobs.

Not sure what others think about ""fully reproducible""--I think it's really important, but it could be a bit tricky if we take dependencies on libraries that rely on random seed generators that might change. @adrinjalali -- do you have thoughts on this? (I might be pulling a cart before the horse here :-)","13","0.267436838695746","Artifact generation and benchmarking","Deployment"
"https://github.com/fairlearn/fairlearn","842770898","issue_comment","https://github.com/Trusted-AI/AIF360/issues/793#issuecomment-842770898","Hey @romanlutz, 

I'll take a stab at this issue and see where I can take it. I'll  be tackling it as part of Pycon 2021 Sprint. Will utilize Scikit Learn based on the specs described above.","20","0.5830374053030303","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","851477373","issue_comment","https://github.com/Trusted-AI/AIF360/issues/793#issuecomment-851477373","@MiroDudik there are so many packages which depend on numpy's random number generators that whenever they need to create something new, they create a new module/function. The old stuff has always been backward compatible generating the same values.","13","0.3093566141582843","Artifact generation and benchmarking","Deployment"
"https://github.com/fairlearn/fairlearn","867026378","issue_comment","https://github.com/Trusted-AI/AIF360/issues/793#issuecomment-867026378","I unassigned @Zuzah who wants to make it available to whoever has time at the moment. Please reply here if you'd like to pick it up!","20","0.4496578690127078","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","881923588","issue_comment","https://github.com/Trusted-AI/AIF360/issues/793#issuecomment-881923588","Hi @romanlutz, I'd like to help here.","20","0.3923719958202718","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","834858602","issue_comment","https://github.com/Trusted-AI/AIF360/issues/792#issuecomment-834858602","Adding the QAI source:
https://sites.google.com/view/queer-in-ai/diversity-guide","14","0.2903762903762902","Documentation","Development"
"https://github.com/fairlearn/fairlearn","836359081","issue_comment","https://github.com/Trusted-AI/AIF360/issues/792#issuecomment-836359081","I have opened an issue for race and ethnicity here: #795","8","0.4531218222493391","Understanding Fairness Definition","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","840606154","issue_comment","https://github.com/Trusted-AI/AIF360/issues/792#issuecomment-840606154","Katta Spiel et al have a great article in ACM Interactions on inclusive ways to collect gender data on surveys:

https://www.researchgate.net/publication/334073420_How_to_do_better_with_gender_on_surveys_a_guide_for_HCI_researchers","8","0.4345583494519667","Understanding Fairness Definition","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","834797515","issue_comment","https://github.com/Trusted-AI/AIF360/issues/791#issuecomment-834797515","Mhmm... wouldn't it be better to have the same nav bar on our documentation pages as we have on the landing page? And the same footer as on the landing page? (footer includes the icons / links to gitter etc.)","14","0.9043091465109808","Documentation","Development"
"https://github.com/fairlearn/fairlearn","834847434","issue_comment","https://github.com/Trusted-AI/AIF360/issues/791#issuecomment-834847434","Of course, but you're proposing something that takes between 5 and 50 hours for an experienced person (sphinx themes are very non-trivial) while I'm proposing a ~15-60 minute fix for a new contributor. :-)
Whenever someone actually wants to come in and create our very own sphinx theme they can certainly unify all the headers and footers, but that's out of scope for what I'm proposing (which is just to get rid of the inconsistency between links and icons). ","14","0.4372705679118981","Documentation","Development"
"https://github.com/fairlearn/fairlearn","834861164","issue_comment","https://github.com/Trusted-AI/AIF360/issues/791#issuecomment-834861164","good point. i wonder if there's a bit of a middle ground--use the current sphinx theme to get a similar look as the landing page nav bar (this would mean removing gitter / stackoverflow, changing the names of sections to match, and replacing github logo).

the second part would be to adjust footer -- at the minimum the copyright statement, and if possible then add icons with links (not sure if sphinx allows that easily).","14","0.8334977259027893","Documentation","Development"
"https://github.com/fairlearn/fairlearn","834873036","issue_comment","https://github.com/Trusted-AI/AIF360/issues/791#issuecomment-834873036","You're already in the 2-10 hour range (and that assumes some familiarity with sphinx). I am really open to having your suggestion as another issue, but this one is just about providing a small item for someone to get started in contributing (hence the tags). I suspect the result will remain on our webpage for at least a year before we make such sweeping changes. Plus, it's a fantastic way to get started using sphinx!","14","0.3232583567635115","Documentation","Development"
"https://github.com/fairlearn/fairlearn","841810305","issue_comment","https://github.com/Trusted-AI/AIF360/issues/791#issuecomment-841810305","Hi @romanlutz, I'd like to work on this issue. Is this issue prepared for the Americas mentored sprint? ","20","0.6862170087976542","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","841824313","issue_comment","https://github.com/Trusted-AI/AIF360/issues/791#issuecomment-841824313","Welcome! It's generally been prepared for anyone getting started. Thanks for picking this and let me know if you have any questions. I will be at the mentored sprint (Americas) today and development sprints Monday/Tuesday, but we can discuss this (or the project more generally) at any time. If you're on the sprints Discord you should find me there with the same alias. After the sprints there is always our Fairlearn [Gitter](https://gitter.im/fairlearn/community). Obviously the best place to ask about this particular issue is right here, but these channels are better in case there's anything else you're curious about. Thanks for contributing!","20","0.6841209451341612","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","841827214","issue_comment","https://github.com/Trusted-AI/AIF360/issues/791#issuecomment-841827214","Will definitely reach out to you if I encounter any issues. Thanks for creating these beginner-friendly issues to help newcomers like me get started contributing to open-source projects! ✨ ","20","0.6073813192457259","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","841838388","issue_comment","https://github.com/Trusted-AI/AIF360/issues/791#issuecomment-841838388","Hey @romanlutz, I just jumped into this issue when building the docs using Sphinx. 

![image](https://user-images.githubusercontent.com/37827647/118403841-08ceca00-b679-11eb-9894-31deafa03fa4.png)

The command I used for this building is `python -m sphinx -v -b html -n -j auto docs docs/_build/html` as it's described at https://fairlearn.org/main/contributor_guide/contributing_documentation.html#contributing-documentation

I also have upgraded the version of `pydata-sphinx-theme` inside the `requirement-dev.text` file as you mentioned in this issue details when setting up the requirements. 

![image](https://user-images.githubusercontent.com/37827647/118404226-a8d92300-b67a-11eb-8f5d-c76ca066cd2b.png)

Any ideas?

","14","0.5276624586969415","Documentation","Development"
"https://github.com/fairlearn/fairlearn","841839130","issue_comment","https://github.com/Trusted-AI/AIF360/issues/791#issuecomment-841839130","@trangology it looks like everything's running fine from what I can tell. The FairlearnDashboard warning is intentional and should actually show up. The first time you build it there should also be a ton of other warnings at the end (often printed in red which makes them quite scary looking), but no errors. Did it finish running? Where there any other messages?
The command you're using with Sphinx mentions `-v` so this is going to be verbose, hence all this output. I do think that's good for development purposes, but it can be a bit strange when using this for the first time.","20","0.3704707842638877","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","841840828","issue_comment","https://github.com/Trusted-AI/AIF360/issues/791#issuecomment-841840828","Oh yeah, forgot about the `-v` flag, which is denoted as `verbose`. Here is the current issue, this generation has been running for more than 5 mins without any changes in the percentage 🤔 

![image](https://user-images.githubusercontent.com/37827647/118404724-be4f4c80-b67c-11eb-9716-e284f6567691.png)
","3","0.3676718761464523","Bias detection metrics validation","Validation"
"https://github.com/fairlearn/fairlearn","841841212","issue_comment","https://github.com/Trusted-AI/AIF360/issues/791#issuecomment-841841212","The first time it takes quite a while I'm afraid. I'd wager around 5-10 minutes. The good news is after that it's a lot faster. It has to run all our examples on the first run and store the outputs. Hang in there! :-) If it takes longer than 15 minutes something is definitely off.","20","0.4179841897233201","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","841846527","issue_comment","https://github.com/Trusted-AI/AIF360/issues/791#issuecomment-841846527","Yes, you are right about the timing. But I need to go off for an hour or so. Will get back to it ASAP. Super excited to get it done! 🙂 ","20","0.5827223851417402","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","841846970","issue_comment","https://github.com/Trusted-AI/AIF360/issues/791#issuecomment-841846970","Good to hear! No worries at all, take your time. Thanks for joining @trangology 😄 ","20","0.8044965786901274","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","855909243","issue_comment","https://github.com/Trusted-AI/AIF360/issues/791#issuecomment-855909243","Closing since it's merged 🙂  Thanks again @trangology ","20","0.4039048200122027","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","841210212","issue_comment","https://github.com/Trusted-AI/AIF360/issues/786#issuecomment-841210212","I agree that we need to expand beyond the tabular data outlook of `MetricFrame` - this discussion has arisen in #756 .

I do wonder whether we should be figuring out what new API would be appropriate, rather than trying to extend `MetricFrame`, though. The latter does its job relatively well, and I wouldn't want that compromised.","15","0.8681596760928708","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","834302089","issue_comment","https://github.com/Trusted-AI/AIF360/issues/785#issuecomment-834302089","I'd be a bit concerned with our policy of not using the word ""bias"" or ""debias"" completely. These words are used in the literature and folks use them to find resources and implementations. I completely agree that we can't completely debias a dataset, or a model, but we do remote some sort of a bias when we use these methods. We can still use these words, but be very clear about what kind of bias it is that the method is removing, instead of saying this method debiases the model/data.","22","0.3793084552578224","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","834567608","issue_comment","https://github.com/Trusted-AI/AIF360/issues/785#issuecomment-834567608","Exactly, but ""Adversarial Debiasing"" isn't a clear name since it misses exactly that point. In fact, it might give the impression that it's a fix for all biases.

Thanks @riedgar-ms for the title fix!","30","0.3629404694122857","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","836339822","issue_comment","https://github.com/Trusted-AI/AIF360/issues/785#issuecomment-836339822","Within the paper itself they even acknowledge that debiasing might be problematic:*""“debias” may not be quite the right word, as all bias is not necessarily removed""*. 

I agree with @adrinjalali that in some cases the usage of ""bias"" or ""de-bias"" might be appropriate, but I wouldn't say that's the case for this specific paper. In particular, there is no exact specification of the type of ""bias"" that is ""removed"" and where it is supposed to originate from. Instead, like most fairness mitigation approaches, the method optimizes for a particular fairness metrics, foregoing the biases from which potential unfairness may have originated. 

My first thought was also to go for something like ""AdversarialFairness"" - although that also still sounds a bit absolute and that it's related to fairness is probably clear from the fact that it's part of Fairlearn. ""AdversarialMitigation"" could be an option.

But I also share @adrinjalali 's concern in terms of the discoverability of the approach...

@fairlearn/fairlearn-maintainers thoughts?","15","0.2269915310308323","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","836347513","issue_comment","https://github.com/Trusted-AI/AIF360/issues/785#issuecomment-836347513","We could also call the class with the letters of the first or last names of the authors. ","7","0.1824283099450885","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","857903475","issue_comment","https://github.com/Trusted-AI/AIF360/issues/785#issuecomment-857903475","I will begin working on this issue, as I have some spare time and this seems like a doable step up from #812.","23","0.4792277614858261","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","897741041","issue_comment","https://github.com/Trusted-AI/AIF360/issues/785#issuecomment-897741041","Hi @stephenrobic, did you have any luck trying to implement this? Please let us know if you have any questions!

If you don't plan to work on this issue anymore that is also perfectly fine. Do let us know so we can let other people know the issue is up for grabs again!","20","0.5539872408293461","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","910077949","issue_comment","https://github.com/Trusted-AI/AIF360/issues/785#issuecomment-910077949","Hi @stephenrobic! Please let me know if you'd still like to work on this issue! ","20","0.2833150784958015","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","922285643","issue_comment","https://github.com/Trusted-AI/AIF360/issues/785#issuecomment-922285643","I would like to work on this issue :)","0","0.0303030303030302","Dataset usage","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","922661052","issue_comment","https://github.com/Trusted-AI/AIF360/issues/785#issuecomment-922661052","Please take and work on this issue! I thought I was prepared to work on
this, but I haven't been able to.

On Sat, Sep 18, 2021, 9:23 AM SeanMcCarren ***@***.***> wrote:

> I would like to work on this issue :)
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/fairlearn/fairlearn/issues/785#issuecomment-922285643>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AGOUXQ3XNK5G5M6VQOJH6KDUCSOELANCNFSM44JKYFAQ>
> .
> Triage notifications on the go with GitHub Mobile for iOS
> <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>
> or Android
> <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>.
>
>
","7","0.9175506915536454","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","930281635","issue_comment","https://github.com/Trusted-AI/AIF360/issues/785#issuecomment-930281635","Hi there, I have some questions regarding this implementation. The paper leaves room for quite some freedom, and we need to make some tough decision choices regarding this. Most notably, I can think of situations in which one may want to change:
- Predictor/adversarial models (make predictor very complex, possibly)
- Output layer. Continuous vs discrete (and for discrete with demographic parity, single binary->sigmoid vs multiclass->cross entropy is another choice). Loss functions depend on this.
- Which metric. The paper speaks about DP, Equality of odds, Equality of opportunity, but they require slightly different scenarios (Equality of odds requires adversary has more inputs and binary values, whereas Equality of opportunity requires predictions are discrete)

One possible implementation is to provide the required code that one would put in a training loop, but not more. Then, a user of this module has to implement their own training loop. Pros: lots of freedom, can do validation at every step, batching how the user would like. Cons: users will have to know precisely what parameters are possible, and must understand the original paper or this function really well.

Another possibility, what I see in AIF360 (among others) also, is to provide the training loop and most choices given above as standard. Pros: Simpler use, maybe we can even use the standard `.fit(X,y)` and `.predict(X)` as the user needn't care how it is fit (what parameters/training loop.) which fits in well with sklearn-style. We can define the different metrics, the user just has to choose. Cons: no custom training loops, so besides possibly some keyword parameters the user cant add things like validation every X epochs or batching, neither can the user define custom adversary models. We don't know how much to train (but the user can supply epochs or something at least)

Side note: Where can I write in the user guide for this? I was thinking under mitigations, but those are about algorithms that implement `.fit(X,y)` and `.predict(X)`, and I don't know whether this algorithm will have this.

EDIT: @adrinjalali mentioned using `partial_fit` to fit with sklearn standards and this seems like a good option!","28","0.3499526103783272","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","933777338","issue_comment","https://github.com/Trusted-AI/AIF360/issues/785#issuecomment-933777338","Just a couple of links:
* [API description](https://scikit-learn.org/stable/glossary.html#term-partial_fit) of `partial_fit`
* examples of SGD API in sklearn (including `partial_fit`): [SGDClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier), [SGDRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html#sklearn.linear_model.SGDRegressor)

[tagging @Dref360 in case he has some thoughts re. API for minibatch training]","15","0.4185424918687405","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","841763513","issue_comment","https://github.com/Trusted-AI/AIF360/issues/784#issuecomment-841763513","Hello! I would like to work on this if that's okay. Will ask further questions once I start looking into it.","20","0.3607628004179729","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","841774331","issue_comment","https://github.com/Trusted-AI/AIF360/issues/784#issuecomment-841774331","@galuhsahid Certainly! Welcome to Fairlearn! As you probably noticed the issue description isn't very detailed, so if you think it's helpful you can outline a plan for the implementation in this issue as well. That's entirely up to you, though. We're happy to review PRs, too. If you'd like to discuss anything further outside this issue we have a [Gitter](https://gitter.im/fairlearn/community) channel. Don't hesitate to ask questions :-)","20","0.7428412566027242","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","858906915","issue_comment","https://github.com/Trusted-AI/AIF360/issues/784#issuecomment-858906915","@galuhsahid I just wanted to check in how it's going. There's no particular time pressure, but if you need help let us know. Also, if you're not planning to continue we can make this available to someone else. We're also starting to do pair programming sometimes, so if that's interesting to you please don't hesitate to ask.","20","0.7938219972118278","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","860081305","issue_comment","https://github.com/Trusted-AI/AIF360/issues/784#issuecomment-860081305","@romanlutz Hello! Sorry for the delay, I'm currently working on the implementation plan and will reach out to ask for feedback before going ahead with the code.","20","0.6383721945681595","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","860098682","issue_comment","https://github.com/Trusted-AI/AIF360/issues/784#issuecomment-860098682","No problem at all! Reach out if you want to discuss anything :-)","20","0.3546878177750662","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","864544636","issue_comment","https://github.com/Trusted-AI/AIF360/issues/784#issuecomment-864544636","Hello! I would like to confirm a few things regarding the implementation:

1. Will the input type be pandas DataFrame?
2. For the output, should we return an array of the new weights?","17","0.4864433811802233","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","865256395","issue_comment","https://github.com/Trusted-AI/AIF360/issues/784#issuecomment-865256395","We typically accept a variety of input types. See the `fit` method of `ThresholdOptimizer`, for example: https://fairlearn.org/main/api_reference/fairlearn.postprocessing.html#fairlearn.postprocessing.ThresholdOptimizer.fit

![image](https://user-images.githubusercontent.com/10245648/122808989-6363d300-d282-11eb-9952-ba100e73b3c3.png)

In terms of API I would imagine that we have
```
class KamiranCaldersReweighing(...):
    def __init__(self, ...):
        ...
    def fit(X, y, *, sensitive_features):
        ...
```
After calling `fit` the weights would be stored on the object (let's say in a member called `weights`, so we could either retrieve it using 
```
reweighing_obj = KamiranCaldersReweighing(...)
reweighing_obj.fit(X, y, sensitive_features=sensitive_features)
w = reweighing_obj.weights
```
or perhaps with a method `get_weights()` but it feels odd to write a custom getter like that. @adrinjalali @MiroDudik may have thoughts on this. I checked the scikit-learn docs for conventions but didn't really find anything similar (preprocessing that results in weights). Regardless of how we pass it on to the estimator we use for training we definitely need to create the weights internally and store them on the object.

I imagine all of this would go into the `fairlearn.preprocessing` module.

I'm still unsure about naming - can we come up with something better than what I put above using the author names? Or are we fine with that?","23","0.6362420351184397","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","865796837","issue_comment","https://github.com/Trusted-AI/AIF360/issues/784#issuecomment-865796837","Reweighing is gonna be tricky to have scikit-learn compatible until we have the ""resamplers"" on the scikit-learn side. For more info you can have a look here: https://github.com/scikit-learn/enhancement_proposals/pull/15","32","0.6340157018123119","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","865839490","issue_comment","https://github.com/Trusted-AI/AIF360/issues/784#issuecomment-865839490","### re. API
If I recall correctly the implementation in AIF360 does something similar to what @romanlutz proposes. Quite a few of my students had issues with that, because it wasn't clear to them that they should retrieve the weights and pass them separately to the estimator. I can imagine we could add an (optional?) estimator argument in which case it would be used more like a wrapper than a separate pre-processing module, but I can also see how that could open another can of worms... @adrinjalali do you expect this is something that will be taken up soon from the scikit-learn side? 

### re. naming
I don't like it a ton, but given that fair ml researchers haven't been very productive in coming up with unique names, I do think going with the (first) author makes the most sense. If we want to adopt this we should probably also update our older classes though...","23","0.4819230924424","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","865853776","issue_comment","https://github.com/Trusted-AI/AIF360/issues/784#issuecomment-865853776","I wouldn't necessarily wait for hit @hildeweerts , but we also don't really have to. The meta estimator alternative mentioned [here](https://github.com/scikit-learn/enhancement_proposals/blob/c64044de8332b070eae9c7049a027133952d17eb/slep005/proposal.rst) which also has a prototype implementation on this PR:https://github.com/scikit-learn/scikit-learn/pull/13269 would be something we could already do.","28","0.5083823622445751","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","869189800","issue_comment","https://github.com/Trusted-AI/AIF360/issues/784#issuecomment-869189800","Hi all, thank you for the answers! If I can summarize so far:
- the input will accept various types, namely: np.ndarray, pd.DataFrame, pd.Series, list depending on the parameters
- as for the output, the weights will be stored on the object
- the code will go to the `fairlearn.preprocessing` module

Shall we proceed with implementing the meta estimator alternative? I’m assuming this will also be similar to https://github.com/fairlearn/fairlearn/blob/main/fairlearn/postprocessing/_threshold_optimizer.py where it takes an estimator argument, am I understanding it correctly? Thank you!","23","0.3568019430088395","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","942064661","issue_comment","https://github.com/Trusted-AI/AIF360/issues/784#issuecomment-942064661","That sounds all good to me @galuhsahid ","20","0.5761643278421804","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","1012049167","issue_comment","https://github.com/Trusted-AI/AIF360/issues/784#issuecomment-1012049167","Hi, @galuhsahid! Do you still intend to work on this? No worries if not!

If I receive no response in 2 weeks (before January 27) I will reassign the issue so other folks who are interested can work on it.","20","0.6118819776714515","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","842655261","issue_comment","https://github.com/Trusted-AI/AIF360/issues/782#issuecomment-842655261","Working on Solutionism trap","25","0.5269473256050439","Research","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","918438817","issue_comment","https://github.com/Trusted-AI/AIF360/issues/782#issuecomment-918438817","Is this still open to do? I see someone is assigned to it but it has been quite a while.","20","0.3291536050156741","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","918466609","issue_comment","https://github.com/Trusted-AI/AIF360/issues/782#issuecomment-918466609","This is almost done thanks to @lgfunderburk's PR where all 5 traps are described at the same time. If you need help finding another issue please reach out!","20","0.3609481915933529","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","918482153","issue_comment","https://github.com/Trusted-AI/AIF360/issues/782#issuecomment-918482153","I am working on fixing conflicts and minor deets
Will have a few updates Wed/Thur this week","24","0.4018529241459178","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","834090770","issue_comment","https://github.com/Trusted-AI/AIF360/issues/776#issuecomment-834090770","Absolutely! I talked about this with @riedgar-ms today as well. Let's do it! ","20","0.4112554112554112","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","834097862","issue_comment","https://github.com/Trusted-AI/AIF360/issues/776#issuecomment-834097862","Done and gone.","0","0.0303030303030302","Dataset usage","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","833913873","issue_comment","https://github.com/Trusted-AI/AIF360/issues/772#issuecomment-833913873","Welcome @wcheung-code ! This is very exciting and I'm glad to see this topic finally approached in Fairlearn.

Regarding the datasets: if you can find them on the internet I don't see a reason why we couldn't add them to the `datasets` module. I'm pretty sure I found some admissions data from UC Berkeley when we last thought about Simpson's paradox. That said, we discussed datasets quite a lot a few months ago and reached the conclusion that one needs to add extensive descriptions, caveats, background information, etc. about the datasets. I suggest you create separate issues for each dataset you intend to add. Then you can add that information into the issue and later into the dataset documentation. At least that's my recommendation.

In general, I recommend breaking your work down as much as possible. For example, describing what counterfactual analysis is could be its own PR, the dataset additions could be their own PRs, and then the notebook, of course. Perhaps you also plan to add functionality to the Fairlearn package (e.g. metrics or algorithms), which could be yet another set of PRs. The smaller the changes the easier it will be to get them merged. As you can see with many of my own PRs its quite hard to review if they're too big and tackle lots of things. At that point we'd need to ask you to break them up into smaller pieces.

In the meanwhile, don't hesitate to reach out with questions on how things work. I don't quite know what you mean with the multi-class question, but perhaps our recent [FAQ](https://fairlearn.org/main/faq.html) addition on the topic helps (search for ""Does Fairlearn support multi-class classification?"").","20","0.509936999726086","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","834696076","issue_comment","https://github.com/Trusted-AI/AIF360/issues/772#issuecomment-834696076","Thanks for the warm welcome @romanlutz! :)

I agree and am well aligned with breaking down the work into manageable issues. Thanks for letting me know! We can reserve this issue for the documentation phase of my work (which will only use synthetic data). Once I am able to get a first working iteration, I plan to open up new issues regarding dataset choices, and potential areas where I can contribute code to the project. Thanks!

The question about multiclass support was referencing to the paper rather than FairLearn. Specifically, the CMU researchers' analysis is only focused on a few binary classification settings, so extending counterfactual analysis to the multiclass setting is still a question to think about. It's great to know that FairLearn supports multiclass problems; I'm sure I'll be leveraging a lot of those existing capabilities.

Again these questions are more long term focused, so I think it's best to address these questions in separate tickets. 

Thanks again for the welcome and the thoughts!




","20","0.3583818480370204","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","964176992","issue_comment","https://github.com/Trusted-AI/AIF360/issues/772#issuecomment-964176992","@wcheung-code : I'm in the process of reviewing the issues and I'm wondering what's the status here. Let us know if you did something on the VW side that we could link to.","20","0.85311004784689","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","833648048","issue_comment","https://github.com/Trusted-AI/AIF360/issues/769#issuecomment-833648048","Measurement and Fairness is still on my desk (and I wouldn't mind reading it again 🥰), do you already have a list of key terms in mind? I'd be interested in collaborating on this issue, but I lack the STS background (at the moment).","8","0.870670002780094","Understanding Fairness Definition","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","833916229","issue_comment","https://github.com/Trusted-AI/AIF360/issues/769#issuecomment-833916229","@michaelamoako (together with @brkifle ) just created #771 today as well. Would be great if the four of you (@shimst3r , @mmadaio , @michaelamoako , @brkifle ) could outline your ideas here in detail to decide who can do which part :-)
For good measure I'll also tag @hannawallach in case she has any thoughts.","20","0.8557853093519316","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","834072825","issue_comment","https://github.com/Trusted-AI/AIF360/issues/769#issuecomment-834072825","I just realized #707 also discussed this. @hildeweerts @LisaIbanez @riedgar-ms and @LeJit participated there, too, and may have thoughts, too :-) ","20","0.5770121598147078","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","836139444","issue_comment","https://github.com/Trusted-AI/AIF360/issues/769#issuecomment-836139444","@romanlutz could you changed linked PR to #787 ","20","0.25008325008325","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","840593176","issue_comment","https://github.com/Trusted-AI/AIF360/issues/769#issuecomment-840593176","> Measurement and Fairness is still on my desk (and I wouldn't mind reading it again 🥰), do you already have a list of key terms in mind? I'd be interested in collaborating on this issue, but I lack the STS background (at the moment).

Ah, thanks for flagging @shimst3r! I should have been more clear in the issue. This is referring to the sub-sections of construct validity from the Measurement and Fairness paper, including:

- face validity
- content validity
  - contestedness
  - substantive validity
  - structural validity
- convergent validity
- discriminant validity
- predictive validity
- hypothesis validity
- consequential validity ","8","0.8100649350649354","Understanding Fairness Definition","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","832091177","issue_comment","https://github.com/Trusted-AI/AIF360/issues/768#issuecomment-832091177","Hi @MartaMarchiori !

One option would be to take a screenshot of the dashboard. Not ideal, I know, but the dashboard is being removed from Fairlearn at the moment #766 in favor of simpler plots. #766 has a few screenshots of what that looks like, although you don't really need to wait for any PRs to get merged since the functionality already exists. With any `MetricFrame` you can simply plot it, e.g.,

```
>>> from sklearn.metrics import precision_score, recall_score, accuracy_score
>>> from fairlearn.metrics import false_positive_rate, true_positive_rate, selection_rate
>>> metrics = {
...     'accuracy': accuracy_score,
...     'precision': precision_score,
...     'recall': recall_score,
...     'false positive rate': false_positive_rate,
...     'true positive rate': true_positive_rate,
...     'selection rate': selection_rate,
...     'count': lambda y_true, y_pred: y_true.shape[0]}
>>> metric_frame = MetricFrame(metrics, y_true, y_pred, sensitive_features=sex)
>>> metric_frame.by_group.plot.bar(
...     subplots=True, layout=[3,3], legend=False, figsize=[12,8],
...     title='Show all metrics')  # doctest: +SKIP
```
![image](https://user-images.githubusercontent.com/10245648/117039565-c642e000-acbd-11eb-9db5-a1f079f4df7e.png)
... and matplotlib plots can be saved easily.

If you are really set on the dashboard, there's a newer version in the [raiwidgets package](https://github.com/microsoft/responsible-ai-widgets). The `_create_group_metric_set` function from Fairlearn can give you all the metrics in a storeable format to pass to the `FairnessDashboard` later on when you want to look at them again. Or you can just use it out of the box with Azure (see [this blog post](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-machine-learning-fairness-aml)) since your metrics are persisted in the cloud there and the dashboard can be viewed anytime in AzureML Studio.

I hope this helps!

I'm leaving this issue open since we should probably put this answer in the Q&A section of our website. 
TODO:
- [ ] Create a Q&A section, something along the lines of: ""What happened to the `FairlearnDashboard`?"" [answer should point to the new repo and to our plotting functionality through `MetricFrame`]
- [ ] add a line to the newly created plotting documentation in #766 to show how one can save the generated matplotlib plots.","12","0.5257090565157043","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","833435683","issue_comment","https://github.com/Trusted-AI/AIF360/issues/765#issuecomment-833435683","You can already give `MetricFrame` a custom metric, so long as it conforms to the `f(y_true, y_pred)` pattern of sklearn (there are examples of this in the documentation, although perhaps not particularly highlighted). So long as `y_true` and `y_pred` are arrays of equal length, the type of the items is irrelevant to `MetricFrame` - it just passes them on to the supplied metric function.

We do know that not all data fit into the tabular model of `MetricFrame`, and we're starting to think about ways to support these scenarios as well.","12","0.3673337470265798","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","834329617","issue_comment","https://github.com/Trusted-AI/AIF360/issues/765#issuecomment-834329617","https://github.com/fairlearn/fairlearn/issues/756 highlights this in a more general way","32","0.3546878177750661","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","842792379","issue_comment","https://github.com/Trusted-AI/AIF360/issues/764#issuecomment-842792379","Hi I would like to work on this issue.","0","0.0303030303030302","Dataset usage","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","846667760","issue_comment","https://github.com/Trusted-AI/AIF360/issues/764#issuecomment-846667760","I expect the config values needed will be
```
'binder': {
        'org': 'fairlearn',
        'repo': 'fairlearn', 
        'binderhub_url': 'https://mybinder.org', 
        'branch': binder_branch, 
        'dependencies': './binder/requirements.txt', 
        'use_jupyter_lab': True    
    }
```

As you can tell the branch changes depending on the version since the documentation is built in multiple versions. I'm hoping the same version parsing will work for us (https://github.com/scikit-learn/scikit-learn/blob/309f135c3284d7db6e23ca81a87948c7066a3949/doc/conf.py#L282 and following), but it's not clear that it will. You may have to try it out. If that doesn't work, just use ""main"" as the branch. The dependencies file is a new file (which is mentioned in the issue description). I have no particular opinion on jupyter lab, but you can try it with `False` and `True` and see if it works.","24","0.2805866699672009","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","846674616","issue_comment","https://github.com/Trusted-AI/AIF360/issues/764#issuecomment-846674616","Thanks @romanlutz for your quick reply. Is the requirements.txt used in `dependencies` pointing to a folder `binder/requirements.txt` in our case?

If you look  at scikit docs, [the requirements.txt](https://github.com/scikit-learn/scikit-learn/blob/309f135c3284d7db6e23ca81a87948c7066a3949/doc/binder/requirements.txt) requirements.txt there is actually nothing. So what should be there in requirements.txt in our case? 

Also it would be very helpful if you can point to some link which you used to setup sphinx_gallery configuration for our project, to get a better idea of the library used.","21","0.4111423018786439","Installation and shell commands","Deployment"
"https://github.com/fairlearn/fairlearn","846688343","issue_comment","https://github.com/Trusted-AI/AIF360/issues/764#issuecomment-846688343","Seems like scikit-learn has a special setup with a separate repo for this. You can check their binder repo to see what's in the requirements file. Probably just the main dependencies we have in our requirements.txt plus the package itself. I'd just try it out by pointing at an empty file and if it fils try adding dependencies. Binder is free so there's no real cost associated with trying this.

You can check the Sphinx Gallery docs https://sphinx-gallery.github.io/stable/index.html
It's a very cool tool with lots of functionality that we're not even using right now.","21","0.4111335194952943","Installation and shell commands","Deployment"
"https://github.com/fairlearn/fairlearn","858907827","issue_comment","https://github.com/Trusted-AI/AIF360/issues/764#issuecomment-858907827","@kurianbenoy I just wanted to check in how it's going. There's no particular time pressure, but if you need help let us know. Also, if you're not planning to continue we can make this available to someone else. We're also starting to do pair programming sometimes, so if that's interesting to you please don't hesitate to ask.","20","0.7849438696896324","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","861200066","issue_comment","https://github.com/Trusted-AI/AIF360/issues/764#issuecomment-861200066","@romanlutz, was a bit busy the past few weeks. I couldn't work on it for a few weeks now, but I would definitely like to continue doing this if there is no time pressure. ","20","0.5961051436555757","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","861233727","issue_comment","https://github.com/Trusted-AI/AIF360/issues/764#issuecomment-861233727","@kurianbenoy there is no pressure at all! Good to know, and let us know if there's any way we can help.","20","0.6499898311978852","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","864422563","issue_comment","https://github.com/Trusted-AI/AIF360/issues/764#issuecomment-864422563","@romanlutz how can I bring the docs website up? Is it by the command: `make doc`?

Also another question, does the team prefer to use `Jupyter labs` or `Jupyter notebooks`?","14","0.3170114506231417","Documentation","Development"
"https://github.com/fairlearn/fairlearn","864423552","issue_comment","https://github.com/Trusted-AI/AIF360/issues/764#issuecomment-864423552","https://fairlearn.org/main/contributor_guide/contributing_documentation.html should help 🙂 

I don't have a preference between lab and notebook. How does it affect the task?","14","0.4719166184134339","Documentation","Development"
"https://github.com/fairlearn/fairlearn","864423914","issue_comment","https://github.com/Trusted-AI/AIF360/issues/764#issuecomment-864423914","Yeah it doesn't affect the task in hand. One issue, I am facing now is when passing dependencies in binder config, I am getting the below error

![image](https://user-images.githubusercontent.com/24592806/122648028-d23b1200-d144-11eb-8f99-fef3f3abd409.png)

Any idea why, I am getting this error?","21","0.5390017081040046","Installation and shell commands","Deployment"
"https://github.com/fairlearn/fairlearn","864425302","issue_comment","https://github.com/Trusted-AI/AIF360/issues/764#issuecomment-864425302","Also another thing is I am able to get the run binder button: 
![image](https://user-images.githubusercontent.com/24592806/122648229-d87dbe00-d145-11eb-8c33-59f43b5f0b9b.png)

Yet on selecting it, I am getting this error

![image](https://user-images.githubusercontent.com/24592806/122648328-71acd480-d146-11eb-9dc9-bcb568bf7133.png)

I will be trying to debug these issue for a couple of hours more and see if I can get a solution. If any help is available it would be great as well :)","21","0.324666287087999","Installation and shell commands","Deployment"
"https://github.com/fairlearn/fairlearn","864426159","issue_comment","https://github.com/Trusted-AI/AIF360/issues/764#issuecomment-864426159","@romanlutz just noticed when checking our docs/conf.py that the copyright is hardcoded. Can we make it dynamic like the Scikit-learn docs configuration [1] ? Also should we give a start year like scikit docs?

Can I create a ticket for it?

[1] https://github.com/scikit-learn/scikit-learn/blob/309f135c3284d7db6e23ca81a87948c7066a3949/doc/conf.py#L88","14","0.3385408917323811","Documentation","Development"
"https://github.com/fairlearn/fairlearn","864445712","issue_comment","https://github.com/Trusted-AI/AIF360/issues/764#issuecomment-864445712","I am a bit confused on what should we give for the `notebook_dir` parameter in binder configuration. In case of `scikit-learn` and `sphinx-gallery` configuration, they don't have any folder named notebooks in their repo. But that is not the case of fairlearn which has a notebooks folder. 

Reference link:
https://sphinx-gallery.github.io/stable/configuration.html#binder-links","21","0.4461845915412538","Installation and shell commands","Deployment"
"https://github.com/fairlearn/fairlearn","864542877","issue_comment","https://github.com/Trusted-AI/AIF360/issues/764#issuecomment-864542877","maybe this would be of some help? https://github.com/scikit-learn/scikit-learn/tree/main/.binder","31","0.4158451989777291","Testing","Maintenance"
"https://github.com/fairlearn/fairlearn","864546901","issue_comment","https://github.com/Trusted-AI/AIF360/issues/764#issuecomment-864546901","I will check it out. Maybe the postbuild file maybe helpful to solve the issue.","21","0.6328671328671329","Installation and shell commands","Deployment"
"https://github.com/fairlearn/fairlearn","865244758","issue_comment","https://github.com/Trusted-AI/AIF360/issues/764#issuecomment-865244758","A few clarifications:
- Ignore the `notebooks` directory. These are only notebooks that existed before we started using `sphinx-gallery`. Eventually they'll either be migrated to the gallery format (basically using `.py` files with the magic markers to separate cells) or deleted.
- Examples/notebooks to render on the webpage are in the `examples` directory. So you can use whatever scikit-learn specifies for notebook directory (which may just be nothing in case they use a default). This should fix the path issue for `plot_quickstart_selection_rate.py`
- Copyright: I don't particularly care to be honest, but if you like feel free to make the year dynamic. I guess the start year would be 2018.
- In the issue description I referred to the `requirements.txt` file from scikit-learn. Did you add one for Fairlearn? That might explain the `binder/requirements.txt` issue.","14","0.4274433341457472","Documentation","Development"
"https://github.com/fairlearn/fairlearn","866223278","issue_comment","https://github.com/Trusted-AI/AIF360/issues/764#issuecomment-866223278","Ok, I am ignoring the notebooks section and using default value for notebooks. Also I have added requirements.txt in .binder folder.","21","0.4270369400052395","Installation and shell commands","Deployment"
"https://github.com/fairlearn/fairlearn","828996232","issue_comment","https://github.com/Trusted-AI/AIF360/issues/759#issuecomment-828996232","@michaelamoako the `FairlearnDashboard` in Fairlearn is deprecated and will be deleted shortly. Are you referring to the `FairnessDashboard` in `raiwidgets`? If so, this sounds like a feature request for [that repo](https://github.com/microsoft/responsible-ai-widgets) rather than Fairlearn.

Fairlearn is adding plotting functionality at the moment and multiple sensitive feature support is on our radar for that. The corresponding PR is #561 ","15","0.4413855045298346","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","829285278","issue_comment","https://github.com/Trusted-AI/AIF360/issues/759#issuecomment-829285278","Yep, dashboard issues are out of scope for this repo now.","24","0.4334038054968286","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","829283103","issue_comment","https://github.com/Trusted-AI/AIF360/issues/758#issuecomment-829283103","I agree this is pretty useful, and for whoever would like to take it up, here's what I've used in my scripts. Feel free to take and generalize it.

``` python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc
from sklearn.metrics import RocCurveDisplay

def plot_auc(df, name, ax=None):
  y_pred = np.asarray(df.y_pred).astype(float)
  y_true = np.asarray(df.FraudLabel == ""true"").astype(int)

  fpr, tpr, _ = roc_curve(y_true, y_pred, pos_label=1)
  roc_auc = auc(fpr, tpr)
  display = RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc, estimator_name=name)
  display.plot(ax=ax)
  return display

plt.figure(figsize=(8, 6))
ax = plt.gca()
overall = plot_auc(df, ""overall"", ax)
women = plot_auc(df_women, ""women"", ax)
men = plot_auc(df_men, ""men"", ax)
nans = plot_auc(df_nan, ""unspecified"", ax)
```

And the corresponding one for precision recall curves:

``` python
import numpy as np
from sklearn.metrics import (precision_recall_curve,
                             PrecisionRecallDisplay,
                             average_precision_score)

def plot_precision_recall(df, name, ax=None):
  y_pred = np.asarray(df.y_pred).astype(float)
  y_true = np.asarray(df.FraudLabel == ""true"").astype(int)

  precision, recall, _ = precision_recall_curve(y_true, y_pred)
  average_precision = average_precision_score(y_true, y_pred)
  disp = PrecisionRecallDisplay(
    precision=precision, recall=recall,
    average_precision=average_precision,
    estimator_name=name
  )
  disp.plot(ax=ax)
  return display

plt.figure(figsize=(8, 6))
ax = plt.gca()
overall = plot_precision_recall(df, ""overall"", ax=ax)
women = plot_precision_recall(df_women, ""women"", ax)
men = plot_precision_recall(df_men, ""men"", ax)
nans = plot_precision_recall(df_nan, ""unspecified"", ax)
```","2","0.5712661322417417","Performance measurement","Validation"
"https://github.com/fairlearn/fairlearn","829292465","issue_comment","https://github.com/Trusted-AI/AIF360/issues/758#issuecomment-829292465","Yes, we should definitely include this!

Perhaps adding a separate `roc_curve_grouped(y_true, y_pred, sensitive_features)` that returns the fpr, fnr, and thresholds for each group would be nice as well. That makes it easier to plot specific thresholds for each group on top of the curves, which can be quite insightful imo.","2","0.3812575090108132","Performance measurement","Validation"
"https://github.com/fairlearn/fairlearn","829332175","issue_comment","https://github.com/Trusted-AI/AIF360/issues/758#issuecomment-829332175","Thanks! There is just one piece I am missing

Given I have a dataframe (df) with labeled sensitive features, I want to create dataframes that are a subset of this, using sensitive features. Something like: 

**create_sensitive_dfs(df, sensitive_features)**
Input: dataframe, list of sensitive features 
Output: Smaller dataframes (i.e: df_women, df_men, df_nans)

Example: sensitive_features = [age, race] 
Output: df_kid_SA, df_adult_MENA, etc.. 

This would then allow me to use your script above ","9","0.3998894049988939","Feature engineering methodology","Design"
"https://github.com/fairlearn/fairlearn","829355358","issue_comment","https://github.com/Trusted-AI/AIF360/issues/758#issuecomment-829355358","Resolved: Combining all of the above @kstohr here's my workaround solution for cartesian product to create the smaller data frames - though how it would be embedded in the MetricFrame or where this same product happens is something I could not find.

Where sensitive features is a list of the sensitive feature columns (defined elsewhere)

```
def sensitive_pdfs(pdf, sensitive_features): 
    distinct_features_vals = []
    df_dict = {}
    for feature in sensitive_features:
        distinct_vals = set(df[feature].to_list())
        distinct_features_vals.append(distinct_vals)
    distinct_features_combos = list(product(*distinct_features_vals))
    for features in distinct_features_combos:
        query = "" & "".join([f""{sensitive_features[i]} == '{features[i]}'"" for i in range(len(sensitive_features))])
        filtered_df = df.query(query)
        key = tuple(sorted(features))
        df_dict[key] = filtered_df 
    return df_dict


def plot_auc(df, y_pred, y_true, name, ax=None):
  y_pred_r = np.asarray(df[y_pred]).astype(float)
  y_true_r = np.asarray(df[y_true]).astype(int)

  fpr, tpr, _ = roc_curve(y_true_r, y_pred_r, pos_label=1)
  roc_auc = auc(fpr, tpr)
  display = RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc, estimator_name=name)
  display.plot(ax=ax)
  return display


sensitive_pdfs_dict=sensitive_pdfs(pdf, sensitive_features)
plt.figure(figsize=(8, 6))
ax = plt.gca()
for grouping in sensitive_pdfs_dict: 
    group_plot= plot_auc(sensitive_pdfs_dict[grouping],""matching_score"",""golden_label"", grouping, ax)
```
    ","2","0.8422296358076183","Performance measurement","Validation"
"https://github.com/fairlearn/fairlearn","829448464","issue_comment","https://github.com/Trusted-AI/AIF360/issues/758#issuecomment-829448464","Hi all! 

Here is some code that splits pdfs based on a list of columns (the columns must only have boolean values)

Example use:
`sensitive_pdfs(df, [('color', ['blue','red']), ('city', ['ny','nj'])], 'pdf')`

Example in colab notebook:
[Notebook](https://colab.research.google.com/drive/1gwClFsgLqNkexFiSXeKwCBu4Iu2awRTn?usp=sharing)

```
def sensitive_pdfs(pdf, sensitive_features, title): 
  # Feature we are splitting on
  sensitive_feature, values = sensitive_features[0]

  # Safety check: guarantee that feature is boolean
  if pdf[sensitive_feature].isin(values).all():
    # Remove feature we are about to split by
    sensitive_features.pop(0)
    
    # For title tracking
    v0 = values[0]
    v1 = values[1]

    # Apply condition to split pdf 
    tmp1 = pdf[pdf[sensitive_feature] == values[0]]
    tmp2 = pdf[pdf[sensitive_feature] == values[1]]

    # Update titles for end pdfs
    title1 =  title + '_' + str(v0) + '_'
    title2 =  title + '_' + str(v1) + '_'

    # Base case check - no more features to split by
    if len(sensitive_features) == 0:
      return [(title + str(v0), tmp1)] + [(title + str(v1), tmp2)]
    else:
      # Recurse, continuing to split by sensitive features
      return sensitive_pdfs(tmp1, sensitive_features.copy(), title + '_' + str(v0) + '_') + \
             sensitive_pdfs(tmp2, sensitive_features.copy(), title + '_' + str(v1) + '_')
  else:
    raise ValueError('Oops - never should happen')
","9","0.7095337251904373","Feature engineering methodology","Design"
"https://github.com/fairlearn/fairlearn","829862619","issue_comment","https://github.com/Trusted-AI/AIF360/issues/758#issuecomment-829862619","Reopening since we need a nice method in fairlearn to do this. ","6","0.2808623144193613","API expansion","Development"
"https://github.com/fairlearn/fairlearn","831271204","issue_comment","https://github.com/Trusted-AI/AIF360/issues/758#issuecomment-831271204","I agree that this is a super useful addition. I also like @hildeweerts suggestion to consider extending both sklearn's [roc_curve](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html#sklearn.metrics.roc_curve) and [plot_roc_curve](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.plot_roc_curve.html#sklearn.metrics.plot_roc_curve).

Until we find somebody willing to implement this, can we tease out the API we would like to see? We would use the same pattern for other curves that we discussed and come up in fairness literature like `calibration_curve` and `cdf_curve`.

For plotting, my preference is to have API of the following form:
* `plot_roc_curves(*, y_true, y_score, sensitive_features, **kwargs) -> matplotlib.axes.Axes`

For just getting `fpr`, `tpr`, `thresholds` (but no plotting), we could have
* `roc_curves(*, y_true, y_score, sensitive_features, **kwargs)`
  * I am less sure about the return format here. But one idea would be to return three dictionaries `fpr`, `tpr`, `thresholds` that would be indexed by the sensitive feature values (or a tuple of sensitive feature values if we have multiple sensitive features).
  * Another idea would be to do something similar to threshold optimizer, where we return `tpr` and `threshold` (a generalized notion of threshold) as a function of `fpr`, where the range of `fpr` values would be the same for all sensitive features. This I think is actually a much more useful output format for any further processing.
","2","0.4860046440681257","Performance measurement","Validation"
"https://github.com/fairlearn/fairlearn","831289188","issue_comment","https://github.com/Trusted-AI/AIF360/issues/758#issuecomment-831289188","I agree, let's get the conversation started (which may be relevant to the other plotting functionality as well? @romanlutz )

* `plot_roc_curve` 
   * If we want to be consistent with sklearn we should use something like: ```plot_roc_curve(Estimator, X, y, sensitive_features,  *, sample_weight=None, drop_intermediate=True, response_method='auto', name=None, ax=None, pos_label=None, **kwargs)``` returning a `RocCurveDisplay` (or similar). 
   * I would also be okay with using `y_score` directly in the API, as it allows for a bit more flexibility wrt the estimator, although that is at the risk of people accidentally using `y_pred`.
* `roc_curve(y_true, y_score, sensitive_features, *, pos_label=None, sample_weight=None, drop_intermediate=True)`
   * I don't see a need for **kwargs here 
   * I think returning a dictionary makes sense! Could you elaborate a bit on the other idea? I don't fully understand yet what that would look like.

Is there a specific reason you want to put `*` at the beginning? I'm always a bit wary about that because it's too easy to mess up the order of `y_true` and `y_score` or the difference between `y_score` and `y_pred`.","2","0.4187110092266283","Performance measurement","Validation"
"https://github.com/fairlearn/fairlearn","831412475","issue_comment","https://github.com/Trusted-AI/AIF360/issues/758#issuecomment-831412475","Re. compatibility with sklearn, I noticed that they use `estimator, X, y` signature, but it seems very limiting (or forcing some workarounds with pass-through estimators). So, in this case, I was proposing to intentionally deviate from the sklearn pattern (that's why I would also use a different name, i.e., `plot_roc_curves`). I would make all of the arguments keyword-only to prevent any confusion with the sklearn format.

Re. `*` at the beginning: I mostly want to ensure keyword-only arguments, because some common libraries have the opposite convention from sklearn around the order of `y_true` vs `y_pred`. I wouldn't mind allowing both `y_pred` and `y_score` (as interchangeable), so the signature could be something like:
* `roc_curve(*, y_true, y_score=None, y_pred=None, sensitive_features, ... )`
","2","0.4716147679554996","Performance measurement","Validation"
"https://github.com/fairlearn/fairlearn","842442857","issue_comment","https://github.com/Trusted-AI/AIF360/issues/758#issuecomment-842442857","@romanlutz Ok, I'll pick this up. Not sure how long it will take me, but will dive in and work through it. ","20","0.7574348132487667","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","842447458","issue_comment","https://github.com/Trusted-AI/AIF360/issues/758#issuecomment-842447458","Sounds great! Let us know if you have questions! We're happy to help! I'll also keep checking Discord during the sprint, of course.","20","0.7872032426172557","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","842639908","issue_comment","https://github.com/Trusted-AI/AIF360/issues/758#issuecomment-842639908","Quick note: There are contexts in which no negative samples in the ground truth (""No negative samples in y_true""), in which case an ROC curve can not be created","12","0.2894176136363637","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","842643743","issue_comment","https://github.com/Trusted-AI/AIF360/issues/758#issuecomment-842643743","@michaelamoako ok, so you want me to detect those cases and handle them with a try/except? Any other cases that would arise that we should handle? ","15","0.7054622659718205","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","842682632","issue_comment","https://github.com/Trusted-AI/AIF360/issues/758#issuecomment-842682632","@romanlutz @michaelamoako  New to this repo.. Couple quick q's before I build out this class/function. 

- I see you have a dir `metrics` and a dir `postprocessing` which has some plotting functionality in it already.  Where do you want this feature to live? `metrics` or `postprocessing` ? 

- Also this function already exists:  _calculate_tradeoff_points which --at first glance-- does appear to compute ROC points.  Do we need to just extend this to process the data grouped by the sensitive features? 

- If we're plotting the ROC curve for each subgroup, that's just a line plot with traces. It sounds like Fairlearn is moving away from having a dashboard and instead enabling plotting as a set of utilities. Is that right? In which case do we want to build a base `_line_plot` and `_grouped_line_plot` class in Matplotlib or maybe plot.ly (interactive) which would be re-usable? Then inherit that class into other metrics classes that might benefit from plotting?  

- Related: I see this plotting functionality here: https://github.com/fairlearn/fairlearn/blob/main/fairlearn/postprocessing/_plotting.py ... do we want to add to or extend this module?  i.e. build out plotting utilities in the same module? ","30","0.2997419741974199","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","843424241","issue_comment","https://github.com/Trusted-AI/AIF360/issues/758#issuecomment-843424241","@kstohr The ROC curves would be more general than the `postprocessing` functionality in that they could be generated for all models (not just the ones from `postprocessing`). I would suggest creating a new file for it under the `metrics` folder.

The postprocessing method has a lot of custom code that we can switch out at a later point. For now, I would completely ignore it since it has somewhat more comprehensive requirements. Same for the plotting capabilities of that module, they're very custom to the mitigation technique used there. Let's ignore it for now.

You're right about the dashboard moving away (in fact #766 deletes it), so this should just be its own standlone utility. We're all in favor of reusing things that already exist. Perhaps some of the code snippets from this thread are useful to have ROC plots with multiple lines (one per group)? I think matplotlib supports this already by just passing in the same axes to subsequent plotting commands.","15","0.3523964619855031","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","843462491","issue_comment","https://github.com/Trusted-AI/AIF360/issues/758#issuecomment-843462491","@romanlutz Ok great. So, I'll build a module under metrics: `metrics/_roc.py` If we want to refactor later to make things more re-usable, we can.

Yep. Saw the code snippets and incorporating as needed. 

In terms of plotting,  you can use plt.subplots() to add traces to the figure, but sklearn already has a class [RocCurveDisplay](https://github.com/scikit-learn/scikit-learn/blob/15a949460/sklearn/metrics/_plot/roc_curve.py#L135) and it looks like you can pass an existing ax to the function which *should* enable us to add traces to the figure for the each of the sensitive values. ","30","0.3715380317663019","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","843485743","issue_comment","https://github.com/Trusted-AI/AIF360/issues/758#issuecomment-843485743","Yes! I was hoping we could reuse what `sklearn` already has. In many ways, a lot of what Fairlearn does in assessment builds on `sklearn` by disaggregating metrics so this is yet another way we're doing so. Thanks @kstohr !","15","0.5346889952153109","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","843515291","issue_comment","https://github.com/Trusted-AI/AIF360/issues/758#issuecomment-843515291","Another very closely related but also valuable curve: Threshold vs. [Metric]. For example, to plot Threshold vs. [Recall]:

_Assume y_true/y_pred defined elsewhere_

```
  _, recall, thresholds = precision_recall_curve(y_true_r, y_pred_r)
  plt.plot(thresholds, recall[:-1], label=name) 
  plt.xlabel(""Threshold"")
  plt.ylabel(""Recall"")
```

the ROC curve function similarly also returns threshold, so there is potential to plot that on the X-axis there too ","12","0.576946943841142","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","843545729","issue_comment","https://github.com/Trusted-AI/AIF360/issues/758#issuecomment-843545729","@michaelamoako Yep. Will keep precision_recall_curve in mind. Assuming you'd also do that for each sensitive value (i.e. by group.) ","12","0.3425028381800714","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","843558836","issue_comment","https://github.com/Trusted-AI/AIF360/issues/758#issuecomment-843558836","@kstohr feel free to stick with one thing per PR. Smaller PRs are easier to review and typically get merged significantly faster 😄 @michaelamoako 's suggestion can be another PR unless it's just a few extra lines. Keep in mind that adding ROC curves should include at least a small unit test and a short user guide section, so this is already quite a bit.","20","0.6165099268547544","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","843565309","issue_comment","https://github.com/Trusted-AI/AIF360/issues/758#issuecomment-843565309","Agreed with @romanlutz - and yes (per group)","15","0.4777303233679074","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","843595622","issue_comment","https://github.com/Trusted-AI/AIF360/issues/758#issuecomment-843595622","@romanlutz @michaelamoako Roger that. I think Michael was just pointing out that the code should ideally be easy to adapt for this other purpose. At least that's how I undrestood it. ","7","0.4188995215311006","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","846071165","issue_comment","https://github.com/Trusted-AI/AIF360/issues/758#issuecomment-846071165","@michaelamoako @romanlutz Yokee, the ref code snippets are up and running. Quick and dirty plot: 

<img width=""589"" alt=""Screen Shot 2021-05-21 at 9 03 35 AM"" src=""https://user-images.githubusercontent.com/8218231/119166934-16d37080-ba14-11eb-865f-7f655c093920.png"">

I am thinking of testing out using the existing MetricFrame to do the data splitting. The benefit of using MetricFrame is that this would be a standard way of handling splitting the data on sensitive values. You get the 'overall' curve as well as the curves for the sensitive values, and any methods we add to the MetricFrame class would be inherited by the ROC Curve class. The downside is that the `_roc_utils.py` would have a dependency on MetricFrame. Thoughts before I head down this path? 
","2","0.4289567326216542","Performance measurement","Validation"
"https://github.com/fairlearn/fairlearn","846085730","issue_comment","https://github.com/Trusted-AI/AIF360/issues/758#issuecomment-846085730","I don't think that a dependency on `MetricFrame` would be a bad thing. Or do you mean to inherit from it?","32","0.262861169837914","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","846098147","issue_comment","https://github.com/Trusted-AI/AIF360/issues/758#issuecomment-846098147","Hoping just to import it. I'll dig into it. However, I need to pass y_test,
y_pred, y_pred_proba. In addition, we need not just the sensitive values
for multiple features but the cartesian product of the sensitive value
features. So, it may not work.

On Fri, May 21, 2021 at 9:30 AM Richard Edgar ***@***.***>
wrote:

> I don't think that a dependency on MetricFrame would be a bad thing. Or
> do you mean to inherit from it?
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/fairlearn/fairlearn/issues/758#issuecomment-846085730>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AB6WM57VBVG4TTCA73RZC63TO2DBDANCNFSM43X6STVQ>
> .
>


-- 
Kas Stohr
+1 646 554 7671
","7","0.4079584197601504","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","846100436","issue_comment","https://github.com/Trusted-AI/AIF360/issues/758#issuecomment-846100436","@kstohr I just tagged you in a comment above related to Cartesian product -though where this process is happening in the MetricFrame is not clear to me","2","0.3610047846889953","Performance measurement","Validation"
"https://github.com/fairlearn/fairlearn","846104901","issue_comment","https://github.com/Trusted-AI/AIF360/issues/758#issuecomment-846104901","@kstohr : my suggestions:
* you could just start with `plot_roc_curves`
* it would be good to figure out the invocation format on this issue (either at the same time as PR or before PR)
* I wouldn't worry about integrating things internally with MetricFrame yet... I expect that we'll need to re-implement and re-factor MetricFrame because it's 10x slower than an equivalent pandas code (I'll create an issue on that hopefully next week), but if it simplifies your life (rather than makes it more complicated), go for it
* One idea re. MetricFrame would be to just use the following ""metric"" (maybe this is what you had in mind?):
```python
def metric(y_true, y_pred):
    return (y_true, y_pred)
mf = MetricFrame(metric, y_true, y_pred, sensitive_features=sensitive_features)
mf.by_group  # this will contain the split of the data y_true and y_pred
             # and consider all combinations if multiple sensitive features are provided
```","2","0.5895805344081204","Performance measurement","Validation"
"https://github.com/fairlearn/fairlearn","846113116","issue_comment","https://github.com/Trusted-AI/AIF360/issues/758#issuecomment-846113116","@MiroDudik Yep that's exactly what I had in mind, but I have to pass a third parameter...looks like that's permitted in the docs and it will split on it and extract the groups. Not sure if it will handle the cartesian split. Still getting up to speed on this codebase. ","30","0.3593550180706141","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","846115915","issue_comment","https://github.com/Trusted-AI/AIF360/issues/758#issuecomment-846115915","Cartesian split is already handled I believe, you just provide two columns of sensitive features and it'll consider all combinations. See [here](https://fairlearn.org/v0.6.2/auto_examples/plot_new_metrics.html#intersections-of-features).

MetricFrame also supports additional column parameters, e.g.:
```python
def metric(y_true, y_pred, y_pred_proba):
    return (y_true, y_pred, y_pred_proba)
mf = MetricFrame(metric, y_true, y_pred, sensitive_features=sensitive_features, 
                 sample_params = {'y_pred_proba': y_pred_proba})
mf.by_group  # this will contain the split of the data y_true, y_pred, and y_pred_proba
             # and consider all combinations if multiple sensitive features are provided
```","2","0.9475155484980814","Performance measurement","Validation"
"https://github.com/fairlearn/fairlearn","846125679","issue_comment","https://github.com/Trusted-AI/AIF360/issues/758#issuecomment-846125679","This would be tied to the NaN issue in MetricFrame right @MiroDudik ? ","2","0.3306693306693306","Performance measurement","Validation"
"https://github.com/fairlearn/fairlearn","846134398","issue_comment","https://github.com/Trusted-AI/AIF360/issues/758#issuecomment-846134398","@michaelamoako : I think you're thinking about #800? We don't have to worry about that here, because we're not using mf.difference() or other aggregates (so the above code shouldn't raise any errors).","15","0.2130987292277615","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","846186532","issue_comment","https://github.com/Trusted-AI/AIF360/issues/758#issuecomment-846186532","@miro Well, there you go. `mf.by_group` should work then. Tks.

On Fri, May 21, 2021 at 10:19 AM MiroDudik ***@***.***> wrote:

> Cartesian split is already handled I believe, you just provide two columns
> of sensitive features and it'll consider all combinations. See here
> <https://fairlearn.org/v0.6.2/auto_examples/plot_new_metrics.html#intersections-of-features>
> .
>
> MetricFrame also supports additional column parameters, e.g.:
>
> def metric(y_true, y_pred, y_pred_proba):
>     return (y_true, y_pred, y_pred_proba)mf = MetricFrame(metric, y_true, y_pred, sensitive_features=sensitive_features,
>                  sample_params = {'y_pred_proba': y_pred_proba})mf.by_group  # this will contain the split of the data y_true, y_pred, and y_pred_proba
>              # and consider all combinations if multiple sensitive features are provided
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/fairlearn/fairlearn/issues/758#issuecomment-846115915>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AB6WM5ZSJGK7HT336WLYCH3TO2I2FANCNFSM43X6STVQ>
> .
>


-- 
Kas Stohr
+1 646 554 7671
","2","0.7640919445019674","Performance measurement","Validation"
"https://github.com/fairlearn/fairlearn","854222027","issue_comment","https://github.com/Trusted-AI/AIF360/issues/758#issuecomment-854222027","y'all this is coming along:

I think the best course of action is to build on MetricFrame as it will create a common language across the Fairlearn API. So you can split by group and use the series index to plot a subset of the sensitive feature subgroups (remember is the cartesian product, so selecting a smaller set to plot will be important.)  

TODO: 
- Determine how best to instantiate plot figure, axis (right now there's a bit of redundant code; might instantiate with the class to have a default figure, ax)
- Separate functions to add 'Overall"" and ""Baseline"" traces to enable users to toggle these traces on/off 
- logging, error handling
- Add additional AUC score and related metric functionality by sensitive feature group... if we're generating the AUC score anyway, why not return it 
- unit tests
Overall thoughts on this basic approach. 

```
class roc_auc: 
    """"""
    Provides utilties for generating auc scores, roc curves
    and plotting roc_curves grouped by sensitive features. 
    
    Parameters
    ----------
    y_true : List, pandas.Series, numpy.ndarray, pandas.DataFrame
        The ground-truth labels for classification. (i.e. results of clf.predict())
        
    y_score : List, pandas.Series, numpy.ndarray, pandas.DataFrame
        
    sensitive_features : List, pandas.Series, dict of 1d arrays, numpy.ndarray, pandas.DataFrame
        The sensitive features which should be used to create the subgroups.
        At least one sensitive feature must be provided.
        All names (whether on pandas objects or dictionary keys) must be strings.
        We also forbid DataFrames with column names of ``None``.
        For cases where no names are provided we generate names ``sensitive_feature_[n]``.
    """"""
    def __init__(self,
                 y_true, 
                 y_score, 
                 sensitive_features): 
        """"""
        Initiate class with required parameters to generate metric. 
        TODO: validate input
        """"""
        self.y_true = y_true
        self.y_score = y_score 
        self.sensitive_features = sensitive_features
        self.ns_probs = [0 for n in range(len(self.y_true))]
        
    @staticmethod
    def splitter(y_true, y_pred): 
        """"""
        Placeholder function to enable splitting of dataframes using 
        existing MetricFrame class. 
        """"""
        return (y_true, y_pred)
    
    @staticmethod
    def plot_auc(y_true, y_score, name, ax=None, pos_label=1, **kwargs):
        """"""
        Plot auc curves. 
        """"""
        
        #Establish plot figure if not already generated
        if not ax: 
            # Establish plot figure
            plt.figure(figsize=(8, 6))
            ax = plt.gca()
        else: 
            self.ax = ax
            
        fpr, tpr, _ = roc_curve(y_true, y_score, pos_label=pos_label, **kwargs)
        roc_auc = auc(fpr, tpr)
        display = RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc, estimator_name=name)
        display.plot(ax=ax)
        return display
    
    def split_by_group(self): 
        """"""
        Splits data by sensitive feature subgroups. 
        See: Fairlearn.MetricFrame for more detail. 
        
        Note: MetricFrame requires y_pred (clf.predict). However, ROC curves and AUC scores 
        are generated using y_score (clf.predict_proba). 
        Method substitutes y_score (type:float) for y_pred (type:int) to conform to MetricFrame 
        required params. 
        (MetricFrame support regression and therefore allows values of type float to be 
        passed as y_pred.) 
        Admittedly this is a little weird. Alternately, you could pass `sample_params` to MetricFrame, 
        but not sure that's any cleaner. 
        """"""
        mf = MetricFrame(
            metric = self.splitter, 
            y_true = self.y_true, 
            y_pred = self.y_score, 
            sensitive_features = self.sensitive_features,
            #sample_params = {'y_score': y_score}
                        )
        self.sensitive_series = mf.by_group
        return self.sensitive_series
    
    def plot_roc_groups(self, 
                        sensitive_index=None, 
                        ax=None): 
        
        
        #Establish plot figure if not already generated
        if not ax: 
            # Establish plot figure
            plt.figure(figsize=(8, 6))
            ax = plt.gca()
        else: 
            self.ax = ax
        
        # Establish which combinations of sensitive features to plot
        if not sensitive_index: 
            sensitive_index = self.sensitive_series.index

        # Plot baseline - 'no skill'
        # i.e. performance of classifier is equivalent to random selection
        ns_auc = roc_auc_score(self.y_true, self.ns_probs)
        ns_fpr, ns_tpr, _ = roc_curve(self.y_true, self.ns_probs)
        ax.plot(ns_fpr, ns_tpr, linestyle='--', label='Baseline (AUC = 0.50)')

        # Plot overall model performance
        overall_auc = roc_auc_score(self.y_true, self.y_score)
        overall_fpr, overall_tpr, _ = roc_curve(self.y_true, self.y_score)
        ax.plot(overall_fpr, overall_tpr, label=f'Overall (AUC = {round(overall_auc, 2)})')
        
        # Plot ROC Curves by group
        for name in sensitive_index: 
            grp = self.sensitive_series[name]
            grp_y_true = self.sensitive_series[name][0]
            grp_y_score = self.sensitive_series[name][1]
            group_plot = plot_auc(
                y_true=grp_y_true, 
                y_score=grp_y_score, 
                name=name, ax=ax)
        return ax
```","2","0.756678977625679","Performance measurement","Validation"
"https://github.com/fairlearn/fairlearn","854292998","issue_comment","https://github.com/Trusted-AI/AIF360/issues/758#issuecomment-854292998","Tagging @alexquach who has started work on #235 . Both of these are generically looking at 'plots for MetricFrame' so we should try to have a common API pattern (even if the details will be different)","15","0.7566188197767147","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","854299345","issue_comment","https://github.com/Trusted-AI/AIF360/issues/758#issuecomment-854299345","Totally agree. Was going to write a generic method to establish the plot
trace. @alexquach if you have something going, please share.

On Thu, Jun 3, 2021 at 6:38 PM Richard Edgar ***@***.***>
wrote:

> Tagging @alexquach <https://github.com/alexquach> who has started work on
> #235 <https://github.com/fairlearn/fairlearn/issues/235> . Both of these
> are generically looking at 'plots for MetricFrame' so we should try to have
> a common API pattern (even if the details will be different)
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/fairlearn/fairlearn/issues/758#issuecomment-854292998>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AB6WM57TNKUGECFJWTRKWG3TRAVCHANCNFSM43X6STVQ>
> .
>
","15","0.5112472851380703","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","854839226","issue_comment","https://github.com/Trusted-AI/AIF360/issues/758#issuecomment-854839226","@alexquach only just started this week, so there's still some playing around with 'how to plot this' before we get too far into the API design. I do like your approach of having a separate class, since there's no need to deal with `MetricFrame` internals. For the error bar case, it's more likely to be 'bring the `MetricFrame` precomputed and tell us the mapping of metrics to error bars' though.

We can meet to talk, if you think that would be helpful (and we can figure out timezones).","15","0.7224744791163494","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","854862059","issue_comment","https://github.com/Trusted-AI/AIF360/issues/758#issuecomment-854862059","What I was thinking was to build a standard plot that is “styled” the
Fairlearn way, whatever that is...and then call that and instantiate it if
the user does not pass their own fig, ax. I was going to do it as part of
the class.

If it’s in the class and then somebody looks at the code, It should be
clear to them how they can change the defaults to suit their needs by
either passing their own figure or updating parameters of the class
instance. And, for those not familiar with plotting or just needing a
convenience method, the defaults should meet their basic needs.

In the ROC case, we’re taking advantage of scikit to provide default axis
labels, etc. So even less to configure...

Thoughts...

On Fri, Jun 4, 2021 at 9:05 AM Richard Edgar ***@***.***>
wrote:

> @alexquach <https://github.com/alexquach> only just started this week, so
> there's still some playing around with 'how to plot this' before we get too
> far into the API design. I do like your approach of having a separate
> class, since there's no need to deal with MetricFrame internals. For the
> error bar case, it's more likely to be 'bring the MetricFrame precomputed
> and tell us the mapping of metrics to error bars' though.
>
> We can meet to talk, if you think that would be helpful (and we can figure
> out timezones).
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/fairlearn/fairlearn/issues/758#issuecomment-854839226>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AB6WM5ZZ5YDHAVB73MOSB23TRD2TRANCNFSM43X6STVQ>
> .
>
","15","0.5387712275179345","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","854890008","issue_comment","https://github.com/Trusted-AI/AIF360/issues/758#issuecomment-854890008","Styling shouldn't be done in this code I think. That's why people can provide their own `ax`. That's something @adrinjalali pointed out in the past with other plots, too.

Although, now I'm not sure you actually meant ""styling"" (as in https://matplotlib.org/stable/gallery/style_sheets/style_sheets_reference.html) or just axis labels, legend, etc.. Regardless, it shouldn't be preconfigured and people should be allowed to configure it however they like. I don't think that will affect the implementation in any way, other than the fact that we shouldn't set any options internally (e.g., figure size, shouldn't be set in our code).","15","0.6055775609441921","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","854936577","issue_comment","https://github.com/Trusted-AI/AIF360/issues/758#issuecomment-854936577","@romanlutz Here's how scikit learn handles it. 

[RocCurveDisplay](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.RocCurveDisplay.html#sklearn.metrics.RocCurveDisplay)

What they have done is offered the full convenience of a labeled plot. However, those who want to provide their own fig, ax, or overwrite the defaults can. They use the default figure size, font, etc. Notice how they return the figure and the axis as 'self' which enables the user to configure the plot further if they wish. 

I am following the above approach.  In fact, in the case of Roc Curves, I am actually using RocCurveDisplay.  

Because plotting is specific to the metric you are plotting, I think providing labeled plotting functions that add convenience to the relevant modules makes more sense than building a generic shared plotting utility. Basically adapting common metrics to return data by subgroup and the associated plots to handle plotting sensitive features by subgroup.","15","0.4433119361554477","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","864019761","issue_comment","https://github.com/Trusted-AI/AIF360/issues/758#issuecomment-864019761","Hi all. I pushed a draft PR for this last night.  Take a look at the basic approach and give me feedback when you have a moment.

https://github.com/fairlearn/fairlearn/pull/869/files

 It's not passin CI/CD checks yet, still needs. 
 
- more functional testing; 
- unit tests 
- improved example documentation 
etc. 

","32","0.5167629022872744","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","881224903","issue_comment","https://github.com/Trusted-AI/AIF360/issues/757#issuecomment-881224903","FYI I tried setting this up tonight. The YAML pipeline definitions were easy to link to the new ADO organization. However, it only auto-queues the pipelines on one org, so that's still currently the ""old"" organization until I can make sure everything works properly in the new one. So obviously I kicked off a run of one of our pipelines to check whether it does indeed work properly. Surprisingly, I encountered the following issue:
```
##[error]No hosted parallelism has been purchased or granted. To request a free parallelism grant, please fill out the following form https://aka.ms/azpipelines-parallelism-request
```
I've filled out that form and it will take up to 3 business days for a response. Stay tuned!","31","0.293066815190709","Testing","Maintenance"
"https://github.com/fairlearn/fairlearn","883199733","issue_comment","https://github.com/Trusted-AI/AIF360/issues/757#issuecomment-883199733","As of right now I've migrated all pipelines and disabled all but the release pipeline in the old organization. The release pipeline is a bit special due to the required service connections to PyPI and test-PyPI. I've created those for now, but we have to run an actual release to confirm that they work.

If any issues occur please flag them here @fairlearn/fairlearn-maintainers ","32","0.7107847872991337","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","884482376","issue_comment","https://github.com/Trusted-AI/AIF360/issues/757#issuecomment-884482376","I found two more issues:
- The release didn't ask for approval to progress to uploading to PyPI (test). What do we have to set to make it wait for that?
- It failed on that upload https://dev.azure.com/fairlearn/Fairlearn/_build/results?buildId=53&view=logs&j=4ae3a96a-b4ee-5b04-2d89-6bbd7e6eea8b&t=fdbebb0b-6c1c-59d8-48cb-529d65e04071 Apparently some twine endpoint needs to be set up. @riedgar-ms  any idea?","13","0.5774044795783929","Artifact generation and benchmarking","Deployment"
"https://github.com/fairlearn/fairlearn","884818777","issue_comment","https://github.com/Trusted-AI/AIF360/issues/757#issuecomment-884818777","The approval is controlled by the 'environment' portion of the job:
https://github.com/fairlearn/fairlearn/blob/a68e74ec90ce99f00446b0b8fe53203f61f640b3/devops/pypi-release-new.yml#L169
which point at:
https://dev.azure.com/responsibleai/fairlearn/_environments
Somewhere in that, it specifies the 'maintainers' security group as needing to sign off.","4","0.2679332386363637","Installation and shell commands","Deployment"
"https://github.com/fairlearn/fairlearn","885349555","issue_comment","https://github.com/Trusted-AI/AIF360/issues/757#issuecomment-885349555","@riedgar-ms Excellent! I found it and set it to the ""Fairlearn team"" which includes all @fairlearn/fairlearn-maintainers 

![image](https://user-images.githubusercontent.com/10245648/126729162-4797e84c-1076-416c-8541-964ad43768ac.png)


However, I still don't know how to fix the twine upload. Any idea? Is there some additional setup that needs to happen? ","13","0.3538707386363636","Artifact generation and benchmarking","Deployment"
"https://github.com/fairlearn/fairlearn","885358397","issue_comment","https://github.com/Trusted-AI/AIF360/issues/757#issuecomment-885358397","What's the problem exactly? That environment setup should mean approvals are required.","21","0.296969696969697","Installation and shell commands","Deployment"
"https://github.com/fairlearn/fairlearn","885358860","issue_comment","https://github.com/Trusted-AI/AIF360/issues/757#issuecomment-885358860","The issue that happened the first time was (copying from above):
- It failed on that upload https://dev.azure.com/fairlearn/Fairlearn/_build/results?buildId=53&view=logs&j=4ae3a96a-b4ee-5b04-2d89-6bbd7e6eea8b&t=fdbebb0b-6c1c-59d8-48cb-529d65e04071 Apparently some twine endpoint needs to be set up.

I'm rerunning right now since the approval is set up, but something tells me the approval has nothing to do with this part.","13","0.541006394217403","Artifact generation and benchmarking","Deployment"
"https://github.com/fairlearn/fairlearn","885361048","issue_comment","https://github.com/Trusted-AI/AIF360/issues/757#issuecomment-885361048","Ahh sorry. Thought it was still not prompting for approval. That looks like the creds may be missing?","13","0.5868921775898519","Artifact generation and benchmarking","Deployment"
"https://github.com/fairlearn/fairlearn","885455436","issue_comment","https://github.com/Trusted-AI/AIF360/issues/757#issuecomment-885455436","https://dev.azure.com/fairlearn/Fairlearn/_build/results?buildId=75&view=logs&j=4ae3a96a-b4ee-5b04-2d89-6bbd7e6eea8b&t=fdbebb0b-6c1c-59d8-48cb-529d65e04071

Where do I set the creds? I created the service connection https://dev.azure.com/fairlearn/Fairlearn/_settings/adminservices and uploaded the auth tokens from PyPI and PyPI Test there. I thought that would do it.","13","0.676136363636364","Artifact generation and benchmarking","Deployment"
"https://github.com/fairlearn/fairlearn","885564257","issue_comment","https://github.com/Trusted-AI/AIF360/issues/757#issuecomment-885564257","Hmmm.... that sounds right. It's been a while since I set this up :-/","20","0.4871995820271684","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","886170475","issue_comment","https://github.com/Trusted-AI/AIF360/issues/757#issuecomment-886170475","Still no progress on this, but it occurred to me that we have some references to the old org in our docs. Searching for `responsibleai/fairlearn` yielded 4 results, so those will need to be removed as part of this issue as well.","14","0.4031843862352336","Documentation","Development"
"https://github.com/fairlearn/fairlearn","825063679","issue_comment","https://github.com/Trusted-AI/AIF360/issues/756#issuecomment-825063679","Resolving this issue would also enable #676 .","31","0.1711076280041798","Testing","Maintenance"
"https://github.com/fairlearn/fairlearn","830460920","issue_comment","https://github.com/Trusted-AI/AIF360/issues/756#issuecomment-830460920","We definitely should be figuring out how to offer metrics for other scenarios. However, I'm a little concerned about a few points. Doing (1) is going to break all existing code. Now, while we hardly have the usage of `sklearn` (say), that's still not a good thing to do. And I think that doing (2) sort of does the opposite - once you've accepted `**kwargs`, you can't add any more arguments of your own, since you might clash with something a user has.

For (2), we could add a `shared_sample_params` _dictionary_ and explode it when we invoke the metric functions.

For the rest, would it be wiser to say ""`MetricFrame` is for tabular data; we also offer X, Y, Z .....""","15","0.6942572195736754","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","831274785","issue_comment","https://github.com/Trusted-AI/AIF360/issues/756#issuecomment-831274785","If we want to implement (1), we would not break everything right away. We would start off by issuing deprecation warnings. Would that work, @riedgar-ms @romanlutz @hildeweerts @adrinjalali ? 

I just think that our current API is really limiting us, so I'd rather fix this sooner rather than commit to the future of docs that describe all kinds of workarounds.

I want to hold off discussing (2), because I acknowledge that there are a variety of considerations... I think that we have several options there. It would be a non-breaking change, so there's less urgency than with (1), which, if we decide to go that route, would be nice to accomplish before SciPy tutorial (at least the version where we have a new format, and issue warnings if somebody is using the old format). ","20","0.3142446361424463","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","831278780","issue_comment","https://github.com/Trusted-AI/AIF360/issues/756#issuecomment-831278780","Is there a way of detecting whether an argument was passed by position rather than by keyword?","2","0.484566596194503","Performance measurement","Validation"
"https://github.com/fairlearn/fairlearn","831302922","issue_comment","https://github.com/Trusted-AI/AIF360/issues/756#issuecomment-831302922","I am generally not opposed to Step 1 (although I don't really see the need for changing `metric` into `metrics`).

When we get to Step 2 things get a bit tricky IMO. I am afraid that trying to fit all different kinds of learning tasks into one `MetricFrame` will become extremely confusing for novice users.  An alternative might be to leave `MetricFrame` as is and create a new data structure for other types of tasks (e.g., differentiate between things like `SupervisedMetricFrame`, `UnsupervisedMetricFrame`, `ReinforcementMetricFrame`).","15","0.5450709627924819","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","831344553","issue_comment","https://github.com/Trusted-AI/AIF360/issues/756#issuecomment-831344553","Re. `metric` -> `metrics`--the rationale is that we have `sensitive_features` and `control_features`. I've also noticed that in the tutorials I keep writing things like:
```python
# Define metrics
metrics = {
    'accuracy': skm.accuracy_score,
    'precision': skm.precision_score,
    'recall': skm.recall_score,
    'false_positive_rate': flm.false_positive_rate,
}

mf = MetricFrame(metrics, ...)
```
So it seems that `metrics` is the more appropriate name--it makes no sense to change the name in isolation, but if we decide to implement (1), then it would be an opportunity to change this as well.

@riedgar-ms : the transitional API (i.e., for the purposes of deprecating the old API) would be something like:
`MetricFrame(*args, metrics, y_true, y_pred, sensitive_features, control_features, sample_params)`","2","0.3459431571599821","Performance measurement","Validation"
"https://github.com/fairlearn/fairlearn","831397504","issue_comment","https://github.com/Trusted-AI/AIF360/issues/756#issuecomment-831397504","I'm not terribly opinionated on this issue, but if we move to something like `MetricFrame(*, metrics, sensitive_features, control_features, sample_params, **shared_sample_params)` then we better have extremely good documentation. Reading this in an API documentation I would have no clue what to do with that. In contrast, the current signature is very intuitive to me. Obviously that can be fixed with a couple of nice examples right in the API doc and updating the user guide, but I want to make sure this is on our radar (i.e., no code PR without updating the docs). 

I would also suggest providing a few examples of what that would look like if we have different kinds of metrics (e.g. accuracy needs `y_true` and `y_pred`, selection rate doesn't need a second set of `y`s) in the same set of `metrics` passed in.

Finally, I'm wondering whether the right thing to do here would be to add a warning that `MetricFrame` will have keyword-only args starting with the next version so people should adjust their scripts. [If so, we should make the switch to `metrics` instead of `metric` right away, otherwise it breaks people again.]","15","0.4002536260600777","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","831417006","issue_comment","https://github.com/Trusted-AI/AIF360/issues/756#issuecomment-831417006","@romanlutz -- i agree than step (2) requires some care, but what do you think about carrying out step (1), with the intermediate API of the form:
```python
MetricFrame(*args, metrics=None, y_true=None, y_pred=None,
            sensitive_features, control_features=None, sample_params=None)
```
This would support the current behavior when 1-3 positional arguments are provided, but in those cases it would give a warning that this calling format is being deprecated.","2","0.5389363343709255","Performance measurement","Validation"
"https://github.com/fairlearn/fairlearn","831443732","issue_comment","https://github.com/Trusted-AI/AIF360/issues/756#issuecomment-831443732","I think that there are a few different things going on here, and I'd like to separate the different threads.

Firstly, I think that the idea behind `shared_sample_params` is a good one. Indeed, I considered putting something like that in last autumn when I was writing `MetricFrame`. I didn't because it wasn't a 'must have' feature. However, as I said above, I think it should be a `Dict[str,vector]` and not `**kwargs`. Going with the latter makes it harder for us to add any other named arguments of our own (since whatever name we pick, we're bound to break someone) at a later date. We 'explode' the `Dict` when the metric functions themselves are invoked.

Secondly, I certainly don't think that `MetricFrame` is the last word on these sort of metrics. Indeed, I'm sure that areas like text or vision may well need similar but different functionality (I don't know enough about them to say). But like @hildeweerts , I think that it may well be better to take each by itself, and figure out what works best for that domain, rather than trying to shoehorn them into `MetricFrame`.

I can see the logic in renaming the `metric` argument to `metrics` (it is a potential break, but only if people are naming their arguments). I'm not sure I understand the motivation behind forcing named arguments. I think that would make the interface much less `skearn`-like.","15","0.5517724959518855","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","831550928","issue_comment","https://github.com/Trusted-AI/AIF360/issues/756#issuecomment-831550928","@riedgar-ms : one of the reason to have keyword-only parameters is so that you can skip them when they're not needed. For example, if we went with the signature:
```python
MetricFrame(*, metrics, y_true=None, y_pred=None, sensitive_features, control_features=None, sample_params=None)
```
Then it would be okay to drop `y_true` and `y_pred` and provide all the metric parameters via `sample_params`.
","2","0.6760445868375387","Performance measurement","Validation"
"https://github.com/fairlearn/fairlearn","831557424","issue_comment","https://github.com/Trusted-AI/AIF360/issues/756#issuecomment-831557424","Re. `shared_sample_params`, I think that `**shared_sample_params` would make the pattern more similar to what we do right now with `y_true` and `y_pred`. We could take advantage of [`inspect.signature()`](https://docs.python.org/3/library/inspect.html#inspect.signature) and throw a warning if there's a conflict. In those rare cases, the users could use `sample_params` as a workaround.

But I also don't mind if `y_true=None` and `y_pred=None` are grandfathered in and anything else needs to be provided via an actual dictionary `shared_sample_params` rather than `**shared_sample_params`.","2","0.564432723135795","Performance measurement","Validation"
"https://github.com/fairlearn/fairlearn","831615145","issue_comment","https://github.com/Trusted-AI/AIF360/issues/756#issuecomment-831615145","> @riedgar-ms : one of the reason to have keyword-only parameters is so that you can skip them when they're not needed. For example, if we went with the signature:
> 
> ```python
> MetricFrame(*, metrics, y_true=None, y_pred=None, sensitive_features, control_features=None, sample_params=None)
> ```
> 
> Then it would be okay to drop `y_true` and `y_pred` and provide all the metric parameters via `sample_params`.

If the use case is ""we can drop the current parameters, and have a different set of parameters instead"" then we should probably be creating a new class to handle the second set of parameters. Right now, I feel that `MetricFrame` does the fairly simple task of taking an `sklearn`-style `(y_true, y_pred)` metric, and turning it into one with grouping (via the sensitive features). I'd prefer to keep it that way, and work out the best API for a new use case from a clean sheet.

It may be that there is a more general API hiding within, and that we end up changing `MetricFrame` to wrap (or subclass) that. But I think it would still be better to keep the existing `MetricFrame` API around, doing its simple job.","15","0.6201600257327808","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","831665576","issue_comment","https://github.com/Trusted-AI/AIF360/issues/756#issuecomment-831665576","I feel like this discussion could benefit from some concrete examples. Like @riedgar-ms (and @hildeweerts I think) I'm currently struggling to see the benefit of complicating the currently very intuitive API, but perhaps with 3-5 examples of what this would look like for actual metrics (say, one of the existing ones we support, and then whichever new ones we might be able to accommodate through this) we might see the benefits (?)","15","0.9347444838696528","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","831752302","issue_comment","https://github.com/Trusted-AI/AIF360/issues/756#issuecomment-831752302","> It may be that there is a more general API hiding within, and that we end up changing MetricFrame to wrap (or subclass) that. But I think it would still be better to keep the existing MetricFrame API around, doing its simple job.

I was thinking the same! 

I think the majority of users will be looking for classification/regression metrics and may not even be familiar with reinforcement learning. Having to look at examples to understand how to use an API even in the 'simplest' scenario, signals that it is not intuitive.

I like @romanlutz idea of writing out some examples!","15","0.6985229743850434","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","832290198","issue_comment","https://github.com/Trusted-AI/AIF360/issues/756#issuecomment-832290198","Is there resistance even to Step 1, which would just modify the current API to:
```python
MetricFrame(*, metrics, y_true=None, y_pred=None, sensitive_features, control_features=None, sample_params=None)
```
I don't think that it would make the API more confusing, would it?

I think that there are good use cases that have nothing to do with reinforcement learning and that we have already run into (dataset-only metrics like demographic parity, streaming metrics which are currently being implemented, and word-error rate-like settings which are requested as well). But let me create some short examples to demonstrate.","2","0.3618005388265025","Performance measurement","Validation"
"https://github.com/fairlearn/fairlearn","833439582","issue_comment","https://github.com/Trusted-AI/AIF360/issues/756#issuecomment-833439582","It would break existing code, and require us to bump the version to v0.7.0. If we do that, I think we should release v0.7.0 as soon as possible after making the change. The long gap between v0.4.6 and v0.5.0 (the latter introducing the `MetricFrame` itself) has given us quite a lot of trouble, since the things in the repo were incompatible with the package on PyPI.

I would not be overjoyed at defaulting `y_true` and `y_pred` to `None`, though. That is not how `sklearn` metrics work.

At the same time, you could also add the `shared_sample_params` dictionary, since I think that is a great thing to have.","32","0.3494712039015836","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","834479376","issue_comment","https://github.com/Trusted-AI/AIF360/issues/756#issuecomment-834479376","Based on the discussion in the community call, I'm happy with the following being done:

1. Renaming `metric` to `metrics` in the argument list
2. Switching to keyword arguments
3. Adding `shared_sample_params` (as a dictionary, not `**kwargs`)

My only concern is orchestrating a swift release, since these are breaking changes.

When it comes to metrics for other problem types, I would like to know more before deciding to support those by extending `MetricFrame` (rather than creating a new class).","15","0.6146912159570389","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","834495573","issue_comment","https://github.com/Trusted-AI/AIF360/issues/756#issuecomment-834495573","The reasoning behind `shared_sample_params` is a bit unclear to me still. It seems like a pretty niche situation but I might be missing something? 

It seems to me that generally speaking the sample_params will be similar for different metrics of the same scenario (e.g., either classification or reinforcement learning). And in the niche cases that it's not, you might as well just create two `MetricFrame` objects.

So for me at least some examples would still be appreciated, @MiroDudik :D","15","0.3795768524780213","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","834804110","issue_comment","https://github.com/Trusted-AI/AIF360/issues/756#issuecomment-834804110","I know... examples... soonish?

@riedgar-ms : re. swift release, my suggestion re. staging is

#### 1. backward compatible implementation with deprecation warning:
```python
MetricFrame(*args, metrics, y_true=None, y_pred=None, sensitive_features, control_features=None, sample_params=None)
```
#### 2. move to keyword-only arguments:
```python
MetricFrame(*, metrics, y_true=None, y_pred=None, sensitive_features, control_features=None, sample_params=None)
```
#### 3. addition of any other arguments like `shared_sample_params` (but that could happen later or together with 2)

Examples are coming next week... promise!","2","0.5762213976499694","Performance measurement","Validation"
"https://github.com/fairlearn/fairlearn","834807434","issue_comment","https://github.com/Trusted-AI/AIF360/issues/756#issuecomment-834807434","I'm not keen on allowing `None` for `y_true` and `y_pred`; not until we're really sure we want to expand on `MetricFrame` in this way. Otherwise, I'm OK with those two steps. I think that 3 could come before 2 actually. Right now, I don't think that (2) should happen before v0.8.0.","20","0.3953366518208014","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","834821745","issue_comment","https://github.com/Trusted-AI/AIF360/issues/756#issuecomment-834821745","@riedgar-ms: I believe that (1) must have `y_true=None` and `y_pred=None` otherwise we cannot be backward compatible plus able to issue deprecation warning... so in a sense (1) might be forced. We can discuss whether (2) should have `None` or not after I show those promised examples...","32","0.3486886805272455","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","835330201","issue_comment","https://github.com/Trusted-AI/AIF360/issues/756#issuecomment-835330201","Hmmm.... good point.","20","0.5515558267236121","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","841889274","issue_comment","https://github.com/Trusted-AI/AIF360/issues/756#issuecomment-841889274","Finally, some examples... in those examples I'm considering two alternative proposals for the API (they slightly differ from the original proposal in my issue at the top):

#### Alternative A: Optional `y_true` / `y_pred` / `sensitive_features`

```python
MetricFrame(*, metrics, y_true=None, y_pred=None, sensitive_features=None, control_features=None, sample_params=None)
```

#### Alternative B: Also allow any keyword arguments and internally use `inspect.signature` to decide who gets what

```python
MetricFrame(*, metrics, y_true=None, y_pred=None, sensitive_features=None, control_features=None,
            sample_params=None, **direct_sample_params)
```
The idea is that `**direct_sample_params` would contain sample parameters and each of them would be passed to all the metrics whose signature includes that parameter name. We retain the nested dictionary `sample_params` as a way to deal with naming conflicts (but there's also a work-around using lambda expressions).

Now the scenarios. I've written them down purely technically, but Scenarios 1 & 2 came up in various notebooks, Scenario 3 is already under a PR, and Scenario 4 comes up in settings with censored data (e.g., many allocation settings).

### Scenario 1: Classification metrics and scoring metrics in the same metric frame

For example, let's say you train a logistic model and would like to evaluate both its classification accuracy as well as the area under the ROC curve.

Say we have:
```python
y_true = # vector of binary classification labels
sf = # vector of sensitive features
y_pred = estimator.predict(X) # binary predictions
y_score = estimator.predict_proba(X)[1] # scores
```

Current approach is something like this:
```python
metrics = {
    `accuracy`: skm.accuracy_score,
    `auc`: lambda y_true, y_pred, y_score: skm.roc_auc_score(y_true, y_score)
}
mf = MetricFrame(metrics, y_true, y_pred, sensitive_features=sf, 
                 sample_params={
                     'auc': {'y_score': y_score}})
```

#### Alternative 1A: Optional `y_true` / `y_pred`

```python
metrics = {
    `accuracy`: skm.accuracy_score,
    `auc`: skm.roc_auc_score
}
mf = MetricFrame(metrics=metrics, y_true=y_true, sensitive_features=sf, 
                 sample_params={
                     'accuracy': {'y_pred': y_pred},
                     'auc': {'y_score': y_score}})
```

#### Alternative 1B: Also allow any keyword arguments and internally use `inspect.signature` to decide who gets what

```python
metrics = {
    `accuracy`: skm.accuracy_score,
    `auc`: skm.roc_auc_score
}
mf = MetricFrame(metrics=metrics, y_true=y_true, y_pred=y_pred, y_score=y_score, sensitive_features=sf)
```

### Scenario 2: Multiple models in the same metric frame

Say we have three models giving predictions `y_pred1`, `y_pred2`, `y_pred3` that we wish to compare. The current approach is something like:
```python
metrics = {
    `model1_accuracy`: skm.accuracy_score,
    `model2_accuracy`: lambda y_true, y_pred, y_pred2: skm.accuracy_score(y_true, y_pred2),
    `model3_accuracy`: lambda y_true, y_pred, y_pred3: skm.accuracy_score(y_true, y_pred3),
}
mf = MetricFrame(metrics, y_true, y_pred, sensitive_features=sf, 
                 sample_params={
                     'model2_accuracy': {'y_pred2': y_pred2},
                     'model3_accuracy': {'y_pred3': y_pred3}})
```

#### Alternative 2A: Optional `y_true` / `y_pred`

```python
metrics = {
    `model1_accuracy`: skm.accuracy_score,
    `model2_accuracy`: skm.accuracy_score,
    `model3_accuracy`: skm.accuracy_score
}
mf = MetricFrame(metrics=metrics, y_true=y_true, sensitive_features=sf, 
                 sample_params={
                     'model1_accuracy': {'y_pred': y_pred1}
                     'model2_accuracy': {'y_pred': y_pred2},
                     'model3_accuracy': {'y_pred': y_pred3}})
```

#### Alternative 2B: Also allow any keyword arguments and internally use `inspect.signature` to decide who gets what

```python
metrics = {
    `model1_accuracy`: lambda y_true, y_pred1: skm.accuracy_score(y_true, y_pred1),
    `model2_accuracy`: lambda y_true, y_pred2: skm.accuracy_score(y_true, y_pred2),
    `model3_accuracy`: lambda y_true, y_pred3: skm.accuracy_score(y_true, y_pred3),
}
mf = MetricFrame(metrics=metrics, y_true=y_true,
                 y_pred1=y_pred1, y_pred2=y_pred2, y_pred3=y_pred3, sensitive_features=sf)
```
(Note that the solution from 2A would still be allowed. I think that 2A is cleaner than 2B.)

### Scenario 3: Streaming metrics

This is the scenario from #670 (and #655). The idea is to support adding data and updating a metric frame in a streaming fashion.

The currently proposed initialization is this:
```python
mf = MetricFrame(skm.recall_score, y_true=[], y_pred=[], sensitive_features=[], streaming=True)
```

#### Alternative 3A: Optional `y_true` / `y_pred` / `sensitive_features`
```python
mf = MetricFrame(skm.recall_score, streaming=True)
```

### Scenario 4: Metrics that don't use `y_true` and `y_pred`

In cost-sensitive learning, the metric parameters would be `y_pred` and `costs` (for each example, providing a vector of costs for predicting each of the classes). In contextual bandits, the metric parameters would be `actions_taken`, `rewards`, `propensities`, `actions_target` (or something like that).

Below, assume that there are metrics defined for the above two settings, say:
```python
cost_sensitive_loss(*, costs, y_pred)  # for cost-sensitive learning
inverse_propensity_score(*, actions_taken, rewards, propensities, actions_target) # for contextual bandits
```

Current approach:
```python
def cost_sensitive_loss_wrapper(y_true, y_pred, *, costs):
    return cost_sensitive_loss(costs=costs, y_pred=y_pred)

def ips_wrapper(y_true, y_pred, *, actions_taken, rewards, propensities, actions_target):
    return inverse_propensity_score(actions_taken=actions_taken, rewards=rewards,
                                    propensities=propensities, actions_target=actions_target)

cost_sensitive_loss_mf = MetricFrame(cons_sensitive_loss_wrapper,
    dummy_y_true, y_pred, sensitive_features=sf,
    sample_params = {'costs': costs})

ips_mf = MetricFrame(ips_wrapper,
    dummy_y_true, dummy_y_pred, sensitive_features=sf,
    sample_params = {'actions_taken': actions_taken, 'rewards': rewards,
                     'propensities': propensities, 'actions_target': actions_target})
```

#### Alternative 4A: Optional `y_true` / `y_pred`

```python
cost_sensitive_loss_mf = MetricFrame(metrics=cost_sensitive_loss,   # wrapper not needed
    y_true=y_true,
    sample_params = {'costs': costs})

ips_mf = MetricFrame(metrics=inverse_propensity_score,
    sample_params = {'actions_taken': actions_taken, 'rewards': rewards,   # wrapper not needed
                     'propensities': propensities, 'actions_target': actions_target})
```

#### Alternative 4B: Also allow any keyword arguments and internally use `inspect.signature` to decide who gets what

```python
cost_sensitive_loss_mf = MetricFrame(metrics=cost_sensitive_loss,
    y_true=y_true,
    costs=costs)

ips_mf = MetricFrame(metrics=inverse_propensity_score,
    actions_taken=actions_taken, rewards=rewards,
    propensities=propensities, actions_target=actions_target)
```
","2","0.7567167765967474","Performance measurement","Validation"
"https://github.com/fairlearn/fairlearn","841889642","issue_comment","https://github.com/Trusted-AI/AIF360/issues/756#issuecomment-841889642","@romanlutz , @riedgar-ms , @hildeweerts : please take a look at the examples above, and let me know what you think about Alternative A vs Alternative B (which just extends Alternative A). In particular, do any of you see issues with Alternative A?","15","0.4381977671451356","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","842305737","issue_comment","https://github.com/Trusted-AI/AIF360/issues/756#issuecomment-842305737","I'm still not enamoured of accepting `**kwargs` in a function signature. It means that if we *ever* want to add new arguments to the signature, we're going to break someone. That is why I've been arguing for using a regular dictionary there. I can see that further using `inspect.signature` is something which can give a 'wow factor' in a demo. It is also a way of enabling users to elevate typos into _really_ subtle bugs.

I still feel that we should leave `MetricFrame` largely as-is (with the exception of converting to keyword-only arguments.... note that this does _not_ involve making `y_true` and `y_pred` optional in the above proposal, since there is not a transition API which accepts the positional arguments with a warning). It does a fairly simple thing well, so keep it that way.

I would be happy to start up a fresh discussion group about what a more general disaggregated metric should look like (and as I mentioned above, it's entirely possible that the existing `MetricFrame` would end up being refactored to use that feature).","15","0.4887953967152442","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","842341510","issue_comment","https://github.com/Trusted-AI/AIF360/issues/756#issuecomment-842341510","Thanks a lot for these examples - this makes things a lot clearer for me! Given these examples, I find Alternative A pretty intuitive. If I understand correctly, the most basic scenario with just `y_true` and `y_pred` doesn't really change apart from having to name the arguments, right?

At a first glance, I am a bit concerned that Alternative B contains too much ""magic"" behind the scenes which makes it more difficult to read (and debug) code. I do understand that writing all the dictionaries can be tedious. Adding subclasses for specific types of problems (e.g., RL) would already go a long way I think (and is much easier to properly document without overwhelming novice users).","15","0.5097818576645943","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","842530462","issue_comment","https://github.com/Trusted-AI/AIF360/issues/756#issuecomment-842530462","@riedgar-ms : what do you think about Alternative A? I think it would go a long way towards addressing the main issues and it doesn't introduce `**kwargs`.

@hildeweerts : correct, our current functionality is supported by both Alternative A and Alternative B by just requiring to name `y_true`, `y_pred`, and `metrics`.","2","0.3488653148919026","Performance measurement","Validation"
"https://github.com/fairlearn/fairlearn","842564339","issue_comment","https://github.com/Trusted-AI/AIF360/issues/756#issuecomment-842564339","I'd say ""Alternative A with `y_true` and `y_pred` as required arguments.""","2","0.5679374389051812","Performance measurement","Validation"
"https://github.com/fairlearn/fairlearn","842598611","issue_comment","https://github.com/Trusted-AI/AIF360/issues/756#issuecomment-842598611","@riedgar-ms : What are the downsides of going with Alternative A? The upside is that we can cover scenarios 1-4 without workarounds.","2","0.5795319745497678","Performance measurement","Validation"
"https://github.com/fairlearn/fairlearn","842708238","issue_comment","https://github.com/Trusted-AI/AIF360/issues/756#issuecomment-842708238","The only workaround required now would be to supply a 'dummy' column for one or the other. Obviously making them optional doesn't break anything which works now doesn't break anything (beyond the general breakage of moving to keyword-required), but it does open up new failure modes, which might not be that user-friendly; we should at least see what new errors can result.","32","0.3481434058898846","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","843235220","issue_comment","https://github.com/Trusted-AI/AIF360/issues/756#issuecomment-843235220","@romanlutz , @hildeweerts , @riedgar-ms : it seems that we at least have a consensus towards the initial steps:

*Step 1:* The intermediate API in the next minor release; backward compatible, but throws a warning that we're deprecating the current API in favor of keyword-only API (see Step 2), starting in 0.8.0:
```python
MetricFrame(*args, metrics, y_true=None, y_pred=None,
            sensitive_features, control_features=None, sample_params=None)
```
[We must support `y_true=None` and `y_pred=None`, so we can be backward compatible without creating an obscure signature.]

*Step 2:* In 0.8.0, we can move to keyword only, but (unless we come to a different decision by then) for now force `y_true` and `y_pred`:
```python
MetricFrame(*, metrics, y_true, y_pred,
            sensitive_features, control_features=None, sample_params=None)
```

*Step 3:* If we decide, we can adopt Alternative A or Alternative B from [my comment above](https://github.com/fairlearn/fairlearn/issues/756#issuecomment-841889274) in a way that would be backward compatible.

Thoughts?","2","0.5428736517719562","Performance measurement","Validation"
"https://github.com/fairlearn/fairlearn","843247801","issue_comment","https://github.com/Trusted-AI/AIF360/issues/756#issuecomment-843247801","I'm a bit confused - why do we need to change the API in step 1 (apart from metric -> metrics) - can't we just throw a warning?","15","0.7083732057416269","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","843254991","issue_comment","https://github.com/Trusted-AI/AIF360/issues/756#issuecomment-843254991","@hildeweerts -- if we go directly to Step 2, it will break all the current code that uses positional arguments rather than keywords. So, `mf = MetricFrame(skm.accuracy_score, y_true, y_pred)` will stop working suddenly.
","2","0.5481807696340567","Performance measurement","Validation"
"https://github.com/fairlearn/fairlearn","843257375","issue_comment","https://github.com/Trusted-AI/AIF360/issues/756#issuecomment-843257375","@MiroDudik yeah I understand - I was just wondering why we need to change anything about our current code in Step 1, apart from metric -> metrics and throwing a warning. We currently have `MetricFrame(metric, y_true, y_pred, *, sensitive_features, control_features=None, sample_params=None)` right? ","2","0.7334049505393668","Performance measurement","Validation"
"https://github.com/fairlearn/fairlearn","843265176","issue_comment","https://github.com/Trusted-AI/AIF360/issues/756#issuecomment-843265176","@hildeweerts -- it's an implementation issue. As far as I know, we cannot tell whether the user was passing the arguments as keyword arguments or as positional arguments if we use the signature `MetricFrame(metric, y_true, y_pred, *, sensitive_features, control_features=None, sample_params=None)`, whereas the signature in Step 1 would be able to tell how things are passed. Or were you thinking about throwing the warning **always** even when people pass all the arguments as keyword arguments?","2","0.7734344437422892","Performance measurement","Validation"
"https://github.com/fairlearn/fairlearn","843269544","issue_comment","https://github.com/Trusted-AI/AIF360/issues/756#issuecomment-843269544","Ahh yes I see, that makes sense - thank you for the explanation!","15","0.4918414918414917","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","843318935","issue_comment","https://github.com/Trusted-AI/AIF360/issues/756#issuecomment-843318935","I'm OK with this plan.","24","0.329153605015674","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","854701288","issue_comment","https://github.com/Trusted-AI/AIF360/issues/756#issuecomment-854701288","How do we want to handle this discussion going forward? We have the transition to keyword-only arguments in hand, but this was originally about handling other metrics which don't use `y_true` and `y_pred` as arguments. Would it make more sense to start a new thread just about the latter (given the digressions), or would people prefer to keep the discussion going here?

Whichever we pick, I would like to get this discussion moving again.

[Edit] We may also need to have some actual meetings to thrash out the issues.","15","0.4275224940567542","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","855904015","issue_comment","https://github.com/Trusted-AI/AIF360/issues/756#issuecomment-855904015","In the absence of other comments, I'll keep going here for now :-)

I have spoken with @michaelamoako a little about this and he gave the example of word error rates being pre-computed, and that his scenario needed the mean (or some other statistic) rate for each subgroup identified by the sensitive features.

With the current `MetricFrame` you have to do something like this:
```python
def my_mean(y_true, y_pred):
    assert len(y_true) == len(y_pred)
    return np.mean(y_pred)

my_frame = MetricFrame(metrics=my_mean, y_true=precompute_wer, y_pred=precompute_wer, sensitive_features=s_f)
```
Obviously, this all looks a little odd (FWIW, `selection_rate()` looks a lot like `my_mean()`).","12","0.2995755601618795","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","855937687","issue_comment","https://github.com/Trusted-AI/AIF360/issues/756#issuecomment-855937687","There is also an alternative to @MiroDudik 's example of sneaking extra arguments in through `sample_params` above: use `DataFrame.to_dict(orient='records')` and `DataFrame.from_records()` to pack and unpack the data. Something like:
```python
y_pred = DataFrame(data={'predictions':preds, 'scores':scores}).to_dict(orient='records')

def my_metric(y_true, y_pred):
    df.from_records(y_pred)
    predicted_classes = df['predictions']
    predicted_probabilities = df['scores']
   # etc.

mf = MetricFrame(metrics=my_metric, y_true=y_true, y_pred=y_pred, sensitive_features=s_f)
```
This might feel more natural for some parameters, rather than smuggling them in through `sample_params`.","2","0.6300054112554115","Performance measurement","Validation"
"https://github.com/fairlearn/fairlearn","856987207","issue_comment","https://github.com/Trusted-AI/AIF360/issues/756#issuecomment-856987207","@riedgar-ms , @michaelamoako : I was thinking about the use case that you're talking about--when the metric is provided on a per example basis, and the task is to just evaluate averages over different slices. For that setting, one natural API would be something like

```python
df = DataFrame({'metric1': [0.1, 0.2, 0.3, 0.1], 'metric2': [1, 0, 0.5, 0.5], 'sf': ['a', 'b', 'a', 'b']}) 
mf = MetricFrame(metric_values=df[['metric1', 'metric2']], sensitive_features=df['sf'])
```

Your example with `to_dict` and `from_records` looks a bit too cryptic for my liking, but as a workaround it's okay. I think that a more elegant solution would be to make sure that slicing preserves the data types (so if `y_true` is a DataFrame, the slices will be too, with the same column names). That's how sklearn does it at a few places. For example, train/test split preserves the type of the provided arguments:
https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html ","15","0.3415167252443652","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","857060552","issue_comment","https://github.com/Trusted-AI/AIF360/issues/756#issuecomment-857060552","@MiroDudik I like this, but rather than adding lots and lots of possible arguments to `MetricFrame` that only really work for some precomputed use cases, I would suggest having a function, let's call it `from_frame` where you specify the columns
```
df = DataFrame({'metric1': [0.1, 0.2, 0.3, 0.1], 'metric2': [1, 0, 0.5, 0.5], 'sf': ['a', 'b', 'a', 'b']}) 
mf = MetricFrame.from_frame(metric_value_cols=['metric1', 'metric2'], sensitive_feature_cols=['sf'])
```
As a user I certainly would be confused about which args to provide if we keep on adding more and more. It's already quite a few that our average user probably doesn't care about (such as control features).","15","0.5947997150528797","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","857551938","issue_comment","https://github.com/Trusted-AI/AIF360/issues/756#issuecomment-857551938","I agree with @romanlutz and I like the suggestion of a `from_frame` function - though we might need to find a way to make it very clear that this considers precomputed scores.

TBH, as a data scientist, I probably wouldn't even think to use Fairlearn in this case, because it's so easy to do it with pandas' `groupby`. But I do see the added value of being able to use the same data structure for these different types of use cases.","15","0.4950986974620652","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","857657337","issue_comment","https://github.com/Trusted-AI/AIF360/issues/756#issuecomment-857657337","We've got two things going on here:

- Where we have a precomputed metric where we just need to slice it up by the sensitive features
- Where there are multiple 'columns' in `y_pred` and/or `y_true`

I definitely agree that not adding more arguments to the constructor is preferable - the static `from_frame()` approach @romanlutz  suggests would be better if there's a precomputed metric to be sliced.

For the second point, I can see that supporting multicolumn DataFrame arguments for both could be useful. Would that do enough to support @michaelamoako 's use cases?","15","0.7707185995157896","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","857684373","issue_comment","https://github.com/Trusted-AI/AIF360/issues/756#issuecomment-857684373","**Re. static initializer** `from_frame()`: That works for me. I like it. However, some comments:
* What do we do about the [streaming API](https://github.com/fairlearn/fairlearn/issues/655) here? I think that this is actually a very nice use case for streaming metrics, because aggregation is super easy, but it would lead to a more complicated `add_data()`. This makes me think that rather than a static initializer, we should actually have a subclass `MetricFrameFromValues` or something like that, which would have a different constructor and a different `add_data()`. I'm generally not a big fan of subclassing in Python, but this might be a perfect use case.
* Re. @hildeweerts question, why would a data scientist want this. I can think of the following:
  * There's some extra functionality: difference(), ratio(), etc., and (hopefully soon), we will have streaming, and possibly error bars.
  * If they want to build on some of our examples that use `MetricFrame`, this would be an easy drop-in option.
  * This feature is actually being requested by a team that's looking to use Fairlearn, so there are _some_ data scientists that are interested :-)
 ","15","0.7088609766978963","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","858678067","issue_comment","https://github.com/Trusted-AI/AIF360/issues/756#issuecomment-858678067","Having a subclass `MetricFrameFromValues` feels odd - you'd be subclassing to close off functionality rather than adding it. We would also be adding quite a bit of code to handle a case which is very easy to handle with a simple wrapper function. Indeed, rather than making a new class, I'd rather provide a function factory which could do what `selection_rate()` does now (indeed, `selection_rate()` could be its first use case). And document it, if people wanted to use `y_true` as their single column, rather than `y_pred`.","15","0.62700382666253","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","862396452","issue_comment","https://github.com/Trusted-AI/AIF360/issues/756#issuecomment-862396452","Ping @hildeweerts @MiroDudik @michaelamoako ","20","0.6731601731601731","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","863093794","issue_comment","https://github.com/Trusted-AI/AIF360/issues/756#issuecomment-863093794","Re. `MetricFrameFromValues`: from a user perspective I think I would prefer a `from_frame` (or maybe `from_precomputed`?) function over a subclass, because it seems more natural considering the familiar `pandas` API that has 'normal' initialization and `from_csv()` etc.  I think the `add_data()` problem should be pretty easy to solve by adding an attribute `precomputed_` (or whatever you'd like to call it) that determines which `add_data()` is used.

Re. function factory suggested by @riedgar-ms : this is for a different problem with varying columns `y_pred`/`y_true`, right? Or also for the precomputed metrics? In that case, could you expand a bit on what that would look like?","15","0.6320800900768586","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","863207096","issue_comment","https://github.com/Trusted-AI/AIF360/issues/756#issuecomment-863207096","The factory function would be something like:
```python
def expand_y_pred(y_true, y_pred, f):
    assert len(y_true) == len(y_pred)
    return f(y_pred)

def make_metric_from_y_pred_function(function):
    return functools.partial(expand_y_pred, f = function)
```
in which case we could define:
```python
selection_rate = make_metric_from_y_pred_function(np.mean)
```
(note I have only written the code above, not actually tried it). There could be a similar `make_metric_from_y_true_function()` as well.

Does that make more sense @hildeweerts ?","12","0.4989161689891617","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","863257177","issue_comment","https://github.com/Trusted-AI/AIF360/issues/756#issuecomment-863257177","Ah, yes, I see. Thanks for the explanation! 

I can see the added value of the factory function, but I wonder how easy it would be for (novice) Fairlearn users to identify the need for it - particularly for those who skip the user guide.

I still think there's added value in having specific data structures for different types of ML tasks, rather than having to wrap functions to fit into the supervised learning paradigm - if that makes sense? E.g., with named arguments it seems like we should be able to handle `y_true`/`y_pred` for supervised problems, and perhaps `labels`/`labels_true`/`labels_pred` for unsupervised problems, etc. Just to make things easier for people who just want to use more standard `scikit-learn`-like metrics.

But I am not sure what's the best way forward here... looking forward to hearing other people's thoughts :)","25","0.4108984914731323","Research","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","863505346","issue_comment","https://github.com/Trusted-AI/AIF360/issues/756#issuecomment-863505346","Can you expand on `labels / labels_true / labels_pred` ? What's the third category for - I've generally been thinking of that '`y_pred` is anything coming from the model, and `y_true` is anything coming from the original data.'

If we did start allowing different names, how would me handle routing arguments? I'm concerned that if we try looking at our list of arguments, and using reflection on the supplied metric functions, we'll end up creating a source of subtle bugs. Think `sample_weight` vs `sample_weights`.","12","0.43270279617953","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","863814691","issue_comment","https://github.com/Trusted-AI/AIF360/issues/756#issuecomment-863814691","> Can you expand on labels / labels_true / labels_pred ? What's the third category for - I've generally been thinking of that 'y_pred is anything coming from the model, and y_true is anything coming from the original data.'

That is correct! `labels` is the equivalent `y` for clustering problems. For some reason there exist both supervised and unsupervised metrics to evaluate clustering results (https://scikit-learn.org/stable/modules/classes.html#clustering-metrics), hence the three categories.","12","0.5248273287287661","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","863819660","issue_comment","https://github.com/Trusted-AI/AIF360/issues/756#issuecomment-863819660","> If we did start allowing different names, how would me handle routing arguments? I'm concerned that if we try looking at our list of arguments, and using reflection on the supplied metric functions, we'll end up creating a source of subtle bugs. Think sample_weight vs sample_weights.

I was thinking we could have a separate `SupervisedMetricFrame` that directly routes `y_true` and `y_pred` (if they're supplied) and `UnsupervisedMetricFrame` that directs `labels`, `labels_pred`, `labels_true`. In this way people who have specific use cases don't need to work with function factories. We could still have the generic `MetricFrame` + function factories to facilitate people who require more flexibility.","15","0.6382181669819371","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","864020240","issue_comment","https://github.com/Trusted-AI/AIF360/issues/756#issuecomment-864020240","My issue with MetricFrame was that it computed the metric on instantiation which meant that the other methods could not be accessed unless I passed a metric. So you'll see in my attempt at a [roc_curves](https://github.com/fairlearn/fairlearn/pull/869/files) class that there is a psuedo metric (tks @MiroDudik, for that solution) to enable that class to take advantage of MetricFrame's `by_group` method.  

What do you think of introducing static methods within the MetricFrame class for helper functions like splitting the data by sensitive feature? ","2","0.326469257988134","Performance measurement","Validation"
"https://github.com/fairlearn/fairlearn","864042945","issue_comment","https://github.com/Trusted-AI/AIF360/issues/756#issuecomment-864042945","@kstohr I'm a bit confused by what you mean by:

>  Note: MetricFrame requires y_pred (clf.predict). However, ROC curves and  AUC scores are generated using y_score (clf.predict_proba).  This method  substitutes y_score (type:float) for y_pred (type:int) to conform to the MetricFrame required params.

The _argument_ is obviously called `y_true` but its type is certainly not required to be `int`.

","2","0.6809962189709027","Performance measurement","Validation"
"https://github.com/fairlearn/fairlearn","864046283","issue_comment","https://github.com/Trusted-AI/AIF360/issues/756#issuecomment-864046283","> > Can you expand on labels / labels_true / labels_pred ? What's the third category for - I've generally been thinking of that 'y_pred is anything coming from the model, and y_true is anything coming from the original data.'
> 
> That is correct! `labels` is the equivalent `y` for clustering problems. For some reason there exist both supervised and unsupervised metrics to evaluate clustering results (https://scikit-learn.org/stable/modules/classes.html#clustering-metrics), hence the three categories.

@hildeweerts  that sort of leads to the question ""Then what is `y` as opposed to `y_true` and `y_pred`?"". Scanning through the link to `sklearn` it appears that it's used for cases where the ground truth isn't known? If so, then would you always have either `y_true` and `y_pred` (`s/y/labels/` as appropriate) OR just `y` ? ","12","0.5283056934188758","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","864064225","issue_comment","https://github.com/Trusted-AI/AIF360/issues/756#issuecomment-864064225","> @hildeweerts that sort of leads to the question ""Then what is `y` as opposed to `y_true` and `y_pred`?"". Scanning through the link to `sklearn` it appears that it's used for cases where the ground truth isn't known? If so, then would you always have either `y_true` and `y_pred` (`s/y/labels/` as appropriate) OR just `y` ?

I'm not sure if I understand your question correctly so sorry in advance if this is all obvious to you. But generally `y` is used for ground-truth during training, my guess is that the authors decided to not call it `y_true` here because there is no explicit counterpart `y_pred`. For evaluation we need to be more specific, hence `y_true` and `y_pred` (or `y_score`). Similarly, they use `labels` to refer to cluster labels and only append `_true` and `_pred` in cases where there is an explicit ground-truth. ","12","0.5013352620451765","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","864073632","issue_comment","https://github.com/Trusted-AI/AIF360/issues/756#issuecomment-864073632","@hildeweerts it might be that I'm just not fully used to the conventions - based on your description, it sounds like `y` and `y_pred` (similarly `labels` and `labels_pred`) are interchangeable - both refer to things coming from a model. Is that correct?","25","0.3385408917323811","Research","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","864078333","issue_comment","https://github.com/Trusted-AI/AIF360/issues/756#issuecomment-864078333","> @hildeweerts it might be that I'm just not fully used to the conventions - based on your description, it sounds like `y` and `y_pred` (similarly `labels` and `labels_pred`) are interchangeable - both refer to things coming from a model. Is that correct?

To make things more confusing for you generally speaking `y`/`y_true` are interchangeable and `labels`/`labels_pred`. The reason for this is that by default clustering problems don't really have a ""ground truth"" (otherwise we would probably be better off doing classification). The ""Evaluation and assessment"" section of [this wikipedia page](https://en.wikipedia.org/wiki/Cluster_analysis) describes some of the issues with evaluating clusterings pretty well I think.","12","0.3891578904894485","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","864082095","issue_comment","https://github.com/Trusted-AI/AIF360/issues/756#issuecomment-864082095","@hildeweerts coming back to the API, though, are there metrics which would accept _three_ parameters - `labels`, `labels_true` and `labels_pred` ? It sounds like the answer is that there aren't, but I'd like to make sure I'm understanding things correctly.","15","0.509081498211933","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","864090037","issue_comment","https://github.com/Trusted-AI/AIF360/issues/756#issuecomment-864090037","> @hildeweerts coming back to the API, though, are there metrics which would accept _three_ parameters - `labels`, `labels_true` and `labels_pred` ? It sounds like the answer is that there aren't, but I'd like to make sure I'm understanding things correctly.

You're right, there aren't. So that's a similar issue as metrics that don't use `y_true` (only `y_pred`) or that use `y_score` instead of `y_pred`. ","12","0.4546894119340676","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","864095837","issue_comment","https://github.com/Trusted-AI/AIF360/issues/756#issuecomment-864095837","@riedgar-ms  

> @kstohr I'm a bit confused by what you mean by:
> 
> > Note: MetricFrame requires y_pred (clf.predict). However, ROC curves and  AUC scores are generated using y_score (clf.predict_proba).  This method  substitutes y_score (type:float) for y_pred (type:int) to conform to the MetricFrame required params.
> 
> The _argument_ is obviously called `y_true` but its type is certainly not required to be `int`.

Yes. It's a nomenclature issue. When I see `y_pred`  I think it's the predicted labels. In fact, MetricFrame also allows you to pass other things in its place. Yes, it works. It's just not clear for the user. 

And, in fact, all I needed to do was split the data by sensitive feature.  I strongly feel like that should be a class/module on its own. Then you can use it in combination or with custom classes to compute metrics. 

I totally agree with @riedgar-ms but would frame it a little differently: 

> We've got two things going on here:

> * Where we have a precomputed metric where we just need to slice it up by the sensitive features
> * Where there are multiple 'columns' in `y_pred` and/or `y_true`

Another way to think about it is we have two tasks to perform: 
a) splitting the data by sensitive feature (universally needed and MetricFrame currently has a great approach) 
b) passing split data to various metrics. 

I think Fairlearn will be easier to maintain if we don't have to worry about how the data should be split and how it should be passed to a given metric in a single module/class. I suggest we just offer a class/module for splitting the data. And then worry about applying metrics to the split data in separate modules, which may import/depend on the module that splits the data such that we always handle that consistently. I feel the current module is trying to do too much in one class. 

","2","0.4330437314954131","Performance measurement","Validation"
"https://github.com/fairlearn/fairlearn","865021132","issue_comment","https://github.com/Trusted-AI/AIF360/issues/756#issuecomment-865021132","> Yes. It's a nomenclature issue. When I see `y_pred` I think it's the predicted labels. In fact, `MetricFrame` also allows you to pass other things in its place. Yes, it works. It's just not clear for the user.

I'm not really sure about how best to cope with that. One solution (which would of course break _everything_) would be to have our own names for the inputs - say `from_data` and `from_model` which should avoid the problem of existing conventions. There's then the question of how to route them to the underlying metric function. Currently we do that positionally - first argument is the 'true' column, the second is the 'predicted' column. Now since @MiroDudik 's recent PR moved `MetricFrame` itself in the direction of keyword-only arguments, one could criticise this as inconsistent too. But having to match arbitrary argument names to `MetricFrame` to arbitrary argument names in the underlying functions is... not going to be trivial or unconfusing.","15","0.3185250008579567","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","865842852","issue_comment","https://github.com/Trusted-AI/AIF360/issues/756#issuecomment-865842852","I think this is an important discussion and I'd love to hear thoughts from other @fairlearn/fairlearn-maintainers as well here!","20","0.6708724253208237","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","865854505","issue_comment","https://github.com/Trusted-AI/AIF360/issues/756#issuecomment-865854505","It's quite a long conversation here, could you maybe summarize what we've got so far here, for the rest of us to be able to understand the conversation?","25","0.6056292461986411","Research","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","865993066","issue_comment","https://github.com/Trusted-AI/AIF360/issues/756#issuecomment-865993066","### Summary

Per @adrinjalali 's request :-)

I think that we've got a few things going on here:

1. It seems that when users see `y_true` and `y_pred` they may think that `MetricFrame` only works for classification metrics, rather than more generally (really, they are `column_from_data` and `column_from_model` and even that meaning is only really imposed by the underlying metric function)
2. Sometimes the two 'columns' might actually need to be multiple columns
3. Some metrics only take one of these (e.g. `selection_rate()`) and users might not want to write a wrapper for this

@kstohr  has also suggested that the 'split up by sensitive and conditional features' part of `MetricFrame` may well work better as a separate piece of functionality.","15","0.5042689670426895","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","869655759","issue_comment","https://github.com/Trusted-AI/AIF360/issues/756#issuecomment-869655759","Pinging..... @adrinjalali  @hildeweerts  @MiroDudik  @michaelamoako @kstohr ","20","0.656647116324536","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","869697580","issue_comment","https://github.com/Trusted-AI/AIF360/issues/756#issuecomment-869697580","@riedgar-ms Yep. That sums it up. 

My reason for spitting out the `group_by` sensitive feature function is so that it can be used for descriptive stats and exploration. If you are running a metric and you are getting an error, it may be because a class is underrepresented, not represented at all, or is 'all true.' In these cases, we may need to know which class is impacted in order to handle it. In addition, it is quite common to inspect the data you are working with before passing it to a metric. 

Note:  It is ok, even preferred, if it is still a method in the class. My issue is that when you instantiate the class, it currently forces you to run a metric. It would be great to be able to perform the `group_by` function independently of running the metric. ","15","0.4172077922077922","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","876532796","issue_comment","https://github.com/Trusted-AI/AIF360/issues/756#issuecomment-876532796","# Summary

This is a second attempt to summarise the discussion to date, with the goal of reaching a conclusion. The previous summary was overly summarised.

## Current Status

As proposed at the the beginning of the thread, we are in the process of moving to requiring named arguments for `MetricFrame`. We have just released `v0.7.0` which issues a warning if positional arguments are used; with `v0.8.0` we plan to make named arguments mandatory. At that point, the signature of `MetricFrame` should be:
```python
metrics = {
  'recall': skm.recall_score,
  'accuracy': skm.accuracy_score
}

s_p = {
  'recall' : { 'sample_weights' : s_w }
  # No sample params for accuracy
}

mf = MetricFrame(metrics=metrics, y_true=y_t, y_pred=y_p,
                 sensitive_features=s_f,
                 control_features=c_f,
                 sample_params=s_p)
```
Note that the metric functions themselves are dispatched using positional arguments - the basic signature is assumed to be `metric_fn(y_true, y_pred)`. Now that we are requiring named arguments, this is an inconsistency.

## Functionality Gaps

The feedback above (and from other sources) has highlighted several gaps in the functionality of `MetricFrame`. Specifically:

- Some metrics don't take `y_pred` but `y_score` instead (often `y_pred` implies a label from a classification)
- Some metrics take more than two arguments, e.g. `inverse_propensity_score(*, actions_taken, rewards, propensities, actions_target)`
- We may have a precomputed metric (such as word error rate) and we just need it aggregated
- Some metrics only need one argument (such as `selection_rate` for which we have a custom function which ignores the `y_true` argument)
- A user may have multiple models which they wish to compare in a single `MetricFrame`

There are actually work arounds for all of these using the existing `MetricFrame`. These generally involve some combination of custom metric functions, making `y_true` and/or `y_pred` into a list of dictionaries, and filling extra arguments out via `sample_params`. The question is whether we can make the process easier.

### A note on precomputed metrics

In the above, one proposed solution for the precomputed metric problem was to have a static factory method `MetricFrame.from_precomputed()`. However, I don't think this is needed - the precomputed metrics are still per-sample, and need to be aggregated somehow. The aggregation function (such as `mean()` or `min()` is then the 'metric'. We should make sure this is documented, though.

## Proposed solutions

There are two basic ways to provide other columns - as `**kwargs` or a separate dictionary. In the following, we assume that the existing `y_true` and `y_pred` arguments get folded in eventually , although there would have to be intermediate releases (similar to the switch to named arguments).

### Providing `**kwargs`

In this case, the signature of `MetricFrame` would become
```python
mf = MetricFrame(metrics, 
                 sensitive_features=s_f,
                 control_features=c_f,
                 sample_params=s_p,
                 **direct_sample_params
)
```
and it would be invoked like:
```python
mf = MetricFrame(metrics=metrics,,
                 sensitive_features=s_f,
                 control_features=c_f,
                 sample_params=s_p,
                 y_true=y_t, y_score=y_s
)
```

However, `**kwargs` has the downside that, it is hard to add other required arguments in future, since we can be sure that whatever name we pick, we will break _someone_. Personally, I also think it makes documentation more difficult to read.

### Providing a dictionary

Rather than provide individual arguments, we can have a `direct_sample_params` dictionary. The signature would then be:
```python
mf = MetricFrame(metrics, 
                 sensitive_features=s_f,
                 control_features=c_f,
                 sample_params=s_p,
                 **direct_sample_params
)
```
and it would be invoked like:
```python
mf = MetricFrame(metrics=metrics,,
                 sensitive_features=s_f,
                 control_features=c_f,
                 sample_params=s_p,
                 direct_sample_params= { 'y_true'=y_t, 'y_score'=y_s }
)
```
This is somewhat clunkier, but does avoid the problems associated with adding extra arguments in future.

## Dispatching the Metric Functions

Regardless of how we accept the non-`y_true`/`y_pred` arguments, we then have to decide how to call the individual metric functions.

The main question is whether we are going to
1. Expect all of the metric function to accept all of the arguments passed in `direct_sample_params` (or `**direct_sample_params`); or
2. Use reflection (`inspect.signature()` I believe) to examine which functions want which arguments

Option (2) is obviously much more flexible; for example, one could do something like:
```python
metrics = {
  'metric_a' : metric_using_y_pred,
  'metric_b' : metric_using_y_score
  'metric_c' : metric_using_y_true_y_pred
  'metric_d' : metric_using_y_true_y_score
}

mf = MetricFrame(
                 metrics=metrics,,
                 sensitive_features=s_f,
                 control_features=c_f,
                 sample_params=s_p,
                 y_true=y_t, y_pred=y_p, y_score=y_s
)
```
(or similarly for the non-`**kwargs` option) and expect things to Just Work.

There are two problems with using reflection in this way. First, consider the example of a precomputed metric such as `word_error_rate`. We might want to use `numpy.mean()` as the aggregator.... but the argument for that is called `a` and not `word_error_rate`. Users would still have to write a wrapper function to make the names match.

Secondly, using reflection like this opens up a huge trap - a typo in the argument name can cause really subtle problems. If the metric function considers an argument to be mandatory (likely for something like `y_true` or `y_score`), then an error will be thrown. However, for an optional argument (think `sample_weight` vs `sample_weights` if a user were trying to pass the weights to every metric, rather than using `sample_params`) then the typo will simply stop the argument being passed, which might not be noticed for a long time (ask me about the value of `IMPLICIT NONE` and why zero initialisation is a travesty).

## Working with Streaming

PR #670 is working to add streaming to `MetricFrame`. I think that all of the above should be compatible with that, but we should keep it in mind.

Tagging @adrinjalali  @hildeweerts  @kstohr @michaelamoako  @MiroDudik ","2","0.4333617255241287","Performance measurement","Validation"
"https://github.com/fairlearn/fairlearn","877069394","issue_comment","https://github.com/Trusted-AI/AIF360/issues/756#issuecomment-877069394","> Note that the metric functions themselves are dispatched using positional arguments - the basic signature is assumed to be metric_fn(y_true, y_pred). Now that we are requiring named arguments, this is an inconsistency.

I personally don't mind this inconsistency.

As for the rest of the proposal, I find it way too complicated, and not very user friendly, and hard to explain to users.

I would rather settle for a middle ground where we leave `MetricFrame` unchanged, and have a `DatasetMetricFrame` which accepts only one `y`. An API which supports all possible cases is an overly complicated one which is designed to handle 100% of the cases, we could settle for 95% and have a much simpler API, and have the other 5% use-cases do the appropriate workarounds.

We should of course accept extra input required by the metric. For example, in the case of `inverse_propensity_score`, if I understand correctly, we can have something like:

    inverse_propensity_score(
        y_pred=actions_taken,
        y_true=actions_target,
        rewards=rewards,
        propensities=propensities
    )
    
And with [SLEP006](https://github.com/scikit-learn/scikit-learn/pull/20350) sklearn could accept something like:

    make_scorer(inverse_propensity_score, score_params=[""rewards"", ""propensities""])

to get a scorer. Note that this has removed the need for us to do any introspection, but we have a scorer object which is a callable but also has a state.

An alternative here, since we don't actually do slicing to re-call a scorer, is to have an object which only accepts precomputed scores, like

    PrecomputedMetricFrame(
        auc=auc_score(...),
        propensity=propensity_score(....),
        ...,
        sensitive_attributes=...
    )

Or something like that.","30","0.3957110692798878","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","884919612","issue_comment","https://github.com/Trusted-AI/AIF360/issues/756#issuecomment-884919612","First, my apologies @adrinjalali  for not responding sooner.

For `MetricFrame` itself, I've very happy with leaving it with its current form (as I mentioned above).

I'm not quite sure what you're describing with `make_scorer()`. Are you saying that users would make a call along the lines of:
```python
my_metric_fn = make_scorer(inverse_propensity_score, score_params=[""rewards"", ""propensities""])
```
and then pass `my_metric_fn` to `FlexibleMetricFrame` (or whatever we call it... I'm not a huge fan of that name). This object would provide the `get_metadata_request()` method which is [mentioned in SLEP006](https://scikit-learn-enhancement-proposals.readthedocs.io/en/latest/slep006/proposal.html#solution-each-consumer-requests), and which `FlexibleMetricFrame` could then use to route the arguments. So the user's invocation would be something like:
```python
func_dict = { 'recall' : recall_score,
            'inv_propensity': my_metric_fn }

flex_mf = FlexibleMetricFrame(metrics=func_dict,
                              y_true=y_t, y_pred=y_p, rewards=rewards, propensities=propensites,
                              sensitive_features=sf)
```
Since `recall_score` won't have `get_metadata_request()` it will only be passed `y_true` and `y_pred` (positionally). In contrast, `my_metric_fn` will respond that it wants `rewards` and `propensities` as well, and be passed them. We avoid the 'hole' of dropped arguments due to spelling errors because `make_scorer()` can check that its `score_params` all appear in the signature of the underlying function, and similarly `FlexibleMetricFrame` can check its `**kwargs` list against the results of `get_metadata_request()`.

Presumably, `FlexibleMetricFrame` (or its improved cognomen) would _not_ accept the `sample_params=` dictionaries of `MetricFrame` because that would be taken care of through `get_metadata_request()` ?","2","0.3835663393972042","Performance measurement","Validation"
"https://github.com/fairlearn/fairlearn","894346180","issue_comment","https://github.com/Trusted-AI/AIF360/issues/756#issuecomment-894346180","Pinging @adrinjalali  .... did I get what you meant by `make_scorer()` ?","20","0.4531218222493391","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","895141380","issue_comment","https://github.com/Trusted-AI/AIF360/issues/756#issuecomment-895141380","So what I was trying to say, is that handling the metadata routing is really not that easy, and most APIs we've tried to design are messy. The API in sklearn applies to scorers i.e. `scorer(estimator, X, y)`, rather than scoring functions i.e. `score(y_true, y_pred)`. Adding `get_metadata_request()` is certainly a way forward, but it's not an easy one.

Alternatively, we could let `AlternativeMetricFrame` to accept a scorer instead of scoring functions, and then it will be exactly in the scope of SLEP006, which would make things much easier.

I guess for now, we could have something like:

``` python
metrics = AlternativeMetricFrame(
    metrics={'roc': roc_func, 'inverse_propensity': inverse_propensity_score}
    y_true=y_true,
    y_pred=y_pred,
    rewards=rewards,
    propensities=propensities,
    routing={'inverse_propensity': ['rewards', 'propensities']}
)
```

We could also accept a scorer instead of the scoring method, but the user would also need to pass the estimator. I kinda would prefer that if we're gonna do routing.","30","0.3782149513356803","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","910093509","issue_comment","https://github.com/Trusted-AI/AIF360/issues/756#issuecomment-910093509","I don't have a lot to add to this discussion, but I agree that leaving `MetricFrame` as is and adding an alternative more flexible version would be preferable.

pinging @fairlearn/fairlearn-maintainers to move the discussion forward","15","0.4602724492632747","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","824298407","issue_comment","https://github.com/Trusted-AI/AIF360/issues/755#issuecomment-824298407","I believe that those are the `_mean_underprediction` and `_mean_overprediction` metrics:
https://github.com/fairlearn/fairlearn/blob/2d795006c045bdaeab1a22cd1893d5c63d27f099/fairlearn/widget/_fairlearn_dashboard.py#L135
You can obviously call them directly (or hand them to a `MetricFrame`). I don't recall offhand why we put the underscore prefix on them.","12","0.5236920351742901","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","826153894","issue_comment","https://github.com/Trusted-AI/AIF360/issues/755#issuecomment-826153894","Thanks for the reply. I got the correct values through MetricFrame.
Earlier, I tried calling them directly like, for a protected attribute named 'Gender' I want the individual mean overprediction and the individual mean underprediction (for values 0 and 1).

print(_mean_overprediction(ground_truth, predicted_outcomes))
print(_mean_underprediction(ground_truth, predicted_outcomes))

But according to this code, the output would be for overall gender variable and not for individual values (0 and 1).","12","0.6360722610722613","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","845542142","issue_comment","https://github.com/Trusted-AI/AIF360/issues/755#issuecomment-845542142","I'm not sure I understand whether there's anything else left you need @adidutt . Can you confirm?","24","0.3651867512332628","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","846341771","issue_comment","https://github.com/Trusted-AI/AIF360/issues/755#issuecomment-846341771","Yes @romanlutz I got my answer to this issue. ","20","0.4555903866248696","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","823865180","issue_comment","https://github.com/Trusted-AI/AIF360/issues/753#issuecomment-823865180","If you're passing a dictionary,  `_process_feature()` will try to convert the dict to a pandas dataframe using ` df = pd.DataFrame.from_dict(features)`. This only works if you're using 1d array (which is also specified in the API), but  I suppose the confusing part is that pandas actually requires the array to have shape `(length,)` rather than `(length,1)` (is there some technical term for the former vs the latter?). In the pandas docs they call this ""array-like"", not sure whether they mean the exact same thing with this term as sklearn.

I am not sure whether we should try to internally deal with this by converting the array, as this might result in weird behavior when people use an ""actual"" multidimensional array. However, I do think we should be clearer in our docs (e.g., replace ""dict of 1d arrays"" with ""dict of array-like"") and try to provide a more meaningful error description.","17","0.5552302500992459","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","823871576","issue_comment","https://github.com/Trusted-AI/AIF360/issues/753#issuecomment-823871576","> If you're passing a dictionary,  `_process_feature()` will try to convert the dict to a pandas dataframe using ` df = pd.DataFrame.from_dict(features)`.

I understand why this happens, but from the users' perspective, this is irrelevant. From the users' perspective, there's no reason for one of those variations to work and not the other. We should do the right validations to make sure those odd potential behaviors you mention don't happen, but I don't see why we'd force the user to import pandas when they don't need to, and to me what I have is a completely legit way of working with the `MetricFrame` and comes quite intuitively to me.","15","0.5362757265487641","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","823912336","issue_comment","https://github.com/Trusted-AI/AIF360/issues/753#issuecomment-823912336","I don't think they'd need to import pandas, right? in your current code replacing `reshape (-1,1)` by `reshape(-1,)` would solve the issue. Regardless, I agree that this is a pretty intuitive way to define an array and we shouldn't bother the user with it. I was mostly thinking that there's probably a reason pandas does not accept this type of shape directly (but I'm probably being too naive here, lol).","17","0.3941346454233053","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","823918775","issue_comment","https://github.com/Trusted-AI/AIF360/issues/753#issuecomment-823918775","> in your current code replacing `reshape (-1,1)` by `reshape(-1,)` would solve the issue.

That's true, which I didn't try, and then `ValueError: If using all scalar values, you must pass an index` is really a misleading error message lol.","17","0.7562969140950788","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","823921644","issue_comment","https://github.com/Trusted-AI/AIF360/issues/753#issuecomment-823921644","> then `ValueError: If using all scalar values, you must pass an index` is really a misleading error message lol.

Yeah, it's not very informative lol. Even as a frequent pandas user I am often still confused by some of their error/warning messages(`SettingWithCopyWarning` gets me every time )","17","0.8222284961415397","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","824027968","issue_comment","https://github.com/Trusted-AI/AIF360/issues/753#issuecomment-824027968","So what is the fix we need here? We could certainly do something to address this particular case. However, you then start getting into worrying about whether a vector of length n is the same as an (n,1) array, a (1,n) array or a (1,1,n,1,1,1) array. I'd prefer to delegate that to pandas... perhaps we could put a try/catch around the conversion, so that we can at least be explicit to the user as to what we tried to do, and embed the error from pandas?","17","0.4313797463191811","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","824036522","issue_comment","https://github.com/Trusted-AI/AIF360/issues/753#issuecomment-824036522","I think applying a `reshape(-1,)` on everything before passing to pandas would make sense. It doesn't matter what the original shape is, as long as there are only `n` items there.","17","0.4767942583732057","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","824049482","issue_comment","https://github.com/Trusted-AI/AIF360/issues/753#issuecomment-824049482","You mean iterate over the dictionary, check if each value is a numpy array (because it might be a list), and if so call reshape? What do we do if some of the values are lists?","17","0.6937849835939012","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","824053145","issue_comment","https://github.com/Trusted-AI/AIF360/issues/753#issuecomment-824053145","There may be a few very specific scenarios in which this could lead to unexpected results e.g., if the user accidentally passes the ""wrong"" array that happens to be of shape (n/2, 2), they wouldn't receive any notification whatsoever that they actually passed a multidimensional array. I admit this is quite an unlikely scenario, but I like @riedgar-ms 's idea of at least letting the user know we tried to do some conversions.","17","0.3820302622456056","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","824089203","issue_comment","https://github.com/Trusted-AI/AIF360/issues/753#issuecomment-824089203","Checking and correcting the number of dimensions is not really something odd. I agree maybe we don't want to do a `reshape` on everything, but doing certain dimension changes is not that odd, eg.:

https://github.com/scikit-learn/scikit-learn/blob/114616d9f6ce9eba7c1aacd3d4a254f868010e25/sklearn/decomposition/_nmf.py#L95

> What do we do if some of the values are lists?

Passing anything to `asarray` or sklearn's `check_array` fixes those inconsistencies. We can either use them or have our own validation which accepts and corrects all those issues. I really don't think changing from `(n, 1)` to `(1, n)` is something we need to warn the user about. Users very quickly get warning fatigue and ignore those warnings.

","30","0.3295797029974245","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","824124196","issue_comment","https://github.com/Trusted-AI/AIF360/issues/753#issuecomment-824124196","If we limit the reshape to a limited set of straightforward shapes (like the (n,1) and (1,n)), I agree a warning wouldn't be necessary.

Would it make sense to still replace the pandas ValueError message with a more informative one (e.g., a suggestion to check the dimensions of the array)?","17","0.4585627138818628","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","824129841","issue_comment","https://github.com/Trusted-AI/AIF360/issues/753#issuecomment-824129841","> Would it make sense to still replace the pandas ValueError message with a more informative one (e.g., a suggestion to check the dimensions of the array)?

Yeah that error really puzzled me. A more verbose message with a suggested solution would probably help.","17","0.470564896096811","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","824139947","issue_comment","https://github.com/Trusted-AI/AIF360/issues/753#issuecomment-824139947","I'd really rather wrap [the call to `DataFrame.from_dict()`](https://github.com/fairlearn/fairlearn/blob/2d795006c045bdaeab1a22cd1893d5c63d27f099/fairlearn/metrics/_metric_frame.py#L522) in a try/catch block which tells the user exactly what we tried to do, and passes on the pandas error. The whole point of using that call was so that we didn't need to do any processing ourselves.","17","0.4624533434554311","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","824156698","issue_comment","https://github.com/Trusted-AI/AIF360/issues/753#issuecomment-824156698","Sure, as long as the error message is clear enough and gives the user a solution, I'd be happy with that.","5","0.2141701714655451","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","824148827","issue_comment","https://github.com/Trusted-AI/AIF360/issues/752#issuecomment-824148827","Hi! Thank you for your question. 

It is possible to measure disparities for multi class classification problems, as long as the underlying metric supports multi class classification.

'Standard' group fairness metrics such as equalized odds (=differences in FPR and FNR) and demographic parity (=differences in selection rate) can be a bit tricky to compute for multi class classification scenarios, as ""positives"" and ""negatives"" are not directly defined. One way to deal with this is to view the problem as multiple binary classification problems and aggregate the results (see for example the different options for the 'average' parameter of [precision_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html) documentation). We currently do **not** provide multi class false_positive_rate/FNR/selection_rate out of the box in Fairlearn.

However, scikit-learn does provide some performance metrics that can be applied to multi class classification problems, such as `accuracy_score` and `precision_score`. This may be relevant for you if you're trying to identify potential cases of quality of service harm (i.e., the model does not work equally well for all groups). You can use fairlearn's `MetricFrame` or `make_derived_metric` to explore disparities of these performance metrics across sensitive groups.  

For example, if we are interested to see if the model is less accurate for some groups compared to others, we can use scikit-learn's `accuracy_score` in combination with fairlearn's `MetricFrame` as follows:

```python
from sklearn.metrics import accuracy_score
from fairlearn.metrics import MetricFrame

# some random numbers
y_true = np.array([0,1,2,1,0,0,1,1])
y_pred = np.array([0,1,1,2,0,0,1,0])
sensitive_features = np.array([0,0,0,0,1,1,1,1])

# compute metric frame
mf = MetricFrame(metric=accuracy_score, 
                  y_true=y_true, y_pred=y_pred, 
                  sensitive_features=sensitive_features)
# print results
print(mf.by_group) # series with accuracy for each sensitive group
print(mf.difference()) # difference in accuracy between the two sensitive groups
```

Which will output:
```
sensitive_feature_0
0     0.5
1    0.75
Name: accuracy_score, dtype: object

0.25
```
Similarly, you could explore other performance metrics in a disaggregated way, such as recall, precision, etc. 

Alternatively, you can explicitly decompose your multi-class problem in multiple binary problems and compute the relevant metrics (e.g., `equalized_odds_difference`) for each class separately. So for example, you can compute the FPR for class 0 by replacing all 0's in `y_true`and `y_pred` by 1 (indicating that the instance belongs to class 0), and all 1's and 2's by 0 (indicating these do not belong to class 0).

Does this answer your question? Please let me know if anything is still unclear.","12","0.4767078579597756","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","824165511","issue_comment","https://github.com/Trusted-AI/AIF360/issues/752#issuecomment-824165511","@fairlearn/fairlearn-maintainers do we want to add explicit mullticlass support for e.g., fpr and fnr? I am a bit worried that e.g., something like macro/weighted/etc. averaging would compress too much information in a single number. Alternatively we could write a section in the user guide to explain to people how they can compute a fairness metric for each class separately. wdyt?","15","0.4620210919770392","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","824588097","issue_comment","https://github.com/Trusted-AI/AIF360/issues/752#issuecomment-824588097","We're facing the same in our chest x-ray classification use case and will just treat it as separate binary classification problems. I share your concern about condensing too much information into a number and I'm extremely (positively) surprised that we basically already support the accuracy score case!

A user guide section seems like the right choice. If others have ideas on how to handle this I'm more than willing to listen, though.","15","0.4627354627354627","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","824605115","issue_comment","https://github.com/Trusted-AI/AIF360/issues/752#issuecomment-824605115",">  I'm extremely (positively) surprised that we basically already support the accuracy score case!

The wonders of being scikit-learn compatible :D","15","0.4120625137574288","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","824640015","issue_comment","https://github.com/Trusted-AI/AIF360/issues/752#issuecomment-824640015","I'm happy to add fairness related metrics for multiclass cases, but metrics in those situations are tricky business. I'd be happy to include anything which is already at least half established in the community, rather than inventing our own ways of computing them.

For MetricFrame, as @hildeweerts mentions, as long as the underlying metric supports it, it's supported, I'd be happy with adding at least a section to the user guides explaining how users could use it for multi-class case.","15","0.5224947373574189","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","824896594","issue_comment","https://github.com/Trusted-AI/AIF360/issues/752#issuecomment-824896594","The idea of better inter-operability with confusion matrix has come up. One way how to enable it would be to have a bit of support for metrics that output pandas series (these would be automatically treated as multiple separate metrics).

Currently, we can do the following:
```python
import numpy as np
from sklearn.metrics import accuracy_score, confusion_matrix
from fairlearn.metrics import MetricFrame
import pandas as pd

# some random numbers
y_true = np.array([0,1,2,1,0,0,1,1])
y_pred = np.array([0,1,1,2,0,0,1,0])
sensitive_features = np.array([0,0,0,0,1,1,1,1])

# pandified confusion matrix with clarifying column annotations 
def confusion_matrix_pd(y_true, y_pred):
    matrix = confusion_matrix(y_true, y_pred, normalize='all')
    return pd.DataFrame(matrix).rename(
               index={0: ""true=0"", 1: ""true=1"", 2: ""true=2""},
               columns={0: ""pred=0"", 1: ""pred=1"", 2: ""pred=2""}).stack()

# construct metric frame
mf = MetricFrame(metric=confusion_matrix_pd, 
                  y_true=y_true, y_pred=y_pred, 
                  sensitive_features=sensitive_features)

# mf.by_group needs to be transformed to work properly
pd.DataFrame(mf.by_group.to_dict()).T.rename_axis(index='sensitive_feature_0')
```
Which outputs this:
```
                    true=0               true=1               true=2
                    pred=0 pred=1 pred=2 pred=0 pred=1 pred=2 pred=0 pred=1 pred=2
sensitive_feature_0
0                     0.25    0.0    0.0   0.00   0.25   0.25    0.0   0.25    0.0
1                     0.50    0.0    NaN   0.25   0.25    NaN    NaN    NaN    NaN
```
However, `mf.difference()` etc. will throw errors. It might be nice to basically enable API such that we would obtain the above output without needing to the the transformation step and with `mf.difference()` etc. working columnwise.
","12","0.4430089872105081","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","833257663","issue_comment","https://github.com/Trusted-AI/AIF360/issues/752#issuecomment-833257663","@hildeweerts  - If we are treating multi-class problem as multiple binary problem the could you help me understand how a new sample will be classified ? Are we going to take average or weighted of multiple binary result metric to classify?","23","0.3865496205921738","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","833283274","issue_comment","https://github.com/Trusted-AI/AIF360/issues/752#issuecomment-833283274","Hi @kumarnav03. Good question. What I meant was that you can treat it as multiple binary classification problems during the *evaluation* of the classifier. You can still use a multi class classifier to classify the samples. 

Does that make sense? Let me know if anything is still unclear!","23","0.4225561672370182","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","876107033","issue_comment","https://github.com/Trusted-AI/AIF360/issues/752#issuecomment-876107033","Action item: add a short example like the one shown above by @hildeweerts to the user guide section on metrics with grouping: https://fairlearn.org/v0.6.2/user_guide/assessment.html#metrics-with-grouping","14","0.4564531513684052","Documentation","Development"
"https://github.com/fairlearn/fairlearn","914076290","issue_comment","https://github.com/Trusted-AI/AIF360/issues/752#issuecomment-914076290","Hi @romanlutz. I did this simple action item as my first issue, and I am running into some issues. I think I have to create a branch and then a pull request, but I don't have write access to fairlearn. Is this necessary or am I doing this wrong? :)

EDIT: SORRY! I read over https://github.com/romanlutz/fairlearn-pycon-sprint a bit too quickly - I do need to fork :)","20","0.3724355354790137","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","914205674","issue_comment","https://github.com/Trusted-AI/AIF360/issues/752#issuecomment-914205674","No worries! I admit it's something to get used to and I really should take some time to integrate the sprint instructions into our contributor guide. I'm glad you were able to figure it out! Looks good! I added a few comments but nothing major, so we should be able to merge this pretty soon.","20","0.7166096315032485","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","842560364","issue_comment","https://github.com/Trusted-AI/AIF360/issues/751#issuecomment-842560364","That would be everything under `examples`, `fairlearn`, and the test directories if I'm not mistaken.
Whoever takes this up may want to look into `flake8` documentation. We have a `setup.cfg` file in the repository's root directory that configures what `flake8` does. Perhaps one can restrict which directories it should consider.","21","0.5481807696340568","Installation and shell commands","Deployment"
"https://github.com/fairlearn/fairlearn","842863264","issue_comment","https://github.com/Trusted-AI/AIF360/issues/751#issuecomment-842863264","Hi @romanlutz am I able to take this up? Was looking at the `setup.cfg` file and one solution is to state the directories that `flake8` should ignore. Would that be an acceptable solution? https://flake8.pycqa.org/en/latest/user/options.html

e.g. in setup.cfg

```
[flake8]
exclude =
   .git/
    build/
    docs/
    notebooks/
```","21","0.7140504423113119","Installation and shell commands","Deployment"
"https://github.com/fairlearn/fairlearn","842864573","issue_comment","https://github.com/Trusted-AI/AIF360/issues/751#issuecomment-842864573","That's great! I was thinking about this earlier today and I think that is indeed the best solution. Thanks @bthng !","30","0.6039464411557435","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","843671125","issue_comment","https://github.com/Trusted-AI/AIF360/issues/751#issuecomment-843671125","Thank you for the guidance, raised a PR!","5","0.4039048200122028","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","818998140","issue_comment","https://github.com/Trusted-AI/AIF360/issues/742#issuecomment-818998140","For whoever wants to contribute, you can do a `git grep fairlearn.github.io` to find instances where you need to change.","32","0.2722385141739981","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","819181047","issue_comment","https://github.com/Trusted-AI/AIF360/issues/742#issuecomment-819181047","@riedgar-ms & @riedgar-ms    #743 is the pull request I did.  First pull request I've done, thanks for posting this as a first issue it was a good way to learn how pull requests work.  Would you let me know if I did anything wrong?
","31","0.4777630163851457","Testing","Maintenance"
"https://github.com/fairlearn/fairlearn","819204605","issue_comment","https://github.com/Trusted-AI/AIF360/issues/742#issuecomment-819204605","Hey @BrandonGoding , in order to link your Pull Request to the issue (and have the issue automatically close when the Pull Request is approved) you should include a reference in your commit/PR to the issue number (the same way you referenced your Pull Request in your comment with a #) with a word like 'fix', 'close', or 'resolve' before it. Check out this link for more details:

https://docs.github.com/en/github/managing-your-work-on-github/linking-a-pull-request-to-an-issue

It also looks like you failed one of the tests that was setup related to ""signing off"" your commit - you can see the test states on the page for your Pull Request #743 and click on 'details' to see what went wrong. Try cancelling your current Pull Request, fixing that issue and then creating a new one!","31","0.4177691972820019","Testing","Maintenance"
"https://github.com/fairlearn/fairlearn","819488536","issue_comment","https://github.com/Trusted-AI/AIF360/issues/742#issuecomment-819488536","I ended up closing the last pull request and opening another.  I went back through and made the fixes suggested in comments","20","0.3224082934609251","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","817757501","issue_comment","https://github.com/Trusted-AI/AIF360/issues/738#issuecomment-817757501","We've had this discussion a while ago, although I'm having trouble tracking down where exactly. What I remember from that time:
- CHANGES.md is meant for ALL changes to the package (excluding, for example, fixing a build pipeline since that's not going on PyPI) made, regardless of how small. This is mainly for developers, not users.
- The website should have something similar, but reduced to just the most important changes for users. Example in scikit-learn: https://scikit-learn.org/stable/auto_examples/release_highlights/plot_release_highlights_0_24_0.html
- [upgrading instructions](https://fairlearn.org/v0.6.0/user_guide/migrating_versions/index.html): this is somewhat related and we've neglected that again in the last few releases

I'm definitely not saying that we need to stick with that system, but I do see the point of the separation into ALL changes and a well-described set of important changes to users. It's basically serving a different audience (at least in my mental model). Wdyt? @fairlearn/fairlearn-maintainers ","32","0.5612346169114729","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","817764304","issue_comment","https://github.com/Trusted-AI/AIF360/issues/738#issuecomment-817764304","I like the idea of release highlights a lot (with the added bonus that it's very tweet-able #fairlearn)!

Upgrading instructions sound like something that we probably should be doing (although they also don't sound very fun to be doing, lol). Ideally we don't add too many breaking changes which will hopefully make future upgrading instructions very minimal.","32","0.5005147486683674","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","817771396","issue_comment","https://github.com/Trusted-AI/AIF360/issues/738#issuecomment-817771396","Upgrading instructions could go into the release highlights, to keep it simple if we want.

The changelog is something like the one you have here:
https://scikit-learn.org/stable/whats_new/v0.24.html

It does include everything which changes something for the users, but it's still different from the release highlights, and it doesn't have to include CI commits for instance.

The idea is that the changelog is updated at the same time as the PRs usually, if the PR touches on something that users should know about. But not all users care about all those changes, hence the existence of the release highlights.","32","0.8535403689326232","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","886507312","issue_comment","https://github.com/Trusted-AI/AIF360/issues/738#issuecomment-886507312","I've started taking a stab at this. My upcoming PR moves the previously called CHANGES.md to the website (it's a lot harder than it sounds! 🤣 ). From there we can add a line per PR, and then distill that into highlights (basically a kind of TL;DR section I'm thinking...).

Open questions:
- Where should this live? You can tell I just merged it with the installation and migration guide.
- Should this be linked from other pages?","20","0.5344968793244657","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","816966485","issue_comment","https://github.com/Trusted-AI/AIF360/issues/737#issuecomment-816966485","I like this a lot. The `lambda x, y: len(x)` isn't super straightforward if you're not familiar with how to write a metric function. Should we just define the `count` function like we did with `selection_rate` so that people can import it and pass it without having to write the code themselves?

Totally agree on documenting this. I guess the tasks here would be
- [ ] to look at the user guide of `MetricFrame` and add that new `count` metric wherever we have 2 or more metrics passed to the `MetricFrame`, and write a short sentence or two on what this does (i.e., ""show the number of people in the corresponding group"")
- [ ] to look through notebooks and add `count` whenever we have a `MetricFrame`.","15","0.5194578545363887","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","817590737","issue_comment","https://github.com/Trusted-AI/AIF360/issues/737#issuecomment-817590737","I like the convenience of a separate `count` function, it makes it more beginner friendly I think. Do we need to open a separate issue for that?","15","0.6172386691118674","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","817614546","issue_comment","https://github.com/Trusted-AI/AIF360/issues/737#issuecomment-817614546","I think the same issue/pr could do both the doc and the function.","30","0.4039048200122027","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","821566124","issue_comment","https://github.com/Trusted-AI/AIF360/issues/737#issuecomment-821566124","To clarify the `count` function proposal. I'm assuming we're just talking about adding `fairlearn.metrics.count`, but not doing anything with the `MetricFrame` API right?

","15","0.7760657421674368","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","821785369","issue_comment","https://github.com/Trusted-AI/AIF360/issues/737#issuecomment-821785369","> To clarify the `count` function proposal. I'm assuming we're just talking about adding `fairlearn.metrics.count`, but not doing anything with the `MetricFrame` API right?

Correct :)","15","0.735086288897948","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","825155351","issue_comment","https://github.com/Trusted-AI/AIF360/issues/737#issuecomment-825155351","Hey I can take this issue up!","24","0.2833150784958013","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","825565904","issue_comment","https://github.com/Trusted-AI/AIF360/issues/737#issuecomment-825565904","Sure, it's yours @vamsidesu5 ","20","0.2833150784958013","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","842643334","issue_comment","https://github.com/Trusted-AI/AIF360/issues/737#issuecomment-842643334","I will begin working on this issue now. Thanks for your guidance @romanlutz ","13","0.2702297702297702","Artifact generation and benchmarking","Deployment"
"https://github.com/fairlearn/fairlearn","842653359","issue_comment","https://github.com/Trusted-AI/AIF360/issues/737#issuecomment-842653359","Thanks @stephenrobic ! @vamsidesu5 let me know if you want to do any other issues! I'd be happy to find you something.","20","0.7403805496828753","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","842857352","issue_comment","https://github.com/Trusted-AI/AIF360/issues/737#issuecomment-842857352","@romanlutz Hi - I was very busy the past 2 weeks and wasn't able to contribute. So sorry! Would you be able to find me another issue that would be interesting to work on?","20","0.656647116324536","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","850201596","issue_comment","https://github.com/Trusted-AI/AIF360/issues/737#issuecomment-850201596","Closing since @stephenrobic 's PR #812  was merged a week ago.","20","0.76014173998045","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","877651332","issue_comment","https://github.com/Trusted-AI/AIF360/issues/736#issuecomment-877651332","Sounds like a good issue to delve into the source code, I take it. :nerd_face:

","4","0.2574535679374389","Installation and shell commands","Deployment"
"https://github.com/fairlearn/fairlearn","877726790","issue_comment","https://github.com/Trusted-AI/AIF360/issues/736#issuecomment-877726790","Excellent! Reach out if you have questions 🙂 And if you post a PR it's always great to post it with a screenshot of what the result looks like. Otherwise you basically force the reviewers to build the docs themselves (since our CircleCI build isn't correctly picking up the branch version (see #819).","32","0.4049509176269739","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","877775883","issue_comment","https://github.com/Trusted-AI/AIF360/issues/736#issuecomment-877775883","Haven’t thought of the screenshot, that’s really good advice. 👍

To make sure I understood the task correctly: I’ll go through functions and methods and document which of them uses external dependencies, correct?

And what’s the difference between a soft and a hard dependency and should I mark them differently?","7","0.3813616221297165","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","877803327","issue_comment","https://github.com/Trusted-AI/AIF360/issues/736#issuecomment-877803327","Hard dependencies are the ones included in `setup.py` and are installed with `pip install fairlearn` on a fresh environment (along with their dependencies which we don't care about and don't document).

You don't need to document hard dependencies. And those are the `import` statements you see on top of the files. Soft dependencies are usually ones which are done in a try-except clause or as a part of `importorskip` statement. They are only required if the user uses certain functionalities, but are not installed by default when the user installs the package.","21","0.6677352183681299","Installation and shell commands","Deployment"
"https://github.com/fairlearn/fairlearn","877810835","issue_comment","https://github.com/Trusted-AI/AIF360/issues/736#issuecomment-877810835","Got it, thanks @adrinjalali! ","20","0.239138371668492","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","878390075","issue_comment","https://github.com/Trusted-AI/AIF360/issues/736#issuecomment-878390075","With a bit of thinking, I came up with the following way of identifying the soft imports:

```shell
grep --exclude-dir .venv --include=""*.py"" -Er ""importorskip|^\s{4,}.*import"" fairlearn
```

Among a few false positives, this yields

| File                                              | Dependency |
| ------------------------------------------------- | ---------- |
| fairlearn/fairlearn/postprocessing/_plotting.py   | matplotlib |
| fairlearn/test/unit/postprocessing/test_plots.py  | matplotlib |
| fairlearn/test/unit/postprocessing/test_smoke.py  | lightgbm   |
| fairlearn/test/unit/reductions/test_smoke.py      | lightgbm   |
| fairlearn/test_othermlpackages/test_pytorch.py    | torch      |
| fairlearn/test_othermlpackages/test_tensorflow.py | tensorflow |
| fairlearn/test_othermlpackages/test_xgboost.py    | xgboost    |

Is this sufficient or should I check every source file manually due to possible false negatives? :thinking: ","4","0.6045584362597827","Installation and shell commands","Deployment"
"https://github.com/fairlearn/fairlearn","878394912","issue_comment","https://github.com/Trusted-AI/AIF360/issues/736#issuecomment-878394912","This is consistent with my mental model! We can skip the teat files, of course, so only 1 is left. There are several other PRs that will add matplotlib to the metrics module, so we'll need to keep in mind to update the table then!","30","0.3681936553030303","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","878401560","issue_comment","https://github.com/Trusted-AI/AIF360/issues/736#issuecomment-878401560","Apart from adding the table to the documentation somewhere sensible, should I also add the necessary `extras_require` to `setup.py`?

/edit: I realized there already is a `extras_require` key in the `setup.py`, referring to a list that currently only contains `customplots` and the corresponding `requirements_customplots.txt` that in turn only contains a pinned dependency on `matplotlib`. Is changing this too much of a change to incorporate it here?","21","0.4596396598868786","Installation and shell commands","Deployment"
"https://github.com/fairlearn/fairlearn","878719624","issue_comment","https://github.com/Trusted-AI/AIF360/issues/736#issuecomment-878719624","We actually don't want it to be a part of `setup.py` and there's a reason behind having it in a requirements file 🙂 This way you can just specify `pip install -r requirements-customplots.txt` without having to care about the contents. Long-term there is #645 to improve this structure a bit following scikit-learn's approach, but this particular issue here is really only about creating the table in the docs. 

Now the next question is where that ""sensible"" location in the docs is, of course. I don't see a related section in the website redesign Figma, so I think we need to choose one ourselves. Right now our installation instructions are in the ""Getting Started"" section, but we should have a more thorough set of installation instructions. ~~In fact, I just saw that we have a doc bug there~~ 🙁  [see UPDATE no.2 below]
![image](https://user-images.githubusercontent.com/10245648/125378246-3ebfc000-e343-11eb-9392-f1436b52afb4.png)

So I suppose the best thing to do is to add an installation section to the user guide which includes
- the full installation instructions (""basic"" installation, and with extras) as well as the overview table that indicates which methods/modules require which extras (I'm writing this very generically here but it's really just `matplotlib`).
- the existing version migration guide

And then the link from the ""Getting Started"" should point at that new section, of course instead of the ~~broken~~ `advanced_install`.

@fairlearn/fairlearn-maintainers any thoughts and/or objections? 🙂 

UPDATE: ~~Amazingly, the advanced installation section still exists! I somehow assumed it must have been removed since the link doesn't work, but it's still there! https://fairlearn.org/main/contributor_guide/development_process.html#advanced-installation-instructions. As it turns out the section tag is gone, so I'll quickly add it in my v0.7.0 PR #879 since I expect it to get merged first and I really want that broken link fixed for the v0.7.0 webpage.~~

UPDATE no.2: I confused myself a bit here... the `advanced_install` tag issue was there on v0.6.2 and has since been fixed already, so nothing to be done about that. Still, you can add a link to your more general installation section from the ""Getting Started"" page. ","14","0.4516413467294404","Documentation","Development"
"https://github.com/fairlearn/fairlearn","879713910","issue_comment","https://github.com/Trusted-AI/AIF360/issues/736#issuecomment-879713910","Okay! Now that @hildeweerts gave her thumbs up, I'll go ahead today and work on the PR. :muscle: ","20","0.8222350897510136","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","883432452","issue_comment","https://github.com/Trusted-AI/AIF360/issues/736#issuecomment-883432452","@romanlutz 

> We actually don't want it to be a part of setup.py and there's a reason behind having it in a requirements file slightly_smiling_face This way you can just specify pip install -r requirements-customplots.txt without having to care about the contents.

I'm wondering if `pip install -r requirements-customplots.txt` is something users are gonna be doing. We're passed the time when users would clone the repo and install from source. They should be ideally `pip install`ing the package, and most people shouldn't be looking at the repo at all.","21","0.6516082688163459","Installation and shell commands","Deployment"
"https://github.com/fairlearn/fairlearn","883484490","issue_comment","https://github.com/Trusted-AI/AIF360/issues/736#issuecomment-883484490","I was referring to contributors here because users can already pip install with the corresponding extra. Anyway, there's already another issue #645 to convert all of these into something more like what sklearn has.","21","0.296757382744644","Installation and shell commands","Deployment"
"https://github.com/fairlearn/fairlearn","815879757","issue_comment","https://github.com/Trusted-AI/AIF360/issues/734#issuecomment-815879757","Sounds good!","15","0.2659352142110763","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","817760359","issue_comment","https://github.com/Trusted-AI/AIF360/issues/734#issuecomment-817760359","If there are no objections, I'd like to go with our current repo state for `v0.6.1` - skipping #662 for now. Please let me know by 0800 EDT on Tuesday (2021-04-13) if you have any concerns. As usual, during a release, there will be some changes made without the usual review delays, as `main` gets its version bumped.","32","0.4236777715038584","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","818692995","issue_comment","https://github.com/Trusted-AI/AIF360/issues/734#issuecomment-818692995","Branch point tagged, branch cut, and PR out (#739) to update `main` for this release. I've also kicked off the _test_ release pipeline for v0.6.1 (which reminds me that that's still MS-only.... we need to fix that).","32","0.8426325447602044","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","818801145","issue_comment","https://github.com/Trusted-AI/AIF360/issues/734#issuecomment-818801145","Turned out that there were two problems:
- Update to `pydata-sphinx-theme` was breaking our docs
- I'd forgotten to update the path to `yarn` in the release pipeline.

PR #740 addresses those. Merged to `main`, being cherry-picked to the release branch (#741).

Also have #739, which updates `main` with the new version and the link from the static page to the versioned documentation.","32","0.5097691493995394","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","818984710","issue_comment","https://github.com/Trusted-AI/AIF360/issues/734#issuecomment-818984710","Published to Test-PyPI:
https://test.pypi.org/project/fairlearn/

I've just noticed that the links are to fairlearn.github.io. Since they're not broken, I've just opened an issue on it, flagged as being a Good First Issue.","32","0.4854178528500031","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","819097345","issue_comment","https://github.com/Trusted-AI/AIF360/issues/734#issuecomment-819097345","Now on PyPI:
https://pypi.org/project/fairlearn/

Normal PR procedures will now resume","26","0.5383675464320625","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","815884099","issue_comment","https://github.com/Trusted-AI/AIF360/issues/730#issuecomment-815884099","Agreed! I've always liked that scikit-learn consistently adds a ""Read more in the User Guide."" link at the bottom of the description.","14","0.4767942583732059","Documentation","Development"
"https://github.com/fairlearn/fairlearn","877828687","issue_comment","https://github.com/Trusted-AI/AIF360/issues/730#issuecomment-877828687","Can I work on this issue? As a first level, we can add a link at the end of each section to point read more in User Guide and Example Notebooks. After which we can add individual examples in places were it's required","20","0.414458486207365","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","878110294","issue_comment","https://github.com/Trusted-AI/AIF360/issues/730#issuecomment-878110294","Sounds good, @kurianbenoy! Let us know if you need help finding the appropriate section in the user guide for each class/function.

I think we can split this up into several issues, at least one for links to the user guide and another for adding examples. Could you open a new issue for adding examples, @kurianbenoy? You can link the current issue in the description.","14","0.6321809645753306","Documentation","Development"
"https://github.com/fairlearn/fairlearn","814431915","issue_comment","https://github.com/Trusted-AI/AIF360/issues/729#issuecomment-814431915","Have discussed separately with Sebastian. Although this particular function is going to be removed from Fairlearn, I would like to fix this here. The issue is that the function dictionaries are setup incorrectly.","15","0.4960925039872408","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","828831125","issue_comment","https://github.com/Trusted-AI/AIF360/issues/728#issuecomment-828831125","I had a similar issue with other combinations of optimization metric and constraints: 

I think because the constraints are related to parity (and not upperbounds) these types of solutions are **possible**. Commenting to track if progress is made on this, as I have the same question","28","0.3451819230365943","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","829034125","issue_comment","https://github.com/Trusted-AI/AIF360/issues/728#issuecomment-829034125","I've managed to trace this to `_merge_columns` in `_input_validation.py`. Evidently, if you use `np.apply_along_axis` to merge multiple text labels it cuts them off. Example:
```
>>> np.apply_along_axis(lambda row: "","".join([str(i) for i in row]), axis=1, arr=np.array([[""abc"", ""def""], [""abdddddddddddddddddddddddddddddddd"", ""danfaenofeinfaoieraj""], [""a"", ""b""]]))
array(['abc,def', 'abddddd', 'a,b'], dtype='<U7')
```
That's also what's happening in your example since some of the sensitive feature group names are fairly long. Clearly, that shouldn't be an issue, so I'm currently looking into what can be done. It seems like `np.apply_along_axis` may just not be terribly suitable for string operations. I'll dig a little deeper to figure out how to do this properly. Stay tuned!","17","0.3704350039083092","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","830594227","issue_comment","https://github.com/Trusted-AI/AIF360/issues/728#issuecomment-830594227","I have a fix and I'm sending out a PR momentarily.","32","0.6551092318534179","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","843388614","issue_comment","https://github.com/Trusted-AI/AIF360/issues/728#issuecomment-843388614","Closing this since #763 fixed the issue and v0.6.2 contains the fix.","24","0.6007728289607485","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","808108405","issue_comment","https://github.com/Trusted-AI/AIF360/issues/721#issuecomment-808108405","I will try to pick this up based on some of my existing course materials.","25","0.4716949716949717","Research","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","812585072","issue_comment","https://github.com/Trusted-AI/AIF360/issues/721#issuecomment-812585072","Also, instead of saying ""if this use case -> use this metric"", we could just add some information on each metric's documentation about when using it makes sense (just trying to make this issue so that it can be broken into smaller bits of work)","15","0.5153907496012758","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","809566279","issue_comment","https://github.com/Trusted-AI/AIF360/issues/720#issuecomment-809566279","I obviously prefer the first suggestion but I'm biased :P I'm happy either way.","32","0.3165933528836756","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","813975277","issue_comment","https://github.com/Trusted-AI/AIF360/issues/720#issuecomment-813975277","@fairlearn/fairlearn-maintainers do any of you have a preference for either pandas or scikit-learn format?","14","0.4334038054968286","Documentation","Development"
"https://github.com/fairlearn/fairlearn","814090638","issue_comment","https://github.com/Trusted-AI/AIF360/issues/720#issuecomment-814090638","So long as everything happens automatically, I'm not fussed.","20","0.7134532134532133","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","814359171","issue_comment","https://github.com/Trusted-AI/AIF360/issues/720#issuecomment-814359171","@riedgar-ms it depends a bit on what you mean with automated, I believe these solutions are actually not 100% automated, because you have to create the table with instructions and such (not sure though, I don't have a ton of experience with sphinx). However, I don't think we are adding new classes, etc. *that* often so maintenance-wise it shouldn't result in much overhead after the initial set up.","14","0.3565441650548033","Documentation","Development"
"https://github.com/fairlearn/fairlearn","815452257","issue_comment","https://github.com/Trusted-AI/AIF360/issues/720#issuecomment-815452257","I think creating pages per module is nice and simple (and consistent with what we're doing so far, not that that should matter...). It's not a hill I'm willing to die on, though, so either way is fine.

The overall idea with overview tables is fantastic and I'm wholeheartedly in support. :-)","20","0.4578392621870883","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","833323286","issue_comment","https://github.com/Trusted-AI/AIF360/issues/720#issuecomment-833323286","Not sure if it's related to this one, but one reason I prefer single page docs is that when I go [here](https://fairlearn.org/v0.6.1/api_reference/fairlearn.metrics.html), I don't even know how many items are there. It's quite confusing. This could be mitigated with another list on the right side maybe which would show the ToC of the page, cc @riedgar-ms 

![Screenshot from 2021-05-06 10-06-19](https://user-images.githubusercontent.com/1663864/117263909-07202d80-ae53-11eb-826b-e5a7c9edb138.png)
","14","0.6343402973837757","Documentation","Development"
"https://github.com/fairlearn/fairlearn","834166427","issue_comment","https://github.com/Trusted-AI/AIF360/issues/720#issuecomment-834166427","I don't know how hard it is to add the list on the right side since that depends mostly on the pydata sphinx theme. We can certainly move to separate pages, though, just like 
- sklearn: https://raw.githubusercontent.com/scikit-learn/scikit-learn/main/doc/modules/classes.rst
- pandas: https://github.com/pandas-dev/pandas/tree/master/doc/source/reference","14","0.3705953058771431","Documentation","Development"
"https://github.com/fairlearn/fairlearn","876104099","issue_comment","https://github.com/Trusted-AI/AIF360/issues/720#issuecomment-876104099","A first step would be to also enable the ""right sidebar""

![image](https://user-images.githubusercontent.com/10245648/124860099-f0d34280-df65-11eb-8f98-6772fd7f0bb2.png)


https://pydata-sphinx-theme.readthedocs.io/en/latest/user_guide/sections.html#the-right-in-page-sidebar","9","0.3313782991202347","Feature engineering methodology","Design"
"https://github.com/fairlearn/fairlearn","883579586","issue_comment","https://github.com/Trusted-AI/AIF360/issues/720#issuecomment-883579586","Let me help. :)","0","0.0303030303030302","Dataset usage","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","883625535","issue_comment","https://github.com/Trusted-AI/AIF360/issues/720#issuecomment-883625535","@shimst3r go ahead 🙂 ","20","0.582027168234065","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","885058890","issue_comment","https://github.com/Trusted-AI/AIF360/issues/720#issuecomment-885058890","@romanlutz if I understand the pydata sphinx documentation correct, I have to add a [my own template](https://pydata-sphinx-theme.readthedocs.io/en/latest/user_guide/sections.html#add-your-own-html-templates-to-theme-sections) for this, as [this issue](https://github.com/sphinx-doc/sphinx/issues/6316) points out there is no way of generating TOC items from functions etc. at the moment.

Any opinions how this should look like? :smile: ","14","0.398551802807122","Documentation","Development"
"https://github.com/fairlearn/fairlearn","885096241","issue_comment","https://github.com/Trusted-AI/AIF360/issues/720#issuecomment-885096241","You're not trying to change the layout I think, but rather to split pages into more pages and have overview pages. Correct me if I'm wrong!

That means you might be fine with just using another layer of index.rst per module (?)

It may be a good idea to compare with scikit-learn which already does what @hildeweerts is looking for. I'll try and check but may not get to it this week.","20","0.3657841299593878","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","899059082","issue_comment","https://github.com/Trusted-AI/AIF360/issues/720#issuecomment-899059082","Let me ping this to make clear I'm still on it, just didn't find the time to actually work on it. :sweat_smile: ","20","0.6270772238514176","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","922959125","issue_comment","https://github.com/Trusted-AI/AIF360/issues/720#issuecomment-922959125","@shimst3r any chance you've found some time to work on this?

As we're adding examples (e.g., #944) I think having individual pages becomes even more important!","14","0.3002357872674875","Documentation","Development"
"https://github.com/fairlearn/fairlearn","812527842","issue_comment","https://github.com/Trusted-AI/AIF360/issues/719#issuecomment-812527842","I would like to work on this!","0","0.0303030303030302","Dataset usage","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","812530905","issue_comment","https://github.com/Trusted-AI/AIF360/issues/719#issuecomment-812530905","That is awesome, @enharten! I have assigned the issue to you. Please feel free to ask questions if anything is unclear.","20","0.8274560552140622","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","813971678","issue_comment","https://github.com/Trusted-AI/AIF360/issues/719#issuecomment-813971678","@enharten I got an e-mail notification with a question from you but I don't see here on Github - do you still need some clarification?","7","0.4053030303030305","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","814097943","issue_comment","https://github.com/Trusted-AI/AIF360/issues/719#issuecomment-814097943","@hildeweerts I don't know why I naively thought that you wouldn't see it; I thought better of my question. I'll submit a PR shortly","20","0.6186778820230776","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","814100796","issue_comment","https://github.com/Trusted-AI/AIF360/issues/719#issuecomment-814100796","No worries, it was a good question - but great to hear you figured it out 👍","20","0.8044965786901274","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","806157403","issue_comment","https://github.com/Trusted-AI/AIF360/issues/716#issuecomment-806157403","Agreed! There's no to-do list other than the usual development process description: https://fairlearn.org/v0.6.0/contributor_guide/development_process.html

In general, you don't need to worry about anything unless the gated pipelines fail since they'd prevent you from merging the PR. If you want to quickly check what the new content looks like you can use the doc build command mentioned [here](https://fairlearn.org/main/contributor_guide/contributing_example_notebooks.html#formatting-of-example-notebooks) but it takes a while when running for the first time.","20","0.332867132867133","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","806170426","issue_comment","https://github.com/Trusted-AI/AIF360/issues/716#issuecomment-806170426","With to-do list I actually meant a list of changes we would like to make to the website content, rather than the development process - but I can totally see why you interpreted it differently, haha.","20","0.5749716181992836","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","806298024","issue_comment","https://github.com/Trusted-AI/AIF360/issues/716#issuecomment-806298024","Clearly, I'm too much in ""checklist mode"" these days :-)
I typically favor small changes over big ones (in part because the big ones tend to get stuck at PR stage due to many comments as I regularly demonstrate...) so writing issues for small chunks that need changing actually helps with that.","20","0.4732366932614151","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","807002343","issue_comment","https://github.com/Trusted-AI/AIF360/issues/716#issuecomment-807002343","Re: community call notes, I've been taking notes on our weekly calls [here](https://hackmd.io/@mmadaio/meeting-notes/edit), but we should probably move those onto something on GitHub if we want to make them more accessible and/or collaboratively editable","20","0.7227177832605809","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","807888804","issue_comment","https://github.com/Trusted-AI/AIF360/issues/716#issuecomment-807888804","@mmadaio I thought to have them editable collaboratively they needed to be on hackmd? We tried using the Wiki here, but it certainly doesn't have the same real-time edit functionality.","20","0.5653348774367883","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","808056598","issue_comment","https://github.com/Trusted-AI/AIF360/issues/716#issuecomment-808056598","@mmadaio @romanlutz I think the main question is: what would be the goal of putting them on the website? 

If the goal is to give people an idea of the kind of topics that are discussed, a paragraph on the website will also do. If the goal is for the community to make it easier to retrieve previous discussions, I'm not sure if it matters a lot whether it is on the website or in a hackmd (at least for those who have previously attended a call). If the goal is to keep us accountable, it makes sense to put them on the website but in that case they should probably be polished which can cause a lot of overhead. I'm not sure if that makes a lot of sense given the current level of maturity/scale of the project.

Thoughts?","20","0.8110047846889955","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","809562629","issue_comment","https://github.com/Trusted-AI/AIF360/issues/716#issuecomment-809562629","One of the purposes of having the notes is for an outsider to understand what happens in the community, and for that I don't think it matters where we put things.","20","0.8339123721289331","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","809607298","issue_comment","https://github.com/Trusted-AI/AIF360/issues/716#issuecomment-809607298","Agreed. I don't think anyone has the time to polish the notes. I did that the first few months and it wasn't really worth the overhead. How about we take the notes that are on the webpage already, put them into @mmadaio 's hackmd, remove them from the website, and link to @mmadaio 's hackmd for those interested in the notes?","20","0.6521641824448322","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","816292655","issue_comment","https://github.com/Trusted-AI/AIF360/issues/716#issuecomment-816292655","Hey I'm new and would love to contribute to this project - would like to pick this issue up! ","20","0.6122922776148584","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","816483607","issue_comment","https://github.com/Trusted-AI/AIF360/issues/716#issuecomment-816483607","Hi @vamsidesu5, welcome to the project! I have assigned the issue to you. Do you have enough information to get started? Please let us know if you have any questions.","20","0.5679374389051809","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","818312391","issue_comment","https://github.com/Trusted-AI/AIF360/issues/716#issuecomment-818312391","Yup going through codebase now and trying to understand where to make edits! Where's the best place to ask any questions? On this issue or in Gitter?","11","0.5142887954275853","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","818322180","issue_comment","https://github.com/Trusted-AI/AIF360/issues/716#issuecomment-818322180","@hildeweerts Hey I'm running into this issue when pushing to git, any permissions need to be changed for me:
<img width=""1172"" alt=""image"" src=""https://user-images.githubusercontent.com/24753774/114477775-d25de500-9bca-11eb-8bb1-b8efaff55ef7.png"">
","29","0.3434665122563212","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","818494812","issue_comment","https://github.com/Trusted-AI/AIF360/issues/716#issuecomment-818494812","@vamsidesu5 if the questions are related to this topic/issue then this is the best place :-)
If they're unrelated then it depends:
- if it's something that might be generally useful (even to other contributors or users) then perhaps a [""discussion""](https://github.com/fairlearn/fairlearn/discussions) is the best place
- if it's probably specific to your situation then Gitter might be best

The error you encountered is a common one and related to our [dev process](https://fairlearn.org/v0.6.0/contributor_guide/development_process.html#development-process). Essentially, the `fairlearn/fairlearn` repo is access-restricted, so everyone works off of their own fork and creates pull requests from their fork to the `fairlearn/fairlearn` repo. If you have questions about this process I'd be happy to help you out, just let me know.","32","0.2874806458748064","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","820683084","issue_comment","https://github.com/Trusted-AI/AIF360/issues/716#issuecomment-820683084","@romanlutz @hildeweerts Thanks for all the help - updating code now and will make a PR soon! ","20","0.6328671328671329","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","820706772","issue_comment","https://github.com/Trusted-AI/AIF360/issues/716#issuecomment-820706772","@romanlutz @hildeweerts I made the edits, but I want to be able to test the website. Is there any command I can run locally to see how the website looks with the changes?","20","0.2355661881977672","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","820744953","issue_comment","https://github.com/Trusted-AI/AIF360/issues/716#issuecomment-820744953","Here is the current PR: https://github.com/fairlearn/fairlearn/pull/745","32","0.5761643278421802","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","820930372","issue_comment","https://github.com/Trusted-AI/AIF360/issues/716#issuecomment-820930372","@vamsidesu5 Great question! This command should build the website locally: `python -m sphinx -v -b html -n -j auto docs docs/_build/html`
Then you can look into that directory at the end of the command and open the pages you care about. In your case I think that would be the contributor guide. This command is a bit of a shortcut compared to what the build pipelines are running since they need to build the webpage for all previously released versions, too, not just for the `main` branch. But that shouldn't really matter in this case, so I'm sure this will be sufficient (and much faster to iterate!).","32","0.3529715122635477","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","820985537","issue_comment","https://github.com/Trusted-AI/AIF360/issues/716#issuecomment-820985537","@romanlutz I think it would be great if we can put those instructions in the contributor guide!","14","0.360948191593353","Documentation","Development"
"https://github.com/fairlearn/fairlearn","830713542","issue_comment","https://github.com/Trusted-AI/AIF360/issues/716#issuecomment-830713542","@hildeweerts merged the corresponding PR in, so I'll close this issue. Thanks a ton, @vamsidesu5 !","20","0.8013587835651892","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","805613563","issue_comment","https://github.com/Trusted-AI/AIF360/issues/715#issuecomment-805613563","I'm also happy either way.","20","0.329153605015674","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","806158332","issue_comment","https://github.com/Trusted-AI/AIF360/issues/715#issuecomment-806158332","I don't understand the ordering by functionality, since I'd expect dataset to be first, but I don't particularly care about the ordering :-)

It's set in this file https://github.com/fairlearn/fairlearn/blob/main/docs/api_reference/index.rst","0","0.4475750238462102","Dataset usage","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","806169349","issue_comment","https://github.com/Trusted-AI/AIF360/issues/715#issuecomment-806169349","Haha, yeah, I guess it's a lot more subjective than alphabetically. I was clicking through the website to find some good first issues and I noticed this was a bit strange.","20","0.3859049788068577","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","807005026","issue_comment","https://github.com/Trusted-AI/AIF360/issues/715#issuecomment-807005026","Happy either way, but +1 for alphabetically","14","0.2659352142110764","Documentation","Development"
"https://github.com/fairlearn/fairlearn","782576391","issue_comment","https://github.com/Trusted-AI/AIF360/issues/705#issuecomment-782576391","Closing singe there is no description. We'll reopen if you could explain the issue further :) ","24","0.4675123326286117","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","782576294","issue_comment","https://github.com/Trusted-AI/AIF360/issues/703#issuecomment-782576294","This is fixed by re-running the tests. Not ideal, but not a big issue. Hmm","24","0.2722385141739981","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","783282405","issue_comment","https://github.com/Trusted-AI/AIF360/issues/703#issuecomment-783282405","To avoid re-running all tests we can also hardcode a few re-tries to OpenML calls, perhaps with a short time delay. That usually fixed my problems with API calls during a benchmark project I did a few years ago.","24","0.4692264257481648","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","784504266","issue_comment","https://github.com/Trusted-AI/AIF360/issues/703#issuecomment-784504266","This is causing issues with some of our internal testing as well. We are investigating whether we can host the Census dataset on a CDN (there is retry logic there, but we've had problems which have defeated that).","24","0.2607717803030304","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","784632709","issue_comment","https://github.com/Trusted-AI/AIF360/issues/703#issuecomment-784632709","Definitely seems like an openml issue to me, and not because of removed datasets but unavailable service.
PapersWithCode also recently added datasets: https://paperswithcode.com/datasets","27","0.4188995215311005","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","807949067","issue_comment","https://github.com/Trusted-AI/AIF360/issues/703#issuecomment-807949067","Thanks for linking @hildeweerts . Sounds like this should be resolved now. Can we close this issue?","20","0.2799154334038055","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","808057437","issue_comment","https://github.com/Trusted-AI/AIF360/issues/703#issuecomment-808057437","I think so. We can always re-open if we still encounter any issues.","24","0.6253813300793166","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","824615365","issue_comment","https://github.com/Trusted-AI/AIF360/issues/703#issuecomment-824615365","Reopened this issue (see https://github.com/fairlearn/fairlearn/pull/732#issuecomment-824583523) as well as the one the at OpenML repo (https://github.com/openml/OpenML/issues/1092).

@riedgar-ms you mentioned you've previously had some retry logic there, what was the reason that didn't work?","24","0.6517719568567025","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","824752055","issue_comment","https://github.com/Trusted-AI/AIF360/issues/703#issuecomment-824752055","Most likely, the retry logic was only in the AzureML Notebooks - there was never a significant problem here. Can you point me at a build where this has been happening? I've just done a quick swing past, and I'm seeing mainly green.","24","0.7832082290797886","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","824788183","issue_comment","https://github.com/Trusted-AI/AIF360/issues/703#issuecomment-824788183","I don't know of a specific build, but @romanlutz mentioned you've encountered this issue several times over the past few weeks (see https://github.com/fairlearn/fairlearn/pull/732#issuecomment-824583523).","24","0.7651733473059119","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","876097762","issue_comment","https://github.com/Trusted-AI/AIF360/issues/703#issuecomment-876097762","FWIW I haven't seen this in quite a while. Thanks @hildeweerts for raising it with OpenML folks, and obviously thanks to the OpenML folks who evidently made a few improvements on their side. 

@hildeweerts do you want to keep this open?","20","0.5083823622445752","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","876278242","issue_comment","https://github.com/Trusted-AI/AIF360/issues/703#issuecomment-876278242","@romanlutz let's close for now, if the issue pops up again we can always reopen :)","24","0.511988011988012","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","893220754","issue_comment","https://github.com/Trusted-AI/AIF360/issues/703#issuecomment-893220754","Reopening since I'm seeing tons of OpenML failures in our pipelines 🙁 ","5","0.4053030303030304","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","897655878","issue_comment","https://github.com/Trusted-AI/AIF360/issues/703#issuecomment-897655878","Hi all, sorry about the hickups, we're doing a bunch of work on the backend migrating to Kubernetes, and occasionally response may be slower. We're definitely planning to resolve it completely.","31","0.332159366057671","Testing","Maintenance"
"https://github.com/fairlearn/fairlearn","897665505","issue_comment","https://github.com/Trusted-AI/AIF360/issues/703#issuecomment-897665505","Thank you for letting us know, @joaquinvanschoren!","13","0.3274917853231109","Artifact generation and benchmarking","Deployment"
"https://github.com/fairlearn/fairlearn","776256750","issue_comment","https://github.com/Trusted-AI/AIF360/issues/698#issuecomment-776256750","Rather than in the Fairlearn repo, this might work better as a separate repo in the Fairlearn org. The description above doesn't quite feel to me like something that fits in with the Fairlearn PyPI package itself.

Could you elaborate a bit on how you calculate fairness metrics on these recognisers, and what material you're providing to help users explore the fairness issues in these data @wguyman ? Do you actually use Fairlearn in your code?

Tagging @adrinjalali  and @hildeweerts who are our external maintainers, and can provide a non-Microsoft perspective.","13","0.3575466200466202","Artifact generation and benchmarking","Deployment"
"https://github.com/fairlearn/fairlearn","776274718","issue_comment","https://github.com/Trusted-AI/AIF360/issues/698#issuecomment-776274718","I'd just like to echo what @riedgar-ms expressed. If this is focused on fairness analysis of such a system then I can see why hosting it under the Fairlearn org in a separate repo makes sense. It would definitely need some documentation to elaborate on fairness criteria, potential harms, etc. And finally, for new repos I think we'd want a certain commitment to maintain that repo. Not that I expect this to be a lot of work, but it's probably a non-zero effort.
Also adding @MiroDudik since he's on the steering committee which (I think?) makes such decisions.","20","0.4484505801145541","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","776275951","issue_comment","https://github.com/Trusted-AI/AIF360/issues/698#issuecomment-776275951","I'm not sure if the script using Azure Cognitive Services would be appropriate for this repo or org in general, but I find the idea of having scripts which would run certain benchmarks on datasets interesting.

I believe this falls under the category of having guidelines on certain use cases, or real world scenarios. We've talked about this before, and I know there's ample interest in the matter. However, it's rather tricky to define metrics against which we should test a dataset or a method since it depends on the use case and not only the dataset.

I'm sure @MiroDudik would have quite a bit to add on that.","15","0.3695157092396354","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","781814247","issue_comment","https://github.com/Trusted-AI/AIF360/issues/698#issuecomment-781814247","Thanks all! Yes, the plan is to publish fairness results of the service algorithms and then separately publish a python script customers can use on their own data that evaluates performance of cognitive services or other services across standard True Positive, False Positive, and ROC curves as used in the NIST report. 

The script doesn't use fairlearn code, so the question it sounds like is whether it could live in a different fairlearn repo? ","13","0.4386363636363638","Artifact generation and benchmarking","Deployment"
"https://github.com/fairlearn/fairlearn","781878852","issue_comment","https://github.com/Trusted-AI/AIF360/issues/698#issuecomment-781878852","To me it sounds like a repo under the Microsoft org may be more suitable for this case?","13","0.5679374389051811","Artifact generation and benchmarking","Deployment"
"https://github.com/fairlearn/fairlearn","830713659","issue_comment","https://github.com/Trusted-AI/AIF360/issues/698#issuecomment-830713659","Closing this issue. If there's any new information we should take into account please reopen! Thanks!","24","0.3508158508158508","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","776278284","issue_comment","https://github.com/Trusted-AI/AIF360/issues/697#issuecomment-776278284","Oh. Can tags be moved? Or what's the proposed fix?","32","0.7134532134532136","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","777709599","issue_comment","https://github.com/Trusted-AI/AIF360/issues/697#issuecomment-777709599","I believe tags can be moved:
http://blog.iqandreas.com/git/how-to-move-tags/
However, we won't find out if it's in the right place until we rebuild the docs... and I believe we can't trigger than manually?","32","0.8222350897510133","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","777730642","issue_comment","https://github.com/Trusted-AI/AIF360/issues/697#issuecomment-777730642","It's auto-triggered upon check in. There's a way to add another trigger for manual triggering to the YAML, called `workflow_dispatch` but you'd need to get that into the `main` branch at which point it would auto-trigger :-)  It may come in handy in the future, though, if you still want to do it...","32","0.8033917153183208","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","777752550","issue_comment","https://github.com/Trusted-AI/AIF360/issues/697#issuecomment-777752550","After a brief battle with vim, I think that this is fixed. However, we don't know until the next doc build/publish","14","0.6380549682875263","Documentation","Development"
"https://github.com/fairlearn/fairlearn","807950594","issue_comment","https://github.com/Trusted-AI/AIF360/issues/697#issuecomment-807950594","I confirmed that it's fixed. Thanks @riedgar-ms !","24","0.649989831197885","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","763687993","issue_comment","https://github.com/Trusted-AI/AIF360/issues/683#issuecomment-763687993","I'm assuming you didn't mean Python 2.7.17 above....

This is not a use case we've considered. Can you tell us what you're trying to do?","15","0.5827223851417401","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","777740481","issue_comment","https://github.com/Trusted-AI/AIF360/issues/683#issuecomment-777740481","@riedgar-ms , i meant python 3.7!","32","0.2659352142110763","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","759153968","issue_comment","https://github.com/Trusted-AI/AIF360/issues/676#issuecomment-759153968","@LeJit can you elaborate on why it's better to have this separate from `MetricFrame`?","20","0.3911088911088911","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","759209677","issue_comment","https://github.com/Trusted-AI/AIF360/issues/676#issuecomment-759209677","@MiroDudik and I have been discussing how we can use entropy metrics to assess the distribution of labels and sensitive attributes in a dataset (to facilitate the creation of model cards and datasheets). I think it would be better to have it separate `MetricFrame`, so we can apply entropy metrics directly to the dataset rather than the outputs of a trained model.","15","0.3704935064935065","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","759622058","issue_comment","https://github.com/Trusted-AI/AIF360/issues/676#issuecomment-759622058","Aren't there metrics which ignore `y_pred`? That's how we support dataset metrics, don't we?","15","0.3905180840664714","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","760643669","issue_comment","https://github.com/Trusted-AI/AIF360/issues/676#issuecomment-760643669","> Aren't there metrics which ignore `y_pred`? That's how we support dataset metrics, don't we?

That's what I thought and the reason why I asked :-) An example for what @adrinjalali means here is `selection_rate`. It doesn't matter what `y_true` is there because we only care about `y_pred`. One could imagine the same thing the other way round, of course.","15","0.3865496205921739","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","761117224","issue_comment","https://github.com/Trusted-AI/AIF360/issues/676#issuecomment-761117224","We can implement it in `MetricFrame` for now, but I don't feel it's intuitive how to use `MetricFrame` to compute dataset metrics without actually looking at the code. Also, there's something I dislike about having to pass in a list of dummy values for `y_pred` to compute dataset metrics.","12","0.4086190625983013","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","761602487","issue_comment","https://github.com/Trusted-AI/AIF360/issues/676#issuecomment-761602487","One thing to note is that a lot of metrics which need only one `y` can be applied to both the dataset and the output of the model, but they don't require `y_true` and `y_pred` at the same time. I agree that this can be confusing to users, and if they pass these metrics to tools such as `GridSearchCV`, there may be inconsistencies or unclearness in whether `y_true` is used or `y_pred` in that setting (`y_pred` should be used in this example). So I agree the API could be improved.","15","0.3516582664426607","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","891089659","issue_comment","https://github.com/Trusted-AI/AIF360/issues/668#issuecomment-891089659","Hi @romanlutz, I would like to pick this up","20","0.582027168234065","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","891399627","issue_comment","https://github.com/Trusted-AI/AIF360/issues/668#issuecomment-891399627","Sure thing, @bram49 ! To be quite honest, I'm not entirely sure where this will lead. You can play around with the samples we have in the documentation https://fairlearn.org/main/user_guide/assessment.html#plotting-grouped-metrics and see if there's a good way to showcase the control features.

You can basically do the same thing with control features as with sensitive features since the `by_group` result is just the same kind of `DataFrame`. The difference comes only through aggregation which isn't really shown. Instead of showing the bars for all groups next to each other I was just wondering whether we could have some kind of separation between the control groups. It's probably not doable with a single command like we have it in the docs linked above, because you'd need to call additional plotting commands. Perhaps the plotting code @alexquach is adding in #857 can be extended to add separators between the groups. In any case, we may just come out of this exploration with the decision not to do anything special (and that's perfectly fine!). If you want to do something more well-defined I'd suggest #666. I can walk you through what's been done on that before since I pair-programmed with @rishabhsamb on that. Just let me know what you think now that I've shared some of my thoughts 🙂 ","15","0.445051738616793","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","892574193","issue_comment","https://github.com/Trusted-AI/AIF360/issues/668#issuecomment-892574193","Thanks for the fast and thorough reply!
I will dive into this when #896 is finished. Think I will start with creating the separation lines and see if this will look nice. 
If it would not work out I will take a look at #666.","20","0.6912060082088897","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","825803185","issue_comment","https://github.com/Trusted-AI/AIF360/issues/667#issuecomment-825803185","I think that there's a simple approach to cover some instances of this: https://github.com/fairlearn/fairlearn/pull/561#issuecomment-825766115","6","0.2969696969696969","API expansion","Development"
"https://github.com/fairlearn/fairlearn","834156511","issue_comment","https://github.com/Trusted-AI/AIF360/issues/667#issuecomment-834156511","This is already possible now with the `MetricFrame.by_group.plot.bar` approach shown in #766 . Closing the issue!","24","0.5527836504580691","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","753357027","issue_comment","https://github.com/Trusted-AI/AIF360/issues/666#issuecomment-753357027","The function should either accept a metric or the user should compute the metrics outside the function and pass the scores to the plotting method.","12","0.534688995215311","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","753553513","issue_comment","https://github.com/Trusted-AI/AIF360/issues/666#issuecomment-753553513","You're right, of course! I missed the metric function. Thanks!","30","0.3792963188936344","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","833025393","issue_comment","https://github.com/Trusted-AI/AIF360/issues/666#issuecomment-833025393","@rishabhsamb expressed some interest in this, so I wanted to make sure we agree on all the basics. In my description above it looks like a standalone function. I think that's also our only option since `MetricFrame` doesn't take multiple predictions. That is, unless we're willing to extend `MetricFrame` towards multiple models/sets of predictions(?). I'm assuming we don't (at least not in the short term), so @rishabhsamb could implement this as 
```
plot_model_comparison(
    metrics,
    y_true, 
    y_preds={""model 1"": y_pred_1, ""model 2"": y_pred_2, ... }, 
    sensitive_features, show_plot=True)
```
But there are still a few different things to choose here such as:
- Should `metrics` include some indication on which one is on the x-axis and which one is on the y-axis? Or do we just randomly choose that?
- Related: Is `metrics` a set of arbitrarily many metrics? If so, do we generate all combinations as charts? Or do we restrict it to just two? In that case, would it be simpler to provide both explicitly as `x_metric`, `y_metric`. In the existing dashboard x is always the ""performance"" metric and y is always the ""fairness"" metric. We could also make that restriction.
- How do we specify the metrics? Simply putting `selection_rate` isn't sufficient, since we may want to evaluate the `group_max`, `group_min`, `ratio`, or `difference`. 

After thinking through these questions I'm thinking of the following
```
plot_model_comparison(
    x_metric=accuracy_score,  # no default value, this is just for illustrative purposes
    x_metric_aggregator=None,  # optional, default None (means ""overall"")
    y_metric=selection_rate,  # no default value, this is just for illustrative purposes
    y_metric_aggregator=""group_max"",  # optional, default None (means ""overall"")
    y_true, 
    y_preds={""model 1"": y_pred_1, ""model 2"": y_pred_2, ... }, 
    sensitive_features, show_plot=True)
```
which would then proceed to plot overall accuracy on the x-axis (the value you get from `MetricFrame.overall`) and demographic parity difference or, in other words, `MetricFrame.difference()` based on `selection_rate`. If the aggregation isn't needed it can be left out (i.e., it's an optional arg). I would also expect that `y_preds` has to be a dictionary (or `DataFrame`?), not just a single prediction, because this is meant to be a scatter plot after all.

@fairlearn/fairlearn-maintainers wdyt? Also, @rishabhsamb please chime in with your own thoughts. I realize I may be missing some pieces or skipping over important details.","12","0.5275640546799135","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","833250173","issue_comment","https://github.com/Trusted-AI/AIF360/issues/666#issuecomment-833250173","> Just like the functions in #561 this should live in the metrics module.

I'm not sure, an `inspection` module makes more sense to me for this.

I guess ideally, the user would have like a few different models and maybe one or two or more grid search CV done on a few hyper parameter sets, and would like to visualize those results.

Have you looked in to the outputs [`GridSearchCV`](https://scikit-learn.org/dev/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV)? We could utilize `cv_results_` and `multimetric_` for instance, just an idea to explore.

> - How do we specify the metrics? Simply putting `selection_rate` isn't sufficient, since we may want to evaluate the `group_max`, `group_min`, `ratio`, or `difference`. 

I would discourage us from incorporating these concepts into the signature of such a function. I would rather have simple metric functions as X and Y, and whatever logic needs to be there, is in that function. We have a bunch of `y_pred` (or estimators for that matter), one `y_true`, and the sensitive feature, and that's enough.","30","0.2300121917184448","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","833281198","issue_comment","https://github.com/Trusted-AI/AIF360/issues/666#issuecomment-833281198","Regarding the module, it seemed just intuitive to have it with metrics since this is basically a metrics visualization. That's not a terribly important point for me, though, so whatever works for people is fine by me.

Note that this isn't just for `GridSearch` either. I can plug in my original model, a postprocessed model, and an `ExponentiatedGradient` model and compare the three of them without touching `GridSearch` at all. Ideally I'd like it to be generic enough to support such use cases.

I understand the concern with exposing so many knobs on the metric aggregation. However, I'm not sure how else you will achieve the same outcome. This isn't just the simple metric, but aggregations of that metric between the groups. If I need to plot the overall accuracy score vs. selection rate difference there isn't really another way to express that (as far as I can tell) other than
- having two args per axis like I described above (metric + aggregator)
- creating a new object that encapsulates metric + aggregator, and pass that object for each axis. I don't like creating new objects that people aren't familiar with, though.
- defining different functions `plot_model_comparison_*` where the `*` would be all kinds of variations of aggregating a certain way on x and y axis. That's obviously a ton of combinations and not really a good option.

If I've missed another option please point it out.","15","0.521159968273629","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","833282582","issue_comment","https://github.com/Trusted-AI/AIF360/issues/666#issuecomment-833282582","So for me to understand, if I have 3 models and a sensitive attribute which has 2 groups in it, how many points do I want to see in that plot? I was assuming 3.","12","0.5186257479251111","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","833289795","issue_comment","https://github.com/Trusted-AI/AIF360/issues/666#issuecomment-833289795","That matches my expectation exactly. But for each of those 3 models I have many choices for what the axes should be. Let's say I want to plot accuracy vs. selection rate. I'll just go with my earlier API proposal which would result in
```
plot_model_comparison(
    x_metric=accuracy_score,
    y_metric=selection_rate,
    y_true, 
    y_preds={""model 1"": y_pred_1, ""model 2"": y_pred_2, ... }, 
    sensitive_features, show_plot=True)
```
How does this function internally know whether I want
- the overall accuracy and selection rate (i.e., ignoring sensitive features)
- the min (or max, although in this case that wouldn't make sense) accuracy or selection rate
- the difference between max and min accuracy and selection rates
- the ratio of min and max accuracy and selection rates
- OR most likely: a combination thereof, like what I specified in my example with overall accuracy of the model (aka ""performance metric"") and the selection rate difference (aka ""fairness metric"").

I see no way of expressing that without args, hence:
```
plot_model_comparison(
    x_metric=accuracy_score,  # no default value, this is just for illustrative purposes
    x_metric_aggregator=None,  # optional, default None (means ""overall"")
    y_metric=selection_rate,  # no default value, this is just for illustrative purposes
    y_metric_aggregator=""group_max"",  # optional, default None (means ""overall"")
    y_true, 
    y_preds={""model 1"": y_pred_1, ""model 2"": y_pred_2, ... }, 
    sensitive_features, show_plot=True)
```","12","0.6637644550739439","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","833293604","issue_comment","https://github.com/Trusted-AI/AIF360/issues/666#issuecomment-833293604","The `plot_model_comparison` would expect the metric function to have one of the two signatures:

- `metric(y_true, y_pred, sensitive_attribute)`
-  `metric(y_true, y_pred)`

Internally we can call the first one, in case it raises something, we call the second. 
The user would call as:

``` python
plot_model_comparison(
    x_metric=accuracy_score,
    y_metric=partial(equalized_odds_ratio, method=""max""),
    y_true, 
    y_preds={""model 1"": y_pred_1, ""model 2"": y_pred_2, ... }, 
    sensitive_features, show_plot=True)
```","12","0.6054241934694272","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","833316205","issue_comment","https://github.com/Trusted-AI/AIF360/issues/666#issuecomment-833316205","I had thought about a similar option, but I still think it would cause some extra trouble for users. `equalized_odds_ratio` is already defined, but what about just getting the group with minimum accuracy? Thinking about this a bit more, you can define variations of aggregations for each kind of metric (at least min, max, diff, ratio), but we didn't want that many custom ones like `equalized_odds_ratio`. Otherwise we'll end up with #metrics x #aggregations (and potentially multiplied by # options within those aggregations, as you've demonstrated with `method=""max""`) custom functions. 

That leaves users with something like this:
```
def accuracy_score_min(y_true, y_pred, sensitive_features):
    MetricFrame(accuracy_score, y_true, y_pred, sensitive_features=sensitive_features).group_min()
plot_model_comparison(..., y_metric=accuracy_score_min,...)
```
It's more flexible than what I proposed in that my proposal can't accommodate the `method` argument. It does still feel like quite a lot to write given that the key idea is easy to express. One could put it all in the same line with a `lambda` but that certainly doesn't help in making this readable.

The middle path would be to still go with your suggestion, but create ""shortcuts"" like `equalized_odds_ratio` for the most common ones. I imagine there are somewhere between 10 and 20 of them so perhaps it's not quite as dramatic as I explained with my estimate above.

I certainly like having fewer arguments and for me that pretty much overrules any concerns I've mentioned. I'm just wondering whether there's another way to avoid creating so many custom functions.","15","0.3767802700826765","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","833323937","issue_comment","https://github.com/Trusted-AI/AIF360/issues/666#issuecomment-833323937","I very much prefer to make the user write 3 lines of code rather than having an API which is really hard to understand and remember.

Ideally for more complicated cases, [`make_derived_metric`](https://fairlearn.org/v0.6.1/api_reference/fairlearn.metrics.html#fairlearn.metrics.make_derived_metric) would be the way users can give their custom metric function.","15","0.6838241570635935","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","833626284","issue_comment","https://github.com/Trusted-AI/AIF360/issues/666#issuecomment-833626284","I like @adrinjalali's proposal above. Just a request to make all the arguments keyword only!","23","0.228752642706131","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","834400242","issue_comment","https://github.com/Trusted-AI/AIF360/issues/666#issuecomment-834400242","[@romanlutz : for the specific use case that you're mentioning, we already support [accuracy_score_group_min](https://fairlearn.org/v0.6.1/user_guide/assessment.html#scalar-results-from-metricframe), but of course, i don't want to add too many entries to that table.]","15","0.4144584862073651","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","877450714","issue_comment","https://github.com/Trusted-AI/AIF360/issues/666#issuecomment-877450714","This issue is still up for grabs, but whoever wants to do it can start from where @rishabhsamb and I left off which is captured in this branch: https://github.com/rishabhsamb/fairlearn/tree/refactored-input

There's quite a bit of input validation we needed to refactor first because the validation is almost identical to what we're doing for mitigation techniques, but doesn't require certain parts of it. I suppose that could be its own PR to start with, and maybe I should get that out of the way before someone does this...","32","0.317157490396927","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","915173237","issue_comment","https://github.com/Trusted-AI/AIF360/issues/666#issuecomment-915173237","Hi! I worked on this :) I'm still a bit new and I appreciate feedback/help. I have a few questions also. You can check out my progress on https://github.com/SeanMcCarren/fairlearn/tree/model_comparison_plot 

1) Currently, I'm having trouble with `from fairlearn.metrics import plot_model_comparison`. It won't seem to import. I tried adding an import in `metrics/__init__.py`, to no avail. I also added it at the bottom, in the `__all__` (this is new for me so I assume something must be wrong here).

2) Secondly, I'm not sure about protocol here, do I make a PR at this point? I don't want to waste the precious CI server every time I make a silly commit haha, but its nice to have tests checked.

3) Lastly (for now hihi), I saw that there are tests using `pytest.mark.mpl_image_compare` for other plotting scenarios, but that those are *disabled*. Should I also write those tests, test it locally, and then disable them? ","32","0.4735966605464276","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","915178099","issue_comment","https://github.com/Trusted-AI/AIF360/issues/666#issuecomment-915178099","For (1) have you done a `pip install -e .` from the root of your local copy? Or have you made accidentally installed Fairlearn from PyPI in your Python environment?

For (2), definitely go ahead and make the PR. You'll only trigger builds when you do a push, not on every commit. And CPU time isn't _that_ precious.

For (3), I don't think we have a good solution for plotting tests. You can write an example notebook to show their use, but that would end up being a manual check.","32","0.5804519876019381","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","915603802","issue_comment","https://github.com/Trusted-AI/AIF360/issues/666#issuecomment-915603802","I agree with everything @riedgar-ms wrote, and I'm SUPER excited you're interested in this issue!!!

I would completely ignore the matplotlib tests since I could never get them to work in CI. This requires a separate issue perhaps at some point, but the main thing is that it renders right on the webpage which we need to check every time we make a change to the corresponding page. So, in summary, @riedgar-ms is right in saying that we should have documentation with an example. However, I would recommend adding a user guide page as well, ideally based on the example notebook (if you search for `literalinclude` you'll find examples of what I mean... if those don't make sense please reach out!).","14","0.31890221230712","Documentation","Development"
"https://github.com/fairlearn/fairlearn","876091233","issue_comment","https://github.com/Trusted-AI/AIF360/issues/665#issuecomment-876091233","As far as I remember we wanted to get our mitigation techniques to be passed into `Pipeline`s from sklearn. That's not possible right now since we require `sensitive_features` which currently can't be passed to the `Pipeline`. I believe @adrinjalali mentioned that passing things like `sensitive_features` might be possible at some point. I found the following SLEP which I *think* refers to this https://scikit-learn-enhancement-proposals.readthedocs.io/en/latest/slep006/proposal.html
@adrinjalali if you have any other updates or thoughts to share please add 🙂 ","15","0.4337538379388601","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","876653286","issue_comment","https://github.com/Trusted-AI/AIF360/issues/665#issuecomment-876653286","What we can do, is to pass a `Pipeline` to our mitigation techniques, which includes the preprocessing part. There's a bug in validating the input in our side which prevents us from doing so, but otherwise it would work.","23","0.5048919690175295","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","886023096","issue_comment","https://github.com/Trusted-AI/AIF360/issues/665#issuecomment-886023096","I just tried it out locally. It actually just works! I'll send out a (few?) PR(s) asap to create a notebook that shows this and then reference it everywhere else. This will also be super useful in #614 .","24","0.3417065390749602","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","886163094","issue_comment","https://github.com/Trusted-AI/AIF360/issues/665#issuecomment-886163094","Update: I created #916 but I can't seem to extract the code WITH output from the examples to show in the new user guide for #614  which is a shame because it would mean that I have to duplicate it after all. I thought I might be able to use the same example and just `literalinclude` all over the docs. @adrinjalali do you know of a fix for that? I created https://github.com/sphinx-gallery/sphinx-gallery/issues/851 in case sphinx-gallery folks have ideas.

Additionally, `Pipeline` seems to fail `check_is_fitted` so I check for the `vars(pipeline)` where end in `_` but don't start with `__` (which is what `check_is_fitted` does) and it's indeed `[]` so it's supposed to fail. Does `check_is_fitted` not apply to `pipeline`?
A workaround might be to check for such a case and set something like `estimator.fitted_ = True` after the `fit` call in our mitigation techniques. After all, when we call `fit` we know for sure that they're fitted.","32","0.3797928262213977","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","887317601","issue_comment","https://github.com/Trusted-AI/AIF360/issues/665#issuecomment-887317601","I personally wouldn't be too bothered by not having the output there, having the bit of code is a good middle ground (at least for now) anyway.

As for the `Pipeline` issue, yep, it's our bad, and working on it, see https://github.com/scikit-learn/scikit-learn/pull/9741","32","0.3322462291072156","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","751362814","issue_comment","https://github.com/Trusted-AI/AIF360/issues/660#issuecomment-751362814","Hi, I'm interested in contributing to the project. Is a good issue for someone new, or is it too involved?","20","0.7897116324535682","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","752695357","issue_comment","https://github.com/Trusted-AI/AIF360/issues/660#issuecomment-752695357","Hi @shngt ! Welcome to the Fairlearn community! I think the currently suggested change in this issue is to create a separate `ThresholdOptimizer` instance per control feature value, e.g. if we have three of them (`high`, `medium`, and `low` from the feature `income`) then we'd pass only the subset of `X` and `y` corresponding to the control feature value to each instance. That sounds simple enough to do. However, the potentially tricky part is the structure (more below). I think this is a fine issue to start on once we sort out what that should look like. [I was planning to open a few other issues that would be great starting points, so I'll do that now to give you some more choices. I'd be happy to walk you through any of them if you have questions.]

Re: structure --> Since @MiroDudik 's change a little while ago we already have a fairly nice separation of logic in `ThresholdOptimizer` and `InterpolatedThresholder`. At first glance this should be as simple as replacing [this line](https://github.com/fairlearn/fairlearn/blob/43532d83656b4c3cced275f5de9930761bd44f01/fairlearn/postprocessing/_threshold_optimizer.py#L227) with a for-loop to do the same with each control feature, and then repeating that process for the prediction methods. Alternatively, if no control features are specified, one would just use the entire dataset. The passing of `control_features` is currently a bit obscure since it's not explicitly spelled out in the API. I'm opening a ""discussion"" (https://github.com/fairlearn/fairlearn/discussions/664) for that, but it is certainly possible to pass `control_features` even now. For `ThresholdOptimizer` this currently results in an exception (by design since that's what this issue is for).

@MiroDudik and @riedgar-ms  may have thoughts on this, and perhaps @adrinjalali since we've talked about `ThresholdOptimizer` code before. Obviously everyone's feedback would be highly appreciated.","15","0.3906688226857056","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","752900564","issue_comment","https://github.com/Trusted-AI/AIF360/issues/660#issuecomment-752900564","We should also probably have a bunch of validation and maybe configurable warnings for the cases where due to `control_features` each estimator would receive only a handful of samples and therefore would not be a suitable estimator.","23","0.4827769886363636","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","743210115","issue_comment","https://github.com/Trusted-AI/AIF360/issues/655#issuecomment-743210115","As discussed in our call, this certainly sounds like something Fairlearn should support. However, `MetricFrame` doesn't keep track of anything - in fact, it doesn't really care what the contents of `y_true` and `y_pred` really are, and certainly doesn't require that the incoming data can be formed into a confusion matrix. That would be something for the individual metrics themselves to support.

The way I see this working is that each metric would be responsible for its own accumulations, and we'd make a copy of the relevant object for each detected sensitive feature combination. The `update(...)` method on the `MetricFrame` would in turn call the update method on each of those underlying metric objects.","15","0.6702930577276129","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","743277985","issue_comment","https://github.com/Trusted-AI/AIF360/issues/655#issuecomment-743277985","**(1) Re. ""wrapper"" implementation** where all the incoming data is kept around:
* This should not be default, but only enabled by setting a Boolean flag in the constructor.

**(2) To incorporate some initial efficient implementations** (say for the metrics based on keeping track of the contingency table), we could allow strings as metrics, similarly to [pd.SeriesGroupBy.aggregate](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.core.groupby.SeriesGroupBy.aggregate.html), e.g.,
```python
mf = flm.MetricFrame('accuracy_score', y_true, y_pred, sensitive_features=sf)
mf.add_data(y_true_new, y_pred_new, sensitive_features=sf_new)

mf = flm.MetricFrame(['accuracy_score', 'false_positive_rate', 'false_negative_rate'],
                     y_true, y_pred, sensitive_features=sf)
```
We could even support some ""standard"" lists that we have found useful?
```python
mf = flm.MetricFrame('binary_classification_metrics',
                     y_true, y_pred, sensitive_features=sf)
```

**(3) For streaming setting, it would be much nicer to support a constructor / add_data pattern** of the following form:
```python
mf = flm.MetricFrame('accuracy_score')
mf.add_data(y_true_next, y_pred_next, sensitive_features=sf_next)
```
Whereas the current alternative is:
```python
mf = flm.MetricFrame('accuracy_score', [], [], sensitive_features=[])
mf.add_data(y_true_next, y_pred_next, sensitive_features=sf_next)
```
I'm not completely sure how to do this right (without making the consructor work with generic `*args` which would hide the meaning). There might be some alternative via [@classmethod](https://stackoverflow.com/questions/12179271/meaning-of-classmethod-and-staticmethod-for-beginner)? Something like:
```python
mf = flm.MetricFrame.for_streaming('accuracy_score')      # or
mf = flm.MetricFrame.without_data('accuracy_score')       # or?
```
Does that feel right or too fancy? This style is mostly used with `from_...` as in `DataFrame.from_dict(...)`","2","0.6007860971759884","Performance measurement","Validation"
"https://github.com/fairlearn/fairlearn","744544045","issue_comment","https://github.com/Trusted-AI/AIF360/issues/655#issuecomment-744544045","In 1) you talk about a flag-enabled feature. This would fix 3) I think. 
```python
# This work
mf = flm.MetricFrame('accuracy_score', [], [], sensitive_features=[], streaming=True)

# This should raise
mf = flm.MetricFrame('accuracy_score', [], [], sensitive_features=[])
# ValueError(""y_true, y_pred can't be empty if `streaming=False`."")
```","2","0.663464542330522","Performance measurement","Validation"
"https://github.com/fairlearn/fairlearn","745312219","issue_comment","https://github.com/Trusted-AI/AIF360/issues/655#issuecomment-745312219","Going through @MiroDudik 's points....

(1) I don't think `MetricFrame` should be in the business of keeping any data around. That would be the responsibility of the metrics objects themselves. `MetricFrame` should simply slice up the incoming data according to the sensitive (and control!) features, and make one copy of each metric object to keep track of that slice. Then, feed each metric object its own slice of the data (repeat as necessary when the user supplies more data). When the user requests some of the results (via `.overall` or `.by_group` etc.), the `MetricFrame` should go through its collection of metric objects, and get the current value from each.

(2) Is something we've discussed before (apart from the set `binary_classification_metrics`  but that does seem like a reasonable extra). There would simply be a different default implementation based on whether some other `streaming` flag was set.

(3) That classmethod would be a factory :-)","15","0.8289566410741445","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","748256649","issue_comment","https://github.com/Trusted-AI/AIF360/issues/655#issuecomment-748256649","I really like @Dref360's [suggestion above](https://github.com/fairlearn/fairlearn/issues/655#issuecomment-744544045). I am just wondering whether we should allow the user to somehow select the naive ""wrapper"" option, where `y_pred`, `y_true`, `sensitive_features` etc. are memorized.

We could still use `streaming` flag, with the allowed values:
* `None` or `False` (default) -- disable streaming
* `True` -- streaming is delegated to the base metric (if base metric does not support it, we raise)
* `'memorize_data'`

Thoughts?","15","0.5726107226107229","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","748647889","issue_comment","https://github.com/Trusted-AI/AIF360/issues/655#issuecomment-748647889","
> (1) I don't think `MetricFrame` should be in the business of keeping any data around. That would be the responsibility of the metrics objects themselves. `MetricFrame` should simply slice up the incoming data according to the sensitive (and control!) features, and make one copy of each metric object to keep track of that slice. Then, feed each metric object its own slice of the data (repeat as necessary when the user supplies more data). When the user requests some of the results (via `.overall` or `.by_group` etc.), the `MetricFrame` should go through its collection of metric objects, and get the current value from each.

metrics are pretty much functions, they don't have the concept of storing stuff, at least most of the ones we have right now don't. So it'd be the `MetricFrame` which would store data.

> We could even support some ""standard"" lists that we have found useful?

I don't think standard lists would be required, it adds quite a bit of complexity to the API I think, and it's outside the scope of this issue anyway.

> (3) That classmethod would be a factory :-)

We probably don't need a factory, it's just the constructor which takes an extra parameter and makes the `MetricFrame` to store stuff or not.

For the parameter, I'd go with:

```
streaming: bool, default=False
    False: the object does not support streaming and `add_data` would raise.
    True: the object supports streaming.
```

It wouldn't be the users' responsibility to know which metrics support streaming and which ones don't. To begin with, we store the data fed to the `MetricFrame` and pass it each time to the underlying metric. It's not the most efficient implementation, but it does the job. In later iterations, we can start by adding support for certain commonly used metrics by implementing them ourselves in a update-able efficient way and enhance the `MetricFrame` implementation, w/o any change on the user/API side.","15","0.8070047458536663","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","748661664","issue_comment","https://github.com/Trusted-AI/AIF360/issues/655#issuecomment-748661664","For (1) I suppose it depends on how much we're concerned with efficiency (and I will confess ignorance as to how streamable metrics such as those mentioned above do this right now) vs making the new user experience as smooth as possible. Storing all the data as it comes in is certainly inefficient, since we would have to just store it, and reprocess everything for each update call. Specialised streaming metrics could be more efficient - for example by building a confusion matrix, and updating the counts for new data (we can't do this within `MetricFrame` because it's much more general than CM-based classification metrics).

I can certainly see the sense in @adrinjalali 's suggestion.","15","0.7803030303030304","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","749228362","issue_comment","https://github.com/Trusted-AI/AIF360/issues/655#issuecomment-749228362","I am fine with @adrinjalali 's suggestion (i think that might be exactly also what @Dref360 had in mind).

My only concern is whether we should try to be more transparent with the user when we are using the inefficient implementation. Do we issue a warning or is that too annoying? I'm also okay just stating in the documentation which metrics have efficient streaming and that all other metrics just use a non-efficient fallback.","15","0.6270166453265044","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","749475964","issue_comment","https://github.com/Trusted-AI/AIF360/issues/655#issuecomment-749475964","I wouldn't raise a warning, but I'm happy to have that implementation detail explained in the docs with the performance issues related to it.","20","0.5274781715459681","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","749788703","issue_comment","https://github.com/Trusted-AI/AIF360/issues/655#issuecomment-749788703","Ok--let's then just have a boolean flag along the lines of [Frederic's comment](https://github.com/fairlearn/fairlearn/issues/655#issuecomment-744544045) and [Adrin's comment](https://github.com/fairlearn/fairlearn/issues/655#issuecomment-748647889) and with the initial inefficient implementation.

@Dref360 -- do you want to get started with that?

The next step is going to be to enable some efficient implementations. I see several options:

**Option 1**. We allow `pytorch_lightning.metrics.Metric` objects in the constructor, e.g.,

```python
import pytorch_lightning.metrics as plm
import fairlearn.metrics as flm

mf = flm.MetricFrame({'accuracy': plm.Accuracy, 'f1': plm.F1},
                     [], [], sensitive_features=[], streaming=True)
```

This sets up a pattern where we could continue adding support for other metric styles, e.g., we might add `tf.keras.metrics.Metric` etc.

  * the upside is that this will immediately provide access to many metrics beyond `sklearn.metrics`
  * the downside is that we're now creating a dependency on other packages

**Option 2**. We create our own streaming metric interface. The implication is that we would then need to build wrappers for `pytorch_lightning.metrics.Metric`, `tf.keras.metrics.Metric`, etc. The downside is that we are creating another new API for users to deal with.

**Option 3**. We create our own efficient streaming implementation that could be accessed via string, e.g.,
```python
mf = flm.MetricFrame(['accuracy_score', 'f1_score'],
                     [], [], sensitive_features=[], streaming=True)
```
In this case, we would create our own internal API for streaming metrics, but we wouldn't need to obsess about it too much (since we wouldn't be exporting it, so we could easily change it later). This is compatible with Option 1 above (we would be able to build our internal wrappers for this internal API).


Thoughts?","15","0.5006387055567384","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","750347558","issue_comment","https://github.com/Trusted-AI/AIF360/issues/655#issuecomment-750347558","Yes let's start with an unefficient v1. I'll try to get a PR opened before the next meeting in January","20","0.7230669686185698","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","743206297","issue_comment","https://github.com/Trusted-AI/AIF360/issues/654#issuecomment-743206297","After the `pip install --upgrade fairlearn` you were still getting ""Can't import MetricFrame"" ? That is strange. Can you check what version of Fairlearn the notebook thinks it has, with
```python
import fairlearn
print(fairlearn.__version__)
```","21","0.791611311444297","Installation and shell commands","Deployment"
"https://github.com/fairlearn/fairlearn","744626103","issue_comment","https://github.com/Trusted-AI/AIF360/issues/654#issuecomment-744626103","Advanced installation steps (https://fairlearn.github.io/v0.5.0/contributor_guide/development_process.html#advanced-install) are only for people who want to change the Fairlearn code
> While working on Fairlearn itself you may want to install it in editable mode. This allows you to test the changed functionality. 

Make sure you're using the right environment in your notebook (like @riedgar-ms pointed out). It could be that you upgraded a different env than the one that your notebook is using.","21","0.4461105904404874","Installation and shell commands","Deployment"
"https://github.com/fairlearn/fairlearn","745422541","issue_comment","https://github.com/Trusted-AI/AIF360/issues/654#issuecomment-745422541","I have restarted the kernel a couple of times and that didn't fix it. I then restarted the Compute Instance and it started working, I could get past the import error. I have now however a second error:

When I run 

`FairlearnDashboard(sensitive_features=X_test['Gender (code)'], sensitive_feature_names=['Gender (code)'], y_true=y_test, y_pred={""initial model"": preds}) `

I get a warning with pink background saying `The FairlearnDashboard will move from Fairlearn to the raiwidgets package after the v0.5.0 release. Instead, Fairlearn will provide some of the existing functionality through matplotlib-based visualizations.`

And then where I should have the dashboard I get:

```
FairlearnWidget(value={'true_y': [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0…
<fairlearn.widget._fairlearn_dashboard.FairlearnDashboard at 0x7fbd68fcc400>
```

And I get no visual dashboard. I have other visual elements in my notebook (inc. Pandas Profiling, Shap, Matplotlib) and those display without problems. I've tried in two Compute Instances (including one freshly installed, on the previous version) and have the same error. And it also happens in JupyterLab and plan Jupyter. A colleague told me she was only able to get it displaying in VS Code, but that's not an option for me.

The direct library calls work (`MetricFrame`, `.diverence`, `ExponentiatedGradient`, etc.) but not the UI.

I'm running on Edge Chromium beta, but it also happens on Dev, and there's no error in the browser console in either case. I use browser add-ins for security (Ghostery and Privacy Possum) but disabled them to make sure they weren't the cause.

Suggestions would be appreciated. I can also open another issue if required.","24","0.3803815232386662","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","745428440","issue_comment","https://github.com/Trusted-AI/AIF360/issues/654#issuecomment-745428440","I think this is a known issue - I believe that @romanlutz had to back all of the fixes out of the widget due to some compatibility issues. The widget does not load reliably in AzureML Notebooks. If you can stay in AzureML, you could use `azureml-contrib-fairness` to upload your dashboards to AzureML, which would at least give you a place to see them. Otherwise, you will have to run the notebook locally, unfortunately.","24","0.6001423380904223","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","745455779","issue_comment","https://github.com/Trusted-AI/AIF360/issues/654#issuecomment-745455779","Hey @riedgar-ms . Note that I'm not using AzureML Notebooks... I created an Azure ML Compute Instance, and I'm using plain/old JupyterLab/Jupyter. Is the dashboard also not working in this case?","24","0.8366314536527304","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","745497204","issue_comment","https://github.com/Trusted-AI/AIF360/issues/654#issuecomment-745497204","@romanlutz is that combination expected to work?","24","0.231645312182225","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","745600150","issue_comment","https://github.com/Trusted-AI/AIF360/issues/654#issuecomment-745600150","Yes, that's the only combination I'd expect to work on Azure. I'll try this out later today.

Update: This worked as expected for me using Jupyter. JupyterLab is not and has never been supported. It may get supported at some point in the future, but not through Fairlearn, only through raiwidgets.

![image](https://user-images.githubusercontent.com/10245648/102723830-cf5b8380-42bf-11eb-8d04-90c7c357d065.png)

This implementation of the dashboard is sometimes a bit moody and needs a page reload (not kernel restart, but page reload without kernel restart). That's because ipywidgets needs to negotiate with the notebook kernel first if I remember that correctly. That's one of the major reasons why we won't use ipywidgets in raiwidgets anymore, but rather flask. The release of raiwidgets is currently planned for January, so I don't have a workaround other than trying the page reload. If you're up for it I'd be happy to jump on a call with you and see whether there's anything I can do @lokijota .","24","0.7275472734136381","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","799851157","issue_comment","https://github.com/Trusted-AI/AIF360/issues/654#issuecomment-799851157","`raiwidgets` has been released (see [the repo](https://github.com/microsoft/responsible-ai-widgets) or [PyPI](https://pypi.org/project/raiwidgets/) for info) so I'll close this. If @lokijota would like to look into this more I'm still happy to jump on a call. For `raiwidgets` issues I recommend filing issues in the `responsible-ai-widgets` repo, though. ","24","0.75390551477508","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","742803202","issue_comment","https://github.com/Trusted-AI/AIF360/issues/652#issuecomment-742803202","Could you expand a bit on how you feel these notebooks demonstrate a fairness issue? Also, we're in the process of moving Fairlearn into a separate organisation; it's not going to be Microsoft-controlled going forwards.","7","0.5459980975676042","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","742805476","issue_comment","https://github.com/Trusted-AI/AIF360/issues/652#issuecomment-742805476","Hi Richard,

I’ve been working with Eddie de Leone from Microsoft. Eddie may be a better person to explain what the goals are for this effort to work with Fairlearn.

Generally speaking, there have been thoughts of having Smartnoise be either presented along with Fairlearn or at least have a similar theme in its individual site and repo as to that of Fairlearn.

Thanks,

Mike
From: Richard Edgar <notifications@github.com>
Date: Thursday, December 10, 2020 at 4:14 PM
To: fairlearn/fairlearn <fairlearn@noreply.github.com>
Cc: Mike Phelan <mphelan@g.harvard.edu>, Author <author@noreply.github.com>
Subject: Re: [fairlearn/fairlearn] Integration of smartnoise-samples documentation to Fairlearn documentation (#652)

Could you expand a bit on how you feel these notebooks demonstrate a fairness issue? Also, we're in the process of moving Fairlearn into a separate organisation; it's not going to be Microsoft-controlled going forwards.

—
You are receiving this because you authored the thread.
Reply to this email directly, view it on GitHub<https://github.com/fairlearn/fairlearn/issues/652#issuecomment-742803202>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AC7X6GUVOFL52GCICVMOB2TSUE23PANCNFSM4UVSG4YQ>.
","7","0.8751891185240684","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","742811095","issue_comment","https://github.com/Trusted-AI/AIF360/issues/652#issuecomment-742811095","We're aware of the differential privacy project - Eddie shared the office until around March. We're in the process of establishing a steering committee to co-ordinate collaborations such as what you're proposing here. You are of course welcome to make a pull request to add your notebooks (if you look in the `examples` directory, you'll see how to format them). However, before putting in that effort, you might want to talk to the steering committee - I've added @MiroDudik who is working on governance for the Fairlearn organisation.","13","0.3991999561619815","Artifact generation and benchmarking","Deployment"
"https://github.com/fairlearn/fairlearn","742811376","issue_comment","https://github.com/Trusted-AI/AIF360/issues/652#issuecomment-742811376","[tagging @eedeleon]

If you are interested in adopting our website infrastructure, you can definitely re-use our code (it's under MIT license).

If you would like to work out a use case, it just needs to follow [our guidelines](https://fairlearn.github.io/v0.5.0/contributor_guide/contributing_example_notebooks.html). We could discuss your use case in one of our next community calls?

Also, for context, this is the PR re. governance: https://github.com/fairlearn/governance/pull/1.

So if your use case is out of scope of Fairlearn project, you might be interested in bringing your project as an independent project into the Fairlearn organization, which would be a test drive for our project approval process. As things stand, the project would need to fit into the mission to ""provide a set of open source resources to assess and improve fairness of AI systems"" unless we broaden the mission, which would need to be decided by the steering committee... but in any case, it might be best to discuss the various alternatives in our next community call.","13","0.4469279677091394","Artifact generation and benchmarking","Deployment"
"https://github.com/fairlearn/fairlearn","744038695","issue_comment","https://github.com/Trusted-AI/AIF360/issues/652#issuecomment-744038695","Hello Michael and Richard,

The general goal was to adopt some of the similar repo management, release, documentation and sphynx practices from fairlearn to be consistent while also benefiting from the work you have all done in making fairlearn usable.

We don't expect both repos to show up on the same page outside of some external RAI page with links to both.

Best,
Eddie

From: Michael Phelan <mphelan@g.harvard.edu>
Sent: Thursday, December 10, 2020 4:18 PM
To: fairlearn/fairlearn <reply@reply.github.com>; fairlearn/fairlearn <fairlearn@noreply.github.com>
Cc: Author <author@noreply.github.com>; Eduardo de Leon <Eduardo.de@microsoft.com>; eddeleon@outlook.com; Eduardo de Leon <Eduardo.de@microsoft.com>
Subject: [EXTERNAL] Re: [fairlearn/fairlearn] Integration of smartnoise-samples documentation to Fairlearn documentation (#652)

Hi Richard,

I've been working with Eddie de Leone from Microsoft. Eddie may be a better person to explain what the goals are for this effort to work with Fairlearn.

Generally speaking, there have been thoughts of having Smartnoise be either presented along with Fairlearn or at least have a similar theme in its individual site and repo as to that of Fairlearn.

Thanks,

Mike
From: Richard Edgar <notifications@github.com<mailto:notifications@github.com>>
Date: Thursday, December 10, 2020 at 4:14 PM
To: fairlearn/fairlearn <fairlearn@noreply.github.com<mailto:fairlearn@noreply.github.com>>
Cc: Mike Phelan <mphelan@g.harvard.edu<mailto:mphelan@g.harvard.edu>>, Author <author@noreply.github.com<mailto:author@noreply.github.com>>
Subject: Re: [fairlearn/fairlearn] Integration of smartnoise-samples documentation to Fairlearn documentation (#652)

Could you expand a bit on how you feel these notebooks demonstrate a fairness issue? Also, we're in the process of moving Fairlearn into a separate organisation; it's not going to be Microsoft-controlled going forwards.

-
You are receiving this because you authored the thread.
Reply to this email directly, view it on GitHub<https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Ffairlearn%2Ffairlearn%2Fissues%2F652%23issuecomment-742803202&data=04%7C01%7CEduardo.de%40microsoft.com%7C7a43cbb61089480cec3908d89d511c9e%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637432319424650261%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000&sdata=Y2qEg5ykpuUIpF4eOrkdT%2Bxfj%2BdYJL59L1ubQ5e25Ls%3D&reserved=0>, or unsubscribe<https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fnotifications%2Funsubscribe-auth%2FAC7X6GUVOFL52GCICVMOB2TSUE23PANCNFSM4UVSG4YQ&data=04%7C01%7CEduardo.de%40microsoft.com%7C7a43cbb61089480cec3908d89d511c9e%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637432319424660218%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000&sdata=uSHvZVUaoxDUG%2FRioZJzM49S7J34ktTQ0ARJDCrJ8rw%3D&reserved=0>.
","7","0.844543945263369","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","744045514","issue_comment","https://github.com/Trusted-AI/AIF360/issues/652#issuecomment-744045514","Could somebody maybe give some context for the non-Microsoft people?

On Sun., Dec. 13, 2020, 18:15 Mike Phelan, <notifications@github.com> wrote:

> Hello Michael and Richard,
>
> The general goal was to adopt some of the similar repo management,
> release, documentation and sphynx practices from fairlearn to be consistent
> while also benefiting from the work you have all done in making fairlearn
> usable.
>
> We don't expect both repos to show up on the same page outside of some
> external RAI page with links to both.
>
> Best,
> Eddie
>
> From: Michael Phelan <mphelan@g.harvard.edu>
> Sent: Thursday, December 10, 2020 4:18 PM
> To: fairlearn/fairlearn <reply@reply.github.com>; fairlearn/fairlearn <
> fairlearn@noreply.github.com>
> Cc: Author <author@noreply.github.com>; Eduardo de Leon <
> Eduardo.de@microsoft.com>; eddeleon@outlook.com; Eduardo de Leon <
> Eduardo.de@microsoft.com>
> Subject: [EXTERNAL] Re: [fairlearn/fairlearn] Integration of
> smartnoise-samples documentation to Fairlearn documentation (#652)
>
> Hi Richard,
>
> I've been working with Eddie de Leone from Microsoft. Eddie may be a
> better person to explain what the goals are for this effort to work with
> Fairlearn.
>
> Generally speaking, there have been thoughts of having Smartnoise be
> either presented along with Fairlearn or at least have a similar theme in
> its individual site and repo as to that of Fairlearn.
>
> Thanks,
>
> Mike
> From: Richard Edgar <notifications@github.com<mailto:
> notifications@github.com>>
> Date: Thursday, December 10, 2020 at 4:14 PM
> To: fairlearn/fairlearn <fairlearn@noreply.github.com<mailto:
> fairlearn@noreply.github.com>>
> Cc: Mike Phelan <mphelan@g.harvard.edu<mailto:mphelan@g.harvard.edu>>,
> Author <author@noreply.github.com<mailto:author@noreply.github.com>>
> Subject: Re: [fairlearn/fairlearn] Integration of smartnoise-samples
> documentation to Fairlearn documentation (#652)
>
> Could you expand a bit on how you feel these notebooks demonstrate a
> fairness issue? Also, we're in the process of moving Fairlearn into a
> separate organisation; it's not going to be Microsoft-controlled going
> forwards.
>
> -
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub<
> https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Ffairlearn%2Ffairlearn%2Fissues%2F652%23issuecomment-742803202&data=04%7C01%7CEduardo.de%40microsoft.com%7C7a43cbb61089480cec3908d89d511c9e%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637432319424650261%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000&sdata=Y2qEg5ykpuUIpF4eOrkdT%2Bxfj%2BdYJL59L1ubQ5e25Ls%3D&reserved=0>,
> or unsubscribe<
> https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fnotifications%2Funsubscribe-auth%2FAC7X6GUVOFL52GCICVMOB2TSUE23PANCNFSM4UVSG4YQ&data=04%7C01%7CEduardo.de%40microsoft.com%7C7a43cbb61089480cec3908d89d511c9e%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637432319424660218%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000&sdata=uSHvZVUaoxDUG%2FRioZJzM49S7J34ktTQ0ARJDCrJ8rw%3D&reserved=0
> >.
>
> —
> You are receiving this because you are subscribed to this thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/fairlearn/fairlearn/issues/652#issuecomment-744038695>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AAMWG6CFC7SCZ5FWYRXSWKTSUTZE3ANCNFSM4UVSG4YQ>
> .
>
","7","0.8525005885716213","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","744455142","issue_comment","https://github.com/Trusted-AI/AIF360/issues/652#issuecomment-744455142","@adrinjalali this is about another project (with external collaborators) which came under the MS Responsible AI umbrella. It's about adding noise to a model so that its results remain statistically valid, while also preventing an attacker from recovering the data used to train it via repeated requests.

@mikephelan I'd be happy to explain our repo set up to you. However, I don't think we need that discussion here - could you reach out to me directly?","7","0.270951822093828","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","744745404","issue_comment","https://github.com/Trusted-AI/AIF360/issues/652#issuecomment-744745404","I've created a few changes to keep this conversation going. I understand from my partners at MSFT that the ultimate direction will be not to stage our doc under fairlearn, but for this iteration I did so to come up to speed.
https://github.com/mikephelan/fairlearn/tree/652_sn_smpls_to_fl_doc
- script to automate process of converting ipynb to percent-format py
- all smartnoise-samples ipnyb converted to percent-format py
- configuration changes to accomodate python utility files
","25","0.3016340883235081","Research","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","744745720","issue_comment","https://github.com/Trusted-AI/AIF360/issues/652#issuecomment-744745720","One thing that I had trouble with was assigning multiple values to sphinx_gallery_conf.examples_dirs. This appears to ignore the additional target location.
fairlearn/docs/conf.py:
```
sphinx_gallery_conf = {
    #'examples_dirs': '../examples',
'examples_dirs': ['../examples', ‘../examples/opendp’],
    'gallery_dirs': 'auto_examples',
    # pypandoc enables rst to md conversion in downloadable notebooks
    'pypandoc': True,
}
```","14","0.6214386190782726","Documentation","Development"
"https://github.com/fairlearn/fairlearn","745276652","issue_comment","https://github.com/Trusted-AI/AIF360/issues/652#issuecomment-745276652","Have sent response to sphinx query over email. I don't think this issue needs to continue on Fairlearn.","7","0.6039464411557434","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","738478817","issue_comment","https://github.com/Trusted-AI/AIF360/issues/650#issuecomment-738478817","It would be good to know the original legal reasoning, since I agree that the friction produced is painful.","20","0.6120440069484658","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","738625598","issue_comment","https://github.com/Trusted-AI/AIF360/issues/650#issuecomment-738625598","If we're making fairlearn an independent entity, does it matter what the original reason was? I would understand if Microsoft would require Microsoft employees to have the DCO signatures in their commits, but as the fairlearn entity, there's no DCO requirement to contribute to an MIT Licensed project.","20","0.8656552061666468","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","738626503","issue_comment","https://github.com/Trusted-AI/AIF360/issues/650#issuecomment-738626503","I'll reach out to one of the OSI members and ask if they have relevant documents, but we do have all those MIT and BSD licensed projects out there that don't require a DCO. I've only seen it if there's one single commercial entity behind a project.","20","0.920646729157368","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","738743484","issue_comment","https://github.com/Trusted-AI/AIF360/issues/650#issuecomment-738743484","> If we're making fairlearn an independent entity, does it matter what the original reason was? I would understand if Microsoft would require Microsoft employees to have the DCO signatures in their commits, but as the fairlearn entity, there's no DCO requirement to contribute to an MIT Licensed project.

I have a memory that HDF5 ran into trouble because they couldn't dot all i's and cross all t's on who had made code contributions. Having said that, I don't remember the details. Personally, I agree that for a public project on GitHub, it is unlikely that anyone could be able to assert IP rights retroactively. It would just be nice to know whether there was a particular reason for the DCO requirement, or if it was simply a commercial lawyer being paranoid.","20","0.6404621490300806","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","738772134","issue_comment","https://github.com/Trusted-AI/AIF360/issues/650#issuecomment-738772134","Seems like people put a DCO in place to not have to deal with potential cases against the company if somebody submits some code which breaks some copyright law. In my experience from other projects, for fairlearn that would really not be a probable risk.

On the other side, DCO is a point of friction which is counterproductive to attracting more contributors from the community.

So in the interest of the community and having a very low level of risk regarding copyrighted code, I'd say we can just remove it, and add it if at some point in the future we think it's necessary.","20","0.6363894921915401","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","741316733","issue_comment","https://github.com/Trusted-AI/AIF360/issues/650#issuecomment-741316733","Couldn't agree more with @adrinjalali on this. I've put it on the agenda for Thursday's community call. Unless someone objects let's remove it once the governance PR by @MiroDudik is completed.","20","0.6461694186120075","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","741697833","issue_comment","https://github.com/Trusted-AI/AIF360/issues/650#issuecomment-741697833","We also had a meeting with an open source lawyer organized by NumFocus, and I did raise this question. Her assessment was that companies sue companies cause it's worth it, but if the project is organized by individuals, they're too small of a cake for them to get anything worthy out of, therefore the probability of getting sued goes down drastically.

Also, this is only important and relevant for major contributions. The fixes, website improvements, small doc improvements, etc are of no risk, and for the major ones, we can and should of course either trust the contributor or ask if we're allowed to use the code, if there is any suspicion. I personally have been in a situation where I had to contact the author of an implementation to ask if I'm allowed to look at their code, and in that case I wasn't given the permission, so had to implement from the paper itself. But we don't need a DCO for that, we just need to be aware of these cases.","20","0.4282641766773332","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","742193052","issue_comment","https://github.com/Trusted-AI/AIF360/issues/650#issuecomment-742193052","Thanks for elaborating. This is exactly what I wanted to understand. I think that's fair enough. If we want to add major contributions we can just ask the author to explicitly agree. I don't see that as a big deal, especially compared with the burden we put on new contributors with DCO right now.","20","0.6890186266333054","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","742625619","issue_comment","https://github.com/Trusted-AI/AIF360/issues/650#issuecomment-742625619","Fantastic. Let's remove it.","11","0.2975444096133752","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","820572871","issue_comment","https://github.com/Trusted-AI/AIF360/issues/650#issuecomment-820572871","I just uninstalled the DCO app. Closing this item accordingly, but if against all odds it still requires passing the DCO test please let me know!","20","0.3319360754519255","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","820703514","issue_comment","https://github.com/Trusted-AI/AIF360/issues/650#issuecomment-820703514","Woohoo, the brave new world of DCO free contibs","20","0.5383675464320625","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","736813894","issue_comment","https://github.com/Trusted-AI/AIF360/issues/648#issuecomment-736813894","@romanlutz I'm here! I'll quickly go through their model training, and if any interesting thing comes up, I will post in this thread and also add it to the hackmd. ","20","0.7050407219898743","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","737346595","issue_comment","https://github.com/Trusted-AI/AIF360/issues/648#issuecomment-737346595","If the intention here is to demonstrate the value of Fairlearn 0.50, it might be productive to try using the guidelines in [Contributing example notebooks](https://fairlearn.github.io/v0.5.0/contributor_guide/contributing_example_notebooks.html).  If there's a different intention - eg, advocating for not building particular kinds of ML systems, illustrating a concept for educational purposes, reaching an audience other than potential Fairlearn users - it might help to articulate that intention in the pull request description.  That can help sort out feedback on the intention itself vs. feedback on execution.

From the notes, it sounds like there might be an abstraction trap (Selbst et al. 2019) around focusing on the dataset, rather than a deployment context.  [The myth of generalisability in clinical research and machine learning in health care (Futoma 2020)](https://www.thelancet.com/journals/landig/article/PIIS2589-7500(20)30186-2/fulltext) might be helpful for thinking through that:
> ...the desire for geographical generalisability is a proxy for validity... In situations with strong signals and little local variability across sites, this strategy might make sense. However, in many scenarios there are too many practice patterns and other local idiosyncrasies that make learning a broadly applicable model effectively impossible. Instead, machine learning systems in these settings can be viewed as an aspirational form of evidence-based medicine—local data to create local inferences for local patients and clinicians... If hospitals want to have useful machine learning systems at the bedside, the broader research community need to stop focusing solely on generalisability and consider the ultimate goal: will this system be useful in this specific case?

Also, I agree that focusing on the labeling procedure seems critical:

> ...we note that across these models, our source of diagnostic labels for these images must be considered at best “silver” labels, as all currently existing public chest X-ray datasets use automatically determined labels based on natural language processing (NLP) techniques to extract labels from the radiology reports. These silver labels may be incorrect, in ways that could compound with observe biases or model errors, a risk that warrants further investigation. [(Seyyed-Kalantari et al. 2020)](https://arxiv.org/pdf/2003.00827.pdf)

If there isn't even real labeled data, would that influence whether how a practitioner might weight the tradeoffs when deploying into a particular deployment context?  This seems really important.

If the intention in this work is to write notebook code, a minimal step could be to model the upstream errors in labeling and illustrate how that error propagates to any downstream metrics (eg, add 10% labeling error to the dataset, and show how that leads to noise in downstream metrics over 1000 simuations).","8","0.3267016222337101","Understanding Fairness Definition","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","737570101","issue_comment","https://github.com/Trusted-AI/AIF360/issues/648#issuecomment-737570101","Thanks @kevinrobinson . We are explicitly considering different kinds of ways this could be deployed. At this stage this is entirely exploratory and it's hard to say what exactly the output will be. I definitely don't want to make this about showcasing the Python toolkit. Either it fits what we need for this, or it doesn't (in which case this can serve as inspiration for the toolkit). These are all things that are on our mind as well, so that's encouraging.","8","0.3163690076540887","Understanding Fairness Definition","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","736585997","issue_comment","https://github.com/Trusted-AI/AIF360/issues/645#issuecomment-736585997","I'm slightly confused by the initial point.... `setup.py` reads in `requirements.txt` (not to mention`requirements-customplots.txt`) - and all packages in there have lower bounds.

If there's a richer way of describing things, I'd be happy to use it. The one thing which I wouldn't like would be going back to having 'truth' in multiple places.","15","0.3988597313339582","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","736767953","issue_comment","https://github.com/Trusted-AI/AIF360/issues/645#issuecomment-736767953","I believe @adrinjalali  was referring to a pyproject.toml file for environment specific configuration https://github.com/scikit-learn/scikit-learn/blob/master/pyproject.toml

Overall it looks like scikit-learn has a different setup.

It looks like our current setup requires you to install multiple requirements files to get all dev dependencies, whereas @koaning expected just requirements-dev to be sufficient. Perhaps we should be explaining this better in contributor guide & README?","4","0.5003703345892505","Installation and shell commands","Deployment"
"https://github.com/fairlearn/fairlearn","737121159","issue_comment","https://github.com/Trusted-AI/AIF360/issues/645#issuecomment-737121159","These may give a bit of a context on why projects are using `pyproject.toml`:
- https://www.python.org/dev/peps/pep-0621/
- https://www.python.org/dev/peps/pep-0518/
- and pip's interaction with pep517: https://pip.pypa.io/en/stable/reference/pip/#build-system-interface

","7","0.1989874532247413","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","876082623","issue_comment","https://github.com/Trusted-AI/AIF360/issues/645#issuecomment-876082623","I think in terms of where we want to end up #245 and this issue are identical. That goal would be to have equivalents of scikit-learn's
- https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/_min_dependencies.py
- https://github.com/scikit-learn/scikit-learn/blob/main/pyproject.toml

and to eliminate our
- requirements.txt
- requirements-dev.txt
- requirements-customplots.txt

Additionally, it would be nice to have this described with a few lines in the contributing guide. I'll close #245 since this one has the better title.","20","0.2949078412995939","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","732151470","issue_comment","https://github.com/Trusted-AI/AIF360/issues/641#issuecomment-732151470","For the entire package, there is a whitepaper:
https://www.microsoft.com/en-us/research/uploads/prod/2020/05/Fairlearn_whitepaper.pdf
The two main techniques have papers:
https://arxiv.org/pdf/1803.02453.pdf
https://papers.nips.cc/paper/6374-equality-of-opportunity-in-supervised-learning.pdf

I'm not sure of where to get a standardised BibTeX format for these.... @MiroDudik ?","30","0.4064498192938558","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","732198609","issue_comment","https://github.com/Trusted-AI/AIF360/issues/641#issuecomment-732198609","Thanks","0","0.0303030303030302","Dataset usage","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","732201073","issue_comment","https://github.com/Trusted-AI/AIF360/issues/641#issuecomment-732201073","@MiroDudik just sent this:
```
@techreport{bird2020fairlearn,
author = {Bird, Sarah and Dud{\'i}k, Miro and Edgar, Richard and Horn, Brandon and Lutz, Roman and Milan, Vanessa and Sameki, Mehrnoosh and Wallach, Hanna and Walker, Kathleen},
title = {Fairlearn: A toolkit for assessing and improving fairness in {AI}},
institution = {Microsoft},
year = {2020},
month = {May},
url = ""https://www.microsoft.com/en-us/research/publication/fairlearn-a-toolkit-for-assessing-and-improving-fairness-in-ai/"",
number = {MSR-TR-2020-32},
}
```","7","0.4789890607317996","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","732205236","issue_comment","https://github.com/Trusted-AI/AIF360/issues/641#issuecomment-732205236","Will use ","0","0.0303030303030302","Dataset usage","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","802019044","issue_comment","https://github.com/Trusted-AI/AIF360/issues/641#issuecomment-802019044","I find it complicated to find this information on the website.

1. Go to fairlearn.io
2. Click on API Docs
3. Click on ""About""
4. Find the section https://fairlearn.org/v0.6.0/about/index.html#citing-fairlearn","14","0.8434609250398727","Documentation","Development"
"https://github.com/fairlearn/fairlearn","728787402","issue_comment","https://github.com/Trusted-AI/AIF360/issues/639#issuecomment-728787402","Colab is not explicitly supported. Note that after v0.5.0 the dashboard is moving to the new `raiwidgets` package (to be released by the end of the month). We're aiming to provide support for a broader set of environments (including Colab) in `raiwidgets`, although that may not happen at the time of the initial release.

Closing this issue since it won't be addressed in Fairlearn.","24","0.7141125541125543","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","725655817","issue_comment","https://github.com/Trusted-AI/AIF360/issues/633#issuecomment-725655817","That would be `numpy.squeeze()` converting one element arrays to scalars. To be helpful.","17","0.3992952783650458","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","725681186","issue_comment","https://github.com/Trusted-AI/AIF360/issues/633#issuecomment-725681186","From @MiroDudik , it appears that [Numpy have a routine to undo this damage](https://numpy.org/doc/stable/reference/generated/numpy.atleast_1d.html).","31","0.4348729227761486","Testing","Maintenance"
"https://github.com/fairlearn/fairlearn","724394498","issue_comment","https://github.com/Trusted-AI/AIF360/issues/629#issuecomment-724394498","@romanlutz Intended fix in progress. ","20","0.5522810522810524","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","724411494","issue_comment","https://github.com/Trusted-AI/AIF360/issues/629#issuecomment-724411494","@MarsLabAtSLU this was already reported in #593 . Since we're removing the widget and moving it to a dedicated visualization package called `raiwidgets` we've rolled back to the previous version of the widget. I'm certainly interested in your idea for a fix, since that may come in handy in the new repository (which will be public once `raiwidgets` is released, ETA end of November).","24","0.9464563499471096","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","724412977","issue_comment","https://github.com/Trusted-AI/AIF360/issues/629#issuecomment-724412977","@romanlutz sure will share my changes at the earliest.","24","0.6253813300793166","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","727056884","issue_comment","https://github.com/Trusted-AI/AIF360/issues/629#issuecomment-727056884","@romanlutz I’m trying to find the branch/tag where it-was-in the previous state which used ""rai_core_flask FlaskHelper” in widget/Fairlearn_dashboard.py. So that, I can make my changes and raise Pull request. Could you please let me know.","24","0.426633305532388","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","727102532","issue_comment","https://github.com/Trusted-AI/AIF360/issues/629#issuecomment-727102532","@MarsLabAtSLU there was no tag/release version with flask since we rolled the change back before releasing. If you're looking for a specific version this one should work: https://github.com/fairlearn/fairlearn/tree/ce7f27a51a428dae7627c1eed6373fc4e626673f

Note that we will remove the entire dashboard from this repo shortly (I might do it today if I get to it) and it'll move to another repo, so a related PR in this repo won't be accepted because of that. If you have an idea for how to fix it feel free to outline it here or create a branch, though, since we still need to fix it regardless (just in another repo).","24","0.8190065714209698","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","728780655","issue_comment","https://github.com/Trusted-AI/AIF360/issues/629#issuecomment-728780655","@MarsLabAtSLU There was an additional issue to fix. In case you are eager to build a dashboard manually in the meanwhile:

When running two dashboards with different ports they ran into a name collision of the functions with the `route` decorator. To fix that I defined the function without `route` decorator, renamed the function to be unique (by using the associated port below) and then applied `add_url_rule` manually instead of through the `route` decorator
```
def visual():
    ....
visual.__name__ = f""visual{self._service.port}""
self._service.app.add_url_rule('/', endpoint=visual.__name__, view_func=visual)
```
This has to be done for every function with a route decorator, including the one that is used to request metrics.

Since this is  
- neither a real issue the released dashboard is facing (since it's not on flask)
- nor will it become one for Fairlearn because the dashboard will be removed shortly
I am closing this issue and the associated pull request. 

Thanks @MarsLabAtSLU for your suggestion! Stay tuned for the upcoming release of `raiwidgets` which will provide a flask-based version of the fairness dashboard (and some new features).","24","0.3633615628669214","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","721081464","issue_comment","https://github.com/Trusted-AI/AIF360/issues/621#issuecomment-721081464","This sounds like you have `v0.4.6` of Fairlearn installed from `pip`, but are following instructions for `master` ?","21","0.5698379140239606","Installation and shell commands","Deployment"
"https://github.com/fairlearn/fairlearn","721083274","issue_comment","https://github.com/Trusted-AI/AIF360/issues/621#issuecomment-721083274","Yes , I exactly follow instruction from https://github.com/fairlearn/fairlearn/blob/master/notebooks/Binary%20Classification%20with%20the%20UCI%20Credit-card%20Default%20Dataset.ipynb

Pip install fairlearn 0.4.6

I get two error :

1-  from fairlearn.metrics import MetricFrame

2-  sweep_preds = [predictor.predict(df_test) for predictor in sweep.predictors_]




Sent from Mail<https://go.microsoft.com/fwlink/?LinkId=550986> for Windows 10

From: Richard Edgar<mailto:notifications@github.com>
Sent: Tuesday, November 3, 2020 1:16 PM
To: fairlearn/fairlearn<mailto:fairlearn@noreply.github.com>
Cc: Shoresh Khezri<mailto:Shoresh.Khezri@microsoft.com>; Author<mailto:author@noreply.github.com>
Subject: Re: [fairlearn/fairlearn] 'GridSearch' object has no attribute 'predictors_' (#621)


This sounds like you have v0.4.6 of Fairlearn installed from pip, but are following instructions for master ?

—
You are receiving this because you authored the thread.
Reply to this email directly, view it on GitHub<https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Ffairlearn%2Ffairlearn%2Fissues%2F621%23issuecomment-721081464&data=04%7C01%7Cshoresh.khezri%40microsoft.com%7C4c40b75f84424c2a281108d87ff24f3f%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637400025978100497%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=9vTTohfrF2mtcSmZlmenl3mQuOTYFb7veEDoAyiorIo%3D&reserved=0>, or unsubscribe<https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fnotifications%2Funsubscribe-auth%2FALQCF3YJB4GEIXPDMECEWDTSN7YCHANCNFSM4TISTBPQ&data=04%7C01%7Cshoresh.khezri%40microsoft.com%7C4c40b75f84424c2a281108d87ff24f3f%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637400025978100497%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=xxFfESlWcK%2FvGhUo5LHX5yl2pkelrfI3bqNl%2FnWaJOg%3D&reserved=0>.

","7","0.4704446084833343","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","721084482","issue_comment","https://github.com/Trusted-AI/AIF360/issues/621#issuecomment-721084482","We've had quite a few breaking changes unfortunately. We're hoping to get `v0.5.0` out this week, but in the mean time, you can clone the Fairlearn repository and `pip install .` on it, which will give you a match between the library and examples.","21","0.3399759600177137","Installation and shell commands","Deployment"
"https://github.com/fairlearn/fairlearn","721087933","issue_comment","https://github.com/Trusted-AI/AIF360/issues/621#issuecomment-721087933","you mean :
git clone git@github.com:fairlearn/fairlearn.git

pip install -r requirements.txt

","21","0.5679374389051809","Installation and shell commands","Deployment"
"https://github.com/fairlearn/fairlearn","721093096","issue_comment","https://github.com/Trusted-AI/AIF360/issues/621#issuecomment-721093096","Once you've cloned the repo, you can just run `pip install .` from its root. The requirements will be automatically installed (if you don't have them already)","21","0.7334730591214738","Installation and shell commands","Deployment"
"https://github.com/fairlearn/fairlearn","721093739","issue_comment","https://github.com/Trusted-AI/AIF360/issues/621#issuecomment-721093739","Thanks","0","0.0303030303030302","Dataset usage","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","721128446","issue_comment","https://github.com/Trusted-AI/AIF360/issues/621#issuecomment-721128446","Are you unblocked now?","28","0.3274917853231106","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","721154844","issue_comment","https://github.com/Trusted-AI/AIF360/issues/621#issuecomment-721154844","yes , it helped and I can load MetricFrame now
Thanks
","5","0.1711076280041798","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","721162145","issue_comment","https://github.com/Trusted-AI/AIF360/issues/621#issuecomment-721162145","need to clone repo and install ","21","0.6499898311978851","Installation and shell commands","Deployment"
"https://github.com/fairlearn/fairlearn","721435189","issue_comment","https://github.com/Trusted-AI/AIF360/issues/621#issuecomment-721435189","if we clone the git repo then we face another problem in widge and dashboard can not be seen ","24","0.4229483446565296","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","721466069","issue_comment","https://github.com/Trusted-AI/AIF360/issues/621#issuecomment-721466069","Did you `pip install .` or `pip install -e .` - I believe that you have to do the former at least once.","21","0.7335997335997335","Installation and shell commands","Deployment"
"https://github.com/fairlearn/fairlearn","721466955","issue_comment","https://github.com/Trusted-AI/AIF360/issues/621#issuecomment-721466955","Shouldn't make a difference with the flask server anymore as long as you do install it. Where are you running this? ","24","0.2533160789388548","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","721607851","issue_comment","https://github.com/Trusted-AI/AIF360/issues/621#issuecomment-721607851","I used Azure ml notebook and VScode , the problem pip install . has some clash with azure machine learning environment ,and probably we need to create new environment which it would be great that you add this procedure in setup and installation documentation with git clone","21","0.4927443448570207","Installation and shell commands","Deployment"
"https://github.com/fairlearn/fairlearn","721665746","issue_comment","https://github.com/Trusted-AI/AIF360/issues/621#issuecomment-721665746","That sounds exactly like the problem @romanlutz is investigating now.

Are you able to run the notebook on your local machine?","21","0.658753136460143","Installation and shell commands","Deployment"
"https://github.com/fairlearn/fairlearn","721722840","issue_comment","https://github.com/Trusted-AI/AIF360/issues/621#issuecomment-721722840","@riedgar-ms @romanlutz  if you install with pip fairlearn then local machine can see dashboard but you face problem with 'predictors_ and metricsframe and if you clon the rep and pip install . then the problem of 'predictors_ and metricsframe will be solved but then you get error to import dashboard since they are not match..","21","0.4687043488032363","Installation and shell commands","Deployment"
"https://github.com/fairlearn/fairlearn","721750569","issue_comment","https://github.com/Trusted-AI/AIF360/issues/621#issuecomment-721750569","I'm not quite clear on what you're doing. I've just been able to do the following:

1. Create a new conda environment (or whatever you prefer)
1. Run `pip install git+https://github.com/fairlearn/fairlearn.git` (on Windows, I also had to run `pip install --upgrade numpy=1.19.3` thanks to a disagreement between Windows and numpy 1.19.4)
1. Run the [census GridSearch notebook](https://fairlearn.github.io/master/auto_examples/plot_grid_search_census.html) and see the dashboard

Are those steps not working for you?","21","0.6360722610722612","Installation and shell commands","Deployment"
"https://github.com/fairlearn/fairlearn","725862911","issue_comment","https://github.com/Trusted-AI/AIF360/issues/621#issuecomment-725862911","We rolled the dashboard back to the previous version before the v0.5.0 release so this problem shouldn't occur anymore. We validated that AML and local notebooks work with the dashboard in v0.5.0. If that's not working for you please open a new issue (since this one is for different problem).

The previous issue is addressed by versioned documentation. You should find a version selector on the webpage in the top left corner.","24","0.8237851415322192","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","721368611","issue_comment","https://github.com/Trusted-AI/AIF360/issues/619#issuecomment-721368611","![Screen Shot 2020-11-03 at 3 51 27 PM](https://user-images.githubusercontent.com/11827679/98039267-722d7080-1dec-11eb-8bab-a88e7dbd4861.png)
@shoresh57 I encountered the same issue here. Would you like to tell me if you have resolved this issue? If so, how?","16","0.5235826001955036","Testing","Maintenance"
"https://github.com/fairlearn/fairlearn","721371584","issue_comment","https://github.com/Trusted-AI/AIF360/issues/619#issuecomment-721371584","@violazhong 
if you installed with pip then it is not working , you should clone git repo : git clone https://github.com/fairlearn/fairlearn.git then it is working but you maybe face another challenge which is widge that you cannot see dashboard fairlearn ,for that part I just upload dashboard to azure ml with the help of this document and everything worked well https://docs.microsoft.com/en-us/azure/machine-learning/how-to-machine-learning-fairness-aml","24","0.4052515263887328","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","719857678","issue_comment","https://github.com/Trusted-AI/AIF360/issues/611#issuecomment-719857678","@bwuzhang thanks for reporting this!
I'm able to reproduce this with the steps you've outlined. I believe that it isn't a bug, though. Let me try to explain:

The notebook itself outputs this table somewhere earlier:
![image](https://user-images.githubusercontent.com/10245648/97766119-51a2a500-1ad2-11eb-976a-53c06ef8e0c6.png)

From this we can tell that the vast majority of rows (people!) are marked with race ""white"", so the dataset isn't well balanced in terms of race. Further, it looks like almost 97% of the white students passed (label 1), and about 78% of black students passed. Again, this tells us that the dataset isn't well balanced, this time with respect to labels 0 and 1. 

Let's take a step back and examine what `DemographicParity(ratio_bound=0.98)` actually means. What we're hoping to achieve with this formulation is that every group is within 2 percentage points (`1-0.98=0.02`, ignoring the ratio bound slack here for a minute) of the group with maximum selection rate. Let's look at the dataset itself. If we used that as a comparison, we'd get roughly `0.78/0.97`, which is about `0.8` and that's far below `0.98`. So `ExponentiatedGradient` tries to get the selection rates of both groups close to each other (so that min divided by max is at least `0.98`) while at the same time trying to optimize accuracy. 

I actually think it would be intuitive if it used dummy classifiers (constant label 1 for everyone), since that makes the ratio exactly 1. So that's exactly how it starts out. The following iterations are not dummy classifiers, though:

```
'predictors_': 
 0    DummyClassifier(constant=1, strategy='constant')
 1              LogisticRegression(solver='liblinear')
 2              LogisticRegression(solver='liblinear')
 3              LogisticRegression(solver='liblinear')
 4              LogisticRegression(solver='liblinear')
```

From the resulting weights we can tell that it doesn't even include the initial dummy classifier in the final solution:
```
'weights_':
 0    0.000000
 1    0.546098
 2    0.000000
 3    0.000000
 4    0.453902
```
We could look further into the classifiers 1 and 4, but I don't think that was necessarily what you asked. I agree that it is confusing that it keeps printing out
```
DEBUG:fairlearn.reductions._exponentiated_gradient._lagrangian:redY had single value. Using DummyClassifier
```
in every iteration. @MiroDudik is that perhaps because it's the first attempt, and then it tries different values of `mul` from `1` to `10` which produce better results?

I don't quite have a good and simple explanation for why it never occurred with `ratio_bound=0.99`, but always with `ratio_bound=0.98`.

It was quite puzzling to me that the resulting selection rates are so low. Given that the dataset was so imbalanced, I would have expected them to be higher. Given that we care about ranking here it's perhaps not all that important. More important is the order, or the threshold(s) we set for admitting students.
![image](https://user-images.githubusercontent.com/10245648/97766978-6c771880-1ad6-11eb-8f2b-20d43a491d69.png)


I think this is a wonderful reminder for us to finally write a thorough user guide for our reductions techniques. @riedgar-ms started summarizing some of the background on these techniques in the past, and we should perhaps revive that effort. I don't think we can expect users to piece this together without documentation.","3","0.2369321735175393","Bias detection metrics validation","Validation"
"https://github.com/fairlearn/fairlearn","716608845","issue_comment","https://github.com/Trusted-AI/AIF360/issues/610#issuecomment-716608845","It's been a long time since the last release, and we don't have versioned docs. I believe you've come across one of the places where the docs are far ahead of the PyPI installed version. Does it work if you install Fairlearn from git?","32","0.6691339280065793","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","716814264","issue_comment","https://github.com/Trusted-AI/AIF360/issues/610#issuecomment-716814264","After installing the package directly from github, the example in the user guide works as a charm. Thanks","14","0.2794133505877277","Documentation","Development"
"https://github.com/fairlearn/fairlearn","716160669","issue_comment","https://github.com/Trusted-AI/AIF360/issues/609#issuecomment-716160669","A second question comes to mind, why is it that `selection_rate` takes both `y_true` and `y_pred` ? I believe it's done only for API consistency and to be able to pass it to functions like `group_summary` and it uses only `y_pred` for the calculation. If that's the case, I believe it would be good to mention this in the documentation of `selection_rate`","12","0.4514297908664104","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","716611056","issue_comment","https://github.com/Trusted-AI/AIF360/issues/609#issuecomment-716611056","Metrics are all about to change a lot.

You are right that `selection_rate()` requires `y_true` as well for API consistency. In the metrics update I'm working on now, the fact that `y_true` is ignored is mentioned.","15","0.4657124882038376","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","716666233","issue_comment","https://github.com/Trusted-AI/AIF360/issues/609#issuecomment-716666233","Thanks for your answer, glad to know this","20","0.5504179728317662","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","716061830","issue_comment","https://github.com/Trusted-AI/AIF360/issues/608#issuecomment-716061830","Yes! This is related to https://github.com/fairlearn/fairlearn-proposals/blob/master/ui/migration.md . In short:
- The existing `FairlearnDashboard` will be migrated out of Fairlearn shortly (after the v0.5.0 release which will happen as soon as the metrics API is finalized) for reasons outlined in that link.
- At that point we'll complete #561 and update the documentation including the getting started page to show these plots instead. As you pointed out it's much better to have the plots actually render without having to download it first.

I'm hoping we can expand on these charts as well, since they provide very minimal functionality at this point. It's a good start, though! 

I think this issue is a good way to ""track"" the effort to update the docs after the `FairlearnDashboard` is removed, so I'd propose to leave it open. I'll assign it to myself since I promised to take care of the removal and docs update, but I'm very much looking for feedback on the resulting docs. I'll make sure to tag you on the PR(s).

Does this make sense @koaning ? Please keep the feedback & suggestions coming :-)","20","0.3133670638158968","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","843391219","issue_comment","https://github.com/Trusted-AI/AIF360/issues/608#issuecomment-843391219","#770 and #766 are related to this. #770 is already completed and #766 will be shortly, at which point we can release v0.7.0 as the first version without dashboard.","24","0.5142887954275853","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","713240185","issue_comment","https://github.com/Trusted-AI/AIF360/issues/606#issuecomment-713240185","We should be getting that warning actually! The file is not listed in the index file:
https://github.com/fairlearn/fairlearn/blob/master/docs/contributor_guide/developer_call_notes/index.rst","4","0.5069484655471921","Installation and shell commands","Deployment"
"https://github.com/fairlearn/fairlearn","713529138","issue_comment","https://github.com/Trusted-AI/AIF360/issues/606#issuecomment-713529138","I suspected that sphinx was telling the truth :-) This was a reminder to come back to it once other changes are in.","9","0.4334038054968287","Feature engineering methodology","Design"
"https://github.com/fairlearn/fairlearn","712838799","issue_comment","https://github.com/Trusted-AI/AIF360/issues/601#issuecomment-712838799","Couldn't agree more! I haven't looked into how scikit-learn achieves this yet, but I'm hoping that it's compatible with our docs generation setup. 

Would you like to try this out @hildeweerts ? If not I'll mark it as ""help-wanted"".","20","0.4854178528500033","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","712884824","issue_comment","https://github.com/Trusted-AI/AIF360/issues/601#issuecomment-712884824","Thanks for laying out your concerns @hildeweerts ! As we discussed in the metrics meeting, there's certainly plenty of space for improvements in the current docs.","15","0.4742094064127961","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","712920401","issue_comment","https://github.com/Trusted-AI/AIF360/issues/601#issuecomment-712920401","I'd be happy to take a stab, but if you know somebody (e.g., a prospective contributor?) who'd like to try out one of these that's totally fine with me as well!","20","0.8455896545068523","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","713112647","issue_comment","https://github.com/Trusted-AI/AIF360/issues/601#issuecomment-713112647","I'm not aware of anyone who's looking for tasks right now. If you are interested in trying please do :-)  The doc generation is a little quirky if you haven't used `sphinx` before, so please do reach out if you have questions. Both @riedgar-ms and I have worked with it extensively.","20","0.615550072752578","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","713133895","issue_comment","https://github.com/Trusted-AI/AIF360/issues/601#issuecomment-713133895","Something else I'll throw in for the references - it would be nice to use BibTeX `.bib` files. I did find some support in Sphinx, but.... it is a bit patchy compared to the real thing.","30","0.2679332386363636","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","808091286","issue_comment","https://github.com/Trusted-AI/AIF360/issues/601#issuecomment-808091286","Split issue into smaller components in #719 and #720","0","0.4777303233679075","Dataset usage","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","706346268","issue_comment","https://github.com/Trusted-AI/AIF360/issues/593#issuecomment-706346268","@anindya5 thanks for reporting! 

I can reproduce this with the notebook ""Binary Classification with the UCI Credit-card Default Dataset"" which uses the dashboard in two separate cells. The second cell that executes `FairlearnDashboard` doesn't show the dashboard, only blank space, while the first cell shows what's supposed to be in the second cell output.

I'm looking into this.","29","0.5464935064935068","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","724217790","issue_comment","https://github.com/Trusted-AI/AIF360/issues/593#issuecomment-724217790","We had to move back to the previous version of the dashboard for v0.5.0 since the ""new"" version based on flask is not able to run on all the previously supported platforms yet. That will be addressed in the `raiwidgets` package which should be released by the end of the month. The dashboard will be removed from Fairlearn after the v0.5.0 release (which is planned for this week).

This mentioned rollback actually solves your problem of running multiple widgets in the same notebook, so I'm closing this issue. If you run into future issues with the dashboard from `raiwidgets` please open an issue in the corresponding repo (which will be public as soon as the release happens).","24","0.9585760108044232","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","724325402","issue_comment","https://github.com/Trusted-AI/AIF360/issues/593#issuecomment-724325402","ok, where can I find details about the raiwidgets package as of now","24","0.4039048200122027","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","724390984","issue_comment","https://github.com/Trusted-AI/AIF360/issues/593#issuecomment-724390984","@romanlutz and @anindya5 I have an fix for above issue without the dependency of raiwidgets. I’m working on it while I provide the fix to it. I think we still keep issue open.","24","0.6601275917065391","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","724412393","issue_comment","https://github.com/Trusted-AI/AIF360/issues/593#issuecomment-724412393","@anindya5 as mentioned above this package will only be released at the end of the month. The rolled back version of the `FairlearnDashboard` does not show the same issue you described.

@MarsLabAtSLU I commented on your new issue #629 . The fix won't be needed in this repo since the widget is moving out of the repo, but feel free to share if you already have a branch and we can adopt that approach for `raiwidgets`.","24","0.84661340243288","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","701662778","issue_comment","https://github.com/Trusted-AI/AIF360/issues/588#issuecomment-701662778","Thanks for reporting @hildeweerts ! I'm a little confused, though. `ThresholdOptimizer` is deterministic except for the `predict` call (which is why the `random_state` is only available there). Small caveat: it assumes the underlying estimator is deterministic. If that's not the case then all bets are off.

Do you have an example where you observe this? I tried a handful of different types of models and couldn't repro it, but that could very well be due to my example being to simplistic.","23","0.53307421003383","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","701945685","issue_comment","https://github.com/Trusted-AI/AIF360/issues/588#issuecomment-701945685","Thanks, Roman! After some further inspection it seems the problem is some confusion on my side on what `random_state` in the `predict` call does. I assumed that if I run a cell that contains ```to.predict(X, sensitive_features=A, random_state=0)``` twice, I would get the same predictions. But that does not seem to be the case?

This is the full code in which I ran in to this problem first:
```python
import pandas as pd
import numpy as np
from sklearn.datasets import fetch_openml
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from fairlearn.postprocessing import ThresholdOptimizer

# fetch data from OpenML
data = fetch_openml(data_id=42193)
X = pd.DataFrame(data['data'], columns=data['feature_names']).drop(columns=['race_Caucasian', 'c_charge_degree_F'])
y = data['target'].astype(np.int)

# split the data in train-validation-test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0)
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=0)

# Train a simple logistic regression model
lr = LogisticRegression(max_iter=1000, random_state=0)
lr.fit(X_train, y_train)

# Train threshold optimizer
to = ThresholdOptimizer(estimator=lr, constraints='equalized_odds', grid_size=1000)
to.fit(X_train, y_train, sensitive_features=X_train['race_African-American'])
```

My expectation was that running the following cell twice would give the same results, but I get different predictions each time:
```python
# score groups
y_val_pred = to.predict(X_val, sensitive_features=X_val['race_African-American'], random_state=0)
```
I'm running this in a Google Colab with fairlearn version  0.4.6.","10","0.3251465737142248","Model development","Development"
"https://github.com/fairlearn/fairlearn","702317471","issue_comment","https://github.com/Trusted-AI/AIF360/issues/588#issuecomment-702317471","Your assumption about how this should work is correct! 
I can repro the issue
```
>>> to.predict(X_val, sensitive_features=X_val['race_African-American'], random_state=0)
array([1, 0, 0, ..., 0, 0, 1])
>>> to.predict(X_val, sensitive_features=X_val['race_African-American'], random_state=0)
array([0, 0, 1, ..., 0, 0, 1])
>>> to.predict(X_val, sensitive_features=X_val['race_African-American'], random_state=0)
array([0, 0, 0, ..., 0, 0, 1])
>>> to.predict(X_val, sensitive_features=X_val['race_African-American'], random_state=0)
array([0, 0, 0, ..., 0, 0, 1])
>>> to.predict(X_val, sensitive_features=X_val['race_African-American'], random_state=0)
array([0, 0, 0, ..., 0, 0, 1])
>>> to.predict(X_val, sensitive_features=X_val['race_African-American'], random_state=0)
array([0, 0, 0, ..., 0, 0, 1])
>>> to.predict(X_val, sensitive_features=X_val['race_African-American'], random_state=0)
array([0, 0, 0, ..., 0, 0, 1])
>>> to.predict(X_val, sensitive_features=X_val['race_African-American'], random_state=0)
array([0, 0, 0, ..., 0, 0, 1])
>>> to.predict(X_val, sensitive_features=X_val['race_African-American'], random_state=0)
array([0, 0, 1, ..., 0, 0, 1])
>>> to.predict(X_val, sensitive_features=X_val['race_African-American'], random_state=0)
```

Thanks a lot for reporting. I'll dig into this and will update later today!","2","0.9193610895803344","Performance measurement","Validation"
"https://github.com/fairlearn/fairlearn","702763055","issue_comment","https://github.com/Trusted-AI/AIF360/issues/588#issuecomment-702763055","Oh. Excellent catch. @romanlutz we have a bug in lines 112-116 of https://github.com/fairlearn/fairlearn/blob/master/fairlearn/postprocessing/_interpolated_thresholder.py

My suggestion is to replace it by something like:
```python
        random_state = check_random_state(random_state)
        positive_probs = self._pmf_predict(
            X, sensitive_features=sensitive_features)[:, 1]
        return (positive_probs >= random_state.rand(len(positive_probs))) * 1
```
where `check_random_state` is imported from `sklearn.utils`.

By the way, this also reminds me of #243 and https://github.com/fairlearn/fairlearn/pull/282#discussion_r374723526. That should be handled after the above bugfix.","23","0.4947757303778247","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","702764721","issue_comment","https://github.com/Trusted-AI/AIF360/issues/588#issuecomment-702764721","Oh--also worth checking how we do it in `ExponentiatedGradient`.","28","0.4716949716949716","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","697217004","issue_comment","https://github.com/Trusted-AI/AIF360/issues/583#issuecomment-697217004","It's up to you. My only advice is to consider that it is easier to have people demo the library if the library makes it easy to demo the library. That's why I've always added datasets to my other libraries. Running scikit-learn code to fetch a dataset is a small task, but it is a small task that still takes mental effort and therefore makes it harder for a novice to get started. ","20","0.4307137079211001","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","697680947","issue_comment","https://github.com/Trusted-AI/AIF360/issues/583#issuecomment-697680947","Good point! I'll take that as a vote towards (1) or (2).","20","0.6933066933066933","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","712473104","issue_comment","https://github.com/Trusted-AI/AIF360/issues/583#issuecomment-712473104","Given that nobody seems to have anything additional to contribute I take it that we're leaving it as is (option #2 from above). If there's anything additional to consider please reopen. Thanks!","25","0.3313782991202346","Research","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","696405645","issue_comment","https://github.com/Trusted-AI/AIF360/issues/581#issuecomment-696405645","This sounds like a duplicate of #556 . If that's not true please reopen this issue. If it is related to #556 please just comment there. Thank you @anindya5 :-)","24","0.3185592580610373","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","695914041","issue_comment","https://github.com/Trusted-AI/AIF360/issues/580#issuecomment-695914041","@Dref360 this is a great discovery! I suspect we'll find the same issue with `ExponentiatedGradient` since it also relies on passing `sample_weight` to the estimator. What I don't quite understand in your example is how the pipeline knows that it's for that step since the step is named `logistic` but you prefix the key with `logisticregression`. Is that perhaps an issue with the example?

I like your proposed solution! @MiroDudik do you have thoughts (since this would be an API change/extension)?

What this really makes me think about is whether we are looking at this a bit backwards. @adrinjalali may have thoughts on this. If we use a pipeline like the one in your example, shouldn't the mitigation technique be applied after the scaling, i.e. shouldn't the mitigation technique be a part of the pipeline? ","23","0.4763664249860568","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","695922074","issue_comment","https://github.com/Trusted-AI/AIF360/issues/580#issuecomment-695922074","The mitigation should definitely be a part of the pipeline to avoid data leackage. Otherwise your mitigation technique is going to see the whole dataset which it shouldn't.","28","0.5303030303030303","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","696079254","issue_comment","https://github.com/Trusted-AI/AIF360/issues/580#issuecomment-696079254","It would seem odd that our Estimators would have a capability (overriding an argument name) when the `sklearn` estimators do not.  What we might want is the ability to override the `sample_weight` name when calling the underlying Estimator, in case something that we're wrapping uses another name.","23","0.8273041837055679","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","696103915","issue_comment","https://github.com/Trusted-AI/AIF360/issues/580#issuecomment-696103915",">  Is that perhaps an issue with the example?

Yes, sklearn does not know the name of the step, it just show `logisticregression__sample_weight`. In reality, it should be `logistic__sample_weight` (from the name I put in the Pipeline arguments).
","23","0.6693164832178286","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","696108644","issue_comment","https://github.com/Trusted-AI/AIF360/issues/580#issuecomment-696108644","An update on my earlier comment, now that I've woken up a bit more. If I understand @adrinjalali  correctly, when trying to use Fairlearn in a pipeline, the Fairlearn estimator should go into the pipeline itself. The line to construct the pipeline should look something like:
```python
self.estimator = Pipeline([('scaler', StandardScaler()), ('estimator', GridSearch(LogisticRegression(solver='liblinear'), constraint=DemographicParity()))]
```
which definitely makes sense, and should work as-is (we should check this, though). It feels odd to have some external algorithm (such as `GridSearch`) 'reach in' to just one part of the Pipeline.

However, we could consider having the argument name override capability, to allow ourselves a little more flexibility in supported estimators, if we found some common package which (for example) used `sample_weights` instead of `sample_weight`.  We might prefer just to tell users to write a wrapper for such cases, though.","23","0.3247537664298706","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","696234946","issue_comment","https://github.com/Trusted-AI/AIF360/issues/580#issuecomment-696234946","More like, do the `GridSearch` on the `pipeline`, would that not work?","28","0.5820271682340651","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","696337840","issue_comment","https://github.com/Trusted-AI/AIF360/issues/580#issuecomment-696337840","> do the `GridSearch` on the `pipeline`
Isn't that the original example @Dref360 is showing? That seems to be in contrast with your earlier comment @adrinjalali . Perhaps I'm misunderstanding.

What Richard drew up is basically what I meant originally, although I don't know if that's preferable to having `GridSearch` wrap the pipeline. A quick search showed one `sklearn` example with `GridSearchCV` wrapping `Pipeline`, but I can't tell whether that's a pattern to copy.","28","0.4420073424180201","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","696559699","issue_comment","https://github.com/Trusted-AI/AIF360/issues/580#issuecomment-696559699","To me, `GridSearch` is not a _mitigation technique_, it's rather finding a set of hyper parameters which satisfy a set of constraints. Ideally `sklearn`'s API gives you all you need so that you could remove `GridSearch` from `fairlearn` and just use `GridSearchCV`, in which case you'd put the whole pipeline in it. On the other hand, I would put `InterpolatedThresholder` inside a pipeline instead of passing the pipeline to it. Am I missing something?","28","0.4082077922077922","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","697016786","issue_comment","https://github.com/Trusted-AI/AIF360/issues/580#issuecomment-697016786","@adrinjalali I agree that that's how it should be, but right now it isn't. There's one potential problem that (at least in my mind) is keeping us from doing it this way. `GridSearch` (ours, not `sklearn`) creates a grid with certain properties, not just a grid that is created by combining all options from all parameters with each other (which would be the Cartesian Product). [You may argue that that's not even a ""grid"" in the way most people think about it.]
One can either pass some of the information about what the grid should be like (including size as in number of grid points, and limits of the values in each dimensions) or directly pass a predefined grid. Since this isn't just a simple Cartesian Product I'm not entirely sure we can actually use `GridSearchCV` without a workaround. Is it possible to pass non-scalars (say, a column of the predefined grid) to `GridSearchCV`? Let's say the following:

```
args = {'grid' = [[1,0,0,0], [0,0,1,0], ...]}
```

If that works then we could define a small estimator that just does what `GridSearch` right now does, but with that one column vector (single `fit`) as opposed to a grid (`n` calls to `fit`), and then use `GridSearchCV` around the pipeline that contains that estimator.

`InterpolatedThresholder` is from postprocessing, so that's not directly related to this, but perhaps you were trying to point at something else?","23","0.2523369286081149","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","701431260","issue_comment","https://github.com/Trusted-AI/AIF360/issues/580#issuecomment-701431260","@romanlutz Should I go ahead with the PR while we wait for MiroDudik approval? ","20","0.6862170087976542","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","702180670","issue_comment","https://github.com/Trusted-AI/AIF360/issues/580#issuecomment-702180670","If I understand correctly, the basic proposal here is to simply add a new keyword argument to the `GridSearch` constructor. Something like `sample_weight_name='sample_weight'`. I think we should go ahead! @Dref360 : From my perspective you can go ahead and start the PR.

@adrinjalali , @romanlutz , @riedgar-ms : We can pick up the discussion of whether we should get rid of `GridSearch` altogether and rely on `GridSearchCV` in a separate issue--I actually think it should be possible, but there are a few wrinkles to resolve (as @romanlutz is alluding to). Also, I agree with @adrinjalali that we should try to make our `ThresholdOptimizer` and `InterpolatedThresholder` plug into sklearn's pipelines (again a separate issue please).","23","0.2905707861460074","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","702183052","issue_comment","https://github.com/Trusted-AI/AIF360/issues/580#issuecomment-702183052","And I also agree with @romanlutz that we should add the same argument to `ExponentiatedGradient`, but that could be a separate PR.","20","0.3313782991202347","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","693137629","issue_comment","https://github.com/Trusted-AI/AIF360/issues/571#issuecomment-693137629","That's correct, we don't yet support JupyterLab. I'm collecting all that kind of feedback in #484 . JupyterLab has already been reported there so I'll close this issue.

FYI I'm actively working on moving the UI from ipywidgets to flask which should solve this issue. Stay tuned!","24","0.8004920337394567","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","693139702","issue_comment","https://github.com/Trusted-AI/AIF360/issues/570#issuecomment-693139702","This is mostly self-inflicted since we wanted to have a custom title page, so the docs ""root"" is `contents` as you're describing, and not index. I don't consider it to be a big issue to be honest. Adding a link from contents back to the `index` page may be a useful ""band aid"" solution until there's perhaps a solution from pydata-sphinx-theme. Feel free to open a PR!","14","0.5225937792017089","Documentation","Development"
"https://github.com/fairlearn/fairlearn","688865319","issue_comment","https://github.com/Trusted-AI/AIF360/issues/569#issuecomment-688865319","I have a repro written as a test case, and [parameterised with both learners and moments](https://github.com/riedgar-ms/fairlearn/blob/riedgar-ms/pickle-expgrad-test/test/unit/reductions/exponentiated_gradient/test_exponentiatedgradient_pickle.py). I agree that this appears to be related to what the `_Lagrangian` is doing, although I've not had time to dig into that yet.","25","0.3832442067736185","Research","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","688912564","issue_comment","https://github.com/Trusted-AI/AIF360/issues/569#issuecomment-688912564","making the objects pickle-able is really nice, but in the meantime you could try with `cloudpickle` and see if that helps for your usecase.","30","0.435996980480966","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","689446427","issue_comment","https://github.com/Trusted-AI/AIF360/issues/569#issuecomment-689446427","`cloudpickle` does seem to work, thanks!","26","0.3274917853231106","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","678069502","issue_comment","https://github.com/Trusted-AI/AIF360/issues/564#issuecomment-678069502","IMO there are a few issues here:
1. prefit = True is the default, but none of this is super obvious when you suddenly see this error without having heard of prefit.
2. While the middle sentence explains what's perhaps wrong, it doesn't tell you if case (1) is an issue or not, so you can't know whether or not to proceed. ","25","0.3565441650548033","Research","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","843398325","issue_comment","https://github.com/Trusted-AI/AIF360/issues/564#issuecomment-843398325","Whoever takes this up should probably run `ThresholdOptimizer` with `prefit=True` and `prefit=False` with several different kinds of models (e.g., one from `sklearn`, one from `pytorch`, one from `lightgbm`, etc.) and see whether the result is as expected.

The same code should be used for writing test cases. We'd want to cover the same spectrum.

Regardless, the experience should be that we first explain what `prefit` is (i.e., a flag indicating whether the model is already trained and doesn't need to be trained by the postprocessing technique), then note that it's been set a certain way (while indicating the default), and then only explain the options like above. Importantly, it should be clear to a user who doesn't specify `prefit` (i.e., uses the default) whether they can continue or whether they should fix something.

The issue at the heart of this is that we use `sklearn`'s `check_is_fitted` which only works properly when the estimator actually conforms to `sklearn`'s convention. In other cases, `check_is_fitted` may not recognize that a model is already trained and complain like above in @arjsingh's description.","23","0.4660012868195158","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","676811142","issue_comment","https://github.com/Trusted-AI/AIF360/issues/562#issuecomment-676811142","This is related to documentation versioning which @riedgar-ms is addressing in #551 . Once we have that it'll be possible to view the documentation of the version that you're using. Since that's already captured in other issues I'll close this one.","24","0.6308597456822927","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","674487222","issue_comment","https://github.com/Trusted-AI/AIF360/issues/558#issuecomment-674487222","@maurominella you're right in that JupyterLab isn't support yet. It should work in Jupyter on Azure. You wrote ""Azure Compute Instance"" so I assume it's an [AzureML Compute Instance](https://docs.microsoft.com/en-us/azure/machine-learning/concept-compute-instance). The dashboard itself is showing up, just the calculation seems to not finish.

Sometimes it does take a little time to calculate if the dataset size is really large. I understand from your description that that can't be the case since it worked locally with the same data, right? Please confirm the size of the data (# of rows).

Something that jumped out at me from your code is that you're passing the `sensitive_feature_names` as an `np.array` as opposed to a list of strings. It doesn't make a difference when I'm running in v0.4.6, though.

Finally, I can't tell from your code what type `ys_pred` is. It should be a dictionary like `{""unmitigated"": unmitigated_predictor.predict(X_test)}`, but I can't be sure from the partial snippet. If you can reproduce the same issue with a minimal example that would be very helpful.","12","0.285304917807749","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","675885197","issue_comment","https://github.com/Trusted-AI/AIF360/issues/558#issuecomment-675885197","Hi @romanlutz , thanks for your answer. I can't say why, but it works now. I just rebooted the VM.

I confirm it's this AzureML Compute Instance:
![image](https://user-images.githubusercontent.com/32428960/90600276-8c48b100-e1f6-11ea-8f7b-6162e0f84d64.png), even after reboot.

ys_pred is a dict whose first element is the model_id and the second one is the array with the predictions as described at [this link](url)

```
{'mauromi_model_classifier_LR:7': array([0, 0, 0, ..., 0, 0, 1]),
 'mauromi_model_classifier_SVM:6': array([0, 0, 0, ..., 0, 0, 1]),
 'mauromi_model_classifier_CBC:6': array([0., 0., 0., ..., 0., 0., 0.])}

```

Anyway, problem solved. Thanks again, Mauro

","12","0.4819901468674472","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","673272791","issue_comment","https://github.com/Trusted-AI/AIF360/issues/556#issuecomment-673272791","Hi @nina-hua ! Your assessment sounds correct :-)
`GroupLossMoment` has been renamed to `BoundedGroupLoss`, but it's also a little bit more general now. GLM is only usable with `GridSearch`, while BGL is usable with `GridSearch` and `ExponentiatedGradient`. [When using BGL with `ExponentiatedGradient` you need to provide the `upper_bound` argument.] Otherwise they should behave the same way.

Please let me know if there are other questions! We definitely need to write a set of changes from v0.4.6 to v-next and publish them on the webpage (besides CHANGES.md). Since it's also related: the multi-version support on the webpage that @riedgar-ms  is working on will be very useful.","28","0.4972463221425877","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","673559461","issue_comment","https://github.com/Trusted-AI/AIF360/issues/556#issuecomment-673559461","Thanks Roman! Do you have a link to older documentation on how to use GroupLossMoment? ","21","0.296969696969697","Installation and shell commands","Deployment"
"https://github.com/fairlearn/fairlearn","673768652","issue_comment","https://github.com/Trusted-AI/AIF360/issues/556#issuecomment-673768652","I don't unfortunately, but you can use it just like `BoundedGroupLoss` except that there's no `upper_bound` argument: https://fairlearn.github.io/user_guide/mitigation.html#bounded-group-loss

It will only work with `GridSearch`, though.","28","0.6969696969696968","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","673825062","issue_comment","https://github.com/Trusted-AI/AIF360/issues/556#issuecomment-673825062","Thanks Roman! I'm actually trying out GridSearch with the Boston Housing Dataset. But I'm running into an error setting the grid size. I've tried using 10 versus 1000 estimators, but I'm not sure how to mitigate this issue?
`Message:'Generating a grid with {} grid points. It is recommended to use at least {} grid points. Please consider increasing grid_size.'
Arguments: (10, 31828687130226345097944463881396533766429193651030253916189694521162207808802136034115584)`
","28","0.5172982390641528","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","673839775","issue_comment","https://github.com/Trusted-AI/AIF360/issues/556#issuecomment-673839775","Is there a tutorial on how to use GridSearch with regression problems since I've only stumbled upon classification problems?","28","0.3084346651225633","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","673857511","issue_comment","https://github.com/Trusted-AI/AIF360/issues/556#issuecomment-673857511","We don't have one yet. Wow that's a big number. How many groups do you have in that example? The recommended grid size is exponential in the number of groups.

I also want to point out the fairness issues with the Boston housing dataset. More context is available at this link: https://github.com/koaning/scikit-fairness/issues/31","0","0.321785058860391","Dataset usage","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","673869515","issue_comment","https://github.com/Trusted-AI/AIF360/issues/556#issuecomment-673869515","By groups, do you mean the sample size? I did an 80/20 train/test split with the Boston dataset, so my training data set comes out to be 404 samples.

Yup, definitely chose this dataset on purpose because I was assigning the ""B"" feature as a sensitive feature to explore. ","0","0.4065970261622437","Dataset usage","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","673873503","issue_comment","https://github.com/Trusted-AI/AIF360/issues/556#issuecomment-673873503","That's likely the problem. By default it treats the sensitive feature data as individual categories and you probably have roughly 400 different ones. I don't have enough context on this particular dataset to make any recommendations, but I'd suggest thinking about the application context first. What do you envision to do with such a model?

FYI @kevinrobinson and @koaning who have worked with this dataset before.

In general, when the sensitive feature is continuous (e.g. age) binning is the way to go.","8","0.4570655217472466","Understanding Fairness Definition","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","673917687","issue_comment","https://github.com/Trusted-AI/AIF360/issues/556#issuecomment-673917687","Binning ... or use a technique like an InformationFilter as demonstrated [here](https://scikit-lego.readthedocs.io/en/latest/fairness.html#How-it-Works). ","0","0.4334038054968286","Dataset usage","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","674141987","issue_comment","https://github.com/Trusted-AI/AIF360/issues/556#issuecomment-674141987","Thanks for the help Roman and Vincent! I wanted to build a model to predict the median house price with a dataset that's known to be biased to demonstrate how Fairlearn & InterpretML can be used in future projects at work. I'll try out the binning method for now. ","0","0.5425779893865004","Dataset usage","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","674158022","issue_comment","https://github.com/Trusted-AI/AIF360/issues/556#issuecomment-674158022","@nina-hua hello!  If you're trying to make the case at work for using these assessment and mitigation approaches, it can really help to frame the impact in terms of how it reduces real harms to real people.

If you're into research papers, [Language (Technology) is Power: A Critical Survey of “Bias” in NLP (Blodget et al. 2020)](https://www.aclweb.org/anthology/2020.acl-main.485.pdf) suggest some ways to approach fairness work that can lead to more productive conversations than focusing on bias alone. 

More practically, in my experience the aggregation in the Boston housing dataset means it's particularly challenging to use as a kind of proof of concept for this kind of fairness work.  You really need a task that's embedded in some social system to talk about harms, and to translate the impact of fairness metrics to accessible human terms so that various stakeholders can see why the work matters and is worth doing.

I'd be happy to collaborate or chat more about scenarios or datasets that might be more directly relevant for your work, or that might help you with kicking off those conversation on your team.  Feel free to chat here or ping in https://gitter.im/fairlearn/community if you'd like!","8","0.5369285288235501","Understanding Fairness Definition","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","674982826","issue_comment","https://github.com/Trusted-AI/AIF360/issues/556#issuecomment-674982826","Thanks! I'll discuss this with my manager and will take you up on that! ","20","0.6499898311978851","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","674984256","issue_comment","https://github.com/Trusted-AI/AIF360/issues/556#issuecomment-674984256","This is an aside, but would there be any capability of having Fairlearn be compatible with glassbox models from InterpretML? I thought about how Fairlearn's GridSearch could be used to improve an ExplainableBoostingRegressor from InterpretML but because ExplainableBoostingRegressor does not contain a ""sample_weight"" argument, I'm receiving this issue. 

![fairlearn_error](https://user-images.githubusercontent.com/40807472/90420505-e7e73300-e06c-11ea-997b-4e8acfb2df6f.png)
","28","0.4645638049893368","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","675036680","issue_comment","https://github.com/Trusted-AI/AIF360/issues/556#issuecomment-675036680","We require `sample_weight` at this point and EBM doesn't have that currently https://github.com/interpretml/interpret/blob/67da4f4913e01340324f07ea95074784db6c337d/python/interpret-core/interpret/glassbox/ebm/ebm.py

This is already tracked through #4 . Nobody is currently working on this feature, though.  @mesameki FYI

If you really want to use it you could perhaps write a wrapper for your regressor that has `sample_weight` in the `fit` method and handles the reweighting. I definitely recommend filing a feature request with `interpret` to ask for the `sample_weight` in the meanwhile since I can't give any ETA for this feature. I've marked it as ""help-wanted"" in the meanwhile so that contributors know that they can pick it up.","23","0.5337608449641646","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","696855112","issue_comment","https://github.com/Trusted-AI/AIF360/issues/556#issuecomment-696855112","Will the BoundedGroupLoss be available in pip or conda anytime soon?","21","0.3508158508158508","Installation and shell commands","Deployment"
"https://github.com/fairlearn/fairlearn","696988236","issue_comment","https://github.com/Trusted-AI/AIF360/issues/556#issuecomment-696988236","@anindya5 yes! With the next release it'll be available. @riedgar-ms I think the metrics updates & doc updates are the main blockers for v0.5.0, is that correct? If so, do you have a (rough) ETA?","24","0.5849307268931486","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","702301461","issue_comment","https://github.com/Trusted-AI/AIF360/issues/556#issuecomment-702301461","@romanlutz , @riedgar-ms  - any update on ETA? This is really important for me to understand for many reasons.","20","0.3577051655343469","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","702498617","issue_comment","https://github.com/Trusted-AI/AIF360/issues/556#issuecomment-702498617","anindya5 I was discussing this with @riedgar-ms yesterday. My best guess would be end of October but early November is possible. Do the workaround with v0.4.6 to use GroupLossMoment or alternatively installing the development version from the master branch not work for you?","25","0.3265387095174329","Research","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","702901582","issue_comment","https://github.com/Trusted-AI/AIF360/issues/556#issuecomment-702901582","@romanlutz , every time I am trying to clone the repository for advanced install, I am getting permission denied. Not sure why is that. 

Also, there does not seem to be any documentation on GroupLossMoment, even if I use it, I cannot showcase it to others.","21","0.554112554112554","Installation and shell commands","Deployment"
"https://github.com/fairlearn/fairlearn","702902515","issue_comment","https://github.com/Trusted-AI/AIF360/issues/556#issuecomment-702902515","Can you post the command & error? git clone should do it.","21","0.7744890768146583","Installation and shell commands","Deployment"
"https://github.com/fairlearn/fairlearn","702903413","issue_comment","https://github.com/Trusted-AI/AIF360/issues/556#issuecomment-702903413","@romanlutz  - i was able to fork and clone, will try the boundedgrouploss now","21","0.3105228105228105","Installation and shell commands","Deployment"
"https://github.com/fairlearn/fairlearn","719661067","issue_comment","https://github.com/Trusted-AI/AIF360/issues/556#issuecomment-719661067","#613 will provide a place on the website to answer these sorts of questions ahead of a release, and #551 provided the versioned docs, so we should be set up to avoid confusion like this in the future :-)","32","0.5901988636363636","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","672868029","issue_comment","https://github.com/Trusted-AI/AIF360/issues/552#issuecomment-672868029","You may have run into one of our backwards compatibility problems.... did you `pip install` Fairlearn, but download the notebook from the website? If so, then I'm afraid there have been breaking changes (we need to do a new release to fix this, and also provide versioned documentation).

To use the latest notebooks, you need to clone the repository, and install Fairlearn via `pip install -e .`

To use notebooks with the latest Fairlearn release, you need to go to tag `v0.4.6` of the repository, and get the notebooks from that.","21","0.5162004662004664","Installation and shell commands","Deployment"
"https://github.com/fairlearn/fairlearn","673164185","issue_comment","https://github.com/Trusted-AI/AIF360/issues/552#issuecomment-673164185","Yes I did pip install Fairlearn, but download the notebook from the website. Thank you, I will do as you suggested. ","21","0.5383675464320625","Installation and shell commands","Deployment"
"https://github.com/fairlearn/fairlearn","673759704","issue_comment","https://github.com/Trusted-AI/AIF360/issues/549#issuecomment-673759704","This is almost certainly because of our `intersphinx` mappings. Entertainingly, search for `sphx_glr_auto_examples` (the reference given) turns up a lot of hits for SciKit-Learn. But the name is defined by `sphinx-gallery`","14","0.4670002780094521","Documentation","Development"
"https://github.com/fairlearn/fairlearn","674450928","issue_comment","https://github.com/Trusted-AI/AIF360/issues/549#issuecomment-674450928","Closed by #560 ","24","0.239138371668492","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","668787143","issue_comment","https://github.com/Trusted-AI/AIF360/issues/544#issuecomment-668787143","Tagging @gaugup ","3","0.2343260188087775","Bias detection metrics validation","Validation"
"https://github.com/fairlearn/fairlearn","668789176","issue_comment","https://github.com/Trusted-AI/AIF360/issues/544#issuecomment-668789176","@parul100495 thanks for pointing this out. I will create a PR to fix this.","24","0.6731601731601733","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","670781730","issue_comment","https://github.com/Trusted-AI/AIF360/issues/544#issuecomment-670781730","@romanlutz @parul100495 we could close this issue as the above is merged.","24","0.7335997335997336","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","670785183","issue_comment","https://github.com/Trusted-AI/AIF360/issues/544#issuecomment-670785183","Closing as it is now done. Thanks @gaugup for the quick response :)","31","0.2500832500832499","Testing","Maintenance"
"https://github.com/fairlearn/fairlearn","687197284","issue_comment","https://github.com/Trusted-AI/AIF360/issues/543#issuecomment-687197284","I've been prodding this, and changing `.size()` to `.count()` in `UtilityParity.load_data()` doesn't seem to be a complete fix. For starters, making that change results in almost every test of ExpGrad failing. This is perhaps not surprising, since `size()` returns an integer, whereas `count()` will return a DataFrame.

I have a test case [in a branch](https://github.com/riedgar-ms/fairlearn/blob/riedgar-ms/vacuous-constraint-fix/test/unit/reductions/exponentiated_gradient/test_vacuousconstraints.py) which shows the bug.

The actual code doesn't blow up until `signed_weights()` is called in `UtilityParity`. At that point, the `lambda_vec` is an empty Series, so attempts to index it fail miserably.

I don't quite understand enough of what's going on to propose a fix. We could have `signed_weights()` check for an empty `lambda_vec` (similarly for `project_lambda()`) and return an empty Series when that happens. We would also have to adjust everything which looks at `self.index` in `UtilityParity` too, to cope with the vacuous case.","17","0.3381238351625595","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","667378110","issue_comment","https://github.com/Trusted-AI/AIF360/issues/539#issuecomment-667378110","Demographic parity is a constraint for classification problems.  You need to pick a constraint moment suitable for regression, such as bounded group loss.","28","0.5083823622445751","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","667498548","issue_comment","https://github.com/Trusted-AI/AIF360/issues/539#issuecomment-667498548","hi. Thanks for the reply . But doesnot the paper ""Fair Regression: Quantitative Definitions and Reduction-based Algorithms"" regression version constrained with statistical parity too. Thanks","6","0.5758759469696973","API expansion","Development"
"https://github.com/fairlearn/fairlearn","668234292","issue_comment","https://github.com/Trusted-AI/AIF360/issues/539#issuecomment-668234292","That's correct. The existing implementation of `DemographicParity` is for binary classification only, though. I'll convert this into a feature request for DP as a regression constraint.

@MiroDudik does the current workstream with Steven, Leijie, and Mukundh address GridSearch as well?","28","0.3919001590603205","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","668560846","issue_comment","https://github.com/Trusted-AI/AIF360/issues/539#issuecomment-668560846","Thank you","0","0.0303030303030302","Dataset usage","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","667145711","issue_comment","https://github.com/Trusted-AI/AIF360/issues/537#issuecomment-667145711","Thank you for highlighting this. It looks like this is actually a bug in the sample code on:
https://docs.microsoft.com/en-us/azure/machine-learning/how-to-machine-learning-fairness-aml

Specifically in,
```python
#  Create a dictionary of model(s) you want to assess for fairness 
sf = { 'Race': A_test.Race, 'Sex': A_test.Sex}
ys_pred = unmitigated_predictor.predict(X_test)
from fairlearn.metrics._group_metric_set import _create_group_metric_set

dash_dict = _create_group_metric_set(y_true=Y_test,
                                    predictions=ys_pred,
                                    sensitive_features=sf,
                                    prediction_type='binary_classification')
```
the variable `ys_pred` is supposed to be a `(str,array)` dictionary, where the key is the name of the (registered) model. The code in the [example notebooks](https://github.com/Azure/MachineLearningNotebooks/blob/master/contrib/fairness/upload-fairness-dashboard.ipynb) does this, and should work.

Tagging @mesameki ","12","0.6901746799821545","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","667163580","issue_comment","https://github.com/Trusted-AI/AIF360/issues/537#issuecomment-667163580","This is actually a problem on the Microsoft Docs website - the Fairlearn part is actually behaving as expected. I've filed an internal bug to fix the webpage.","14","0.4088694882014297","Documentation","Development"
"https://github.com/fairlearn/fairlearn","664421784","issue_comment","https://github.com/Trusted-AI/AIF360/issues/530#issuecomment-664421784","The version of Fairlearn currently available through PyPI does not have the `datasets` module. In order to use the `datasets` module, you will need to clone the Fairlearn repo, and `pip install` the cloned directory. If you do that, please make sure that you use examples from the clone - we have had a number of breaking API changes since the v0.4.6 release.","21","0.424690031780135","Installation and shell commands","Deployment"
"https://github.com/fairlearn/fairlearn","664564358","issue_comment","https://github.com/Trusted-AI/AIF360/issues/530#issuecomment-664564358","We should actually fix that quickstart to not use the datasets module before releasing the next version. My bad!","32","0.4490456163054027","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","665431835","issue_comment","https://github.com/Trusted-AI/AIF360/issues/530#issuecomment-665431835","Thanks, the older version worked. ","21","0.4777303233679074","Installation and shell commands","Deployment"
"https://github.com/fairlearn/fairlearn","665607821","issue_comment","https://github.com/Trusted-AI/AIF360/issues/530#issuecomment-665607821","Sorry about this - we know we need to get better about breaking changes, and we are planning to provide versioned documentation to help with unavoidable breakages.","32","0.4490456163054027","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","724282056","issue_comment","https://github.com/Trusted-AI/AIF360/issues/524#issuecomment-724282056","Closing this since we're not changing the dashboard anymore before removing it. This will be addressed by `raiwidgets`","24","0.7883101477407526","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","724392977","issue_comment","https://github.com/Trusted-AI/AIF360/issues/524#issuecomment-724392977","> Closing this since we're not changing the dashboard anymore before removing it. This will be addressed by `raiwidgets`

raiwidgets is just been launched Nov, 2 2020. I recommend we could still fix it and have version control take care of rest. Might be when raiwidgets gets released we can close this issue and document.","24","0.8645753307725137","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","724414347","issue_comment","https://github.com/Trusted-AI/AIF360/issues/524#issuecomment-724414347","@MarsLabAtSLU It's just an ""empty release"" to reserve the name. We've already added a warning to the Fairlearn repo to indicate to users that it's going away, and the v0.5.0 release which is planned for tomorrow will be the last one to have the dashboard.

As I explained above, the dashboard was rolled back to the previous ipywidgets-based version and does not show this problem anymore. Rolling it forward to the flask-based version is theoretically possible, but we're removing it before the next release. The only place where the fix may be useful is in the new repository. I'm afraid I can't make the repo public until we have the initial release ready, but if you have an idea for how to fix it I'm interested since we'll encounter the same issue in `raiwidgets` which will be flask-based.","24","0.866187384044527","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","660958056","issue_comment","https://github.com/Trusted-AI/AIF360/issues/523#issuecomment-660958056","or move it out of the repo :P","24","0.4871995820271683","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","661080371","issue_comment","https://github.com/Trusted-AI/AIF360/issues/523#issuecomment-661080371","That doesn't solve the documentation issue, though. Also raises a bunch of other questions we need to think about more, such as whether it stays within fairlearn or becomes a separate package. I think that's a very separate conversation and one that needs to be discussed in a proposal. Let's keep this one about doc.","13","0.3323211235528563","Artifact generation and benchmarking","Deployment"
"https://github.com/fairlearn/fairlearn","712474329","issue_comment","https://github.com/Trusted-AI/AIF360/issues/523#issuecomment-712474329","Closing this since we've removed the updates, added a warning to alert users that the dashboard will move out of the repo, and this documentation update therefore isn't required anymore.","24","0.6419238254100638","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","659692430","issue_comment","https://github.com/Trusted-AI/AIF360/issues/518#issuecomment-659692430","Possibly happening because tempeh depends on shap and one of the Jupyter Notebooks requires tempeh","4","0.3425028381800713","Installation and shell commands","Deployment"
"https://github.com/fairlearn/fairlearn","659831049","issue_comment","https://github.com/Trusted-AI/AIF360/issues/518#issuecomment-659831049","While this is definitely an issue worth investigating this also raises the dependency on `tempeh` as an issue. We could easily remove that dependency if we add the remaining datasets that we use from there directly to `fairlearn.datasets`.","5","0.3404674548172306","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","659838029","issue_comment","https://github.com/Trusted-AI/AIF360/issues/518#issuecomment-659838029","I tried reproing and couldn't with these steps:
```
conda create -n shap_issue_env python=3.7
conda activate shap_issue_env
pip install -e .
pip install -r .\requirements.txt
```
Now it could obviously be that I have C++ installed on my machine from another project and that isn't restricted by conda envs. I don't recall manually installing that, so another idea would be that it's there for everyone and you may have manually uninstalled it?

Could you tell me 
- the python version you used
- the exact commands to create the conda env (perhaps there's some significant difference)
- whether you see any C++ installed under ""Apps & features"". I have the following, for example: 
  <img src=""https://user-images.githubusercontent.com/10245648/87748154-fdd1b180-c7a9-11ea-8760-5eb4968a38c1.png"" width=""350"">


I'll look into removing `tempeh` from requirements.txt since that's bothered me for a while...","21","0.435340470738701","Installation and shell commands","Deployment"
"https://github.com/fairlearn/fairlearn","659999852","issue_comment","https://github.com/Trusted-AI/AIF360/issues/518#issuecomment-659999852","Microsoft still doesn't allow redistribution of the VS compilers :/ 
As a result, setting up C++ compilers on Windows is still very challenging.

You could check the [python's guide](https://docs.python.org/3/using/windows.html) or [scikit-learn's guide](https://scikit-learn.org/0.21/developers/advanced_installation.html#windows) on how to setup a windows machine. It may help.","4","0.4285572583444925","Installation and shell commands","Deployment"
"https://github.com/fairlearn/fairlearn","660477986","issue_comment","https://github.com/Trusted-AI/AIF360/issues/518#issuecomment-660477986","And of course the Windows agents in our build pools all come with Visual Studio:
https://docs.microsoft.com/en-us/azure/devops/pipelines/agents/hosted
(and at work, one of the first things I do on a newly imaged machine is install VS).

Is there an easy way of detecting packages with potentially problematic dependencies like this?","21","0.3041125541125541","Installation and shell commands","Deployment"
"https://github.com/fairlearn/fairlearn","660609510","issue_comment","https://github.com/Trusted-AI/AIF360/issues/518#issuecomment-660609510","Their `setup.py` says they support python 3.6 only, but their CI checks for python 3.5, 3.6, and 3.7. They can't do 3.8 since tensorflow is always lagging behind in terms of supporting latest python releases.

Also, for some reason they don't even include the 3.7 wheel in their release: https://pypi.org/project/shap/#files and that's why installing on a 3.7 environment tries to compile the package from source.

In general, it's important to be very careful with dependencies since it's really easy to get into situations where dependencies cannot be easily satisfied.","4","0.3295797029974245","Installation and shell commands","Deployment"
"https://github.com/fairlearn/fairlearn","660630591","issue_comment","https://github.com/Trusted-AI/AIF360/issues/518#issuecomment-660630591","I ran into the same issue with 3.7 Python 3.6 works without any issues.

Installing this did not work

> error: Microsoft Visual C++ 14.0 is required. Get it with ""Build Tools for Visual Studio"": https://visualstudio.microsoft.com/downloads/

Installing `MSVC v142 - VS 2019 C++ x64/x86 build tools (v14.26)` and `Windows 10 SDK (10.0.19041.0)` allows `Shap` to compile successfully.","4","0.750809760310948","Installation and shell commands","Deployment"
"https://github.com/fairlearn/fairlearn","660736148","issue_comment","https://github.com/Trusted-AI/AIF360/issues/518#issuecomment-660736148","Well on the plus side we don't depend on it, and we can soon remove the dependencies for `tempeh`. @hildeweerts has a PR out to replace the COMPAS dataset from `tempeh` with the openml version of it, and then the only remaining one should be the law school dataset.","0","0.2365223429053216","Dataset usage","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","661119263","issue_comment","https://github.com/Trusted-AI/AIF360/issues/518#issuecomment-661119263","While we should be off this particular issue for `shap` soon, I have a feeling we're going to hit the same underlying problem again. Indeed, I was asked to redo the ADO pipelines for interpret-community and interpret-text based on the work I did for Fairlearn, and.... those had an interesting mix of conda and pip in their setup. Then on the AzureML side, another problem got traced to `pip` not bothering to check the package version against the requirement in `setup.py` if the package was already installed.

@adrinjalali have you any suggestions for managing this form of dependency hell? When we supported Python 3.5, we needed a separate `requirements-3.5.txt` and we could go back to that.... presumably with similarly suffixed conda environment files if/when we add that in? Spawning off specialised files like that strikes me as non-ideal. One of the interpret repos has a script to generate the conda environment file, but that just moves the maintenance headaches to that script (and in its current incarnation, doesn't actually support any Python version than 3.6.8).","32","0.2974414646241271","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","662175156","issue_comment","https://github.com/Trusted-AI/AIF360/issues/518#issuecomment-662175156","@riedgar-ms I like the idea outlined in #516 which is about splitting.","20","0.6530136530136529","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","710026639","issue_comment","https://github.com/Trusted-AI/AIF360/issues/518#issuecomment-710026639","I don't see `shap` in our requirements files any more... this should be fixed?

*Update:* No; `tempeh` depends on it. However, only one notebook depends on `tempeh`","4","0.5250398724082935","Installation and shell commands","Deployment"
"https://github.com/fairlearn/fairlearn","665737108","issue_comment","https://github.com/Trusted-AI/AIF360/issues/516#issuecomment-665737108","Just noticed that `skorch` has a split requirements file (`requirements.txt` and `requirements-dev.txt`) and reads one of them in for the dependencies for `setup.py`:
https://github.com/skorch-dev/skorch/blob/master/setup.py
This is a pattern we should adopt, so we don't have to maintain things in both `requirements.txt` and `setup.py` (which will inevitably lead to breakages).

The main concern would be if we also need to split up on Python versions.","32","0.4082077922077924","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","665919610","issue_comment","https://github.com/Trusted-AI/AIF360/issues/516#issuecomment-665919610","I think we can further split it up, for people who want to build the documentation (sphinx etc.), people who want to run notebook tests (papermill etc.), ...
See the link I posted with the original issue at the top.

The only reason we have requirements.txt be that kitchen sink of everything is because we haven't split it up yet, so that duplication will automatically be solved by splitting it.","13","0.2783458815597258","Artifact generation and benchmarking","Deployment"
"https://github.com/fairlearn/fairlearn","666303484","issue_comment","https://github.com/Trusted-AI/AIF360/issues/516#issuecomment-666303484","The duplication to which I was referring was between `requirements.txt` and `setup.py`. Splitting up the former does not intrinsically fix that.

I'm not sure that further splitting of the `requirements.txt` would be desirable. So long as we can kick out `shap`, `xgboost` and so on from the core developer dependencies, I'm not sure that the remaining differences between 'doc only' and 'full dev dependencies' are great enough to be worth the maintenance hassle. Especially given how 'cheap' new conda environments are.","4","0.4356677078196065","Installation and shell commands","Deployment"
"https://github.com/fairlearn/fairlearn","666932229","issue_comment","https://github.com/Trusted-AI/AIF360/issues/516#issuecomment-666932229","Sure, and if we change our mind we can easily revisit this!","20","0.6127206127206126","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","668654967","issue_comment","https://github.com/Trusted-AI/AIF360/issues/516#issuecomment-668654967","I think this particular issue is closed by my commit #535 . As said above, we can open a new issue if that change proves insufficient.","20","0.5383675464320626","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","656698543","issue_comment","https://github.com/Trusted-AI/AIF360/issues/514#issuecomment-656698543","Thanks for reporting the issue. Could you please also include a [minimal and reproducible example](http://sscce.org/) for us to better diagnose the problem?","29","0.7752615119163161","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","656700971","issue_comment","https://github.com/Trusted-AI/AIF360/issues/514#issuecomment-656700971","Please see #495 ","0","0.0303030303030302","Dataset usage","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","656701995","issue_comment","https://github.com/Trusted-AI/AIF360/issues/514#issuecomment-656701995","Work-around also described here: https://github.com/fairlearn/fairlearn/issues/448#issuecomment-637667403","8","0.1711076280041798","Understanding Fairness Definition","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","651960283","issue_comment","https://github.com/Trusted-AI/AIF360/issues/510#issuecomment-651960283","As noted on the original email thread, setting the logger to 'debug' level will at least get regular output from the algorithms, so users know that _something_ is happening.","7","0.2328082495097108","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","651961289","issue_comment","https://github.com/Trusted-AI/AIF360/issues/510#issuecomment-651961289","Yeah, but that doesn't tell you much. You can sort of infer that each group of log messages is an iteration, but it's impossible to keep count...","17","0.6489389572963056","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","653468933","issue_comment","https://github.com/Trusted-AI/AIF360/issues/510#issuecomment-653468933","I haven't looked into fairlearn's GridSearch implementation so I don't know whether this is relevant, but I believe that in scikit-learn's GridSearchCV progress is printed through the joblib.Parallel dependency.","28","0.428548644338118","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","654369529","issue_comment","https://github.com/Trusted-AI/AIF360/issues/510#issuecomment-654369529","@hildeweerts it’s definitely relevant. @adrinjalali suggested using GridSearchCV for this a while back, so if we make such a change we could get that perhaps for free.","20","0.6704304188380622","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","654371963","issue_comment","https://github.com/Trusted-AI/AIF360/issues/510#issuecomment-654371963","joblib.Parallel looks like the sort of thing we should be using to speed up GridSearch training. Would be a bit of an overkill just for adding a print statement for every iteration, though.","25","0.3322462291072156","Research","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","659520200","issue_comment","https://github.com/Trusted-AI/AIF360/issues/510#issuecomment-659520200","How about using something like tqdm? 
https://tqdm.github.io/","5","0.2659352142110764","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","659535259","issue_comment","https://github.com/Trusted-AI/AIF360/issues/510#issuecomment-659535259","That looks good as an option - please fork and PR :-)","32","0.6731601731601732","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","807984746","issue_comment","https://github.com/Trusted-AI/AIF360/issues/510#issuecomment-807984746","The consensus in #517 was not to use `tqdm` to avoid introducing a new dependency. Instead, `logging` is preferable as @adrinjalali explained in #517 . If anyone wants to start with this task please comment below!","20","0.42267971191738","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","649211700","issue_comment","https://github.com/Trusted-AI/AIF360/issues/508#issuecomment-649211700","I tried a variety of things here already including several sphinx extensions that are supposed to run and capture widget state and display the interactive dashboard in html, but none of them worked. I wasted quite a bit of time figuring out how to debug them, but none of them had any debugging information, so I'm stuck on that front. The next steps could be to 

- contact the creators of the extensions to figure out how to debug them
- read up on `nbconvert` which seems to be part of most of the extensions so there's likely something to do with how it treats the dashboard
- learn more about our dashboard to understand what could be going wrong

However, I'm holding off on this right now until we have clarity on whether the dashboard will even live in this repo long-term. Moving it out of the repo would theoretically make the problem disappear.","24","0.4818501861303418","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","650351911","issue_comment","https://github.com/Trusted-AI/AIF360/issues/508#issuecomment-650351911","> Moving it out of the repo would theoretically make the problem disappear.

... and reappear in a different repo, surely?","24","0.7361156044430065","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","650450621","issue_comment","https://github.com/Trusted-AI/AIF360/issues/508#issuecomment-650450621","Probably. I’m having a hard time imagining that it’s not relevant at all. 

The “solution” of copy pasting 5000 lines of html every time it shows up in the webpage isn’t really acceptable to me so I’ve put this on hold until I figure out something better. Suggestions are most welcome.","20","0.3434492636856003","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","650789749","issue_comment","https://github.com/Trusted-AI/AIF360/issues/508#issuecomment-650789749","Did you end up trying [binder](https://mybinder.org/)? Or whatever equivalent Microsoft Azure has? I thought the idea was to have the dashboards on that platform, and have users go there if they want something interactive. I'm certain with some magic we can have the dashboards in the existing generated examples as well, but I'm not sure if it's a very high priority at this point.","24","0.4307725138711052","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","650804555","issue_comment","https://github.com/Trusted-AI/AIF360/issues/508#issuecomment-650804555","I haven’t looked at binder yet. Not showing the dashboard on the webpage somewhat defeats the purpose of having the notebooks on the page. The point is to illustrate metrics, visualize trade-offs etc., so if it doesn’t show the example is incomplete.","24","0.5185842803030302","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","830714561","issue_comment","https://github.com/Trusted-AI/AIF360/issues/508#issuecomment-830714561","I opened #764 for the Binder button and I'm closing this since we don't have an interactive dashboard much longer in this repo, so the need for this isn't really there anymore. Thanks everyone for the input, though! ","24","0.7139188495120699","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","909280643","issue_comment","https://github.com/Trusted-AI/AIF360/issues/507#issuecomment-909280643","Hi @romanlutz, I'd like to start looking at this! I will review the previous issues but please let me know if you have additional information that can provide context.","20","0.5925837320574163","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","909981931","issue_comment","https://github.com/Trusted-AI/AIF360/issues/507#issuecomment-909981931","Awesome!!! This is such an important one, I'm glad you are interested in starting 🙂

In think the comment linked in the issue description is where I'd start. Even just creating a list of sources would be a great step. Feel free to update this issue with comments on your progress, e.g., ""here's what I found so far"".

We'll likely want a new user guide section on this dataset, but we can discuss specifics when you have something you'd like to add. In terms of objective I'm really thinking of educating readers about the origins of the dataset and why using it is problematic.

 @koaning has a lot of thoughts on this topic, too, so perhaps there's something he'd like to add (?)","20","0.4947757303778248","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","910056047","issue_comment","https://github.com/Trusted-AI/AIF360/issues/507#issuecomment-910056047","I suppose I could add a guide that's pretty much the content of [this video](https://youtu.be/Z8MEFI7ZJlA?t=769). 

It's the material that we could almost pretty much copy from [the scikit-lego fairness tooling](https://scikit-lego.netlify.app/fairness.html). That way we'd also have a guide for the CorrelationRemover component. 

I other folks prefer to pick this up: be my guest.

","0","0.3867776733360188","Dataset usage","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","917722697","issue_comment","https://github.com/Trusted-AI/AIF360/issues/507#issuecomment-917722697","Hi @romanlutz, I went through all the issues cited (though did not do a deep dive into the research papers linked). @koaning  has linked a few resources, here and elsewhere, that provide good starting places!

List of resources:
- The original [paper](https://deepblue.lib.umich.edu/bitstream/handle/2027.42/22636/0000186.pdf?sequence=1&isAllowed=y) that introduced the boston housting dataset
- [Issue 466](https://github.com/fairlearn/fairlearn/issues/466) discusses porting `Demographic Parity Classifier` and `Equal Opportunity` from scikit-fairness to fairlearn, and Kevin suggests using the Boston housing dataset as an example
- Boston housing dataset used for fairness walkthrough: https://scikit-lego.netlify.app/fairness.html, same as https://scikit-fairness.netlify.app/fairness_boston_housing.html. I think this guide is pretty thorough, though
- [Benchmark notebook using boston dataset](https://gist.github.com/koaning/1a606e07c9b27d1889ada4185959d87a) from [issue 430](https://github.com/fairlearn/fairlearn/issues/430#issuecomment-632690069) that tests out `InformationFilter` and `CorrelationRemover`. Findings: (1) the hyperparameter alpha should not be constrained to (0,1) and (2) that two functions should be moved to fairlearn from scikit-fairness, `GramSmidthFilter` (aka `InformationFilter`?) and `CorrelationRemover`. 
- A [discussion in scikit-learn](https://github.com/scikit-learn/scikit-learn/issues/16155) about how one of the columns in the boston dataset encodes systemic racism, and the subsequent `FairnessWarning` now included in the dataset load. I don't know if the problematic column `B` is still in the dataset as implemented in fairlearn or if it was removed?
- In [issue 31 of scikit-fairness](https://github.com/koaning/scikit-fairness/issues/31), the user who raised most of the fetch_boston comments suggests taking a sociotechnical approach to the user guide by explaining how a team of diverse participants, some technical and some not, might be able to offer different perspectives on the dataset. Is that an approach we would want to take in the documentation? So not just highlighting the technical issues that other users have cited before in other comments and other repositories, but maybe a way of using fairlearn to address all the additional feedback different stakeholders might have about the data. 
- The purpose of the boston dataset was originally to measure air quality! I definitely think we should address that, given how differently it's used now.
-[Issue 435](https://github.com/fairlearn/fairlearn/issues/435) discusses scikit-learn phasing out the boston dataset, but given that this issue is a year old it may be overcome by events.

I am curious as to whether you have any ideas about what we might want to touch upon that goes beyond koanig's [tutorial](https://scikit-fairness.netlify.app/fairness_boston_housing.html). Maybe we could adapt that analysis for fairlearn and add some of the in- and post-processing functionalities to test outcomes? 

Overall, I am thinking the user guide post could have a structure as follows:
1. Introduction to the boston housing dataset - where does it come from, where is it currently used, where is it deprecated
2. Issues with the dataset - display via summary statistics and a 'typical' prediction pipeline. Here we can also talk in depth about scikit-learn issue #16155.
3. Run through the diagnostic analysis in the scikit-fairness blog post, adjusting for changes in fairlearn syntax
4. Discussion about what we found, how well different processing methods work at achieving some fairness goal, and general recommendations about using the dataset

Please let me know what you think!","0","0.5094421760538013","Dataset usage","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","918289128","issue_comment","https://github.com/Trusted-AI/AIF360/issues/507#issuecomment-918289128","Thanks @alliesaizan for the summary!!!

Some thoughts:
- `CorrelationRemover` is already part of Fairlearn thanks to @koaning https://fairlearn.org/main/api_reference/fairlearn.preprocessing.html#fairlearn.preprocessing.CorrelationRemover
- We never did anything to the dataset so it should be in the original configuration. In fact, you can try calling `fetch_boston` and the `feature_names` should include the `B` column. For me it shows `['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT']`. I think the main reason for including it in Fairlearn is to show this bad example.
- Writing about the origin of the dataset and then considering how it's been used since is absolutely important. I would say that already captures some of the issues. One can be arbitrarily ambitious with this, but I've found it to be beneficial to start small and iterate on that. Kevin outlined a pretty ambitious agenda that would be awesome to get to IMO, but it also requires a lot of time from several people. This documentation might provide the foundation for this to happen eventually.
- I like your outline. I'm not 100% sure about using the mitigation techniques because I really feel like putting the dataset first. On the other hand, I don't quite know what you have in mind there (as in ""what are you trying to show using the mitigation techniques?"") so I'm definitely open to looking into it. The first three points you made are definitely 100% in line with what I thought of.

I'd also love to hear if others have thoughts, for example @hildeweerts @LeJit @MiroDudik ","20","0.5295776648304269","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","918417201","issue_comment","https://github.com/Trusted-AI/AIF360/issues/507#issuecomment-918417201","@romanlutz thanks! I agree about putting the dataset first, and can hold off on the mitigation techniques for now. I think my intention with thinking about mitigation techniques is to show readers how to address disparities if they're using a similar type of dataset, but I don't want to give the impression that data processing is a panacea for all the issues with the boston data. Given that we for sure want to address the first three bullets, I can start working on sketching those sections out! ","20","0.3177773629519009","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","918961903","issue_comment","https://github.com/Trusted-AI/AIF360/issues/507#issuecomment-918961903","I agree with everything @romanlutz mentioned and I am looking forward to read your drafts @alliesaizan!","20","0.6447751536719507","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","927368422","issue_comment","https://github.com/Trusted-AI/AIF360/issues/507#issuecomment-927368422","Hi @romanlutz and @hildeweerts, [here](https://github.com/alliesaizan/fairlearn-volunteering-files/blob/main/Revisiting%20the%20Boston%20Housing%20Dataset.ipynb) is my first draft of the Boston housing article. I used the same method as @koaning to turn the problem into a classification problem so we could show the typical fairlearn evaluation metrics. I am open to any and all feedback! Please also let me know if/how you'd like to expand the article.","6","0.4165749010971087","API expansion","Development"
"https://github.com/fairlearn/fairlearn","928348815","issue_comment","https://github.com/Trusted-AI/AIF360/issues/507#issuecomment-928348815","There's a lot I like about this and huge kudos for creating this summary. Personally, I struggle with commenting on Jupyter notebooks because there's no way to use a ""comment on line xyz"" functionality. We could either add it as a user guide section (which is what I was thinking) OR an example notebook. In either case you\d need to create a PR against the Fairlearn repo so that we can provide feedback by line.

@hildeweerts @mmadaio how do you feel about user guide vs. example notebook? I'm leaning user guide.

### If it's a user guide section
Copy the content into a restructured text (`.rst`) file? I'm thinking a new file under `docs/user_guide`, perhaps `datasets.rst`. That would need to be listed in the corresponding `index.rst`. This is the first such dataset investigation (of hopefully many!) so we can create subsections/-pages there eventually but you can just use `datasets.rst` to start with. Restructured text is formatted differently, but I wouldn't worry too much about it for now. I think we're just looking high-level and then progressing to details (structure, goals, then content, eventually phrasing, formatting). 

### If it's an example notebook

To have it render in our ""examples"" section you'd need to convert it to a `.py` file. A tool like [jupytext](https://jupytext.readthedocs.io/en/latest/using-cli.html) should do the trick. Then just drop it into `examples and make sure the file name starts with `plot_`

### Advice that applies in both cases

Then you can build the docs using instructions from https://fairlearn.org/main/contributor_guide/contributing_documentation.html

If you need infos on how to create a fork and subsequently a PR: https://github.com/romanlutz/fairlearn-scipy-sprint

I'm sure @hildeweerts and @mmadaio, perhaps also @michaelamoako and @brkifle, would like to chime in once that PR exists.","14","0.3895375910452744","Documentation","Development"
"https://github.com/fairlearn/fairlearn","928441772","issue_comment","https://github.com/Trusted-AI/AIF360/issues/507#issuecomment-928441772","@romanlutz I was also thinking this would go in the user guide, so I can take the rst file route! I pushed the `datasets.rst` file to my forked repository and created the [PR](https://github.com/fairlearn/fairlearn/pull/961) so that everyone can start reviewing. ","14","0.4398082386363638","Documentation","Development"
"https://github.com/fairlearn/fairlearn","929061248","issue_comment","https://github.com/Trusted-AI/AIF360/issues/507#issuecomment-929061248","![image](https://user-images.githubusercontent.com/1019791/135070378-16c13357-2d05-4294-a539-c27c53d2c852.png)

The table seems to have two columns missing: B and LSTAT. This might be conscious, but I would recommend mentioning that you'll get back to these variables later in the tutorial. ","0","0.2966468559688898","Dataset usage","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","929112642","issue_comment","https://github.com/Trusted-AI/AIF360/issues/507#issuecomment-929112642","Hm @koaning I think that is an issue with the formatting; I did not take the columns out intentionally. See how a bunch of columns got caught up in the Description for TAX? I will work on adjusting the rst formatting so those columns do not get squished together.","9","0.3728106755629689","Feature engineering methodology","Design"
"https://github.com/fairlearn/fairlearn","647586185","issue_comment","https://github.com/Trusted-AI/AIF360/issues/504#issuecomment-647586185","I feel like this is a bit backwards. In 90% of the cases people don’t know that there’s something they can help with unless the issue owners say so. That’s why we have to be proactive about marking things as “help wanted” before anyone even asks.

As for the task lists: issues are threads so posting at the bottom gets tedious. That also means it changes through the thread. Task lists are built to be updatable. The project mgmt tool might allow notes on the board with the full list as well, but so far I’ve found it to be too much overhead to use. From what I’ve heard from people they look at issues so that’s probably the best place.

Having separate boards per workstream might work. ","20","0.7013153002416804","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","647818473","issue_comment","https://github.com/Trusted-AI/AIF360/issues/504#issuecomment-647818473","Switched to using projects instead of milestones now and deleted all milestones. The projects are at the repo level to make sure they're visible. I also tagged a few items as ""help wanted"". There will be some documentation-follow-up-PR to make sure this is captured in the contributor guide. The main remaining question is whether this is helpful and clear to contributors.","20","0.5991679921693381","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","648861039","issue_comment","https://github.com/Trusted-AI/AIF360/issues/504#issuecomment-648861039","You could also see here how other projects do ""up for grab"" https://up-for-grabs.net/#/

I find milestones really good for the releases. Project boards work only as long as people maintain them, which can be quite a bit of work. Let's see if they work here :)","20","0.8008560945780676","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","649180832","issue_comment","https://github.com/Trusted-AI/AIF360/issues/504#issuecomment-649180832","I had not heard of this! Just submitted the PR for Fairlearn. Thanks a lot for pointing it out @adrinjalali !

I absolutely agree w.r.t. maintaining the boards. I think we'll need ""owners"" for them that check them at least once a week. I've kind of been keeping track of all issues and PRs even without the boards, so it's not too much extra work. Shall we just start with me managing these or does anyone else want to manage any individual ones? [This is only about keeping the board up to date, no privileges :-) ]

Adding @MiroDudik @riedgar-ms ","20","0.848880228190573","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","649446499","issue_comment","https://github.com/Trusted-AI/AIF360/issues/504#issuecomment-649446499","I _think_ you need to have admin rights to maintain the boards IIRC.","13","0.5522810522810523","Artifact generation and benchmarking","Deployment"
"https://github.com/fairlearn/fairlearn","807958518","issue_comment","https://github.com/Trusted-AI/AIF360/issues/504#issuecomment-807958518","How do people feel about this so far? I don't think anyone's looking at the project boards and I've honestly not been able to consistently keep it up to date. I do like it as a grouping mechanism, but that could just as well achieved through labels.","20","0.3323211235528563","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","809556697","issue_comment","https://github.com/Trusted-AI/AIF360/issues/504#issuecomment-809556697","maintaining a board requires a dedicated person to maintain that board. I personally haven't been able to maintain one.","20","0.8497074491310799","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","809609392","issue_comment","https://github.com/Trusted-AI/AIF360/issues/504#issuecomment-809609392","Exactly. I don't think it's helping anyone either. For several months I did put in an effort to keep it up to date, but I never once heard that anyone actually uses it. My vote is to switch to just using labels. We can add a few new ones if people like that, such as
- organization/administrative
- visualizations
- metrics
- mitigations
- use-case

@fairlearn/fairlearn-maintainers wdyt?","20","0.4530250602914962","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","809613207","issue_comment","https://github.com/Trusted-AI/AIF360/issues/504#issuecomment-809613207","Good point, @romanlutz! If at some point we have a very specific project with a particular timeline for which a board may be useful we can always reinstate it.

I don't have a strong opinion on specific label names, the ones you mention make sense to me. Maybe use case -> educational materials (or add another one for educational materials). but I'm fine either way.","25","0.5264370509215871","Research","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","647453768","issue_comment","https://github.com/Trusted-AI/AIF360/issues/501#issuecomment-647453768","Looked into this, its not related to VS Code or the environment, its a problem of datasize. Its seems at some point over 100,000 rows, the dashboard exceeds maximum call stack. We will look into reworking this with larger datasets in mind. Sorry for the bug, but it may still work in VS code with smaller data set sizes.","11","0.2245201606903735","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","647458219","issue_comment","https://github.com/Trusted-AI/AIF360/issues/501#issuecomment-647458219","The underlying issue is a call to Math.min. This javascript library function is recursive and fails at about this point. Will add a task and tests to make sure the dashboard avoids using Math.min/max in places that could be large arrays.","9","0.3578693509489357","Feature engineering methodology","Design"
"https://github.com/fairlearn/fairlearn","647682636","issue_comment","https://github.com/Trusted-AI/AIF360/issues/501#issuecomment-647682636","thanks, I reduced my dataset to less than 100 000 rows for testing and its working now.
right, a recursive formula can be a problem, I hope you'll be able to solve it.","0","0.283811802232855","Dataset usage","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","654395793","issue_comment","https://github.com/Trusted-AI/AIF360/issues/501#issuecomment-654395793","Assigning this to myself for now since I'm switching the dashboard from ipywidgets to Flask. That alone may not solve all the problems in VS Code yet, but it's a prereq.","24","0.7546065845777657","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","728790132","issue_comment","https://github.com/Trusted-AI/AIF360/issues/501#issuecomment-728790132","The dashboard is moving to a dedicated package upon the next release (> 0.5.0). I'm closing this issue similar to #639 since we're not making updates in this repo anymore. VS code is on our list for environments we want to support, although I don't have a particular timeline yet.","24","0.7697525715874338","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","646884143","issue_comment","https://github.com/Trusted-AI/AIF360/issues/498#issuecomment-646884143","Thanks for sharing! I may have a different mental model of this than others but in my mind this maps almost 1-to-1 as follows:

- “try this right now” —> user guide, section on fairness 
- “domain-specific guides” —> example notebooks (unless these are broader than individual cases, then perhaps it would be more appropriate in a different way such as having domain-specific pages with various notebooks from that domain and additional information relevant to the domain)
- “learning more” doesn’t quite fit into a single place, of course.

As we pointed out this focus would warrant a shift in terms of at least the landing page structure since these are the topics that we want people to engage with.

I’m curious how you feel about the mapping above and what pieces you think need to change to highlight this focus properly. I’ve pointed out the landing page as one but perhaps it’s not limited to that.","25","0.5382853345923168","Research","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","647401631","issue_comment","https://github.com/Trusted-AI/AIF360/issues/498#issuecomment-647401631","Awesome! Are you working on this on a Github repo? Any chance I could get access to it to spy on you? :P","13","0.5403860670764583","Artifact generation and benchmarking","Deployment"
"https://github.com/fairlearn/fairlearn","649712398","issue_comment","https://github.com/Trusted-AI/AIF360/issues/498#issuecomment-649712398","@hildeweerts Sure, I bet you could find it if you poke around :)  If you want to collaborate or chat more feel free to message whenever!","20","0.6470758540822237","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","665856226","issue_comment","https://github.com/Trusted-AI/AIF360/issues/498#issuecomment-665856226","The hackpad [Fairlearn: Sociotechnical “talking points”](https://hackmd.io/nDiDafJ6TMKi2cYDHnujtA) is another smaller way to try working forward on this.","8","0.435996980480966","Understanding Fairness Definition","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","666500534","issue_comment","https://github.com/Trusted-AI/AIF360/issues/498#issuecomment-666500534","FYI, this is an example mockup I shared during the 7/30 call for what this could look like:

<img width=""1329"" alt=""Screen Shot 2020-07-30 at 12 17 44 PM"" src=""https://user-images.githubusercontent.com/1056957/88947558-b09c1800-d25e-11ea-8cf8-143fcd955c1c.png"">


and here's the dump of the HTML for future hacking :)
```
<div class=""container-fluid section-5 py-5"">
    <div class=""container"">
      <div class=""row justify-content-lg-center pt-5 pb-5"">
        <div class=""col-12 text-center"">
          <h2 class=""dark"">Fairness as a sociotechnical challenge</h2>
        </div>
      </div>
      <div class=""row justify-content-lg-center pb-5"">
        <div class=""col-12 col-lg-4 text-center text-lg-left"">
          <div class=""card h-100 benefit-card py-4 px-4"" style=""cursor: pointer;"">
            
            
            <h4 class=""dark pb-2 benefit-text"">Identifying potential tax fraud</h4>
            <p class=""benefit-text"" style=""/*! text-decoration: underline; *//*! color: blue; */"">You're a member of an analytics team in a European country, and brought in to consult about a project that has already started to scale the deployment of models for predicting which tax returns may require further investigation for fraud.  The team has used a model trained in other jurisdictions by a large predictive analytics supplier, and hopes that they can leverage this at a lower cost that would be required to invest in the capability in-house.</p>
            <p class=""benefit-text"" style=""/*! text-decoration: underline; *//*! color: blue; */""><img style=""margin-right: 10px;"" src=""https://avatars0.githubusercontent.com/u/1056957?s=400&amp;v=4"" width=""32""> <span style=""text-decoration: underline; color: blue;"">@kevinrobinson 7/29/20</span></p>
<p class=""benefit-text"" style=""/*! text-decoration: underline; *//*! color: blue; */""><img style=""margin-right: 10px;"" src=""https://avatars1.githubusercontent.com/u/10245648?s=460&amp;u=b88ceec5f790583634c1bec447477cce680eb58b&amp;v=4"" width=""32""> <span style=""text-decoration: underline; color: blue;"">@romanlutz 8/12/20</span></p>
          </div>
        </div>
        
        <div class=""col-12 col-lg-4 text-center text-lg-left"" style=""/*! opacity: 0.5; */"">
          <div class=""card h-100 benefit-card py-4 px-4"" style=""cursor: pointer;"">
            
            
            <h4 class=""dark pb-2 benefit-text"">Debit card fraud investigation</h4>
            <p class=""benefit-text"" style=""/*! text-decoration: underline; *//*! color: blue; */"">You're a data scientist at a Dutch financial services company, and your manager asks you to join an existing team.  This team has deploy a model trained on historical transaction data and now new debit transaction data is arriving.  For each new transaction, the model predicts whether it is potentially fraudulent and then will trigger an alert and inspection by human analysts.  The output that matters for the company is the final decision by the human analyst of whether to block the transaction, allow it but flag for further investigation by another team, or flag the transaction as normal.<a href=""https://arxiv.org/abs/1907.03334"" style=""margin-left: 5px;"">Weerts et al. (2019)</a></p>
            <p class=""benefit-text"" style=""/*! text-decoration: underline; *//*! color: blue; */""><img style=""margin-right: 10px;"" src=""https://avatars1.githubusercontent.com/u/24417440?s=460&amp;v=4"" width=""32""> <span style=""text-decoration: underline; color: blue;"">@hildeweerts 7/12/20</span></p>

          </div>
        </div>

        <div class=""col-12 col-lg-4 text-center text-lg-left"" style=""/*! opacity: 0.5; */"">
          <div class=""card h-100 benefit-card py-4 px-4"" style=""cursor: pointer;"">
            
            
            <h4 class=""dark pb-2 benefit-text"">Candidate screening</h4>
            <p class=""benefit-text"" style=""/*! text-decoration: underline; *//*! color: blue; */"">A potential client asks you if ML can help with predicting job candidates' suitability for jobs based on a combination of personality tests and body language <a href=""https://arxiv.org/pdf/1906.09208.pdf"">Rhagavan et al. (2019)</a></p>
            <p class=""benefit-text"" style=""text-decoration: underline;color: blue;""> @solonbarocas 6/23/20</p>
<p class=""benefit-text"" style=""text-decoration: underline;color: blue;""> @lisaibanez 8/2/20</p>
          </div>
        </div>

      </div>
    </div>
  </div>
```","8","0.3322628033819368","Understanding Fairness Definition","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","677686526","issue_comment","https://github.com/Trusted-AI/AIF360/issues/498#issuecomment-677686526","There's been some recent discussion offline with @nessamilan about this, and it hasn't yet migrated into GitHub or hackpad.  If anyone is interested in learning more or collaborating, please comment here or reach out in gitter.im/fairlearn/community, we'd love to talk more! 👍 ","20","0.4466813463353256","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","646588525","issue_comment","https://github.com/Trusted-AI/AIF360/issues/495#issuecomment-646588525","Did you get the notebook directly from GitHub?

If so, then we unfortunately have breaking changes there. There are [two basic options](https://github.com/fairlearn/fairlearn/tree/master/notebooks#notebooks-and-fairlearn-versioning)
- Clone the repo, which will allow the latest versions of the notebooks to work
- Navigate to the tag corresponding to your installed Fairlearn, and download the notebook there.

We are going to be paying more attention to versioning in future, but that is the current situation.","21","0.4366604186191816","Installation and shell commands","Deployment"
"https://github.com/fairlearn/fairlearn","646595710","issue_comment","https://github.com/Trusted-AI/AIF360/issues/495#issuecomment-646595710","Thanks, @riedgar-ms.
I checked the [release that is tagged with 0.4.6](https://github.com/fairlearn/fairlearn/releases/tag/v0.4.6). According to this release, I should use `predictors = sweep._predictors` instead of `predictors = sweep.predictors_`.
The issue can be closed. Thanks.","11","0.5788830004076642","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","646617112","issue_comment","https://github.com/Trusted-AI/AIF360/issues/495#issuecomment-646617112","Thanks for letting us know that you're unblocked.","28","0.2833150784958014","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","645562892","issue_comment","https://github.com/Trusted-AI/AIF360/issues/486#issuecomment-645562892","*Off-Topic:* I'm still trying to figure out why they picked Hari Seldon rather than Susan Calvin. I will concede that calling an algorithm ""Calvinist"" might have unintended implications.","18","0.489938446969697","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","645596280","issue_comment","https://github.com/Trusted-AI/AIF360/issues/486#issuecomment-645596280","Naming things is hard!  I interpreted it as either a weird kind of hubris or truly expert-level subtext critiquing the wisdom of the Plan :)

fwiw if you want to read more about how science fiction references in CS can influence people's perceptions of whether they belong in the field, check out [Sapna Cheryan's work](https://depts.washington.edu/sibl/publications/).","8","0.3812575090108131","Understanding Fairness Definition","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","646002048","issue_comment","https://github.com/Trusted-AI/AIF360/issues/486#issuecomment-646002048","I'm glad you brought this up, I had NO idea what character they were referring to 😂","20","0.4053030303030304","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","646044508","issue_comment","https://github.com/Trusted-AI/AIF360/issues/486#issuecomment-646044508","> Naming things is hard! I interpreted it as either a weird kind of hubris or truly expert-level subtext critiquing the wisdom of the Plan :)

Suggesting that doing things correctly requires a secret cadre of telepaths is certainly odd :-)

","8","0.3867776733360188","Understanding Fairness Definition","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","644917210","issue_comment","https://github.com/Trusted-AI/AIF360/issues/484#issuecomment-644917210","Please run 
```python
from fairlearn import show_versions
show_versions()
```
and paste the output here. That should provide us with some information on your context.

It's likely that you need to reload the browser page with the jupyter notebook (assuming you're running one) and rerun the cell with the dashboard. This is an issue a few people have faced.

Tagging @rihorn2 for awareness.","29","0.4123982683982684","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","645540972","issue_comment","https://github.com/Trusted-AI/AIF360/issues/484#issuecomment-645540972","![version](https://user-images.githubusercontent.com/60263695/84934672-4871e800-b09d-11ea-9b07-7507644b83ce.jpg)
","22","0.37166849215042","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","645541425","issue_comment","https://github.com/Trusted-AI/AIF360/issues/484#issuecomment-645541425","I tried multiple attempts, using show( ) or display( ), the dashboard is still not showing up...Thank you for your help btw! ","24","0.6839210611452602","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","645541928","issue_comment","https://github.com/Trusted-AI/AIF360/issues/484#issuecomment-645541928","I am working on Jupyter Notebook btw","24","0.7335997335997336","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","645543388","issue_comment","https://github.com/Trusted-AI/AIF360/issues/484#issuecomment-645543388","Did you try reloading the page before rerunning it?

Can you also tell us which version of Fairlearn you're using? `pip freeze` should tell you","24","0.4496578690127078","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","645543910","issue_comment","https://github.com/Trusted-AI/AIF360/issues/484#issuecomment-645543910","Yes, I did, tried reloading multiple times, and even restart kernel and re-run everything. ","24","0.8044965786901273","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","645543997","issue_comment","https://github.com/Trusted-AI/AIF360/issues/484#issuecomment-645543997","Which browser/OS is this?","2","0.2833150784958016","Performance measurement","Validation"
"https://github.com/fairlearn/fairlearn","645544706","issue_comment","https://github.com/Trusted-AI/AIF360/issues/484#issuecomment-645544706","Chrome on Windows
Fairlearn version is 0.4.6","29","0.6745983323164532","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","645547982","issue_comment","https://github.com/Trusted-AI/AIF360/issues/484#issuecomment-645547982","And the jupyter notebook is on AWS environment (doubt if that matters)","29","0.4200879765395894","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","645548535","issue_comment","https://github.com/Trusted-AI/AIF360/issues/484#issuecomment-645548535","You mean that you have a windows VM in AWS, and you're RDPing to it, and running the notebook there?","29","0.7749266862170089","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","645549806","issue_comment","https://github.com/Trusted-AI/AIF360/issues/484#issuecomment-645549806","Also, what version of Jupyter? As @romanlutz  said, if you could attach your `pip freeze` it would be very helpful.","21","0.4053030303030305","Installation and shell commands","Deployment"
"https://github.com/fairlearn/fairlearn","645554522","issue_comment","https://github.com/Trusted-AI/AIF360/issues/484#issuecomment-645554522","I've just used the following requirements file:
```
scikit-learn==0.23.1
numpy==1.18.5
scipy==1.4.1
pandas==1.0.4
Cython==0.28.4
matplotlib==3.0.3
fairlearn==0.4.6
```
and then doing a regular install of jupyter (and shap). With that, I can load one of our sample notebooks, and load the dashboard in Chrome.

I'm slightly puzzled by one thing - your output from `show_versions()` above implies that you're running on a Linux box. On my laptop, I get:
```
System:
    python: 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)]
executable: c:\users\riedgar\appdata\local\continuum\miniconda3\envs\bug-testing\python.exe
   machine: Windows-10-10.0.19041-SP0

Python dependencies:
       pip: 20.1.1
setuptools: 47.3.0.post20200616
   sklearn: 0.23.1
     numpy: 1.18.5
     scipy: 1.4.1
    Cython: 0.28.4
    pandas: 1.0.4
matplotlib: 3.0.3
    tempeh: None
```","29","0.7533743549027393","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","645563616","issue_comment","https://github.com/Trusted-AI/AIF360/issues/484#issuecomment-645563616","I'm using the AWS server so I think that's why they showed a Linux system?

jupyter == 1.0.0
jupyter-client ==5.2.3
jupyter-consele==5.2.0
jupyter-core==4.4.0
jupyterlab = = 0.32.1
jupyterlab-launcher==0.10.5","29","0.8255197717081125","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","645564294","issue_comment","https://github.com/Trusted-AI/AIF360/issues/484#issuecomment-645564294","I have no idea what's the cause of this issue, I can view an interactive plot through ipywidget...","24","0.5357293868921775","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","645573173","issue_comment","https://github.com/Trusted-AI/AIF360/issues/484#issuecomment-645573173","I'm still confused as to where and how the notebook is running. It almost sounds as if you're using a browser on Windows with the kernel running on a Linux machine in AWS?

If so, are you able to run the notebook locally, on the Windows machine? Or run the whole stack (browser and all) on the Linux box?","29","0.7296157450796626","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","645645789","issue_comment","https://github.com/Trusted-AI/AIF360/issues/484#issuecomment-645645789","I've heard people say it doesn't work on jupyterlab if that's what you're using.

It sounds like you have a Windows machine with Chrome, but the notebook is running on a Linux VM. Has anyone tried running the dashboard in Linux? ","29","0.6688351403947734","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","650320589","issue_comment","https://github.com/Trusted-AI/AIF360/issues/484#issuecomment-650320589","I had a similar problem! I am on a Linux VM (on Google Cloud AI Platform) using Jupyter Lab and upon inspecting the webpage, I see some error messages saying 'Fairlearn-Widget semver range  ^0.1.1 is not registered as a widget module'. I will screen shot below for reference.
![Screen Shot 2020-06-26 at 2 05 47 PM](https://user-images.githubusercontent.com/13242069/85887771-6387da80-b7b6-11ea-99e6-c7d5326c7a03.png)

","29","0.4417316017316018","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","650352858","issue_comment","https://github.com/Trusted-AI/AIF360/issues/484#issuecomment-650352858","Tag @rihorn2 ","11","0.2027168234064786","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","651095161","issue_comment","https://github.com/Trusted-AI/AIF360/issues/484#issuecomment-651095161","@bricha2 The current Fairlearn package does not support visualization in Jupyter Labs, only Jupyter notebook. This is an area we are actively working on addressing, and will update this thread when there is support in Labs. In the meantime, could you check if the visualizations are appearing in a notebook tab?","24","0.8713683223992504","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","651313231","issue_comment","https://github.com/Trusted-AI/AIF360/issues/484#issuecomment-651313231","Yeah, so when I move to Jupyter Notebook (within the GCS), I cannot see the widget here either. The error is slightly different here though. 

![Screen Shot 2020-06-29 at 3 37 48 PM](https://user-images.githubusercontent.com/13242069/86048611-caa7c800-ba1e-11ea-93f7-d81fbfbc2138.png)

I am gonna try it locally to see if I run into any problems.","24","0.3526271893244369","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","651360899","issue_comment","https://github.com/Trusted-AI/AIF360/issues/484#issuecomment-651360899","And this is to confirm it did work locally.","13","0.329153605015674","Artifact generation and benchmarking","Deployment"
"https://github.com/fairlearn/fairlearn","656649362","issue_comment","https://github.com/Trusted-AI/AIF360/issues/484#issuecomment-656649362","It is even not appearing in the Google Colab.","29","0.1949616648411828","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","656656774","issue_comment","https://github.com/Trusted-AI/AIF360/issues/484#issuecomment-656656774","Hi @rihorn2 
Visualizations are not appearing in azure notebooks(which is jupyter notebook) : <fairlearn.widget._fairlearn_dashboard.FairlearnDashboard at 0x7f90943abc88>
","24","0.7897116324535682","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","656692950","issue_comment","https://github.com/Trusted-AI/AIF360/issues/484#issuecomment-656692950","@sagu94271  The fairlearn dashboard does render in a jupyter notebook running on an azure compute instance
![image](https://user-images.githubusercontent.com/29712031/87162579-1dda1000-c294-11ea-83c7-f7b5cd2febf4.png)
As stated above, the dashboard relies on IPython Widget currently, which limits it to jupyter notebooks. The other environments mentioned that are not jupyter notebook-based will not work with the current communication layer. We are working on moving to a local flask service, but setting up the correct forwarding and auth for other cloud environments will require work per environment.
","24","0.7378694924707193","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","656733246","issue_comment","https://github.com/Trusted-AI/AIF360/issues/484#issuecomment-656733246","Thanks for the update @rihorn2 .After refresh It is indeed working with Local jupyter notebook","24","0.6839210611452603","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","661122275","issue_comment","https://github.com/Trusted-AI/AIF360/issues/484#issuecomment-661122275","Hey! This looks like an awesome package and so I wanted to give it a try, buuut ran into the same issues as described above. 

This happens on a local instance of jupyter notebooks:
![image](https://user-images.githubusercontent.com/11074788/87957720-ea2b9100-cab0-11ea-8d13-9260135dcf82.png)

And for info:
![image](https://user-images.githubusercontent.com/11074788/87957763-fca5ca80-cab0-11ea-8cd6-9aa96f4e205c.png)

It's worth noting I'm on `#master` (wanted the `datasets` submodule).","24","0.7199743918053776","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","661124862","issue_comment","https://github.com/Trusted-AI/AIF360/issues/484#issuecomment-661124862","@torfjelde if you're on `master` did you run `pip install .` to make sure the widget was setup? I don't recall the exact reason, but I know that that's required, rather than `pip install -e .`","21","0.5083823622445752","Installation and shell commands","Deployment"
"https://github.com/fairlearn/fairlearn","661171329","issue_comment","https://github.com/Trusted-AI/AIF360/issues/484#issuecomment-661171329","I originally installed using `pip install git+...` but tried what you suggested just now and the same issue occurs :confused: ","21","0.5186257479251111","Installation and shell commands","Deployment"
"https://github.com/fairlearn/fairlearn","661410851","issue_comment","https://github.com/Trusted-AI/AIF360/issues/484#issuecomment-661410851","@torfjelde what's `git+`? Are you trying to use the dashboard or change it? If you're just using it we suggest using `pip install fairlearn`. If you are trying to contribute, then please refer to the contributor guide https://fairlearn.github.io/contributor_guide/development_process.html#advanced-installation-instructions ","21","0.4183238636363636","Installation and shell commands","Deployment"
"https://github.com/fairlearn/fairlearn","661428881","issue_comment","https://github.com/Trusted-AI/AIF360/issues/484#issuecomment-661428881","> what's git+?

Just installing from github using `pip`. But also tried by manually cloning the repo and doing `pip install .`

> Are you trying to use the dashboard or change it?

Wanted to use it, but also wanted to use the `fairlearn.datasets` submodule which wasn't available on the last release (i.e. what I get if I do `pip install fairlearn`)
","21","0.5164621998532034","Installation and shell commands","Deployment"
"https://github.com/fairlearn/fairlearn","661865074","issue_comment","https://github.com/Trusted-AI/AIF360/issues/484#issuecomment-661865074","@torfjelde Does the dashboard work if you just `pip install fairlearn` rather than trying to use it directly from git? There have been some big changes in the dashboard recently, so we'd like to be sure which 'side' of them your issue is occurring.","21","0.4391221633374099","Installation and shell commands","Deployment"
"https://github.com/fairlearn/fairlearn","662174035","issue_comment","https://github.com/Trusted-AI/AIF360/issues/484#issuecomment-662174035","The datasets module is only available on the master branch, so either clone and install or wait for the next release. The code there is pretty minimal so you can also just copy it.","32","0.3232812385354757","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","678259527","issue_comment","https://github.com/Trusted-AI/AIF360/issues/484#issuecomment-678259527","Despite the comments, I'm having the same problems running the dashboard on a brand new Azure Machine Learning Compute Instance:

I executed:
```
!pip install --upgrade scikit-learn>=0.22.1
!pip install fairlearn
```
It still doesn't show the dashboard, but also doesn't give an error. I am not sure, but it looks like the dashboard can't find the widget's `js` libraries, as it tries to load them relative to the current URL, but it does not seem like Jupyter actually exposes those URLs? Hence a `404`?

![image](https://user-images.githubusercontent.com/525867/90878968-842f7380-e3a6-11ea-84a5-f164b4c78dc6.png)

Thanks
Clemens","24","0.3735908573792533","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","678375306","issue_comment","https://github.com/Trusted-AI/AIF360/issues/484#issuecomment-678375306","@csiebler due to current limitations you need to restart the kernel and reload the page (in your browser) and then rerun after your installation. We're working on addressing this at the moment and that won't be necessary anymore in about a month. Thanks for your patience.","24","0.7766205425779896","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","684083491","issue_comment","https://github.com/Trusted-AI/AIF360/issues/484#issuecomment-684083491","I'm having the same problem in an Azure ML Compute Instance Jupyter Notebook. I created a dedicated environment as follows:

```
conda create -y -n fair python=3.6 scikit-learn pandas numpy
conda activate fair
pip install azureml-sdk[notebooks] azureml-contrib-fairness fairlearn==0.4.6
conda install -y ipykernel
python -m ipkernel install --user --name=aml-fair
conda deactivate
```
Restarted the kernel (and in fact restarted Jupyter on the Compute Instance), but running code to display the widget just results in the following text output:

```
<fairlearn.widget._fairlearn_dashboard.FairlearnDashboard at 0x7f3442d2cc88>
```

It worked a week or so ago, and the only thing that's changed is the release of azureml-sdk 1.13 today.

Same error stack as @csiebler above (see screenshot below).
@romanlutz, restarting the kernel and refreshing doesn't resolve it for me. Even restarting the entire compute instance and rerunning the notebook doesn't help!

![image](https://user-images.githubusercontent.com/11771134/91778431-45b86680-eba7-11ea-8da2-9bfa1901b8f8.png)

","24","0.4760657421674369","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","684121925","issue_comment","https://github.com/Trusted-AI/AIF360/issues/484#issuecomment-684121925","I was able to repro this with these steps @GraemeMalcolm and spend the past couple hours trying various things with @KeXu444 .

Short-term:
- Using a new compute and installing with pip worked (with the usual reloading of page & kernel)
- Using a new compute and installing with conda worked, although it was very flaky with sometimes several retries required to make the dashboard show. For me it seemed to help to run `conda install -c conda-forge ipywidgets` after the fairlearn installation (i.e. after line 3 in your snippet above) and the dashboard showed up consistently afterwards.

Long-term: I'm working on replacing the `ipywidgets` based dashboard with a flask based dashboard that shouldn't have this issue and that shouldn't have the issue requiring restarts of page & kernel. I'm hoping to have a prototype ready by the end of the week, but it would only become available with the next release, of course.","24","0.6654014688637906","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","695055342","issue_comment","https://github.com/Trusted-AI/AIF360/issues/484#issuecomment-695055342","Update: #578 adds Flask to do the metric calculations as opposed to ipywidgets. This should solve the issue of the dashboard not showing up on the first call after installation.

To address issues in other environments we have future work coming up. Note that this dashboard will move out of this repository & package after the next release which is why we've added a warning. More info will be in that warning and we'll point you add the package as soon as it's published.","24","0.8571815473503658","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","728791472","issue_comment","https://github.com/Trusted-AI/AIF360/issues/484#issuecomment-728791472","We decided to roll back the flask changes before v0.5.0 since we observed regressions in previously supported environments. Additionally, the dashboard is moving out of Fairlearn with the next release. Closing this issue similar to #639 since we won't be fixing this in this repo. For the new dashboard please refer to `raiwidgets` once it gets released at the end of the month!","24","0.7043862606285352","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","759104604","issue_comment","https://github.com/Trusted-AI/AIF360/issues/484#issuecomment-759104604","Hi @romanlutz I am trying to use the fairness dashboard in a local jupyter notebook instance on my laptop but am having some difficulties. I installed fairlearn via pip. Here is my information: 
`
System:
    python: 3.8.5 (default, Jul 28 2020, 12:59:40)  [GCC 9.3.0]
executable: /usr/bin/python3
   machine: Linux-5.4.0-58-generic-x86_64-with-glibc2.29
Python dependencies:
    Cython: None
matplotlib: 3.3.3
     numpy: 1.19.0
    pandas: 1.1.5
       pip: 20.0.2
     scipy: 1.4.1
setuptools: 45.2.0
   sklearn: 0.24.0
    tempeh: None
`

When I run the below code in a cell (after following the [Fairlearn Dashboard: 2.2.1 Set-up and single model assessment](https://fairlearn.github.io/master/user_guide/assessment.html#fairlearn-dashboard) instructions),
`
from fairlearn.widget import FairlearnDashboard
FairlearnDashboard(sensitive_features=A_test, sensitive_feature_names=['race', 'gender'], y_true=y_test, y_pred=y_pred)
`

I get this as the output.
`
<fairlearn.widget._fairlearn_dashboard.FairlearnDashboard at 0x7fa16c4e5fa0>
`

Above, I see you noted that for the new dashboard we should refer to raiwidgets--How do we use raiwidgets with the fairlearn dashboard? Has the fairlearn documentation been updated with the instructions of how to use the dashboard now with raiwidgets? Please let me know. Thanks so much!","29","0.4993527251591768","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","759156811","issue_comment","https://github.com/Trusted-AI/AIF360/issues/484#issuecomment-759156811","Hi @mjorgen1 ! Did you try restarting the kernel? The current `FairlearnDashboard` is built on `ipywidgets` and often doesn't show up the first time because of some internals (which is one of the main reasons why we don't use that anymore in `raiwidgets`). If that still doesn't help I'd try reloading the Jupyter notebook page in your browser. 

Clearly, this kind of experience isn't ideal and a good motivation to improve things. That's why we're about to release `raiwidgets` as a separate package. There were a few delays but it's currently planned for the end of this month. By then we'll update the Fairlearn documentation as well. Currently it justs says that the `FairlearnDashboard` is deprecated and people should use `raiwidgets`, but I'll add some pointers on how to use `raiwidgets` before we release.","24","0.5966590001210509","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","797835021","issue_comment","https://github.com/Trusted-AI/AIF360/issues/484#issuecomment-797835021","FYI: I met the same problem with Jupyter Lab, but the dashboard works for me in Jupyter Notebook.","24","0.7757401100340579","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","797980287","issue_comment","https://github.com/Trusted-AI/AIF360/issues/484#issuecomment-797980287","@xgchena JupyterLab is not currently supported. I would encourage you to file a feature request in the [raiwidgets package](https://github.com/microsoft/responsible-ai-widgets) for JupyterLab support, since the `FairlearnDashboard` moved to that package. The current version in Fairlearn is deprecated and will go away shortly.","24","0.6628082062864672","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","875515871","issue_comment","https://github.com/Trusted-AI/AIF360/issues/484#issuecomment-875515871","Hi @romanlutz I have updated my fairlearn package to 0.6.2 and I've attached a picture showing my package versions. I'm still having issues with showing the fairlearn widget when using Jupyter Notebook on my local machine, even after restarting my kernel and refreshing the notebook page as well. I've included what I'm trying to run and the output, as well as the Firefox browser console showing some issues. Thanks for your help--this functionality is really useful so I'm hoping to get this sorted.
![image](https://user-images.githubusercontent.com/29387373/124749041-acbb5000-df1b-11eb-88c8-19af34b11f18.png)
![image](https://user-images.githubusercontent.com/29387373/124749278-ee4bfb00-df1b-11eb-8d75-13816bb3bf49.png)
![image](https://user-images.githubusercontent.com/29387373/124749431-205d5d00-df1c-11eb-8c06-b69c3bc46418.png)
![image](https://user-images.githubusercontent.com/29387373/124749662-6ca89d00-df1c-11eb-9faa-37c2b10f8461.png)


","24","0.6413745437266465","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","875768149","issue_comment","https://github.com/Trusted-AI/AIF360/issues/484#issuecomment-875768149","@mjorgen1 thanks for reaching out! If you can send me a minimal example (using the code block rendering in markdown, for example) that reproduces this issue on your machine that would be enormously helpful. Sadly, the widget is rather tricky to debug and the screenshots alone don't give me much of a hint of where things are going wrong.

Please also note that the widget has moved away from Fairlearn and is not included in the latest version v0.7.0 that was released a few hours ago. You can find (a newer version of) it in the raiwidgets package built in https://github.com/microsoft/responsible-ai-widgets if you're interested.","24","0.3025022004275116","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","883549032","issue_comment","https://github.com/Trusted-AI/AIF360/issues/484#issuecomment-883549032","Hi @romanlutz, Given your advice, I updated Fairlearn and downloaded/imported raiwidgets, updated my kernel and refreshed the page, and wala! It worked. Thanks so much for your help! :)","24","0.8115782522562183","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","644805814","issue_comment","https://github.com/Trusted-AI/AIF360/issues/482#issuecomment-644805814","I'm interested to know how this interacts with tags and branches. We'll need to make sure that our release process conforms to expectations.","32","0.3953366518208014","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","644914226","issue_comment","https://github.com/Trusted-AI/AIF360/issues/482#issuecomment-644914226","Me, too. I'll start by reading the scikit-learn code that they use for this purpose sometime later this week.","20","0.8044965786901274","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","644998462","issue_comment","https://github.com/Trusted-AI/AIF360/issues/482#issuecomment-644998462","There are many ways you can do it. The way it's done in scikit-learn is that the CI pushes the docs to the website for each release branch, i.e. any branch whose name looks like a major release, plus sending master's compiled docs to `/dev`. The release manager sets what `stable` points to, manually, in what you see in step 7 of [this guide](https://github.com/scikit-learn/scikit-learn/blob/master/doc/developers/maintainer.rst), which comes after

> For major/minor (not bug-fix release), update the symlink for stable and the latestStable variable


Buf fix releases simply change the release branch and therefore the docs will be updated for that branch.

You can modify this pipeline to have pushing the docs done half manually (with push of a button somewhere) if you want more control.

I'm happy to talk more about it if things are not clear.","32","0.8933950767730929","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","650355908","issue_comment","https://github.com/Trusted-AI/AIF360/issues/482#issuecomment-650355908","One random thought, this may have to wait until we actually start releasing new versions. After all, the old branches (very old!) and tags (more recent) will not have the up-to-date documentation directories, and hence will not build on CircleCI.","32","0.7227177832605806","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","650789246","issue_comment","https://github.com/Trusted-AI/AIF360/issues/482#issuecomment-650789246","My suggestion would be (if human resources allow) is to start setting up the infrastructure which itself is quite a bit of work. With the new release you can then test things much better. I wasn't thinking of having the docs for the older versions that we have, more like setting up things so that we have two versions (last release and the current state), and at the next release that will become three (the existing releases plus what the default branch becomes)","32","0.7246814942776938","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","650804744","issue_comment","https://github.com/Trusted-AI/AIF360/issues/482#issuecomment-650804744","Sklearn also only has newer versions on the webpage so I don’t see any issues with that.","24","0.586892177589852","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","643685631","issue_comment","https://github.com/Trusted-AI/AIF360/issues/478#issuecomment-643685631","This is awesome (your list, not the notebook)! Looking forward to digging deeper into this!!!

IMO this notebook/example is perhaps the one most urgently in need of reworking, so I'm eager to get started once I figure out all the other follow-ups from the developer call we discussed.

One thing I want to definitely also mention w.r.t. this example: this may be one of those cases where tech shouldn't be used at all, or at least not in the form of these COMPAS scores. So if the result is that we just want to take it down I'm definitely not going to object. Just wanted to make sure that people are aware that that's an option. I'd hate to fall into the ""solutionist trap"" from Selbst et al. (""Fairness and Abstraction in Sociotechnical Systems""). ","25","0.7981813862070456","Research","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","644756842","issue_comment","https://github.com/Trusted-AI/AIF360/issues/478#issuecomment-644756842","@romanlutz no problem!  I think it's great progress to even be naming the ""solutionist trap"" here 👍","25","0.3461632453567939","Research","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","651940395","issue_comment","https://github.com/Trusted-AI/AIF360/issues/478#issuecomment-651940395","I found this resource: https://allendowney.github.io/RecidivismCaseStudy/

Seems to analyze various perspectives on the topic. In this particular topic we have to do lots of reading since there’s already so much work out there.","25","0.807440398218496","Research","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","692804199","issue_comment","https://github.com/Trusted-AI/AIF360/issues/478#issuecomment-692804199","@hildeweerts In going through some other work, I ran into another paper that may have helpful framing on this and added it last up above.

That paper is similar to what @michaelamoako has been talking about in terms of ""start with a narrow technical frame, then take a step outward to reveal more context each time, so you can see how much the narrow technical frame missed.""  He might have some other great ideas on this too!","25","0.7434458874458875","Research","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","692878613","issue_comment","https://github.com/Trusted-AI/AIF360/issues/478#issuecomment-692878613","Thanks, Kevin. This is super helpful!","25","0.5188087774294673","Research","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","709381695","issue_comment","https://github.com/Trusted-AI/AIF360/issues/478#issuecomment-709381695","I added Wang et al. 2020 to the description above, and a link to the actual COMPAS survey questions.","8","0.4481704654615313","Understanding Fairness Definition","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","718808804","issue_comment","https://github.com/Trusted-AI/AIF360/issues/478#issuecomment-718808804","Folks things through this may be curious to know that Proposition 25 is on the ballot in California next week, and involves changes to pre-trial detention.  The use of risk assessment algorithms is one part of what's being contested more broadly: https://voterguide.sos.ca.gov/propositions/25/","8","0.6205921737836635","Understanding Fairness Definition","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","643308628","issue_comment","https://github.com/Trusted-AI/AIF360/issues/477#issuecomment-643308628","Thanks for bringing this up! I'm not a huge fan of `default`. I think I best like `main`.

Other options are `develop` (since we're using GitHub workflow), or `latest`, or `current`.","32","0.6939801603478735","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","643326282","issue_comment","https://github.com/Trusted-AI/AIF360/issues/477#issuecomment-643326282","Yes, thank you! I like `main` too.","32","0.1949616648411828","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","643481659","issue_comment","https://github.com/Trusted-AI/AIF360/issues/477#issuecomment-643481659","I’m still baffled that I never noticed this before :-(
I like all your suggestions @MiroDudik 
Let’s see whether @adrinjalali @hildeweerts @kevinrobinson @koaning @MBrouns @riedgar-ms @rihorn2 or others have thoughts as well ","20","0.8375403302334411","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","643486264","issue_comment","https://github.com/Trusted-AI/AIF360/issues/477#issuecomment-643486264","I have a slight preference to `trunk` or `develop`, but don't have anything against `main` either.","32","0.7574348132487667","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","643500362","issue_comment","https://github.com/Trusted-AI/AIF360/issues/477#issuecomment-643500362","One other thing I want to flag is that this relates to something that we'd discussed previously about having a `dev` or `unstable` branch and then a `release` or `stable` branch, so that folks can still make bigger changes even close to a release. ","32","0.5228059138093739","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","643552155","issue_comment","https://github.com/Trusted-AI/AIF360/issues/477#issuecomment-643552155","@hannawallach I do have a proposal out for making our versioning a bit more robust, and that includes starting to have `release` branches again:
https://github.com/fairlearn/fairlearn-proposals/pull/11

As for a new name.... I think I'd prefer something like `trunk` or `main` to `dev` if we want to make sure that our main development is always ready-to-ship.","32","0.7496531041582739","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","643686306","issue_comment","https://github.com/Trusted-AI/AIF360/issues/477#issuecomment-643686306","So many great suggestions! As a non-native English speaker I'm somewhat confused with `trunk`. It doesn't seem intuitive to me. Perhaps someone can explain this to me :-)

`main` seems to have the same kind of intuition as `master` but without the baggage. `dev` or `develop` are intuitive to me as well.

`current` or `latest` may be confusing because people may think that it reflects the version that they get when they run `pip install fairlearn`.

Maybe I'm overthinking this, but those are the thoughts I had when seeing these now.","15","0.4844696969696971","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","643701218","issue_comment","https://github.com/Trusted-AI/AIF360/issues/477#issuecomment-643701218","Just came across this: https://github.com/dfm/rename-github-default-branch/blob/main/README.md","10","0.2391383716684924","Model development","Development"
"https://github.com/fairlearn/fairlearn","643736964","issue_comment","https://github.com/Trusted-AI/AIF360/issues/477#issuecomment-643736964","> So many great suggestions! As a non-native English speaker I'm somewhat confused with trunk. It doesn't seem intuitive to me. Perhaps someone can explain this to me :-)

I think it came from SVN originally but I've always seen it as the trunk of a tree, from which branches spawn

","15","0.3925507116996478","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","643759832","issue_comment","https://github.com/Trusted-AI/AIF360/issues/477#issuecomment-643759832","My only preference is to use what the rest of the community is gonna use since there are a bunch of scripts around which rely on the name of the ""main"" branch. It seems the community is converging on `main` which I'm happy with.","32","0.5257457386363635","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","643999924","issue_comment","https://github.com/Trusted-AI/AIF360/issues/477#issuecomment-643999924","I like `main`, but I'm happy with any of the other suggestions as well.","20","0.3306693306693306","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","644131912","issue_comment","https://github.com/Trusted-AI/AIF360/issues/477#issuecomment-644131912","This implies that GitHub is going to go with `main`:
https://www.theregister.com/2020/06/15/github_replaces_master_with_main/
although I've not yet turned up an official announcement from the GitHub site yet, just the tweet.

Unless we *really* don't like their final choice, I suggest we stick to whatever is chosen by GitHub for the sake of simplicity.","7","0.4272102467978759","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","659934257","issue_comment","https://github.com/Trusted-AI/AIF360/issues/477#issuecomment-659934257","Does anybody know what GitHub is going to go with? I couldn't find any announcements dated later than June 15.","7","0.7335997335997334","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","659952705","issue_comment","https://github.com/Trusted-AI/AIF360/issues/477#issuecomment-659952705","AFAIK they haven't finished the work yet, so we don't know for sure, but I'm almost certain it's `main`.

In so far as the tasks go, I've compiled a list using the feedback I got from different places here: https://discuss.python.org/t/communitys-take-on-changing-master-branch-to-main/4462/12?u=adrinjalali

Not all of them are needed on this repo, but it's a good starting point.","20","0.5042613636363639","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","660068865","issue_comment","https://github.com/Trusted-AI/AIF360/issues/477#issuecomment-660068865","Thanks for compiling that list of steps @adrinjalali . Funnily enough, we had an internal email this week saying that detailed instructions are coming.  I expect there may be some automation on the GitHub side to take care of some of those (e.g. redoing any open PRs) although things like redoing the build definitions are going to remain manual.","32","0.7814361164175748","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","665670256","issue_comment","https://github.com/Trusted-AI/AIF360/issues/477#issuecomment-665670256","FYI: https://github.com/github/renaming","31","0.5188087774294673","Testing","Maintenance"
"https://github.com/fairlearn/fairlearn","665705485","issue_comment","https://github.com/Trusted-AI/AIF360/issues/477#issuecomment-665705485","Thanks for highlighting that update @hildeweerts !

While I can see the argument that we should make the change sooner, I'm inclined to follow the suggestion on that page, and wait for the automation to become available. Although we could use it as a forcing function, to clear our PR backlog :-)","32","0.3895162808206288","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","665921070","issue_comment","https://github.com/Trusted-AI/AIF360/issues/477#issuecomment-665921070","I think I'd rather wait for the seamless move than spend hours fixing things all over the place. I'm not 100% convinced even the seamless move will fix things, though. I'm particularly thinking of the automation that builds the docs and pushes that to the other repo's master branch. If we had more info on what the seamless move will include we could judge whether it's worth waiting or not. What do you think?","32","0.6854312354312357","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","666298063","issue_comment","https://github.com/Trusted-AI/AIF360/issues/477#issuecomment-666298063","I'm okay with waiting a bit longer for the 'seamless' move. Perhaps it'd make sense though to set a go/no go date in case Github takes longer than expected.","32","0.4970999053030301","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","666933257","issue_comment","https://github.com/Trusted-AI/AIF360/issues/477#issuecomment-666933257","Let me see whether I can get some more details on just how seamless this will be... if it's not sufficient I'd say there's no particularly good reason to wait.","32","0.8144074193896257","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","734982392","issue_comment","https://github.com/Trusted-AI/AIF360/issues/477#issuecomment-734982392","I still can't find information on when this will happen. I am more and more thinking that @hildeweerts was right in suggesting a go/no go date :-) 

So I'll just propose doing this during the week of December 21 and volunteer myself. Thoughts?

One option that would probably cause very little disruption is if I try this around the holidays in December, because I don't expect too many people to write code around that time. The parts that may cause some trouble:
- pipelines on Azure, GitHub, and CircleCI need to be repointed at `main` branch
- website repo `fairlearn.github.io` needs to consume `main` branch as opposed to `master` branch","32","0.6029001673173453","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","734983568","issue_comment","https://github.com/Trusted-AI/AIF360/issues/477#issuecomment-734983568","last update was that it should happen by the end of the year: https://github.com/github/renaming#later-this-year-seamless-move-for-existing-repositories-

I'd say ask your colleagues when they're planning to launch this? :P","24","0.7180223285486443","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","753049014","issue_comment","https://github.com/Trusted-AI/AIF360/issues/477#issuecomment-753049014","It says January 2021, that's all I could gather internally as well. Fine, I'll wait another month...","20","0.8274560552140622","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","761603656","issue_comment","https://github.com/Trusted-AI/AIF360/issues/477#issuecomment-761603656","GitHub now properly supports renaming the branch: https://github.com/github/renaming

> Renaming a branch will:
> 
>    Re-target any open pull requests
>    Update any draft releases based on the branch
>    Move any branch protection rules that explicitly reference the old name
>    Update the branch used to build GitHub Pages, if applicable
>    Show a notice to repository contributors, maintainers, and admins on the repository homepage with instructions to update local copies of the repository
>    Show a notice to contributors who git push to the old branch
>    Redirect web requests for the old branch name to the new branch name
>    Return a ""Moved Permanently"" response in API requests for the old branch name
> ","32","0.404030117879405","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","763163642","issue_comment","https://github.com/Trusted-AI/AIF360/issues/477#issuecomment-763163642","In addition to this, we'll also have to manually adjust the website build.","32","0.4504580690627202","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","774407015","issue_comment","https://github.com/Trusted-AI/AIF360/issues/477#issuecomment-774407015","Trying my luck now! Stay tuned.","24","0.5761643278421804","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","774408309","issue_comment","https://github.com/Trusted-AI/AIF360/issues/477#issuecomment-774408309","The PR description of #694 has all the instructions on adjusting your local env.","32","0.4348729227761486","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","642140611","issue_comment","https://github.com/Trusted-AI/AIF360/issues/473#issuecomment-642140611","@MiroDudik is working on the `ThresholdOptimizer()` right now.","23","0.551555826723612","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","644147456","issue_comment","https://github.com/Trusted-AI/AIF360/issues/473#issuecomment-644147456","I want to get #381 in first, but once that's in, adding the costs will only require minor refactoring. The API in #381 is of the form
* `thr_optimizer = ThresholdOptimizer(constraints=..., objective='accuracy_score', ...)`

We also allow objective to be `'balanced_accuracy_score'` and a few other string options.

For the scenario you are thinking about, I can see two API styles that would match up with this incoming PR:

1. `thr_optimizer = ThresholdOptimizer(constraints=..., objective_cost_matrix=[[0,1],[1,0]], ...)`

   where the cost matrix follows the same convention as sklearn's [confusion_matrix](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html)

2. `thr_optimizer = ThresholdOptimizer(constraints=..., objective_costs = {'fp': 1.0, 'fn': 0.8})`

I think I much prefer the second one for clarity. Would that work?","15","0.4522843228200373","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","644150631","issue_comment","https://github.com/Trusted-AI/AIF360/issues/473#issuecomment-644150631","Yes @MiroDudik I prefer the second one too! That's exactly what I was thinking about.

I actually tweaked the class just to include 2 extra parameters Cfp and Cfn, but with the dictionary is more elegant

thanks a lot!

(of course if I can be of any help in the code writing as well, let me know. I am not an expert at all in github contributing, but I can learn!)","15","0.4165550761295443","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","641545674","issue_comment","https://github.com/Trusted-AI/AIF360/issues/472#issuecomment-641545674","I'd say that there's no value to a build which is perpetually red, and hence ignored. Fundamentally, we need to get better about compatibility (which is what my PR over in `fairlearn-proposals` is about). Of course, the build is currently checking _forwards_ compatibility, which is necessarily going to break as features are added (although I believe that the current issues are simply due to naming instability). To keep this build green through such changes, we would need to have tests and notebooks declare their 'minimum supported version.'

Having the documentation versioned is related, and very good to have, but doesn't solve the issue that we have too many breaking changes too often.

Until we get to that v0.5.0 compatibility commitment, it might be worth putting '[Optional]' or something like that on the build name. That will at least reduce the confusion.","32","0.7623202755946118","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","642029154","issue_comment","https://github.com/Trusted-AI/AIF360/issues/472#issuecomment-642029154","I still don't think there should be a CI step which tests against the latest released version, that's not a sensible thing to me really.

We do need to have different versions of the docs online, since API changes always happen and people should see the latest released version online and not what's on master (unless they explicitly go to the latest version of the docs which is built from the master).

And checking for backward compatibility should be clear from checking the tests and running the docs on master itself. If a PR has to change anything in the docs, then it's a breaking change.

Also we don't really need the notebooks to test that, as long as we have good docs and examples which use the API, we'll notice the breaking changes.","32","0.8689455453977628","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","642067808","issue_comment","https://github.com/Trusted-AI/AIF360/issues/472#issuecomment-642067808","> And checking for backward compatibility should be clear from checking the tests and running the docs on master itself.

Do you mean keeping backwards-compatibility tests in master, @adrinjalali ? I had been thinking that we would have builds which would take tests from supported releases, and run them against Fairlearn installed from master.","32","0.8594110115236876","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","642175469","issue_comment","https://github.com/Trusted-AI/AIF360/issues/472#issuecomment-642175469","@adrinjalali  that sounds reasonable to me. I’ll see whether I can carve out some time for doc versioning. Last time I looked at scikit-learn’s logic for that it looked very complicated. ","32","0.7350862888979481","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","642495722","issue_comment","https://github.com/Trusted-AI/AIF360/issues/472#issuecomment-642495722","I don't mean putting anything more than what we already have in master @riedgar-ms . When reviewing a PR, if the PR has to change anything in the tests or the docs to make the CI pass, that means it's probably not backward compatible, and that's enough, isn't it? ","32","0.8758145125577276","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","642643686","issue_comment","https://github.com/Trusted-AI/AIF360/issues/472#issuecomment-642643686","@adrinjalali Agreed. But I do like to have builds which double check things :-)","32","0.3609481915933529","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","643217975","issue_comment","https://github.com/Trusted-AI/AIF360/issues/472#issuecomment-643217975","What I'm trying to say is that it's the wrong check. Certain changes to master break things and that's okay as far as we document it and/or warn the user. master's not supposed to pass all the tests with an outdated (aka released) version. ","32","0.7106085403957743","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","643283357","issue_comment","https://github.com/Trusted-AI/AIF360/issues/472#issuecomment-643283357","If we are trying to provide some level of backwards compatibility, sure `master` should be able to run older tests (within the compatibility window)?","32","0.6890749601275916","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","643530639","issue_comment","https://github.com/Trusted-AI/AIF360/issues/472#issuecomment-643530639","@riedgar-ms I am not even sure that makes sense. There are tests that use things which aren’t around anymore. For example, the current pipeline is failing because a member name was changed from _<name> to <name>_. That isn’t really a breaking change in some ways because it was underscored to begin with. 
What you’re proposing is in essence a different version (different in scope of tests) of the existing pipeline, right? Perhaps I’m missing something.

I certainly see the point in being intentional about documenting breaking changes better for users. In part @kevinrobinson ’s PR will help with that since we get a dedicated place to document how to upgrade from version to version.

That along with versioned docs should go a long way. I’m not opposed to further checks in principle, but they may be better off not being PR checks.","32","0.6696950769405485","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","643550940","issue_comment","https://github.com/Trusted-AI/AIF360/issues/472#issuecomment-643550940","I know that there will be breaking changes in future, but if we keep having breaking changes for every single release on PyPI, we are neither going to get nor keep users. At some point, we're going to have to start making the promise that code that works with `x.y.z` will also work with `x.y.z+1`. Running the tests of an older version (but *within the compatibility window*) against `master` is a way of verifying that compatibility.","32","0.8222272165411845","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","643760558","issue_comment","https://github.com/Trusted-AI/AIF360/issues/472#issuecomment-643760558","> If we are trying to provide some level of backwards compatibility, sure `master` should be able to run older tests (within the compatibility window)?

Imagine we find a bug in a method and fix it, which also changes the output of the method and hence the need to change the tests. Then as a result of your `FIX` your `main`/`master` branch is not backward compatible. This happens quite frequently in my experience and this alone is enough for me not to require full backward compatibility of the `main` branch with the latest release. And if such a fix goes to the `x.y.z+1` release, then those relevant tests are not gonna pass on `x.y.z` and that's intentional and okay.

We also should start using the `.. versionadded` and `.. versionchanged` directives in the docs to help users figure out when/what has changed.","32","0.7607673564638457","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","643829192","issue_comment","https://github.com/Trusted-AI/AIF360/issues/472#issuecomment-643829192","I essentially agree with all points raised by @adrinjalali . I really think we should drop this specific test.

On a very basic level -- I am not sure what's the action associated with not passing the test? When I create a PR or review a PR, I already know whether the API is being changed (the current tests are being updated etc.), so anything related to the changes in API should really be caught then.

The issue of how we deal with backward compatibility is a separate one that we should figure on https://github.com/fairlearn/fairlearn-proposals/pull/11 .
","32","0.6395979020979025","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","644089260","issue_comment","https://github.com/Trusted-AI/AIF360/issues/472#issuecomment-644089260","> And if such a fix goes to the x.y.z+1 release, then those relevant tests are not gonna pass on x.y.z and that's intentional and okay.

But if we regard `x.y.z` as ""supported"" then wouldn't a bug fix get backported as a `post[n]` release (although I'm not sure how PyPI handles those if one just specifies `x.y.z` as the version)?

I suppose as @MiroDudik says, it depends on what we want to call 'supported' and what we want our versions to mean.","32","0.724348204841018","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","644212415","issue_comment","https://github.com/Trusted-AI/AIF360/issues/472#issuecomment-644212415","`post[n]` releases are not used as a bug fix release AFAIK ([PEP440](https://www.python.org/dev/peps/pep-0440/#post-releases)):

> Some projects use post-releases to address minor errors in a final release that do not affect the distributed software (for example, correcting an error in the release notes).

A bug fix release should change the version of the software as I understand.","32","0.4156555285918736","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","644273957","issue_comment","https://github.com/Trusted-AI/AIF360/issues/472#issuecomment-644273957","I see - thanks for highlighting that part of PEP440, which specifically says not to put bugfixes into post releases. I'd been thinking about the [SemVer](https://semver.org/) approach, there the `.z` is explicitly supposed to be backwards compatible. So we've been talking at cross purposes.

However, we should follow the PEP440 way.

I then don't see any easy way to monitor API changes, to make sure that we do have backwards compatibility of the API (short of a lot of 'API only' tests). I know that in theory we know from looking at PRs and the Changes.md, but that's manual, not automatic.","32","0.3447887599745434","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","644292262","issue_comment","https://github.com/Trusted-AI/AIF360/issues/472#issuecomment-644292262","However, I think we're getting a bit off-topic. I think that:

- Further backwards-compatibility discussion needs to go over to my larger proposal in `fairlearn-proposals`
- We should have versioned docs
- The current test-notebooks-against-last-release CI build isn't particularly useful, since it's perpetually red","32","0.6553794829024183","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","641136317","issue_comment","https://github.com/Trusted-AI/AIF360/issues/471#issuecomment-641136317","The Microsoft team doesn't really do `make`, so it's up to us to just expand it, and for now it's been just me. I'm happy to have a richer `Makefile`. For the pre-commit hooks, I rather have a pre-commit hook than having it in the `Makefile` though, so that people on Windows machines could easily use it as well.","21","0.503774771940862","Installation and shell commands","Deployment"
"https://github.com/fairlearn/fairlearn","641160194","issue_comment","https://github.com/Trusted-AI/AIF360/issues/471#issuecomment-641160194","No problems with a `check`/`install` command?","3","0.2316453121822249","Bias detection metrics validation","Validation"
"https://github.com/fairlearn/fairlearn","641170861","issue_comment","https://github.com/Trusted-AI/AIF360/issues/471#issuecomment-641170861","I usually prefer to use `pip install --no-build-isolation -e .` to install, but I definitely wouldn't mind having whatever you like in the makefile. For check I would rather have something like the one you see [here](https://github.com/scikit-learn/scikit-learn/blob/master/.pre-commit-config.yaml). Any reason you don't like it?","21","0.534688995215311","Installation and shell commands","Deployment"
"https://github.com/fairlearn/fairlearn","641337539","issue_comment","https://github.com/Trusted-AI/AIF360/issues/471#issuecomment-641337539","The `Makefile` is definitely a contribution from the wider community - Powershell doesn't come with `make` :-)","13","0.6210007047216349","Artifact generation and benchmarking","Deployment"
"https://github.com/fairlearn/fairlearn","644410571","issue_comment","https://github.com/Trusted-AI/AIF360/issues/471#issuecomment-644410571","Definitely want to echo @adrinjalali and @riedgar-ms 's statements. Feel free to extend however it feels right for you!
Does that resolve the issue? ","20","0.5855383809274296","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","647810861","issue_comment","https://github.com/Trusted-AI/AIF360/issues/471#issuecomment-647810861","OK, looks like this is clear (= please improve/update makefile however you feel is appropriate) and the issue isn't needed anymore. Please reopen if anything else comes up. Thanks!","24","0.5531524926686219","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","640876473","issue_comment","https://github.com/Trusted-AI/AIF360/issues/468#issuecomment-640876473","@MiroDudik is doing extensive refactoring on this right now #381 . That PR addresses this documentation gap. There's still another gap, of course, and that's the missing user guide. I'll get to that soon, since I'm writing all user guides right now. If you take a look at that PR, feel free to add your review and please mention if there are other shortcomings you notice!","20","0.8261756189077862","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","647810235","issue_comment","https://github.com/Trusted-AI/AIF360/issues/468#issuecomment-647810235","Closing this since #381 was merged with documentation for `constraints`. Please reopen if there are further issues. Thanks!","24","0.6270772238514174","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","640847644","issue_comment","https://github.com/Trusted-AI/AIF360/issues/466#issuecomment-640847644","I'll defer to @MiroDudik  on the question of the algorithm itself (but I expect a positive response - we'd like a richer library!), but I'm not a huge fan of the name. We've tended to have the name of the technique be the class name, and then specify the constraint as an argument. This is obviously true for things like reductions, but we do something similar for ThresholdOptimizer.

What sub-package were you thinking of using (creating, I expect)?","15","0.3027191836226744","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","640852091","issue_comment","https://github.com/Trusted-AI/AIF360/issues/466#issuecomment-640852091","It seems awesome to merge efforts! 👍   It'd be super helpful to understand why this approach is valuable, particularly in comparison to what is already in this repository, and other contributions being discussed.  If there's a way to show this in an example scenario or notebook, that'd be amazing.

re: fairness.html, I've shared some feedback on the fairness.html example already over in https://github.com/koaning/scikit-fairness/issues/31, and a specific critique https://github.com/fairlearn/fairlearn/issues/430#issuecomment-632690069, but to recap: if you are trying to predict the median house price in a census tract, I don't understand why you would want to enforce that those predictions are uncorrelated with information about the income of people living in that census tract.  It seems like this is what the [Demographic Parity Classifier](https://scikit-lego.readthedocs.io/en/latest/fairness.html#Demographic-Parity-Classifier) and [Equal Opportunity](https://scikit-lego.readthedocs.io/en/latest/fairness.html#Equal-opportunity) examples are doing, so I'd love to understand more about why someone would use this technique, or about how this would mitigate any real harms.

Alternately, if the idea is that the task is similar to what the Boston dataset is used for in the original [Harrison and Rubinfeld work](https://deepblue.lib.umich.edu/bitstream/handle/2027.42/22636/0000186.pdf?sequence=1&isAllowed=y), it's harder for me to understand or interpret.  The original paper is not using this dataset to do a prediction task - the point of the hedonic pricing methodology in the paper is to learn the relationships between the features they have chosen and the median housing price in that tract.  They then use the weights of that model to understand the relative influence of the ""clean air"" feature, and how much it contributes to the median housing price which they know as ground truth.  So in that context I'm trying to understand the rationale for applying mitigation techniques to the predictions, since their methodology doesn't involve using the dataset to make predictions.

To try to help brainstorm another idea, I noticed that the Zafar paper runs its experiments on the UCI adult income dataset, and a ""bank dataset.""  Perhaps it might be helpful to explore if the original paper has example tasks that can show the value of these approaches?  While the URL in the paper to the bank dataset is broken, at first glance it seems like it refers to [a scenario](https://archive.ics.uci.edu/ml/datasets/Bank+Marketing) where a bank is trying to predict the success of a telemarking phone call campaign to get people to purchase 'term deposits' like bonds.  The paper discretizes age to ""between 25 and 60 or not"" and then shows the technique applied to a binary classifier.  I'm happy to help brainstorm but unfortunately it's also hard for me to understand how enforcing demographic parity on predictions about the success of a telemarketing campaign contributes to mitigating real harms.  I also don't understand paper's references to ""disparate impact"" in the context of marketing campaigns (eg, the company is deciding how many times to call each person and each row in the ""bank dataset"" encodes the bank's salespeople's decisions about how many times to call that home).  EDIT: Here's the references to the two papers on the UCI site for the ""bank dataset"": [A Data-Driven Approach to Predict the Success of Bank Telemarketing (Moro et al. 2014](http://repositorium.sdum.uminho.pt/bitstream/1822/30994/1/dss-v3.pdf) and [Using Data Mining for Bank Direct Marketing: An Application of the CRISP-DM Methodology (Moro et al. 2014)](https://repositorium.sdum.uminho.pt/bitstream/1822/14838/1/MoroCortezLaureano_DMApproach4DirectMKT.pdf).  In the second paper, they also describe how they are not using the dataset to train a model for prediction:

> The business goal is to find a model that can explain success of a contact, i.e. if the   client   subscribes   the   deposit.   Such   model can increase campaign efficiency by identifying the   main characteristics that affect success, helping in a better management of the available resources (e.g. human effort, phone  calls,  time) and selection of a high quality and affordable set of potential buying customers.

To be super clear, I'd love to see this kind of merging and collaboration, and I'm just a curious outsider :)  So I'm trying to share this feedback and asking these kinds of questions in that same spirit 😄 ","0","0.4056089663760896","Dataset usage","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","640854619","issue_comment","https://github.com/Trusted-AI/AIF360/issues/466#issuecomment-640854619","I agree that the current names are not super great but I'm not sure I like the alternative either. Basically the current algorithm is a logistic regression with a constraint on the covariance between the predicted probabilities and the sensitive attributes, that is *a* way of imposing demographic parity, but I'm pretty sure there can be others as well. You easily run the risk of creating god objects and code that is not open/closed. I just took a quick look at the `ThresholdOptimizer` and the presence of a function named ` _threshold_optimization_demographic_parity` confirms this fear.

I think this would make most sense in a `linear_models` package, with names like `EqualizedOddsLogisticRegression` and `DemographicParityLogisticRegression`. 
 
@kevinrobinson Agreed on the point regarding the dataset in the notebook I linked. Other datasets seem much more suitable. We did also test this approach briefly with another dataset on arrests in another bit of experimentation that I did with @koaning although that was more focused on how we could do these kinds of fairness topics in a Bayesian framework. (https://www.mbrouns.com/posts/2020-02-20-prior-has-some-potential/)","28","0.2244326437874824","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","640863498","issue_comment","https://github.com/Trusted-AI/AIF360/issues/466#issuecomment-640863498","@MBrouns thanks!  I see that post, and am guessing the dataset is the same as the one with the same number of records [included in R](https://www.rdocumentation.org/packages/carData/versions/3.0-3/topics/Arrests)?  The R docs say that it's from police records in Toronto, filtered to only include arrests made for possession of small amounts of marijuana, and it doesn't disclose when or how it was collected.

I think it's important to say that predictive policing has been the subject of extensive work by many people, so it'd be amazing to connect the assessment and mitigation techniques here with that work!  If it's helpful, I think these two papers might be the most directly relevant to what you're doing here [An algorithm for removing sensitive information: application to race-independent recidivism prediction (Johndrow and Lum 2017)](https://arxiv.org/pdf/1703.04957.pdf) and [To predict and serve? (Lum and Isaac 2016)](https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/j.1740-9713.2016.00960.x) is shorter and more narrative.  In particular, they discuss why the sociotechnical context around the collection of the dataset is critical in evaluating algorithmic fairness mitigation techniques because of particular forms of observation bias and feedback loops in policing patterns.

FWIW, racial bias in policing is the subject of particular attention and discussion at this moment, at least in the US.  So potential users and audiences for fairlearn may be even more attuned to evaluating any algorithmic approaches that aim to reduce harm in predictive policing systems in a wider sociotechnical context.","8","0.6557516102271561","Understanding Fairness Definition","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","640874676","issue_comment","https://github.com/Trusted-AI/AIF360/issues/466#issuecomment-640874676","> You easily run the risk of creating god objects and code that is not open/closed. I just took a quick look at the ThresholdOptimizer and the presence of a function named _threshold_optimization_demographic_parity confirms this fear.

Just to quickly address this fear: there's a PR by @MiroDudik out right now that generalizes this so that these things aren't specific under the hood. I don't know to what extent this is possible but can you imagine extending it to other constraints (say, true positive parity) in the future? I have yet to read the paper, so I can't say right now. If this is something we're interested in, then perhaps the ""god object"" as you call it isn't the worst idea. The APIs in the repo so far are written in a way where you specify the constraint as an argument in the constructor, so these would be the first ones that have it in the name.
I'm curious what you think about this.

---

I think @riedgar-ms referred to the general name when he wondered whether there's perhaps something better. For example, you could call any classifier that optimizes for Demographic Parity `DemographicParityClassifier`, but it really should be descriptive of the technique that's used.

> I think this would make most sense in a linear_models package, with names like EqualizedOddsLogisticRegression and DemographicParityLogisticRegression.

This sounds much better to me because it describes the technique better.","15","0.2468098597130854","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","640914517","issue_comment","https://github.com/Trusted-AI/AIF360/issues/466#issuecomment-640914517","Yes, I was wondering if there was something equivalent to the `Moment` objects for this algorithm, and if that pattern could be used. It would be nice to have something like:
`est = ConstrainedLogisticRegression( constraint=DemographicParity(), ....)`
although I suspect that `DemographicParity` would not be from our `Moment` hierarchy (since we don't want those becoming god-objects either).","28","0.5257457386363633","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","640928517","issue_comment","https://github.com/Trusted-AI/AIF360/issues/466#issuecomment-640928517","@riedgar-ms those are reductions-method specific. `ThresholdOptimizer` will have string arguments to specify the constraints, at least post-Miro's-PR.","15","0.6120440069484658","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","640966774","issue_comment","https://github.com/Trusted-AI/AIF360/issues/466#issuecomment-640966774","> @riedgar-ms those are reductions-method specific. `ThresholdOptimizer` will have string arguments to specify the constraints, at least post-Miro's-PR.

Indeed - but if there is useful algorithmic work which can be done inside the 'constraint' objects, the same general pattern can be followed, surely? What I don't know is whether there is sufficient commonality to be worthwhile.","15","0.6145910826761892","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","647327215","issue_comment","https://github.com/Trusted-AI/AIF360/issues/466#issuecomment-647327215","@romanlutz @MiroDudik Did you have a look at the implementation in scikit-lego? Is there (apart from naming) any concerns that should be addressed before porting? ","20","0.6839210611452603","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","647928902","issue_comment","https://github.com/Trusted-AI/AIF360/issues/466#issuecomment-647928902","I have to read it in depth, but I don't see a concern since there's a paper backing it up. I'll definitely prioritize this once it's a PR, so please go ahead.

Just to make sure, though: your implementation is different from the paper authors' right? I stumbled upon this repo while looking for the paper https://github.com/mbilalzafar/fair-classification and they have this licensed under GPL which is very restrictive, so I just want to make sure. It doesn't strike me as similar, but I haven't had the time for a closer look yet. I'm curious if you have any idea how your implementation is different. Perhaps they are willing to give us an exception from their license or something like that if theirs has any properties that are useful.","25","0.4210081585081586","Research","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","647964226","issue_comment","https://github.com/Trusted-AI/AIF360/issues/466#issuecomment-647964226","The implementation is indeed different, I have just taken a look through their codebase and it seems like they also support a linear SVM and they also seem to support different constraints, although I'm not really sure yet what those are.

They do use the same solver (cvxpy). 

The implementation of the `EqualOpportunityLogisticRegression` is not backed by a paper. I'll see if I can submit a PR this week, but it will basically be a 1-to-1 port of what's available in scikit-lego. That's why I wanted to check whether there's concerns beforehand.","20","0.4007191819305539","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","648883741","issue_comment","https://github.com/Trusted-AI/AIF360/issues/466#issuecomment-648883741","I looked at the proposed algorithm and I agree that this would be a nice addition (and our first inprocessing algorithm). It solves similar problems as the reductions, but the optimization problems it creates might be simpler to solve in practice.

As you say, the Demographic Parity follows
* [Fairness Constraints: Mechanisms for Fair Classification](https://arxiv.org/pdf/1507.05259.pdf)

Your Equal Opportunity implementation actually follows another approach by Zafar et al:
* [Fairness Beyond Disparate Treatment & Disparate Impact: Learning Classification without Disparate Mistreatment](https://arxiv.org/pdf/1610.08452.pdf)
(see their Eq. 13, where _g<sub>&#952;</sub>_ is set according to Eq. 12, to reflect Eq. 5; note that they encode negative examples as y=-1)

We should think more about the right API, but the algorithm is a go from my perspective.

My current thoughts about the API... I am thinking about something like:
```
from fairlearn.inprocessing import ConstrainedLogisticRegression

est = ConstrainedLogisticRegression('demographic_parity', ...)
est = ConstrainedLogisticRegression('true_positive_rate_parity', ...)
```
But I think that we may want to consider changing the name, to e.g. something like:
* `'demographic_parity'` -> `'bounded_covariance'` with a keyword argument `'upper_bound'`

Do we want to continue discussing API on this issue or would it be better to have a call?","6","0.3840917862531139","API expansion","Development"
"https://github.com/fairlearn/fairlearn","648999118","issue_comment","https://github.com/Trusted-AI/AIF360/issues/466#issuecomment-648999118","Why not 

```
from fairlearn.linear_model import ConstrainedLogisticRegression
```","28","0.4918414918414918","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","649096969","issue_comment","https://github.com/Trusted-AI/AIF360/issues/466#issuecomment-649096969","> 'demographic_parity' -> 'bounded_covariance' with a keyword argument 'upper_bound'

Both the equal opportunity and demographic parity have bounded covariance constraints though. How would we distinguish between them?","3","0.459368774247667","Bias detection metrics validation","Validation"
"https://github.com/fairlearn/fairlearn","650186630","issue_comment","https://github.com/Trusted-AI/AIF360/issues/466#issuecomment-650186630","**Re. modules**: I can see arguments either way:
* linear_model PROS:
  * the same name as in sklearn, so benefit to familiarity
* linear_model CONS:
  * it is possible to mitigate unfairness in logistic regression via reductions or postprocessing or preprocessing, so I don't think that we necessarily what the users to go instinctively to `fairlearn.linear_model` for these needs (?)
  * I don't think that we should try to have a mitigated variant for all estimators in `sklearn.linear_model` 
* inprocessing PROS:
  * for better or worse, the `inprocessing` is now a terminology accepted in parts of the fair ML community, e.g., https://arxiv.org/pdf/1908.09635.pdf and https://aif360.readthedocs.io/en/latest/modules/algorithms.html#module-aif360.algorithms.inprocessing
* inprocessing CONS:
  * I don't necessarily love the name... also, this might become a bit too much of a catch-all module

**Re. function API**: Good points re. covariance, how about the following format:
```
est = ConstrainedLogisticRegression('demographic_parity', covariance_bound=0.1, ...)
est = ConstrainedLogisticRegression('true_positive_rate_parity', covariance_bound=0.1, ...)
```
or maybe:
```
est = ConstrainedLogisticRegression(constraints='demographic_parity', covariance_bound=0.1, ...)
est = ConstrainedLogisticRegression(constraints='true_positive_rate_parity', covariance_bound=0.1, ...)
```

I think that this would play nicely with how we do things elsewhere, e.g.,
```
est = ExponentiatedGradient(base_estimator, DemographicParity(difference_bound=0.1), ...)
est = ExponentiatedGradient(base_estimator, DemographicParity(ratio_bound=0.8), ...)
est = ThresholdOptimizer(estimator=base_estimator, constraints='demographic_parity', ...)
```
","28","0.7361115075400789","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","650564824","issue_comment","https://github.com/Trusted-AI/AIF360/issues/466#issuecomment-650564824","A quick update: I have taken an informal poll among a few fair ML / data mining researchers, and folks raised several additional issues with in-processing (for example, it sounds like data transformation rather than training and so it is confusing--also, it is not as widely adopted as I thought), so I no longer think that we should consider it as a module name. So let's stick to **fairlearn.linear_model**.

We just need to settle the calling API. Let's first figure out how to handle specification of constraints. Let me repeat my two suggestions from above and add a third one:

```
class ConstrainedLogisticRegression(BaseEstimator, LinearClassifierMixin):
    def __init__(self, constraints='demographic_parity', *, covariance_bound=0.0, ...)

class ConstrainedLogisticRegression(BaseEstimator, LinearClassifierMixin):
    def __init__(self, *, constraints='demographic_parity', covariance_bound=0.0, ...)

class LogisticRegression(BaseEstimator, LinearClassifierMixin):
    def __init__(self, *, constraints=None, covariance_bound=0.0, ...)
```
The idea is that `constraints=None` would just revert to the unconstrained behavior. I think that your current implementation has the default corresponding to `covariance_bound=np.inf`--I think it's a bit counter-intuitive, but open to discussion.

Thoughts?
","28","0.3669803978792744","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","749003985","issue_comment","https://github.com/Trusted-AI/AIF360/issues/466#issuecomment-749003985","@MBrouns : Ping. Let us know if you want to resume working on this, or prefer to drop it, or maybe pitch to the community to take over?","20","0.4018529241459179","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","807960647","issue_comment","https://github.com/Trusted-AI/AIF360/issues/466#issuecomment-807960647","@MBrouns any thoughts? :-)","20","0.2975444096133753","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","813878164","issue_comment","https://github.com/Trusted-AI/AIF360/issues/466#issuecomment-813878164","Apologies for the radio silence. It's been busy and I've been avoiding programming in my spare time a bit to focus on other things. 

I'm still happy to pick this up but I can't commit to a deadline. If there's someone lined up who wants to tackle this I'm happy for them to give it a go. If not, I'll try to make a start near the end of the month","20","0.9062207982912832","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","813879747","issue_comment","https://github.com/Trusted-AI/AIF360/issues/466#issuecomment-813879747","I also don't have time, but probably a topic that requires discussing upfront: our implementation of these tools requires a dependency from cvxpy. @MiroDudik is this a dependency you're willing to accept? ","20","0.4471436705257166","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","815301628","issue_comment","https://github.com/Trusted-AI/AIF360/issues/466#issuecomment-815301628","There's no particular urgency AFAIK. @hildeweerts and I have just been going through all issues to add more detail where possible or mark as ""help wanted"" where appropriate, since we're going to participate in developer sprints at PyCon and SciPy this year. 

Thanks for pointing out the `cvxpy` dependency @koaning ! Out of interest I've installed `cvxpy` locally and it brought in a few new ones I didn't have: `ecos`, `scs`, `osqp`, and `qdldl`. This is the `pipdeptree` output in case you're interested:
```
cvxpy==1.1.11
  - ecos [required: >=2, installed: 2.0.7.post1]
    - numpy [required: >=1.6, installed: 1.19.0]
    - scipy [required: >=0.9, installed: 1.4.1]
      - numpy [required: >=1.13.3, installed: 1.19.0]
  - numpy [required: >=1.15, installed: 1.19.0]
  - osqp [required: >=0.4.1, installed: 0.6.2.post0]
    - numpy [required: >=1.7, installed: 1.19.0]
    - qdldl [required: Any, installed: 0.1.5.post0]
      - numpy [required: >=1.7, installed: 1.19.0]
      - scipy [required: >=0.13.2, installed: 1.4.1]
        - numpy [required: >=1.13.3, installed: 1.19.0]
    - scipy [required: >=0.13.2, installed: 1.4.1]
      - numpy [required: >=1.13.3, installed: 1.19.0]
  - scipy [required: >=1.1.0, installed: 1.4.1]
    - numpy [required: >=1.13.3, installed: 1.19.0]
  - scs [required: >=1.1.6, installed: 2.1.2]
    - numpy [required: >=1.7, installed: 1.19.0]
    - scipy [required: >=0.13.2, installed: 1.4.1]
      - numpy [required: >=1.13.3, installed: 1.19.0]
```
... definitely makes me lean towards soft dependency (which is what we do for things like `matplotlib` right now).
@adrinjalali typically has thoughts on such matters, too :-)","21","0.7656464138876201","Installation and shell commands","Deployment"
"https://github.com/fairlearn/fairlearn","815855327","issue_comment","https://github.com/Trusted-AI/AIF360/issues/466#issuecomment-815855327","yeah I've had issues with `cvxpy` before because of its dependencies. It's the kinda package that I would be really worried about for having a hard dependency on.

I would suggest having it as another soft dependency, and having on `pip install fairlearn[complete]` for instance. But we should also start having a very clear table, with methods/modules on one side, and dependencies on the other side, for people to have a clear view about them.","21","0.5003703345892506","Installation and shell commands","Deployment"
"https://github.com/fairlearn/fairlearn","824919561","issue_comment","https://github.com/Trusted-AI/AIF360/issues/466#issuecomment-824919561","I like @adrinjalali 's suggestion to have `cvxpy` as a soft dependency. I like the suggestion re. doc table with soft dependencies. I'm agnostic if we need `complete` install variant--but it seems nice from the user perspective.","21","0.3805485294846998","Installation and shell commands","Deployment"
"https://github.com/fairlearn/fairlearn","640848505","issue_comment","https://github.com/Trusted-AI/AIF360/issues/463#issuecomment-640848505","Thanks for pointing this out. We're following up.","8","0.1507849580138736","Understanding Fairness Definition","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","640934825","issue_comment","https://github.com/Trusted-AI/AIF360/issues/463#issuecomment-640934825","You're absolutely right... there are a few other issues with images that we're aware of. We need to properly set up licensing of artwork (not covered by MIT license, because it's not code) before we merge the illustration source files into the repo--but once that's in, we'll be able to start iterating on better artwork! (Tagging @mesameki and @hannawallach )","20","0.4807292303927567","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","665668411","issue_comment","https://github.com/Trusted-AI/AIF360/issues/463#issuecomment-665668411","Are there any updates regarding the images?","14","0.2027168234064786","Documentation","Development"
"https://github.com/fairlearn/fairlearn","669460184","issue_comment","https://github.com/Trusted-AI/AIF360/issues/463#issuecomment-669460184","@hildeweerts Thanks for following up. Our designer is looking into this and we came up with a list of action items that we would like to share with the community first. We will provide an update of the timeline and action items in our upcoming session.","20","0.4318339975960018","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","639189203","issue_comment","https://github.com/Trusted-AI/AIF360/issues/460#issuecomment-639189203","Interesting! Couple comments below:

- `DemographicParity` compares each individual group's selection rate to the overall selection rate, whereas demographic parity difference looks at min vs. max. This isn't document well and actually came up a few times already. Perhaps this isn't intuitive @MiroDudik 

- Ran this myself and got a warning in trial one `The pivot operation produces a pivot value of: 1.6e-06, which is only slightly greater than the specified tolerance 1.0e-09. This may lead to issues regarding the numerical stability of the simplex method. Removing redundant constraints, changing the pivot strategy via Bland's rule or increasing the tolerance may help reduce the issue.`

- Ran this with a few additional trials and the numbers are always around the same as yours, so it's not just that you had bad luck. For completeness:

```
>>> printout(trial_one)
TRAINING
   accuracy: 0.980 to 0.931
  selection: 0.230 to 0.275
   for male: 0.289 to 0.282
 for female: 0.110 to 0.261

TEST
   accuracy: 0.813 to 0.758
  selection: 0.233 to 0.293
   for male: 0.288 to 0.321
 for female: 0.119 to 0.235

>>> printout(trial_two)
TRAINING
   accuracy: 0.980 to 0.931
  selection: 0.224 to 0.269
   for male: 0.283 to 0.275
 for female: 0.104 to 0.256

TEST
   accuracy: 0.813 to 0.768
  selection: 0.241 to 0.286
   for male: 0.300 to 0.313
 for female: 0.122 to 0.230

>>> printout(trial_three)
TRAINING
   accuracy: 0.980 to 0.928
  selection: 0.227 to 0.273
   for male: 0.286 to 0.277
 for female: 0.107 to 0.264

TEST
   accuracy: 0.818 to 0.768
  selection: 0.232 to 0.283
   for male: 0.284 to 0.308
 for female: 0.124 to 0.232

>>> printout(trial_four)
TRAINING
   accuracy: 0.979 to 0.929
  selection: 0.224 to 0.269
   for male: 0.283 to 0.274
 for female: 0.105 to 0.258

TEST
   accuracy: 0.813 to 0.772
  selection: 0.243 to 0.285
   for male: 0.299 to 0.313
 for female: 0.127 to 0.227
```

- You ran `predict` which is randomized, so to validate that it's not just bad luck that way I ran it with `_pmf_predict` instead. The metrics had to be adjusted a little bit for that, too, since you now get a probability instead of 0/1. Results below for seeds 0 to 4:

```
# seed 0
TRAINING
   accuracy: 0.980 to 0.928
  selection: 0.226 to 0.272
   for male: 0.287 to 0.277
 for female: 0.103 to 0.262

TEST
   accuracy: 0.819 to 0.768
  selection: 0.234 to 0.290
   for male: 0.287 to 0.318
 for female: 0.128 to 0.233

# seed 1
TRAINING
   accuracy: 0.979 to 0.928
  selection: 0.231 to 0.277
   for male: 0.291 to 0.282
 for female: 0.111 to 0.267

TEST
   accuracy: 0.824 to 0.776
  selection: 0.227 to 0.277
   for male: 0.285 to 0.299
 for female: 0.113 to 0.235

# seed 2
TRAINING
   accuracy: 0.980 to 0.930
  selection: 0.223 to 0.270
   for male: 0.282 to 0.275
 for female: 0.105 to 0.260

TEST
   accuracy: 0.809 to 0.760
  selection: 0.238 to 0.291
   for male: 0.297 to 0.318
 for female: 0.118 to 0.237

# seed 3
TRAINING
   accuracy: 0.980 to 0.930
  selection: 0.225 to 0.269
   for male: 0.284 to 0.274
 for female: 0.107 to 0.259

TEST
   accuracy: 0.821 to 0.776
  selection: 0.239 to 0.287
   for male: 0.296 to 0.315
 for female: 0.122 to 0.231

# seed 4
TRAINING
   accuracy: 0.981 to 0.931
  selection: 0.227 to 0.271
   for male: 0.284 to 0.276
 for female: 0.110 to 0.261

TEST
   accuracy: 0.809 to 0.759
  selection: 0.242 to 0.288
   for male: 0.299 to 0.318
 for female: 0.127 to 0.227
```

Note: I had to modify the used metrics. Instead of `accuracy_score` I used `1 - mean_absolute_error` and `selection_rate` had to be modified to work with the floating point numbers as well (in a hacky way for this example as `sum(np.squeeze(np.asarray(y_pred))) / len(y_pred)`.

The results seem to rather reinforce your point, @kevinrobinson , so I'll leave this as is and will let @MiroDudik take a look.","10","0.7068362390063586","Model development","Development"
"https://github.com/fairlearn/fairlearn","641353840","issue_comment","https://github.com/Trusted-AI/AIF360/issues/460#issuecomment-641353840","@romanlutz thanks!  re: DemographicParity, yeah I moved your comment over to https://github.com/fairlearn/fairlearn/issues/444#issuecomment-641349702.

re: the error message, yes I saw that as well on the first run (see https://github.com/scipy/scipy/issues/11940) but ignored it since it's from a third party library.  I've been asking all these questions about one single narrowly technical use case (ie, the quick start), and think that seems good to start.

@MiroDudik's discussion on exponentiated gradient in https://github.com/fairlearn/fairlearn/issues/439 is helpful for putting these together, but it also makes me wonder if this work is all too early, and if any questions about generalization are still open research questions at this point.","20","0.398100553189406","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","641497299","issue_comment","https://github.com/Trusted-AI/AIF360/issues/460#issuecomment-641497299","Have you tried using different estimators? Perhaps the Decision Tree is just overfitting and the mitigation technique has nothing to do with it.","28","0.4496578690127077","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","641512059","issue_comment","https://github.com/Trusted-AI/AIF360/issues/460#issuecomment-641512059","@romanlutz I took the test case verbatim from the quick start, and assumed that the quickstart scenario was making reasonable design choices and illustrating a situation where using fairlearn made sense.  If not then that's probably worth addressing first.","15","0.2730405980036703","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","644245257","issue_comment","https://github.com/Trusted-AI/AIF360/issues/460#issuecomment-644245257","It's definitely overfitting--you see that even on the level of non-constrained predictor. When I added reporting of confidence intervals on test data I got this for  `trial_one`:
```
TRAINING
   accuracy: 0.980 to 0.931
  selection: 0.230 to 0.275
   for male: 0.289 to 0.282
 for female: 0.110 to 0.261

TEST
   accuracy: 0.813±0.004 [n = 9769] to 0.759±0.004 [n = 9769]
  selection: 0.233±0.004 [n = 9769] to 0.292±0.005 [n = 9769]
   for male: 0.288±0.006 [n = 6550] to 0.319±0.006 [n = 6550]
 for female: 0.119±0.006 [n = 3219] to 0.236±0.007 [n = 3219]
```
After I modified the learner to:
```
classifier = DecisionTreeClassifier(min_samples_leaf=10, max_depth=4)
```
I got the following results:
```
>>> printout(modified_trial_one)
TRAINING
   accuracy: 0.843 to 0.826
  selection: 0.163 to 0.158
   for male: 0.214 to 0.163
 for female: 0.061 to 0.148

TEST
   accuracy: 0.847±0.004 [n = 9769] to 0.828±0.004 [n = 9769]
  selection: 0.158±0.004 [n = 9769] to 0.155±0.004 [n = 9769]
   for male: 0.208±0.005 [n = 6550] to 0.159±0.005 [n = 6550]
 for female: 0.055±0.004 [n = 3219] to 0.148±0.006 [n = 3219]

>>> printout(modified_trial_two)
TRAINING
   accuracy: 0.845 to 0.824
  selection: 0.160 to 0.166
   for male: 0.210 to 0.171
 for female: 0.057 to 0.156

TEST
   accuracy: 0.843±0.004 [n = 9769] to 0.824±0.004 [n = 9769]
  selection: 0.166±0.004 [n = 9769] to 0.172±0.004 [n = 9769]
   for male: 0.216±0.005 [n = 6551] to 0.177±0.005 [n = 6551]
 for female: 0.065±0.004 [n = 3218] to 0.163±0.007 [n = 3218]
```
So, we should definitely modify our quick start example! Thanks for catching this.

Overall, we should spell out in the documentation that just like regular classifiers can overfit, so can the mitigated classifiers and that hyperparameters should be tuned to prevent overfitting. As a strategy, we should suggest to tune hyperparameters on the ""regular"" classifier and then apply the same hyperparameters when running `ExponentiatedGradient`.","10","0.7942766075388025","Model development","Development"
"https://github.com/fairlearn/fairlearn","645045607","issue_comment","https://github.com/Trusted-AI/AIF360/issues/460#issuecomment-645045607","This is awesomely helpful, thanks!  I'll leave aside improvements to the quickstart, and try to focus on the conceptual questions so I can understand enough to roll this into a user guide.

First, I get that if the mitigation algorithm is given an overfit classifier, its predictions will be overfit and them mitigation algorithm will violate the bounds of the constraints.  This fundamentally happens because of the difference between the training and test sets.  But this surprised me, I think because I've made inaccurate assumptions about the ""constraints"" and ""guarantees"" that are provided.  My understanding is that if the training and test set are not drawn from the same distribution, there are no theoretical bounds on any of `fairlearn`'s constraints.  Perhaps this is obvious to everyone else, but this seems like it has practical implications that would be worth teaching users about :)

Second, I was wondering what an experiment might look like to probe at this.  I'll share this as a thought experiment first.  Imagine:
1. we made binary classifier that was perfect 95% of the time and random 5% of the time
2. we had a training dataset and a test dataset drawn from the same distribution
3. we ran a `ExponentiatedGradient` for `DemographicParity` on the messed up binary classifier from step 1
4. we looked at the predictions of the mitigated classifier on the test dataset

Would we be able to characterize the bounds of how much the `DemographicParity` constraint is violated on the test dataset, across all trials?  In other words, even if a toy scenario where we can control the error in the classifier, and the distribution drift between training and test, could we characterize the error bounds of `ExponentiatedGradient`?  Perhaps it's better to just make this example, and happy to do that if it'd be a more helpful next step!  I also recognize that if I more deeply understood the implementation code I would be able to explain this better, so if you all think it's helpful for me to go off and understand some particular piece of that, that could be a helpful next step too. 👍 ","10","0.4255461222932369","Model development","Development"
"https://github.com/fairlearn/fairlearn","645054322","issue_comment","https://github.com/Trusted-AI/AIF360/issues/460#issuecomment-645054322","@kevinrobinson I just submitted a PR to update the quickstart to avoid overfitting as described by Miro. That doesn't solve everything, obviously.

The guarantees only apply to the training set. The constraints will not hold on the test set, so a small difference is always expected. Furthermore, this is a randomized predictor as explained above. For more on that I recommend taking a look at the API reference https://fairlearn.github.io/api_reference/fairlearn.reductions.html#fairlearn.reductions.ExponentiatedGradient.predict as well as the paper's section on randomization (where they introduce `Q`).

We absolutely need to address that in the documentation. As you noticed there are currently two efforts by Richard and me to write documentation around basics of reductions approaches, and we should build on those PRs with this kind of info.

Regarding your thought experiment: Are you assuming that the ""random 5%"" are randomly picked once and from there on out consistent? I'm pretty sure Exponentiated Gradient assumes that the underlying estimators are deterministic.","20","0.2706635711142471","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","647808933","issue_comment","https://github.com/Trusted-AI/AIF360/issues/460#issuecomment-647808933","The root of this issue still isn't addressed despite the correction in the Quickstart, of course!

One way to address it would be to have a notebook that illustrates generalization (or lack thereof) by taking something like this example (below is just a suggestion of what could be in that, not strictly the agenda):

1. overfit an estimator and see how Fairlearn's mitigation will overfit
2. add regularization (or other means) to avoid overfitting, and observe that Fairlearn's mitigation won't overfit either
3. discuss when fairness constraints are expected to hold, i.e., on the training set and if the test set follows the same distribution we expect to be reasonably close as well. This will probably also require mentioning randomized classifiers since that can affect results to some extent.

If anyone wants to pick this up just add a message below!","10","0.2963926885364969","Model development","Development"
"https://github.com/fairlearn/fairlearn","638966940","issue_comment","https://github.com/Trusted-AI/AIF360/issues/459#issuecomment-638966940","hi @marcscho!  check out this similar issue, which has some suggestions: https://github.com/fairlearn/fairlearn/issues/448","23","0.2628611698379139","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","638983313","issue_comment","https://github.com/Trusted-AI/AIF360/issues/459#issuecomment-638983313","Thanks @kevinrobinson for linking! Closing this as duplicate","5","0.2299367299367298","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","638888980","issue_comment","https://github.com/Trusted-AI/AIF360/issues/458#issuecomment-638888980","Thank you for the feedback. The matter of `pos_label` has certainly come up recently... I'm not sure but I think the reworked metrics API will actually pass that through to the underlying method. Having said that, the fact that you're asking about it means that (even if it would be passed through) it's not documented properly.

The point about a privileged group is an interesting one. Our metrics are always based on max and min, rather than a particular group. But different groups could get the max and min by different metrics. We probably need to support both - i.e. if `privileged_group=None` then do what we do now, and base on max and min. Otherwise, compare to the specified group (and concoct some way of handling the case where the privileged group might not be the max or min - easy for differences, ratios are not so obvious).","3","0.4295278365045807","Bias detection metrics validation","Validation"
"https://github.com/fairlearn/fairlearn","638943056","issue_comment","https://github.com/Trusted-AI/AIF360/issues/458#issuecomment-638943056","On a related note, disparities between more than two sensitive groups are very difficult to capture in a single number. To be honest, I cannot really think of a practical scenario in which the min/max difference would be sufficient for me to judge the fairness of a model, let alone compare several models. You would always like to know between *which* groups the disparity is large, what the performance is like for the other groups that are not min/max, etc. ","3","0.4902883460152185","Bias detection metrics validation","Validation"
"https://github.com/fairlearn/fairlearn","638992673","issue_comment","https://github.com/Trusted-AI/AIF360/issues/458#issuecomment-638992673","Thanks for raising the issue @hildeweerts !

There's indeed a lot to be improved in terms of the documentation. Ideally our documentation (API reference) should be clear enough that people understand what they're actually calling. Providing a string ""explanation"" is a pattern I haven't seen before, but if it's something people find useful we should consider it. As a user I usually don't use things unless I have an idea of what it does, which is why from my perspective the documentation improvements sound like the obvious place to fix this.

I agree that it gets difficult with the metrics when comparing more than two groups. That's pretty much the value proposition of the dashboard single model view. Beyond that, I couldn't agree more that a single metric (e.g. min/max difference) isn't sufficient. I hope the documentation doesn't say that anywhere. We should always take a look at the entire picture, but that's hard with just numbers and that's where the visualizations come in.

The group summaries do provide you with everything you need to make any sort of comparisons, though, in case you really want to do this without visualizations
```python
>>> from fairlearn.metrics import accuracy_score_group_summary
>>> y_true = [1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0]
>>> y_pred = [1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0]
>>> sensitive_features = [""a"", ""b"", ""b"", ""a"", ""a"", ""a"", ""b"", ""b"", ""a"", ""b"", ""b"", ""b"", ""a"", ""b""]
>>> accuracy_score_group_summary(y_true, y_pred, sensitive_features=sensitive_features)
{'overall': 0.7142857142857143, 'by_group': {'a': 0.8333333333333334, 'b': 0.625}}
```

Even if there are more groups you could always figure out specific differences between certain groups that are relevant to your context.

In terms of follow-ups I think it makes sense to iterate on the documentation and user guide. There's currently a PR out #451 that adds content to the user guide for metrics. That may help as a first step, but I don't see this as the final piece. @hildeweerts thanks again for raising this. It's really helpful to hear what is hard to understand, what you would have expected to see, etc. 

If you have a concrete suggestion please also feel free to submit a PR. We're happy to take any contributions!","25","0.280707871349651","Research","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","639016481","issue_comment","https://github.com/Trusted-AI/AIF360/issues/458#issuecomment-639016481","As I think a bit more, we would probably want to have users select the group via a `selected_group` argument, rather than `privileged_group` since they may be wanting to view comparisons with a group which is getting particular bad results, rather than the best ones.

Other than that, I think that for the metrics code, our first order of business should be to:

- Ensure that `pos_value` is supported by all the classification metrics
- Add the `selected_group` argument for all metrics, and decide on a consistent way of handling differences and ratios in that case

Would that cover your main concerns there @hildeweerts ? I very much agree that we need to have the documentation make it plain that 'fairness isn't just a single number.' I'm a little more leery of trying to generate explanatory text from these. I can see that there is value in that, but it's also a lot of effort, given all the potential little edge cases. We can talk to the guys in our (currently virtual) office who work on interpretability in general, though.","3","0.353622785903845","Bias detection metrics validation","Validation"
"https://github.com/fairlearn/fairlearn","639401168","issue_comment","https://github.com/Trusted-AI/AIF360/issues/458#issuecomment-639401168","> Ideally our documentation (API reference) should be clear enough that people understand what they're actually calling. Providing a string ""explanation"" is a pattern I haven't seen before, but if it's something people find useful we should consider it. As a user I usually don't use things unless I have an idea of what it does, which is why from my perspective the documentation improvements sound like the obvious place to fix this.

Yes, improving documentation would help. Nevertheless, even if you do know what you're calling it is very easy to make mistakes, especially in an exploratory phase, e.g. because you accidentally switched labels or something like that. I don't think textual explanations are necessarily the way to go, as @riedgar-ms mentioned there are many potential edge cases, but I do think it is something we need to think about.

> I agree that it gets difficult with the metrics when comparing more than two groups. That's pretty much the value proposition of the dashboard single model view. 

Using the dashboard is a nice way to circumvent these issues, but there are also many use cases in which the dashboard may not be sufficient or desirable. I wonder whether returning a single number should be the default behavior at all. I can also imagine returning the summary as default behavior while adding specific function arguments for aggregations. At least in this scenario, users would be forced to consciously think about what they are optimizing for.

> Ensure that pos_value is supported by all the classification metrics

Yes! It might already be supported for most if not all - so this would probably mostly be a documentation issue.","15","0.5453093066923951","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","639609533","issue_comment","https://github.com/Trusted-AI/AIF360/issues/458#issuecomment-639609533","These are my suggestions for next steps:
* improve documentation on when to use which metrics; we in fact have a more basic concern, which is to even expose the metrics that are already available (because they are hidden inside the metrics dictionaries right now)
* add `pos_label` to our own base metrics and make sure that it works with the metrics that we derive from `sklearn.metrics`
* in documentation, we should point out strongly that the aggregate metrics are just the initial diagnostic step and if you see that their value is large, you'll want to dig into the distribution of the base metric across the groups and so you should use `<metric>_group_summary`.
  * This was the rationale behind defining the worst-case versions of difference & ratio--they are meant to quantify the overall disparity rather than a disadvantage of a specific group. Thinking about disadvantage requires more nuance, so I'd much rather show the numbers across all groups and let that serve as a starting point for further exploration. 

I'm right now a bit less sure about the benefit of creating an additional functionality around `selected_group` or `privileged_group` as opposed to just using `<metric>_group_summary`. But this might be because I'm not quite sure about the specific metrics that you had in mind and the specific use case scenarios. What are those? (We could also cover that in a call if that's easier.)","15","0.5661091265204374","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","640752310","issue_comment","https://github.com/Trusted-AI/AIF360/issues/458#issuecomment-640752310","For `pos_label`, I've just been experimenting, and it doesn't seem to work. I'll see if I can figure out how to fix that.","32","0.4098997088320931","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","657116533","issue_comment","https://github.com/Trusted-AI/AIF360/issues/458#issuecomment-657116533","Related conversation going on at https://github.com/fairlearn/fairlearn-proposals/issues/12","25","0.6745983323164532","Research","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","808107702","issue_comment","https://github.com/Trusted-AI/AIF360/issues/458#issuecomment-808107702","Close in favor of smaller issue #721. 

@MiroDudik I'm assuming your concerns re. `<metric>_group_summary` are not really relevant anymore with the new metrics API. If this is not the case please open a new issue to address it.","15","0.5037747719408618","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","638884001","issue_comment","https://github.com/Trusted-AI/AIF360/issues/457#issuecomment-638884001","Tagging @MiroDudik who added that part of the metrics API.","15","0.7157869012707726","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","638997065","issue_comment","https://github.com/Trusted-AI/AIF360/issues/457#issuecomment-638997065","Linked @MiroDudik 's proposal https://github.com/fairlearn/fairlearn-proposals/pull/9/files which sheds light on these things.

Although after re-reading this it seems like there's two ways to get at this:
```
demographic_parity_difference(...)
difference_from_summary(selection_rate_group_summary(....))
```
and there's various things one can extract from the summary (difference, ratio, min, max).

Super useful to hear about your expectations in terms of API. Let's wait for @MiroDudik as well in case I missed something.","3","0.3693627048057428","Bias detection metrics validation","Validation"
"https://github.com/fairlearn/fairlearn","639426361","issue_comment","https://github.com/Trusted-AI/AIF360/issues/457#issuecomment-639426361","I did not see this pull request yet - thanks for linking! I don't think it contains arguments on *why* this particular API design is proposed, does it? I might have missed that though. Perhaps we can discuss API design during the developer call this Thursday? It might be a better medium for this type of discussion.","15","0.4400889630247426","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","639623501","issue_comment","https://github.com/Trusted-AI/AIF360/issues/457#issuecomment-639623501","Agreed let's chat on the call. I see an agenda emerging :-)...

But at least some quick arguments that we went through. Parity is a constraint (or a set of constraints), so the metrics should not be called ""parity"". One idea would be to call the metric ""disparity"", so we'd have `demographic_disparity(..., metric='difference')` or something like that. This might be okay, but it becomes verbose when generalizing to things like true positive rate, when we'd end up with `true_positive_rate_disparity(..., metric='ratio')` and we'd probably have a default `metric='difference'`. So we opted for `true_positive_rate_difference` and `true_positive_rate_ratio` instead--it makes clearer what you're measuring (rather than rely on the default) and is less verbose.","3","0.6313801605203797","Bias detection metrics validation","Validation"
"https://github.com/fairlearn/fairlearn","657116582","issue_comment","https://github.com/Trusted-AI/AIF360/issues/457#issuecomment-657116582","Related conversation going on at https://github.com/fairlearn/fairlearn-proposals/issues/12","25","0.6745983323164532","Research","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","769081502","issue_comment","https://github.com/Trusted-AI/AIF360/issues/457#issuecomment-769081502","@hildeweerts do we need this any more? I think that the `MetricFrame` addresses this?","15","0.2808623144193614","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","769232508","issue_comment","https://github.com/Trusted-AI/AIF360/issues/457#issuecomment-769232508","Yes! We can close this issue","24","0.1949616648411828","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","638883461","issue_comment","https://github.com/Trusted-AI/AIF360/issues/456#issuecomment-638883461","That would certainly be useful. @adrinjalali , can you comment from the scikit-learn perspective?","20","0.3911088911088911","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","638896169","issue_comment","https://github.com/Trusted-AI/AIF360/issues/456#issuecomment-638896169","Yep, we use `linkcode`, and the configuration is pretty easy:

https://github.com/scikit-learn/scikit-learn/blob/master/doc/conf.py
https://github.com/scikit-learn/scikit-learn/blob/master/doc/sphinxext/github_link.py
","30","0.7638486778614165","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","638882833","issue_comment","https://github.com/Trusted-AI/AIF360/issues/455#issuecomment-638882833","Thanks for the feedback. I'm definitely not averse to rethinking the file structure. And `_group_metric_set.py` certainly needs a proper home - right now, it exists solely for the AzureML integration work I've been doing, although it could also be used in Fairlearn for a 'save this dashboard' button.","6","0.3001208540351818","API expansion","Development"
"https://github.com/fairlearn/fairlearn","638888951","issue_comment","https://github.com/Trusted-AI/AIF360/issues/455#issuecomment-638888951","For _documentation_ in both API and User Guide, I think we should start with the [Metrics API proposal](https://github.com/fairlearn/fairlearn-proposals/blob/master/api/METRICS.md)
 (which has all the concepts at one place) and add full mathematical definitions etc. to it.

Re. structuring of the modules, I totally agree that we should combine all the little files that you mention! I was thinking using the name `_base_metrics` instead of `_performance` (since they all can be either viewed as performance metrics or used to derive various fairness metrics). But I don't have a strong opinion on this.

Re. content of the package: also please take a look at the metrics that exist but are not currently surfaced in the API documentation: https://github.com/fairlearn/fairlearn/blob/master/test/unit/metrics/test_metrics_engine_dicts.py
The biggest immediate issue is to document these properly in the API docs / user guide. I'd love to be able to generate a ""table"" of metrics for the documentation automatically from these  dictionaries...","14","0.3605979364692503","Documentation","Development"
"https://github.com/fairlearn/fairlearn","638892964","issue_comment","https://github.com/Trusted-AI/AIF360/issues/455#issuecomment-638892964","> PS. I suppose giving each function their own page as in the scikit-learn API reference is already implicit in the documentation proposal, right?

I think that's more of a sphinx generated thing and how they're generated.

I like merging those files, @romanlutz may need to get used to it :P 

> I'd love to be able to generate a ""table"" of metrics for the documentation automatically from these dictionaries...

You could probably do that through extending or customizing sphinx, but I'd recommend having the table in a user guide nicely put in context rather than an automated table.","14","0.4344416491013351","Documentation","Development"
"https://github.com/fairlearn/fairlearn","638916785","issue_comment","https://github.com/Trusted-AI/AIF360/issues/455#issuecomment-638916785","> I think that's more of a sphinx generated thing and how they're generated.

Yes, definitely! This was more of a quick question so I did not have to open yet another issue :P

> Re. structuring of the modules, I totally agree that we should combine all the little files that you mention! I was thinking using the name _base_metrics instead of _performance (since they all can be either viewed as performance metrics or used to derive various fairness metrics). But I don't have a strong opinion on this.

Some metrics, e.g. selection_rate, are not typical evaluation metrics anyways, so _base_metrics.py works even better I think!
","15","0.2446875392933485","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","639021524","issue_comment","https://github.com/Trusted-AI/AIF360/issues/455#issuecomment-639021524","I agree that a handwritten table is likely to look much nicer, but.... if we went that route, we should give some thought to how to make sure that the table evolves with the auto-generated function list.","30","0.5057416267942583","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","639141758","issue_comment","https://github.com/Trusted-AI/AIF360/issues/455#issuecomment-639141758","@adrinjalali I just want to make sure I understand: You mean having single files in the documentation is better than more nesting? I'm totally on board with that :-) Perhaps you've noticed already, but I've moved away from very fine granular documents since that means more clicking.

How about I take some time to get the table into the user guide as a PR and then we see what changes people want? I'm open to pretty much anything. The metrics were a major point of confusion for me at some point, so I'd rather avoid new users having the same experience, and that will only be possible with better documentation. Keep the feedback coming! :-)","20","0.6889475669558661","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","639313897","issue_comment","https://github.com/Trusted-AI/AIF360/issues/455#issuecomment-639313897","> we should give some thought to how to make sure that the table evolves with the auto-generated function list.

Adding a new method/function is not something which happens too frequently in a library, and when it does, we should make sure we put the effort to properly document it, and updating the relevant user guide is a part of that documentation process, so I guess it should come naturally.

> I just want to make sure I understand: You mean having single files in the documentation is better than more nesting?

I was talking about the `.py` files and not the documentation. The documentation is okay as far as I'm concerned, and the layout can always change for the better.","25","0.2842449390167233","Research","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","807970242","issue_comment","https://github.com/Trusted-AI/AIF360/issues/455#issuecomment-807970242","Looking at the user guide we actually do have a table with the performance metrics now: https://fairlearn.org/v0.6.0/user_guide/assessment.html#metrics
![image](https://user-images.githubusercontent.com/10245648/112590394-0611d000-8dc0-11eb-8b4c-06d119d6a3e4.png)

For this particular issue it's probably best if we focus on the organization of the metrics module (code) rather than conflating it with the documentation. #458 touches on documentation a little bit as well, but we may want to start a completely fresh issue with very specific things that should be addressed (and small in scope).

Other than that, it seems like there's agreement on merging files, so this strikes me as an issue that can move forward if there's anyone willing to take up the task. 
","25","0.2918749470473608","Research","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","637651835","issue_comment","https://github.com/Trusted-AI/AIF360/issues/448#issuecomment-637651835","It sounds like you are running this from source on a recent commit?  The issue may be related to  https://github.com/fairlearn/fairlearn/pull/386/files#diff-9a9553e456a6225df4024e931e062bc6R181, where there are changes to this part of the code since the 0.4.6 release.  I'm figuring the example notebook you're trying might not have been updated as part of those code changes.

","32","0.6403290719696969","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","637667403","issue_comment","https://github.com/Trusted-AI/AIF360/issues/448#issuecomment-637667403","I think that the issue is actually the opposite: `predictors_` were added _after_ 0.4.6 and they will be part of a new release, so the notebooks on the `master` branch in the repository have already been updated. I am guessing that you are running a pip-installed Fairlearn package, which does not have `predictors_`.

You have two options
* run an older version of the notebook with the pip-installed Fairlearn. Call `pip freeze` to check your installed version, and then find the notebook on, e.g., https://github.com/fairlearn/fairlearn/tree/v0.4.6
* run Fairlearn as well as notebooks from a cloned repository: https://fairlearn.github.io/contributor_guide/development_process.html#advanced-installation-instructions

Let me know if these solve your issue!","21","0.6054241934694276","Installation and shell commands","Deployment"
"https://github.com/fairlearn/fairlearn","637889574","issue_comment","https://github.com/Trusted-AI/AIF360/issues/448#issuecomment-637889574","Thank you for all help!!! 
It is due to mismatch version. 
I have decided to move to v0.4.6.","24","0.3310782241014799","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","638153462","issue_comment","https://github.com/Trusted-AI/AIF360/issues/448#issuecomment-638153462","Good to hear you're unblocked @jingwora . We are working to improve our backwards compatibility story, since we know we've broken a lot of things recently.","32","0.3467092803030304","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","636371673","issue_comment","https://github.com/Trusted-AI/AIF360/issues/444#issuecomment-636371673","Ah I also found [this description](https://github.com/fairlearn/fairlearn/blob/master/fairlearn/reductions/_exponentiated_gradient/exponentiated_gradient.py#L34) afterward:

>  eps: Allowed fairness constraint violation; the solution is guaranteed to have the error within 2*best_gap of the best error under constraint eps; the constraint violation is at most 2*(eps+best_gap)

This seems important :)  

In reading the code, this doesn't seem to match what I see in the [line](https://github.com/fairlearn/fairlearn/blob/master/fairlearn/reductions/_exponentiated_gradient/exponentiated_gradient.py#L153):

>   `  if (gaps[t] < self.nu) and (t >= _MIN_ITER)` 

This matches my understanding of what I've seen in the paper - that there is no ""best gap"" in this method since we're stopping early as soon as the solution is tolerable in terms of nu.

So I'm still trying to understand these params and how to control them at the abstraction level of ""enforce demographic parity so that when we run `metrics.demographic_parity_difference` afterward, it will be below 0.05.""  (I'm also aware that these questions are only focused on understanding how the constraints are enforced on the training dataset, and we're not even talking about generalization yet.)","10","0.5742079149900376","Model development","Development"
"https://github.com/fairlearn/fairlearn","637575875","issue_comment","https://github.com/Trusted-AI/AIF360/issues/444#issuecomment-637575875","From reading the Moments code I thought I had figured out that from an API perspective that `DemographicParity(difference_bound=0.05)` was the way to ask for something like this, but I was surprised to discover that this didn't work.  I see now that the API in the most recently published 0.4.6 has changed on master.  This looks like it changed a few days ago in https://github.com/fairlearn/fairlearn/pull/424/files#diff-97014611344adf9185e7249ff4c43f4dR41, perhaps in relation to larger discussion in https://github.com/fairlearn/fairlearn-proposals/pull/9/files#diff-44059dad885525fd6f291b8ee1774e15R235.

So I think at this point I'll leave this be :)  Thanks!","32","0.2615411255411256","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","639190869","issue_comment","https://github.com/Trusted-AI/AIF360/issues/444#issuecomment-639190869","I *think* this is because `DemographicParity` looks at difference in the selection rate between each group and the overall selection rate, so that can be below your threshold, say `0.05`, even if the difference between the min and max are more. Example: 50% of samples in group A, 50% of samples in group B

threshold is 0.05
selection rate of A is 0.75
selection rate of B is 0.81
overall selection rate is 0.78
demographic parity difference is 0.06 (max-min)
difference between each group and the overall selection rate is 0.03, i.e., it satisfied the constraint.

@MiroDudik please correct me if that doesn't make sense.","3","0.7193763359738468","Bias detection metrics validation","Validation"
"https://github.com/fairlearn/fairlearn","641349702","issue_comment","https://github.com/Trusted-AI/AIF360/issues/444#issuecomment-641349702","From @romanlutz over in https://github.com/fairlearn/fairlearn/issues/460:

> `DemographicParity` compares each individual group's selection rate to the overall selection rate, whereas demographic parity difference looks at min vs. max. This isn't document well and actually came up a few times already. Perhaps this isn't intuitive @MiroDudik
","3","0.7171567969204599","Bias detection metrics validation","Validation"
"https://github.com/fairlearn/fairlearn","636158991","issue_comment","https://github.com/Trusted-AI/AIF360/issues/439#issuecomment-636158991","@MiroDudik  can certainly provide more details, but a few things ahead:

- Let's call the repo/project Fairlearn, not the algorithm. Fairlearn contains other algorithms, not just the one(s) from the original paper.
- The algorithm they refer to from the Agarwal et al. paper is called `ExponentiatedGradient` in this repo, and the ""deterministic"" one is called `GridSearch`.

So picking `n` is something you do for `GridSearch`, but not for `ExponentiatedGradient`.

The dashboard is not tied to any mitigation techniques. You could use it in a standalone fashion, just analyzing metrics of a model. If you provide data from multiple models it will provide you a comparison view (but you can still drill down into the single model view). None of this requires any sort of mitigation, but some of our examples try to showcase how to use grid search output with the dashboard.

Taking a step back now: there are advantages and disadvantages to every mitigation technique. Fairlearn doesn't necessarily need to push people towards one approach over the other. It would be great if we can provide all tools that people care about, and along with that the educational material to help them decide what makes sense in their particular situation. As mentioned above there are plenty of situations where mitigation may not be required at all. So purely from a project scope standpoint I would say that it would be great to have this as part of the toolbox.

... but when I last tried using it it was rather cumbersome (as you'd expect from research code), and it took months to hear back on the technical difficulties. That said, if there's interest from users I don't see why we couldn't put in the effort to integrate it into Fairlearn. Adding tensorflow as a requirement isn't lightweight at all, but there are certain workarounds like adding it as an extension (similar to what we do with matplotlib right now).
To this day I haven't heard anyone complaining about the lack of mitigation techniques, though, so that's all speculation at this point.","28","0.3427434290893328","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","636181099","issue_comment","https://github.com/Trusted-AI/AIF360/issues/439#issuecomment-636181099","@romanlutz Thanks, this is super helpful! 👍   As you said:

> there are advantages and disadvantages to every mitigation technique... it would be great if we can provide... the educational material to help them decide what makes sense in their particular situation

...this is really what I'm trying to understand better, so thanks!  I'm happy to help package this discussion up into docs if that's helpful after. :)

So first, to check my understanding, `ExponentiatedGradient` code is:

> We proceed iteratively by running a no-regret algorithm for the λ-player, while executing the best response of the Q-player. Following Freund & Schapire (1996), the average play of both players converges to the saddle point. We run the exponentiated gradient algorithm (Kivinen & Warmuth,1997) for the λ-player and terminate as soon as the sub optimality of the average play falls below the pre-specified accuracy ν.

I think I see this all in https://github.com/fairlearn/fairlearn/blob/master/fairlearn/reductions/_exponentiated_gradient/exponentiated_gradient.py#L153.  (EDIT: I moved the questions about understanding the signficance of the `ν` param into https://github.com/fairlearn/fairlearn/issues/444, which looks at this in the specific scenario in the quickstart).

Second, I'm trying to understand when I'd want to use this approach versus the grid search.  For example, in the [credit card example](https://github.com/fairlearn/fairlearn/blob/master/notebooks/Grid%20Search%20for%20Binary%20Classification.ipynb), why does it make more sense to use the `GridSearch` versus just using the `ExponentiatedGradient` method?

It's probably good to cut back down the scope to focusing on the different algorithmic approaches and contexts where they'd make more or less sense (vs other ecosystem/community questions), since I've asked too many questions here. :)","25","0.3510500867382719","Research","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","636548963","issue_comment","https://github.com/Trusted-AI/AIF360/issues/439#issuecomment-636548963","It's absolutely helpful to document this. Feel free to submit PRs as you see fit! :-)

Not sure if I see a question before ""Second"", but yes, the exponentiated_gradient.py file has the logic from the paper. You may also have noticed some logic related to a linear program (in the code `LP` or `linprog` depending on where you look) which is not mentioned in the paper. It's an optimization @MiroDudik added.

`GridSearch` vs `ExponentiatedGradient` is an interesting comparison! One could think of `GridSearch` as the naive or brute force version of `ExponentiatedGradient`. That has certain limits, though. If you have a binary sensitive feature, you effectively end up with a 1-dimensional grid (if you want to call that a grid at all...). Searching that is not too hard. Once your sensitive features get more values or you consider intersectionality the grid dimensionality explodes pretty quickly. [Sidenote: as you're aware intersectionality should certainly be on people's radar]
At each grid point we end up having to fit our estimator. Let's say we have a 10-dimensional grid. To cover this adequately we'd need a lot of grid points, or we may end up not finding a very good solution. `ExponentiatedGradient` approaches this different by iteratively and intelligently picking the next solution until we either get below our tolerance or hit the iteration limit. The paper provides certain theoretical guarantees so as long as your iteration limit is high enough you'll find a solution that suits your requirements.

Taking a step back: this and a lot more should be explained in the [user guide](https://fairlearn.github.io/user_guide/mitigation.html#reductions), perhaps with examples to illustrate. It's something we're working towards. As you've noticed the documentation is lacking on various fronts, so I can't quite say when this gets better. If you feel like improving any part don't feel shy to do so. Thanks!  ","28","0.3237532623814212","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","639566295","issue_comment","https://github.com/Trusted-AI/AIF360/issues/439#issuecomment-639566295","I really like [Cotter et al. 2018](http://jmlr.csail.mit.edu/papers/volume20/18-616/18-616.pdf)--I think that their line of work is very closely related to ours and they present some nice improvements.

They present several algorithms:
* The high level idea behind Algorithm 1 is the same as our `fairlearn.reductions.ExponentiatedGradient`--they solve the same basic optimization problem. We have implemented some additional tweaks that improve the performance:
  * We use exponentiated gradient instead of projected (sub)gradient. I did some initial benchmarking when writing our paper, and exponentiated gradient seemed to work better, so that's what we have implemented.
  * We check for the duality gap, which allows us to terminate early, as soon as we solve the optimization problem.
  * We carry out the _shrinkage step_ from their paper in each iteration (we call in `LP_step`). This is computationally very cheap, but practically very important, because it allows us to solve the problem in around 10 iterations rather than something like 100 or more--which is how long it would take without it (or if you only used it for sparsification at the very end as they suggest).

The key assumption that Algorithm 1 and ExponentiatedGradient make is that the underlying supervised learner (aka ""oracle"" aka ""base estimator"") can solve some hard problems. That's a similar leap of faith that data scientists make when they run _any_ classification algorithm (0/1 classification is NP-hard). In ExponentiatedGradient, we take some additional heuristical steps ([here](https://github.com/fairlearn/fairlearn/blob/fee71c194e5fbc08d2d17f431d30788752adba5c/fairlearn/reductions/_exponentiated_gradient/_lagrangian.py#L101) and [here](https://github.com/fairlearn/fairlearn/blob/fee71c194e5fbc08d2d17f431d30788752adba5c/fairlearn/reductions/_exponentiated_gradient/_lagrangian.py#L173)) to ""encourage"" the oracle to solve these hard problems, but sometimes the oracle might fail. In that case, ExponentiatedGradient might not reduce the duality gap as fast as implied by theory. A more typical consequence is that the calculated duality gap is small, but incorrect, and so the optimality guarantee would not hold. I have observed this behavior in ExponentiatedGradient in situations when the fairness constraints are harder to satisfy--e.g., when `eps` is close to zero. This is the failure that Cotter et al. are trying to address.

* Their Algorithms 2, 3 and 4 introduce the idea of proxy-Lagrangian.
  * This can be advantageous if you are wrapping a supervised learner (i.e., ""reducing to an oracle"") that can solve for arbitrary differentiable objectives--for example, if your are trying to fit a neural net in tensorflow. ExponentiatedGradient can still be applied, but it does not exploit the differentiability structure and so the problems that it is creating for the underlying supervised learner might be harder.
  * Algorithms 2-4 do not reduce to supervised learning, but to _optimization_. This means that you cannot for example easily specify them as ""wrappers"" (or meta-estimators in the sklearn terminology). Also, I don't think they would easily work with classifiers that are not based on optimization--like decision trees or tree ensembles.
  * Algorithm 2 still creates NP-hard subproblems (that are passed to the oracle), but because the problems it creates are differentiable, they might be easier than those created by ExponentiatedGradient. The price that it pays is that its optimality guarantees are weaker than those enjoyed by Algorithm 1 and ExponentiatedGradient. Specifically: note that Eq. 16 has _g&#771;<sub>i</sub>_ in the `inf` whereas the matching expression in Theorem 2 has _g<sub>i</sub>_. That said--it's not clear to me that this distinction matters in practice.
  * Algorithm 3 does not create hard problems, but its scope is more limited, because it assumes that the surrogates are convex--in that case Eq. 16 might bite quite unpleasantly. You could possibly run this algorithm without convexity and it might work in practice, but guarantees don't hold.
  * Algorithm 4 is a heuristic without guarantees, but it might work reasonably well in practice.

So--what's the practitioner to do?

* If the underlying supervised oracle (base estimator) is not optimization based--e.g., when it is fitting a decision tree or an ensemble of trees--then it is not really possible to run Algorithms 2-4.

* If you have access to an optimizer--e.g., you are training something that can be written as a (differentiable) neural net--it makes sense to try one of Algorithms 2-4. In particular, Algorithm 4 seems to do well empirically (although it does not come with guarantees). Besides creating differentiable subproblems it has an additional advantage over ExponentiatedGradient: it does not require solving the subproblems to full precision, and so it can be much faster. But that also opens a can of worms--since you need to tune the learning rate, mini-batch size etc.. So I would still suggest running ExponentiatedGradient at least once (using the hyperparameters that work for the unconstrained version of the problem). And only then embark on Algorithm 4.
","6","0.4800694247099411","API expansion","Development"
"https://github.com/fairlearn/fairlearn","639838591","issue_comment","https://github.com/Trusted-AI/AIF360/issues/439#issuecomment-639838591","@MiroDudik This is amazingly helpful, thank you! 😄  I really appreciate how you're referring to specific reference points in the papers, and pointing out where the code in fairlearn has additional improvements.  I also am thankful for the bit translating this into a practitioner perspective too :)

One quick reaction I have is that there is a lot of complexity to understand how the theoretical work in any of these approaches will apply to any specific problem or scenario. :) So while the 2018 paper says things like:

> Our reductions work for any classifier representation, encompass many definitions of fairness, **satisfy provable guarantees**, and work well in practice.

I think I might have interpreted what terms like 'constraints' and 'guarantees' meant a bit too naively or rigidly.  So this is super helpful in realizing I need to understand this more deeply. 👍 

I'll dig into reading through this more to make sure I'm fully understanding, and although it might take some more questions and clarifications, I'd love to help translate this discussion into user-facing documentation or an example notebook illustrating what you're describing in a particular scenario.","25","0.5437680213263696","Research","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","640849764","issue_comment","https://github.com/Trusted-AI/AIF360/issues/435#issuecomment-640849764","I may have missed another discussion, but @romanlutz created the `tempeh` package, which has been a source of cleaned data. That package is still under the ""Microsoft"" org, though.","4","0.5452344265903586","Installation and shell commands","Deployment"
"https://github.com/fairlearn/fairlearn","640863048","issue_comment","https://github.com/Trusted-AI/AIF360/issues/435#issuecomment-640863048","Just chatted with @riedgar-ms about this and we're very much on board with @koaning 's idea! If there are no license/ownership issues I think that's a great idea. We should probably have some descriptions related to it.

I'm not super opinionated about this and like your PR and suggestions to have datasets in Fairlearn. The only concern is around ownership. Would hate to violate terms of the dataset owner. I tried adding another dataset with the same blueprint you provided, @koaning , and that worked really well. I can add all fairness-related ones we have in `tempeh`, and perhaps that makes the entire package somewhat redundant. In that case I'm happy to get rid of it. It was always meant as a supplemental piece, so that's actually a best case scenario.","25","0.3842755114392931","Research","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","646474549","issue_comment","https://github.com/Trusted-AI/AIF360/issues/435#issuecomment-646474549","https://github.com/fairlearn/fairlearn/pull/494","25","0.1949616648411828","Research","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","648412072","issue_comment","https://github.com/Trusted-AI/AIF360/issues/435#issuecomment-648412072","Closing this and opened #507  as a follow-up to add the corresponding documentation/user guide/notebook (whichever is appropriate).","25","0.3905180840664712","Research","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","632923907","issue_comment","https://github.com/Trusted-AI/AIF360/issues/433#issuecomment-632923907","It's because the file was contributed while we were on MSFT payroll--MSFT employees can contribute to OSS projects as long as we put the copyright notice in the source files we create. If you modify the file, you can of course add your copyright notice to the top (but don't need to). Similarly, if MSFT folks modify files initially contributed by others, we're not expected (but can) add the additional copyright notice. I hope this is not an issue--but we should explain this in our contributor guide probably?","20","0.5641896139821454","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","633012483","issue_comment","https://github.com/Trusted-AI/AIF360/issues/433#issuecomment-633012483","Since there's an MIT license I suppose that technically, you can still argue for openness. But it is a turnoff. 

Contributing to a file that starts with `Copyright (c) Microsoft Coporation. All rights reserved.` feels like I am using my spare time to contribute to something that is owned by a big company rather than a open source project that I am building with peers. 

Also, side note, the link to the contributor guide on the readme is broken; https://fairlearn.github.io/contributor_guide.html. ","20","0.6082324956045095","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","633075313","issue_comment","https://github.com/Trusted-AI/AIF360/issues/433#issuecomment-633075313","I agree. There's also the `SECURITY.md` file which I still think should be removed.","20","0.3105228105228105","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","633112377","issue_comment","https://github.com/Trusted-AI/AIF360/issues/433#issuecomment-633112377","I see your points--let me consult with MSFT lawyers to get to the bottom of the rationale and see what can be relaxed. For example, would it help if we just kept the copyright notice in the LICENSE file? On a basic level, I'm not sure whether we should be preventing any contributor (e.g., folks from Google, Facebook, or any other corporations) for stating their copyright as long as they keep the MIT license.

I think that we're running into a misconception--but a turnoff nevertheless. The way I understand this, in case of the MIT license, the copyright notice is like stating the authorship and so when somebody copies substantial portions of the code, they need to give credit to the author (just by stating their names as given in the copyright notice). Other than that--everybody is free to do as they please with the code. When you update the code, you become a co-author and you can add your name in the form of the copyright notice at the top.

Re. `SECURITY.md`, I'll try to get some clarity around that as well. Last we heard is that until we have an independent entity, e.g., ""a Fairlearn foundation"", we should keep `SECURITY.md`. @romanlutz : is that right?","20","0.6885676934150033","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","633131089","issue_comment","https://github.com/Trusted-AI/AIF360/issues/433#issuecomment-633131089","@vingu was involved in those discussions, but I think you’re right. 

I agree with everything written above. It’s altogether not unusual to have companies holding copyrights, e.g. [angular](https://github.com/angular/angular.js/blob/master/LICENSE) or [Bootstrap](https://github.com/twbs/bootstrap/blob/master/LICENSE). That said, I had the same impression as you. One pattern I found that I like is “copyright X and other contributors” , e.g. [JQuery](https://github.com/jquery/jquery/blob/master/LICENSE.txt) or [Vowpal Wabbit](https://github.com/VowpalWabbit/vowpal_wabbit/blob/master/LICENSE). I’ve been trying to get some clarity from folks with more experience in open source licensing to make sure I am allowed to extend copyright to “all contributors” but with MIT license it really shouldn’t matter as Miro wrote. Nevertheless, giving the strange first impression doesn’t do any good either. I’ll push a little harder to get a resolution internally and will update this thread. 

Regarding the lines in every file: you may recall that we’re thinking of the license for documentation (perhaps CC BY) as something different from the license for code (MIT). For clarity it may be beneficial to have it explicitly stated. That’s another point I want to get a definitive answer on, though.

If you have thoughts in the meanwhile please share!","20","0.7876264962296416","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","633853799","issue_comment","https://github.com/Trusted-AI/AIF360/issues/433#issuecomment-633853799","I had the same impression as @koaning and it made me hesitant to contribute to the project. In addition to the turnoff of providing 'free labor' to a big company, for me, it also includes the fear of being overruled during decision-making.","20","0.6205921737836632","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","634131177","issue_comment","https://github.com/Trusted-AI/AIF360/issues/433#issuecomment-634131177","We've been kicking the governance issue down the road until we're bigger, but it's obvious that this needs addressing now... it also came up in the last [developer call](https://github.com/fairlearn/fairlearn/blob/master/docs/community/developer_call_notes/2020_05_07.rst).","13","0.332159366057671","Artifact generation and benchmarking","Deployment"
"https://github.com/fairlearn/fairlearn","634294046","issue_comment","https://github.com/Trusted-AI/AIF360/issues/433#issuecomment-634294046","For the security.md, the reason we put the MS one there is for the benefit of the FairLearn community.  We spoke with our security experts and they recommended that you should have a channel to communicate security vulnerabilities that isn't posting them publicly, and MS agreed to do it for this project in absence of an alternative. We would be very happy to update it to another process if we have proposals.","13","0.3830649350649351","Artifact generation and benchmarking","Deployment"
"https://github.com/fairlearn/fairlearn","634308474","issue_comment","https://github.com/Trusted-AI/AIF360/issues/433#issuecomment-634308474","Re. `SECURITY.md`--so it sounds like this might be solved by setting up a secure e-mail account that forwards to maintainers?","7","0.4376037027333853","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","634326673","issue_comment","https://github.com/Trusted-AI/AIF360/issues/433#issuecomment-634326673","I think generally yes but we should confirm if there's any other functions we'd want to add. I think also they recommended a process to notify people that use FairLearn that might have been exposed as a result of an issue.","13","0.6504784688995217","Artifact generation and benchmarking","Deployment"
"https://github.com/fairlearn/fairlearn","634348088","issue_comment","https://github.com/Trusted-AI/AIF360/issues/433#issuecomment-634348088","We have both an “internal”/“private” contributor mailing list (read: only accessible by subscribed contributors) and a newsletter through python.org (thanks to @adrinjalali who suggested it). Sounds like they fit both these functions:

- reporting to internal list
- announcements to newsletter 

Note: we haven’t used either of them yet, so you haven’t missed out on anything!

@slbird does that make sense? ","20","0.3846844736019995","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","634530074","issue_comment","https://github.com/Trusted-AI/AIF360/issues/433#issuecomment-634530074","> I think also they recommended a process to notify people that use FairLearn that might have been exposed as a result of an issue.

That's generally needed if there's any sort of SLA or a contractual agreement or something between the parties. Software which comes with an MIT license, very clearly states:

> 
>     THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
>     IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
>     FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
>     AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
>     LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
>     OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
>     SOFTWARE

I'm happy to simply have an email address or something dedicated to security issues, but I don't think it's really needed. It can also be simply a line on the README saying, please report security issues to this email address/web page/etc.

With regards to the copyright notice, sure a lot of projects do that, but that's no the only way to do it. It's true that for many larger projects which are mostly owned by a company that's how it's done, but you could also have a ""funding"" place instead and recognize who supports the project like the one [here](https://scikit-learn.org/stable/about.html#funding).","13","0.5338214040325063","Artifact generation and benchmarking","Deployment"
"https://github.com/fairlearn/fairlearn","637540277","issue_comment","https://github.com/Trusted-AI/AIF360/issues/433#issuecomment-637540277","I'm seeing a PR with activity on this topic which I'm thankful for. Gotta ask though ... what did the lawyers say about the topic? ","20","0.6353985717043042","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","637670644","issue_comment","https://github.com/Trusted-AI/AIF360/issues/433#issuecomment-637670644","There are various topics discussed above, and we haven’t found a resolution for all of them yet. Regarding copyright they said that we can extend it towards contributors so I made that change. The remaining points need further discussions, and we’re on it. Thanks for your patience!

I think @adrinjalali ’s point regarding funding is valid, too. It’s just a unique situation for Microsoft right now and there are a few things we need to iron out for that. Will keep you posted as soon as I know more.","20","0.6409334382140315","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","876072790","issue_comment","https://github.com/Trusted-AI/AIF360/issues/433#issuecomment-876072790","Thread summary:
- add a funding page to highlight which institutions supported contributors
- SECURITY.md is gone
- update from outside this thread: copyright for new files is only for ""Fairlearn contributors"", but no longer Microsoft.

I feel like the funding page should be it's own separate issue, and the other two are completed with no further action required. With that in mind I'd suggest closing the issue. Thoughts? @fairlearn/fairlearn-maintainers ","20","0.7843579789955164","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","876300071","issue_comment","https://github.com/Trusted-AI/AIF360/issues/433#issuecomment-876300071","Sounds good, @romanlutz. I have opened #888 for the funding page and will close this.","20","0.6056292461986411","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","632919573","issue_comment","https://github.com/Trusted-AI/AIF360/issues/432#issuecomment-632919573","Good catch. Done.","15","0.1711076280041797","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","632249346","issue_comment","https://github.com/Trusted-AI/AIF360/issues/431#issuecomment-632249346","I'm always a little bit reluctant on that front because those datasets are probably owned by someone, or distributed with some sort of license. If you have experience with these sorts of things I'm happy to learn more :-)

To sort of work around that I've been dumping all such code into a different package called `tempeh` (https://github.com/microsoft/tempeh) which I'm not super eager to maintain because I simply don't have the time to do this properly, but it provides access to all kinds of datasets (among other thigns).

Curious to hear what you think!","25","0.5268900610197539","Research","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","632255637","issue_comment","https://github.com/Trusted-AI/AIF360/issues/431#issuecomment-632255637","Sofar I've been able to find enough datasets that (at least I think) do not have ownership issues. Thusfar it seemed fine as long as you properly attribute but this is something that we can check on a dataset by dataset basis. You do probably want to be critical here because you don't want to become a package of datasets. 

In any case; one thing to be aware of is that scikit-learn is dropping support for `load_boston` because of the controversy. I agree that the boston dataset should be removed there, but there are lessons in that dataset in terms of fairness that should not be lost. It may be valid to host here, it will certainly help with education. ","0","0.6253950548429071","Dataset usage","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","632265152","issue_comment","https://github.com/Trusted-AI/AIF360/issues/431#issuecomment-632265152","I haven't read up on the details yet, but if there's something to be learned from it I think it would be fabulous to have that illustrated / explained. We've started converting some of the notebooks we currently have into these example notebooks on the webpage https://fairlearn.github.io/auto_examples/notebooks/index.html. The one that's there exists because @adrinjalali put in the effort to get this started. I'll try and do something similar for the other ones we want there.

Do you have a rough idea what this could look like/contain? Any links to read up more on the topic are welcome, of course. Starting to feel like I should open another issue for this specifically :-)

To address the broader question: why isn't there an ML package that provides *all* the datasets?
I know that lots of packages have a few datasets sort of attached to them, but I can't say I like this pattern. Not that that should prevent us from going down that path if it's what everyone else does...","20","0.2780771888263745","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","632266597","issue_comment","https://github.com/Trusted-AI/AIF360/issues/431#issuecomment-632266597","You could make a package with *all* the datasets, but it can get rather heavy. You may download 100s of megabytes if you're not careful. 

I guess nothing is stopping me from making a package that contains good datasets for educational purposes. But then you'll have a dependency in your docs. If the package fails, your docs fail. 

To some extend the `kaggle` library does this. But it requires a login, which is super annoying and it forces datasets to be saved in a user folder (they do that to prevent many downloads I imagine). ","21","0.2672199813838039","Installation and shell commands","Deployment"
"https://github.com/fairlearn/fairlearn","632274278","issue_comment","https://github.com/Trusted-AI/AIF360/issues/431#issuecomment-632274278","I liked the approach of just providing the code to download it from the original location. That means the package is tiny (it's just code), and it can download it on demand. If the data moves elsewhere you lose it, though. And you'd need caching (or keep downloading every time). That may also solve the data licensing issue because the package itself doesn't make data available. So that's kind of what I tried to do with `tempeh` but it's a very raw package...","4","0.298136139320527","Installation and shell commands","Deployment"
"https://github.com/fairlearn/fairlearn","632305284","issue_comment","https://github.com/Trusted-AI/AIF360/issues/431#issuecomment-632305284","For scikit-fairness we were considering hosting it on github and allowing users to download the raw file that way. Another option would be s3 or google storage. ","0","0.2368352108986114","Dataset usage","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","632547067","issue_comment","https://github.com/Trusted-AI/AIF360/issues/431#issuecomment-632547067","Why not use [openml](https://www.openml.org/)? It has tons of datasets, you can easily add a dataset to it, and sklearn has a `fetch_openml` routine which can even return a pandas dataframe. Is there anything in our datasets which prevents them from being on openml?","5","0.4939115929941615","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","632558930","issue_comment","https://github.com/Trusted-AI/AIF360/issues/431#issuecomment-632558930","I never heard of that project before but from clicking around it certainly seems like an option. @adrinjalali are you familiar with the project? It would be nice to have some guarantee that the project will exist for the for-see-able fututre. 

If so, I can start making some `fetch_` methods for [these three datasets](https://scikit-fairness.netlify.app/api/datasets.html). Unless there's other objections.  ","20","0.4571863390179488","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","632562620","issue_comment","https://github.com/Trusted-AI/AIF360/issues/431#issuecomment-632562620","We have been investing quite a bit on the sklearn side to properly support it, and we're we're very much in touch with them as it's been a collaborative project. We have incorporated its usage quite a bit in our examples as well. We casually simply get the data directly w/o having a dedicated `fetch_...` method. So I'd say we're betting on it.

For instance, in [this](https://scikit-learn.org/dev/auto_examples/inspection/plot_permutation_importance.html#sphx-glr-auto-examples-inspection-plot-permutation-importance-py) example you have:

``` python
X, y = fetch_openml(""titanic"", version=1, as_frame=True, return_X_y=True)
rng = np.random.RandomState(seed=42)
X['random_cat'] = rng.randint(3, size=X.shape[0])
X['random_num'] = rng.randn(X.shape[0])

categorical_columns = ['pclass', 'sex', 'embarked', 'random_cat']
numerical_columns = ['age', 'sibsp', 'parch', 'fare', 'random_num']

X = X[categorical_columns + numerical_columns]
```
I think `fetch_...` methods would make sense as a convenience method if there's some preprocessing you want to apply to the data, for instance separating the sensitive features maybe?","17","0.2466511392904236","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","632565384","issue_comment","https://github.com/Trusted-AI/AIF360/issues/431#issuecomment-632565384","If nobody objects, I might start working on a first example for `fetch_boston` then. 

Any comments on the idea for raising `FairnessWarning` while the fetching works?","0","0.6606500843788977","Dataset usage","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","632566961","issue_comment","https://github.com/Trusted-AI/AIF360/issues/431#issuecomment-632566961","Is be happy with having the `FairnessWarning` if we also tell people how to suppress it. As a user I'd like to not have the warning if I know what I'm doing (and also not have it in the CI)","32","0.3551437946342404","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","632569161","issue_comment","https://github.com/Trusted-AI/AIF360/issues/431#issuecomment-632569161","Would this work? 

```python
fetch_boston(..., supress=False)
fetch_boston(..., supress=True)
```","5","0.3707538013587834","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","632573265","issue_comment","https://github.com/Trusted-AI/AIF360/issues/431#issuecomment-632573265","I'd probably call it `warn`, but yeah that works.","25","0.1694971694971694","Research","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","632682693","issue_comment","https://github.com/Trusted-AI/AIF360/issues/431#issuecomment-632682693","@adrinjalali cool to learn about openml.org, I didn't know about that either! 👍 

Adding datasets seems great, although I'm not following what raising a `FairnessWarning` helps with in this context.  I'm imagining that any people calling methods like `fairlearn.load_boston()` are doing so because they are following examples or notebooks the project has created.  It's not like people are coming to `fairlearn` every day for all their dataset needs :)

If folks want to collaborate on specific example notebooks for datasets, I'd be into that (eg, https://github.com/fairlearn/fairlearn/issues/418, https://github.com/fairlearn/fairlearn/issues/419, https://github.com/fairlearn/fairlearn/issues/413, or https://github.com/koaning/scikit-fairness/issues/31).  I think there's a lot of educational value to be had in that direction, particularly in going beyond the mechanics of calling the algorithm.","25","0.4636601222488607","Research","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","632691944","issue_comment","https://github.com/Trusted-AI/AIF360/issues/431#issuecomment-632691944","fyi in poking around openml.org, the first 'study' listed is FairML by @hildeweerts.  It's recently added and doesn't have anything in it yet, but she might separately be working on datasets or have thoughts.","20","0.3246662870879989","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","632739624","issue_comment","https://github.com/Trusted-AI/AIF360/issues/431#issuecomment-632739624","I think leveraging OpenML is a great way for loading data sets - but I am 100% biased as one of my colleagues is the founder of OpenML and I've worked with the team before ;)

I recently came across [this paper](https://arxiv.org/abs/2001.09784) which a.o. lists several data sets that have been used to evaluate fair ml approaches. I've added the ones that are already on OpenML to the study @kevinrobinson mentioned. The statistics on the web interface seem to be broken, which is why it does not look like anything is added yet, but if you go to https://www.openml.org/s/241/data you'll see it contains 5 data sets.

One of the things I might want to work on is a benchmark study on how well different unfairness mitigation approaches work in different scenarios, including both synthetic and 'real' data sets. I've done a benchmarking project using OpenML before (see https://www.openml.org/s/98) and it works quite well for that purpose.","0","0.4514053330587872","Dataset usage","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","632755117","issue_comment","https://github.com/Trusted-AI/AIF360/issues/431#issuecomment-632755117","cool!  I'm wary that adding many datasets and running evaluations across them may invite the abstraction traps outlined in [Selbst et al. (2019)](https://andrewselbst.files.wordpress.com/2019/10/selbst-et-al-fairness-and-abstraction-in-sociotechnical-systems.pdf).

If folks do want to go forward in that direction, one minimal step might be to include a description of how the data was collected and labeled, and a potential deployment context.  The framing of 'dataset' abstracts that away, which then makes it very hard to evaluate how well various algorithmic approaches contribute to assessing or mitigating any real-world harms.","25","0.508488021577027","Research","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","632763424","issue_comment","https://github.com/Trusted-AI/AIF360/issues/431#issuecomment-632763424","I think that we should surface the dataset issues explicitly. If we have functions of the form `fetch_<dataset>` then the docstrings can include any issues re. data collection / labeling and the deployment context. Other alternatives:
* a section in the user guide
* maybe there's a suitable place on OpenML?

The matching `FairnessWarning` could include the link as to where to find this information.","25","0.2531601731601732","Research","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","632826155","issue_comment","https://github.com/Trusted-AI/AIF360/issues/431#issuecomment-632826155","If people agree that the idea of a datasets module is valid then I can close this issue and start a new issue for datasets that I’d like to add. That way we can debate per dataset. 

For now I’ll explore using the openml service to host ‘fetch_boston’ and I’ll make a seperate issue for it. ","0","0.7274766507606356","Dataset usage","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","632861674","issue_comment","https://github.com/Trusted-AI/AIF360/issues/431#issuecomment-632861674","I've made a work in progress PR so we can discuss implementation there. Will close this issue and split it up so that we can debate per dataset. ","20","0.5346889952153112","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","632256459","issue_comment","https://github.com/Trusted-AI/AIF360/issues/430#issuecomment-632256459","Thanks for sharing the video!

Adding @MiroDudik and @hannawallach as well. I think it's interesting, but we need to be very careful in the description / documentation. De-biasing data is usually a tricky topic since there are so many different kinds, and they can affect each other etc., so the way we've been looking at it so far is described in the [documentation](https://fairlearn.github.io/user_guide/fairness_in_machine_learning.html) (quoted below). That's much more focused on ""harms"", that is looking at the outcome or result.
That said, applying this could reduce the unfairness in the outcome at the end of the pipeline. We've discussed adding preprocessing techniques a few times, but never quite got down to it. I think it would be great to add it. 


> AI systems can behave unfairly for a variety of reasons. Sometimes it is because of societal biases reflected in the training data and in the decisions made during the development and deployment of these systems. In other cases, AI systems behave unfairly not because of societal biases, but because of characteristics of the data (e.g., too few data points about some group of people) or characteristics of the systems themselves. It can be hard to distinguish between these reasons, especially since they are not mutually exclusive and often exacerbate one another. Therefore, we define whether an AI system is behaving unfairly in terms of its impact on people — i.e., in terms of harms — and not in terms of specific causes, such as societal biases, or in terms of intent, such as prejudice.
Usage of the word bias. Since we define fairness in terms of harms rather than specific causes (such as societal biases), we avoid the usage of the words bias or debiasing in describing the functionality of Fairlearn.","25","0.5147032199663778","Research","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","632269829","issue_comment","https://github.com/Trusted-AI/AIF360/issues/430#issuecomment-632269829","It's certainly valid to point at the main weakness of the approach: technically it only applies a linear filter. Any non-linear effects may still be in there. It can be mitigated by adding a  `PolynomialFeatures` preprocessing step before filtering, but it remains a caveat. 

It's also valid to point out that you're merely filtering based on what you see in your dataset, which is most likely a biased dataset to even start with. 

That said, we've seen it be reasonably effective at giving you a knob that you can tweak and the main benefit is that you can still apply your favorite machine learning model afterwards.","0","0.5002606468575057","Dataset usage","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","632277415","issue_comment","https://github.com/Trusted-AI/AIF360/issues/430#issuecomment-632277415","Exactly! I think if we do a good job in accurately documenting it then that's a good step forward. 
I'm sure we can find an example where the addition of preprocessing helps overall, or perhaps preprocessing + mitigation (e.g. postprocessing) helps in terms of the observed unfairness in the outcome. That actually fits rather nicely into the ""harms"" perspective.","25","0.4215098495044659","Research","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","632529335","issue_comment","https://github.com/Trusted-AI/AIF360/issues/430#issuecomment-632529335","Are there any objections to adding the `InformationFilter` as an approach? If not, then I can dive into the details of how `fairlearn` handles the sensitive attribute such that my proposal will be compatible code-wise. 
","0","0.3240250033972006","Dataset usage","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","632690069","issue_comment","https://github.com/Trusted-AI/AIF360/issues/430#issuecomment-632690069","I'd love to understand an example scenario (or help collaborate on fleshing out one)!  I've read https://scikit-fairness.netlify.app/fairness_boston_housing.html, watched the talk, and seen other great writing like [this one](https://koaning.io/posts/cost-of-success/), thanks for sharing! 👍 
![image](https://user-images.githubusercontent.com/1056957/82670296-e1325680-9c0a-11ea-8b6f-e48db77ef529.png)

We've chatted a bit about the Boston housing dataset in and of itself, but with regards to `InformationFilter` specifically I can't really follow why you would apply it in that scenario.  For example, if someone wants to predict median home value in a geographic area, why would they want to enforce that those predictions about real estate price are not correlated with incomes for that geographic area?  I'm also trying to understand how using `InformationFilter` to remove information about racial demographics in a geographic area connects with assessing or mitigating real-world harms related to housing.  Finding another scenario might just be all we need :)

@koaning it sounds like there might be other real world contexts where you have used this approach?  Alternately, picking another scenario that uses data points on individual people may work better for illustrating the value.  I'm happy to help brainstorm.","25","0.4449434522765284","Research","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","632823671","issue_comment","https://github.com/Trusted-AI/AIF360/issues/430#issuecomment-632823671","I wouldn’t mind picking another example. Part of the reason why we made the dataset fair against income there is because the effect is more pronounced. I would argue one needs to be careful with incomelevels for house prediction though. It can cause a self fulfilling prophecy.

#### Hypotetical Example

If one would use this dataset to automate house price prediction. If a low income neighborhood is getting gentrifed then this model may cause low income households to get less money for their house because they’re stamped as low-income. 

Either way. The example for the docs is a seperate issue. I’d prefer to focus this issue on the implementation of the `InformationFilter`. ","0","0.8946098149637978","Dataset usage","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","633066434","issue_comment","https://github.com/Trusted-AI/AIF360/issues/430#issuecomment-633066434","I saw the video, but I was wondering if there's a paper or tech report about this--not a requirement, but it might shorten the discussion.

Your `InformationFilter` reminds me of [Impartial predictive modeling: Ensuring fairness in arbitrary models](https://arxiv.org/pdf/1608.00528.pdf). Their approach in Definition 4 is equivalent to applying `InformationFilter` at training time, but then using the original features at the test time (without the sensitive features partialed out).

But I think that the way you [quantify fairness](https://scikit-fairness.netlify.app/fairness_boston_housing.html#Measuring-fairness-for-Regression) in your example is more like [Controlling Attribute Effect in Linear Regression](http://web.lums.edu.pk/~akarim/pub/controlling_attribute_effect_icdm2013.pdf). I think that you'd like to ensure, at least for (generalized) linear models, that the model prediction is uncorrelated with sensitive features, right? But if that's what you want then I think that you need to subtract feature means before doing Gram-Schmidt.

Did I get this right or is there something else that you had in mind to achieve with `InformationFilter`?","0","0.4189656236499414","Dataset usage","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","634560076","issue_comment","https://github.com/Trusted-AI/AIF360/issues/430#issuecomment-634560076","This approach was partially inspired from [this pydata talk](https://www.youtube.com/watch?v=AGgCqpouKSs) but also by thinking about linear algebra operations in general. After implementing it I didn't check the theory because I had evidence that it worked in practice. It may very well be that there's a paper with this idea in mind and I wouldn't mind linking to related works, but this implementation is not based on a paper. I think it was just a whiteboard session I had with @Mbrouns.

You could certainly standardise with a `StandardScaler` or a `QuantileScaler` before passing data to the `InformationFilter` (this is often a good idea even without thinking about fairness) but even without it we've been able to see that it indeed removes correlation with sensitive attributes.

I would argue here that the way one might prefer to quantify fairness is indepedant of the methods one might apply. It may be that there's tools that specialize towards a certain fairness metric but the algorithm designer can (and should) varify emperically if their preferred fairness measure gives good scores when the information filter, or any tool for fairness, is used.","0","0.4216469938565611","Dataset usage","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","635744874","issue_comment","https://github.com/Trusted-AI/AIF360/issues/430#issuecomment-635744874","I completely agree that the mitigation technique can be chosen independently from the choice of the metric and the usefulness is driven by the evaluation criteria. I was mostly curious why you do things the way you do them in `InformationFilter`.

I generally like the idea and I think it would be a nice addition, but I think that the centering step should be always done.

One example to think about is:
* feature1 = [0, 0, 1, 1]
* feature2 = [1, 1, 2, 2]

If feature1 is sensitive, then using just Gram-Schimdt without recentering will give you
* feature2' = [1, 1, 0, 0]

That seems undesirable. If you center first, then you'll get feature2'= [0, 0, 0, 0].","6","0.3431331017537913","API expansion","Development"
"https://github.com/fairlearn/fairlearn","635884478","issue_comment","https://github.com/Trusted-AI/AIF360/issues/430#issuecomment-635884478","> I generally like the idea and I think it would be a nice addition, but I think that the centering step should be always done.

That's fair, but as @koaning mentioned, s scaler can be applied in the pipeline before this step. We should just document that the data is expected to be centered. You could also raise a warning if the input data deviates too much from what `InformationFilter` expects. It's usually much cleaner and nicer if the preprocessing needed for a step is not done in the step itself.","0","0.5393359739799695","Dataset usage","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","635977124","issue_comment","https://github.com/Trusted-AI/AIF360/issues/430#issuecomment-635977124","In principle I am okay with raising a warning, I just want to make sure that the absence of centering is a feature not a bug :-). That's why I wanted more clarity about the intended behavior--the intended behavior should be described as a relationship between input and output rather than as a procedure.

If I understand this correctly, fairness here is formalized as statistical independence--the initial goal is to obtain transformed features that are statistically independent from the sensitive features, while keeping as much information as possible otherwise. If such a thing was possible, then any classifier / regressor based on the transformed features would also be statistically independent of the sensitive features.

Instead of going after this in full generality, one could simply require that the transformed features are uncorrelated with the sensitive features. In symbols, if `x_i` is the original feature vector of the i-th example and `s_i` is a vector of sensitive features of the i-th examples (these would be rows in a data frame, but I'm thinking of them as columns), the goal is to derive transformed feature vectors `z_i` such that:

* `E[z s^T] = E[z] E[s]^T`,
   meaning
   ![equation](https://latex.codecogs.com/png.latex?%5Cfrac1n%20%5Csum_%7Bi%3D1%7D%5En%20z_i%20s_i%5ET%20%3D%20%5Cleft%28%5Cfrac1n%20%5Csum_%7Bi%3D1%7D%5En%20z_i%5Cright%29%5Cleft%28%5Cfrac1n%20%5Csum_%7Bi%3D1%7D%5En%20s_i%5Cright%29%5ET)
   in words: every component of `z_i` is uncorrelated with every component of `s_i`

That requires centering + Gram-Schmidt. And without centering you only achieve

* `E[z s^T] = 0`
   I'm not sure how this achieves independence in the absence of centering

I cannot easily think of any use case where one would use Gram-Schmidt without centering.

The pattern of seemingly combining two steps into one already exists in `sklearn`. In fact `StandardScaler` is doing two things--it is centering and re-scaling by default, and you can disable either of the two aspects if you like. I'd much rather follow a similar pattern here, where we center by default, and you can disable it.

**Update:** I think that when the data matrix is sparse, you want to avoid explicit centering. However, in that case, I still think that it might be a good idea to do implicit centering. In this case, if `x` and `s` are two columns of data matrix; `s` being the sensitive feature and `x` being another feature, you would derived the transformed feature column `z` using the formula:

![equation](https://latex.codecogs.com/gif.latex?z%20%3D%20x%20-%20%5Cfrac%7B%5Cfrac1n%20x%5ET%20s%20-%20%5Cbar%7Bx%7D%5Cbar%7Bs%7D%7D%7B%5Cfrac1n%20s%5ET%20s%20-%20%5Cbar%7Bs%7D%5Cbar%7Bs%7D%7D%5Ccdot%20s)

where `\bar{x}` and `\bar{s}` are the scalars corresponding to the averages of `x` and `s`. I think that I would be okay if we skipped explicit centering (or created a flag for it, with a default either way), but if we applied this modified Gram-Schmidt formula. This modified Gram-Schmidt will nicely deal with things like `x=[0,0,1,1], s=[1,1,0,0]`. We probably need a call.","9","0.5044272827136249","Feature engineering methodology","Design"
"https://github.com/fairlearn/fairlearn","636332294","issue_comment","https://github.com/Trusted-AI/AIF360/issues/430#issuecomment-636332294","I'd prefer to run some benchmarks just to confirm the effect of centering, but assuming it's an improvement I'm fine with centering in the method and having a flag for it that you can manually turn off if you wish. 

For sparse data though ... it may also make sense to make a seperate filter for that. I haven't explored sparse data at all here but I find it plausible that you may want to handle that use-case slightly differently. ","0","0.4769909509548191","Dataset usage","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","640033400","issue_comment","https://github.com/Trusted-AI/AIF360/issues/430#issuecomment-640033400","I just ran some benchmark locally and I am now in favor of having centering in the method. I'll allow a user to turn it off but the effect, albeit subtle, is clear. Without it you'd get a filter that will grab the high correlation but you'd allow the perfect correlation to pass, which is wrong. 

@MiroDudik good observation. I still think that we might want to make a seperate component for sparse but I'll work on a first PR for this component.  ","0","0.5877343444374231","Dataset usage","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","640050440","issue_comment","https://github.com/Trusted-AI/AIF360/issues/430#issuecomment-640050440","Thanks for trying this out! Did you have any thoughts on the modified Gram-Schmidt? It implements the idea ""remove correlation without recentering"". I initially thought of it in the context of sparsity, but I think it makes sense as the main approach too. In that case, I don't think we would need to center--and we would still be removing all the correlation--so it would be a conceptually simpler transformation.","0","0.3232583567635115","Dataset usage","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","640051936","issue_comment","https://github.com/Trusted-AI/AIF360/issues/430#issuecomment-640051936","That certainly something that we can still discuss. I read your comment with the assumption that it was focussed on sparse settings. I might have time tomorrow to write an implementation of it. 

Suppose we have these two approaches, how might you benchmark them? I wouldn't be against multiple filtering methods but it might be nice to have a principled way of comparing them. ","0","0.3584720443875372","Dataset usage","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","640082571","issue_comment","https://github.com/Trusted-AI/AIF360/issues/430#issuecomment-640082571","@MiroDudik I'm looking at your formula that you passed earlier. 

![](https://camo.githubusercontent.com/dd942f464e19dcca69fd5eabff3c60654af18e7b/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f7a253230253344253230782532302d2532302535436672616325374225354366726163316e2532307825354554253230732532302d253230253543626172253742782537442535436261722537427325374425374425374225354366726163316e2532307325354554253230732532302d253230253543626172253742732537442535436261722537427325374425374425354363646f7425323073)

How might you extend this in the case that you have more than one sensitive attribute? Let's say you have zipcode and gender? ","15","0.6117679032477239","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","640805647","issue_comment","https://github.com/Trusted-AI/AIF360/issues/430#issuecomment-640805647","Let me write this up more formally.

Agreed re. benchmarking--the basic idea would be to look at statistical independence between Z and S vs. how much information from X is preserved in Z. We could do it in the context of a specific task or tasks--i.e., for some specific Y we wish to predict (as you do in your worked out example) or in terms of how well we can predict X from Z.","25","0.6196678752070184","Research","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","642301998","issue_comment","https://github.com/Trusted-AI/AIF360/issues/430#issuecomment-642301998","I have written up what I have in mind more formally--alongside a quick proof-of-concept Python example:
[linear-information-filter.pdf](https://github.com/fairlearn/fairlearn/files/4761588/linear-information-filter.pdf)
Let me know what you think.","20","0.3446565297099102","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","642915128","issue_comment","https://github.com/Trusted-AI/AIF360/issues/430#issuecomment-642915128","Whoa ... Extensive 👍 

I think the centered approach that is in the information filter right now is eventually equivalent but your approach has more vectorisation that can happen and because there's less operations it should also be numerically more stable. 

I just wrote a quick implementation, it's less code and substantially faster. 

<img width=""964"" alt=""image"" src=""https://user-images.githubusercontent.com/1019791/84436379-a46ae580-ac33-11ea-8bd8-8bc446fd04af.png"">

There's subtle differences in the numbers. I'm interested in understanding what is causing that but for now I think I can go ahead an rewrite the implementation. I'll add unit tests this weekend. ","0","0.4681139755766616","Dataset usage","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","650807376","issue_comment","https://github.com/Trusted-AI/AIF360/issues/430#issuecomment-650807376","I've been running some benchmarks on the `load_boston` dataset and I fear we have to re-evaluate the method. I just found a huge difference between the original `InformationFilter` that uses Gram-Smidt and the `CorrelationRemover` that uses regression internally. 

Each gif represents the model output. I am taking the boston dataset and applying a linear regression after filtering away information. I've made two gifs that each display three charts. 

1. The 1st chart shows predictions on the x-axis and true values on the y-axis. 
2. The 2nd chart shows predicted values split on a sensitive attribute `B`. I split the column into two groups, the group that has a value B < np.mean(B) and a group with a value B > np.mean(B). You would hope the resulting two histograms to overlap.
3. The 3rd chart shows predicted values split on a sensitive attribute `LSTAT`. I split the column into two groups, the group that has a value LSTAT < np.mean(LSTAT) and a group with a value LSTAT > np.mean(BLSTAT). You would hope the resulting two histograms to overlap.

Here's a gif that displays the effect of alpha on the `Correlation Filter` 

![boston_corr](https://user-images.githubusercontent.com/1019791/85955726-f2e0e980-b980-11ea-9e56-a546565c2889.gif)

Here's a gif that displays the effect of alpha on the `InformationFilter`.

![boston_filter](https://user-images.githubusercontent.com/1019791/85955728-f5434380-b980-11ea-8fcf-c10337c29b52.gif)

Looking at these gifs I noticed that both approaches made a change but that the information filter was much more ""aggressive"". This made me curious, why? 

This led me to another chart. I plot the alpha value on the x-axis and the difference in column groups (for each column B and LSTAT) on the y-axis.

![image](https://user-images.githubusercontent.com/1019791/85955992-ab5b5d00-b982-11ea-86ed-48a686243e03.png)

<details>
  <summary><b>More details on the chart.</b></summary>
  I'll give a code snippet as well to make it more clear what the y-value in this chart entails.

```
X, y = load_boston()
pred = pipeline(alpha).predict(X)
c12_upper, c12_lower = pred[X[:, 12] > X[:, 12].mean()], pred[X[:, 12] <= X[:, 12].mean()]
y = c12_upper.mean() - c12_upper.lower()
```

When y=0, I could argue we have a proxy for equal treatment between groups. 

</details>


It seems like the fact that the `CorrelationRemover` is trying to remove correlation by changeing the original data as little as possible is making it less aggressive and in this benchmark ... it seems like the earlier approach was preferable. 

I'll clean up the notebook and share here as well. ","0","0.7459989183856603","Dataset usage","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","650808684","issue_comment","https://github.com/Trusted-AI/AIF360/issues/430#issuecomment-650808684","Here's the notebook; https://gist.github.com/koaning/1a606e07c9b27d1889ada4185959d87a","8","0.4555903866248696","Understanding Fairness Definition","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","650810604","issue_comment","https://github.com/Trusted-AI/AIF360/issues/430#issuecomment-650810604","Either way, I seem to have learned a lesson for the hyperparameter ... it should not be constrained to (0, 1)","25","0.3508158508158508","Research","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","651698374","issue_comment","https://github.com/Trusted-AI/AIF360/issues/430#issuecomment-651698374","I might have a proposal. Shall we make two transformers. One `GramSmidthFilter` and a `CorrelationRemover`? ","0","0.3709623709623709","Dataset usage","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","651960702","issue_comment","https://github.com/Trusted-AI/AIF360/issues/430#issuecomment-651960702","I'm open to that, but we really need this kind of analysis in the user guide I think. Users need to understand when to use which one. ","25","0.7622128760918796","Research","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","653389265","issue_comment","https://github.com/Trusted-AI/AIF360/issues/430#issuecomment-653389265","@MiroDudik any comments? I think my next step is to write up a proper benchmark. @romanlutz I can run this on boston with linear regression, knn and some other models. got a suggestion for another dataset? ","0","0.4266333055323877","Dataset usage","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","653641838","issue_comment","https://github.com/Trusted-AI/AIF360/issues/430#issuecomment-653641838","Interesting! I'm not sure I understand. Are you standardizing the features before transforming with `InformationFilter`? In that case the two approaches should be mathematically equivalent (but numerically they might behave differently). Also, if I understand your benchmark plots correctly, you are plotting the differences between mean predictions between the two groups? Can you also plot the covariances between predictions and sensitive features? Before deciding next steps, I'll definitely want to check the notebook to have a better sense what's going on.","0","0.3200308617341098","Dataset usage","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","653643058","issue_comment","https://github.com/Trusted-AI/AIF360/issues/430#issuecomment-653643058","@MiroDudik agreed. I'll see if I can make a more clear/elaborate notebook and share it here. The `InformationFilter` does *not* do standardizing. I also assumed equivolence here which is why I find the results suprising. One thing I wonder is if the modified GramSmidt process that I have makes the change with minimum distance in mind. ","15","0.4313797463191812","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","653644614","issue_comment","https://github.com/Trusted-AI/AIF360/issues/430#issuecomment-653644614","I see--if you don't center as the first step then they are not equivalent. I'll wait for the notebook!","0","0.5925806103742042","Dataset usage","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","653653395","issue_comment","https://github.com/Trusted-AI/AIF360/issues/430#issuecomment-653653395","The notebook can be found [here](https://gist.github.com/koaning/bc628caf8f7d905891e8886e7c574fcf).

This chart shows the main conclusion (closed to zero means better for fairness);

![image](https://user-images.githubusercontent.com/1019791/86493256-218c0500-bd71-11ea-9473-71852db4dad6.png)

I've ignored the accuracy metrics for now because that is not the focus. ","0","0.4970999053030305","Dataset usage","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","654370367","issue_comment","https://github.com/Trusted-AI/AIF360/issues/430#issuecomment-654370367","@koaning you could use the adult dataset as well. I have a PR out right now to add it to the datasets module, but the relevant code is just that one line if you don’t want to wait.","0","0.4693039909178232","Dataset usage","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","654431613","issue_comment","https://github.com/Trusted-AI/AIF360/issues/430#issuecomment-654431613","This seems to be the 1st chart for the Adult dataset. Different results, but also because the dataset is very different. This is a model with a lot of dummy variables. 

![image](https://user-images.githubusercontent.com/1019791/86634533-43d08d80-bfd2-11ea-8ddf-732499c55299.png)

I'd say here it looks as if both approaches are equal.","0","0.6403290719696973","Dataset usage","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","654434140","issue_comment","https://github.com/Trusted-AI/AIF360/issues/430#issuecomment-654434140","Updated the notebook here with code for the ADULT dataset https://gist.github.com/koaning/bc628caf8f7d905891e8886e7c574fcf. ","0","0.3822410147991542","Dataset usage","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","660652461","issue_comment","https://github.com/Trusted-AI/AIF360/issues/430#issuecomment-660652461","@romanlutz @MiroDudik ping :) ","20","0.6499898311978851","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","700772119","issue_comment","https://github.com/Trusted-AI/AIF360/issues/430#issuecomment-700772119","@koaning : apologies for the inordinate delay--are you still interested in seeing this through? I can commit a few hours each week to make sure that we make steady progress.","20","0.8048644338118024","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","700984593","issue_comment","https://github.com/Trusted-AI/AIF360/issues/430#issuecomment-700984593","There were two bugs in the notebook. One of them was in the implementation of `CorrelationRemover.fit()`:
* _original_: `self.sensitive_mean_ = X_sensitive.mean()`
  _correct_: `self.sensitive_mean_ = X_sensitive.mean(axis=0)`

The second was in `run_stat()` implementation on **adult**:
* _original_:
```python
    pipe = Pipeline([
        ('filter', filt([0, 1], alpha=alpha)),
        ('mod', alg)
    ])
```
* _correct_:
```python
    pipe = Pipeline([
        ('filter', filt([1, 2], alpha=alpha)),
        ('mod', alg)
    ])
```

With those two bugs fixed, some of the paradoxes that we saw previously disappear. To generate the plots below, I was running [this notebook](https://gist.github.com/MiroDudik/0eb70f05631fb0c240cf3aa29c7d1576).

Besides the disparity metrics that you measured previously, I'm also measuring a rescaled version of covariance (scaled to be comparable to the difference stats). Similarly to the difference stats, covariance=0 here corresponds to greater fairness:

### Boston dataset
![image](https://user-images.githubusercontent.com/20565444/94611075-38a69a00-026f-11eb-9f72-1b2f0630d55e.png)
### Adult dataset
![image](https://user-images.githubusercontent.com/20565444/94611909-6213f580-0270-11eb-95fc-0fbb75fa0143.png)

Linear models (linear regression and logistic regression) behave as expected: `CorrelationRemover` leads to the greatest fairness with alpha close to 1. On the other hand, behavior of `InformationRemover` is less consistent across the two linear models and it does not achieve the same low values of unfairness.

I'm not quite sure what to say about non-linear models except that neither of the two mitigation techniques does particularly well, except for SVR, which appears more similar to a linear model than to the k-nearest neighbors model (but that could be driven by the default choice of hyperparameters).
","28","0.5103365404957142","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","701428787","issue_comment","https://github.com/Trusted-AI/AIF360/issues/430#issuecomment-701428787","Ah yeah. Good catch there. Covariance is also a nice touch. 

I've confirmed these results locally and they convince me to focus on the `CorrelationRemover` part. It's faster, numerically more stable and currently I can't come up with edge cases where the `InformationFilter` approach will be better. If we agree that this `CorrelationRemover` is an appropriate addition I wouldn't mind preparing a PR. 

That said, some folks have been playing with the scikit-lego implementation of `InformationFilter` and it does demonstrate some [other drawbacks](https://twitter.com/davidmasip2/status/1290644405880324096). If the orignal dataset is already balanced, it seems like this method might make it ""unbalanced"" and it seems to ""hickup"" when there's integers around (as opposed to floats floats/categoricals). The method still seems valid, but the docs should reflect that it is to be applied in combination with linear systems and that feedback is perhaps appreciated. 

@MiroDudik this is green for a PR? ","15","0.2584089366166759","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","702739673","issue_comment","https://github.com/Trusted-AI/AIF360/issues/430#issuecomment-702739673","Yes--please go ahead and start `CorrelationRemover` as a PR. In the documentation, we should mention that this is intended as a preprocessing step for (generalized) linear models, and that ""no correlation"" does not mean ""independence"". If you really want to stress the point, we could even rename `CorrelationRemover` to `LinearDependenceRemover` (but that's really a mouthful).

I really like the observation how integer-valued features are impacted--a very nice illustration that the lack of correlation does not mean lack of dependence and that dependence and correlation can go in the opposite directions.","0","0.3633597384634732","Dataset usage","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","741517005","issue_comment","https://github.com/Trusted-AI/AIF360/issues/430#issuecomment-741517005","Closing this since the PR went in.","20","0.2027168234064786","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","630233635","issue_comment","https://github.com/Trusted-AI/AIF360/issues/425#issuecomment-630233635","From your question, I'm guessing that you have an `int-string` dictionary, which could be used for the names?

As a work around, could you apply this dictionary to your sensitive feature column in a map operation when invoking the dashboard? That would avoid having to adjust your existing pipeline.

We can look into allowing a user supplied dictionary for the sensitive feature columns; the exact API of the Dashboard constructor does need some reworking.","9","0.5775309563810592","Feature engineering methodology","Design"
"https://github.com/fairlearn/fairlearn","630373955","issue_comment","https://github.com/Trusted-AI/AIF360/issues/425#issuecomment-630373955","Ah yes, applying the map before using the dashboard works too.
It would be nice to have it in the dashboard though. Are you taking pull requests for a feature like that?","31","0.4188995215311005","Testing","Maintenance"
"https://github.com/fairlearn/fairlearn","630376473","issue_comment","https://github.com/Trusted-AI/AIF360/issues/425#issuecomment-630376473","We welcome pull requests :-)

As I said, we are considering reworking the Dashboard constructor anyway, so that the sensitive features and predictions are handled consistently.","9","0.4092503987240829","Feature engineering methodology","Design"
"https://github.com/fairlearn/fairlearn","807973877","issue_comment","https://github.com/Trusted-AI/AIF360/issues/425#issuecomment-807973877","The dashboard has moved to the `raiwidgets` package and will be removed from `fairlearn` in April. Closing this accordingly.","24","0.6839210611452603","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","718807951","issue_comment","https://github.com/Trusted-AI/AIF360/issues/419#issuecomment-718807951","fwiw there's a question related to this issue on the ballot next week in California https://voterguide.sos.ca.gov/quick-reference-guide/16.htm, if folks are curious to learn more about the sociotechnical context around admissions and ways it is contested.","8","0.5758759469696975","Understanding Fairness Definition","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","646468998","issue_comment","https://github.com/Trusted-AI/AIF360/issues/418#issuecomment-646468998","Thanks for the stimulating discussion today! I had noted down a few things that I'm generally unsure about in the context of credit card/loan/etc. application decisions. I'll just post them here for the next deep dive:

1. If people have little or no credit history, do they (or should they) be treated separately? I'm curious about this because it makes the entire fairness assessment a lot more tricky. Do you compare outcomes as a whole (looking at everyone) or outcomes per bucket (fairness for automatically evaluated cases, and fairness for separately/manually evaluated cases)? Perhaps both? All considerations seem relevant since the fairness of the system as a whole may be affected even if both individual parts provide some sort of parity, because the parity may not hold for the system as a whole.
   The distribution of people in the ""no credit history"" bucket may be very different from the rest as well. I'd expect lots of younger people and immigrants.

2. I'd expect the overall number of possible accepted applications to be restricted, because there's only so much money a financial institution can give away. How does this affect the problem formulation. If we use binary classification we'll get some percentage with 0s and some with 1s. We may be able to get scores or probabilities out of this depending on the technique of choice, so that would allow us to take the top-k and approve them to reach the desired lending amount. Perhaps that's not at all how financial institutions think, so this may be worth checking with practitioners.

3. I'm missing a discussion on why Equalized Odds is what we want to have, and why not other criteria. That's usually the hardest choice. http://www.datasciencepublicpolicy.org/projects/aequitas/ has a nice little chart on how to figure this out, although I don't know if such a chart should even exist. Also, how much deviation from exact parity are we willing to allow? Why?

4. Do we want to talk about how historically there are various factors that influence credit card approval decisions and that those may also be reflected in the data? Obviously this in itself is context-dependent, and would require some digging in literature that we're not really familiar with.

5. Beyond y/n decisions financial institutions have more knobs. One such method is to introduce thresholds above which there's auto-approval, and another one below which there's auto-rejection. In between there may be more manual scrutiny. This was mentioned on the call today. There are other conceivable options such as the perhaps approving an application, but with a lower credit limit. Are we simplifying this too much, or would that be a separate model that decides the credit limit? Again, probably worth checking with practitioners.

6. Selection bias: People who never got a credit card card wouldn't be able to establish any kind of history. This was raised today as well. This ties into point (4) because if historical discrimination causes people to be rejected even today then there's no way for Fairlearn's tools like mitigation to help. This may require special attention to ensure the resulting system doesn't repeat this behavior, but instead at the very least surfaces this in some way. Importantly, this also shouldn't result in too many extra hurdles because that may keep groups that were previously unrepresented in the dataset unrepresented even in the future, and having considerably higher burden to get approved is a harm in itself.

7. This is just additional context for ML in credit lending that I found interesting: https://towardsdatascience.com/my-analysis-from-50-papers-on-the-application-of-ml-in-credit-lending-b9b810a3f38. One of the takeaways is that interpretability is important which may restrict what estimator we can use. The Apple Card scenario serves as an interesting case study here as well. https://www.forbes.com/sites/maikoschaffrath/2019/11/12/with-apple-card-investigated-for-discrimination-can-tech-create-a-better-credit-system/#637131212a98

Really looking forward to continuing the conversation next week.","8","0.3865216730281423","Understanding Fairness Definition","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","646650727","issue_comment","https://github.com/Trusted-AI/AIF360/issues/418#issuecomment-646650727","@romanlutz Yeah, this is amazing! 👍 These are exactly the kinds of questions that I'd hope an engineer or data scientist who cares about fairness might be asking, and I hope there are ways the project can help make this a place for that and even encourage it :)

I also think that this kind of work is different than shipping code, and that sometimes it feels challenging because it's so open-ended and complex.  I view this as similar to the work in finding ""product-market fit"" - it's building understanding and empathy of potential users, as a way to explore how or whether the project can contribute to those users.  If we can find an example where that feels good, then we can roll it into a more concise notebook scenario, and then that's directly useful for validating with potential users, making marketing copy, examples for documentation, etc.

And if we find there either isn't a real demand in that kind of deployment context, or the demand is for something that's different than core values around fairness as a sociotechnical challenge (eg, the real value is related to reducing regulatory risk, and it won't meaningfully influence real-world harms), then it's a signal to keep exploring other deployment contexts where the project might contribute.

That's just my two cents, thanks for sharing such thoughtful questions! 👍 ","8","0.4053959671060043","Understanding Fairness Definition","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","649782601","issue_comment","https://github.com/Trusted-AI/AIF360/issues/418#issuecomment-649782601","Here's two more reference: one that looks at the feedback loops in this scenario, and another that talks about why lending is a particularly challenging scenario for fairness mitigation algorithms.

#### [Delayed Impact of Fair Machine Learning (Liu et al. 2018)](http://proceedings.mlr.press/v80/liu18c/liu18c.pdf)
> [Fairness] constraints are clearly intended to protect the disadvantaged group by an appeal to intuition, a rigorous argument to that effect is often lacking.  In  this  work,  we  formally  examine  under  what  circumstances fairness criteria do indeed promote the long-term well-being of disadvantaged groups measured in terms of a temporal  variable  of  interest.   Going  beyond  the  standard classification setting, we introduce a one-step feed-back model of decision-making that exposes how decisions change the underlying population over time... Reflecting on our findings, we argue that careful temporal modeling is necessary in order to accurately evaluate the impact of different fairness criteria on the population. More-over, an understanding of measurement error is important in assessing the advantages of fairness criteria relative to unconstrained selection.

[ml-fairness-gym](https://ai.googleblog.com/2020/02/ml-fairness-gym-tool-for-exploring-long.html) also has a paper and blog post on simulating this.

#### [POTs: Protective Optimization Technologies (Kulynych et al. 2020)](https://arxiv.org/pdf/1806.02711.pdf)
> To be consistent with the fairness literature [77,86], we assume that the target group is interested in getting higher credit ratings (through decrease in false-negatives). This model that postulates the access to credit as beneficial, however, is naïve. Even though the loan-making process that relies on risk-based pricing has economic backing, by definition it implies that the less finanically advantaged people will get more expensive loans. Hence, an intervention that aims at increasing inclusion of disadvantaged populations in the lending process can be seen as an instance of *predatory inclusion*[76,87,88]. Even if it results in lower loan prices in the short term, it can lead to dispossession in the long run.

Also, @MiroDudik my understanding from the discussion today was that this direction seemed more promising to pursue than auto loans (eg, https://github.com/fairlearn/fairlearn/pull/492), but that the group would need more input from partners to describe the deployment context so we could discuss harms and how fairlearn can mitigate them (as described in the new ordering for https://github.com/fairlearn/fairlearn/pull/490).  Let me know if there are other ways I can help! 👍 

EDIT: two other references, one that's more policy suggestions, but a little more nuts and bolts: [Fair Algorithmic Housing Loans](https://www.aspentechpolicyhub.org/wp-content/uploads/2020/07/FAHL-Policy-Brief.pdf), and another that's looking at estimating a particular definition of group discrimination in traditional mortgage lending markets versus algorithmic systems [Consumer-Lending Discrimination in the FinTech Era (Bartlett et al. 2019)](https://faculty.haas.berkeley.edu/morse/research/papers/discrim.pdf).","8","0.4062143304245371","Understanding Fairness Definition","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","652687755","issue_comment","https://github.com/Trusted-AI/AIF360/issues/418#issuecomment-652687755","I think Kevin has pointed out the majority of the issues/clarifications needed between the population this sample represents and the specific deployment context and population. In addition to the ecological validity issues with the real-world use the features and target (e.g., can banks access these variables, are they allowed to use them in decision-making), there may also be construct validity issues. Missing a payment in a given month may not provide a good representation of creditworthiness/risk, as defaulting usually means consecutive missed payments; it may be overly punitive for some users and/or too forgiving for other who may have had other missed payments sprinkled in during the year.

In addition to construct validity, I think the stakeholders’ views on the features that they value may be important to understand given the highly regulated and institutionalized nature of lending practices. One feature that primarily comes to mind is the FICO score if deployment context is in the US since it seems to be heavily relied on for so many different types of credit and loan decision. Would lenders be willing to adopt a model that does not include it or that doesn’t include at least some of the features that the FICO score summarizes (e.g., on-time payments over extended period of time, balances, age of credit)?

As already pointed out, there is plenty of evidence to suggest that in addition to gender, race/ethnicity is another protected variable that may have significant implications for fairness. With certain hiring and lending practices, there is certainly a possibility that an interaction between gender and race that may intensify disparities; e.g., black women may be at a greater disadvantage than white women; both are at a greater disadvantage than white men.

Also, while it makes sense, I was also curious about the specific rationale for selecting equalized odds. Found this interesting blog post, https://blog.acolyer.org/2018/05/07/equality-of-opportunity-in-supervised-learning/, that also highlighted how an equalized odds model relates to max-profit model (no fairness constraints) in terms of what % of the potential profits it achieves (in this example, EO model achieved 80% of potential profits relative to max profit). While we want to maximize fairness, I’m guessing % of potential benefits will be an important “performance” metric for lenders/stakeholders.

Circling back to deployment context, we are entering unprecedented times (kinda) given the mass unemployment and potential global recession, which will have the creditworthiness of many people take a massive hit and influence the general practices of lenders since I believe they tend to have lower interest rates, but more stringent, narrow standards for creditworthiness/ willingness to take on risk. In a perfect world, we would have training data that included the recent-ish 2008 recession and what ensued. Of course (sadly), there is some evidence to suggest that people in underrepresent groups have been disproportionately impacted by unemployment and shut out of the Payment Protection Program for small businesses. https://www.cbsnews.com/news/women-minority-business-owners-paycheck-protection-program-loans/

","8","0.5437778141980054","Understanding Fairness Definition","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","652812033","issue_comment","https://github.com/Trusted-AI/AIF360/issues/418#issuecomment-652812033","@LisaIbanez this is exactly the kind of thinking that's required here, and I'm very excited to work with you (and others) on this. I totally agree on all your points, and the current situation is certainly something we may want to factor in. 

@kevinrobinson previously pointed out that the UCI Credit Card default dataset happened during a debt crisis and may not generally be applicable, so in a way this turns this around a little bit by saying we should have data from a comparable crisis (although hopefully in the same context that we're applying this to).

I'm definitely 100% on board with evaluating this in every way possible, i.e., with a variety of sensitive features and combinations thereof. Intersectionality likely plays a crucial role here.","8","0.3023550265890099","Understanding Fairness Definition","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","719591469","issue_comment","https://github.com/Trusted-AI/AIF360/issues/418#issuecomment-719591469","@romanlutz If you've started working on this, I'm happy to collaborate!  If not then I don't believe this is actually in-progress, but I can't change that status.","20","0.8285739236747878","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","627437534","issue_comment","https://github.com/Trusted-AI/AIF360/issues/413#issuecomment-627437534","I also added two issues critiquing two existing notebooks in regards to ""fairness is a sociotechnical challenge,"" and added suggestions for minimal improvements too (https://github.com/fairlearn/fairlearn/issues/418 and https://github.com/fairlearn/fairlearn/issues/419).  I imagine those issue may end up leading towards the minimal suggestions, and keep the focus on demonstrating the capabilities of the fairlearn v0.4.6 software package.

So this issue is more about exploring those new different directions, where we see what happens if we center the idea that ""fairness is a sociotechnical challenge"" :)","8","0.5789206134033721","Understanding Fairness Definition","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","628024583","issue_comment","https://github.com/Trusted-AI/AIF360/issues/413#issuecomment-628024583","Thanks for #418 and #419. You're right that we picked them to demonstrate the functionality, and tried to put in a couple of disclaimers to acknowledge that, but we should do it more carefully (and in case of LSAC consider picking a different motivating example altogether). Your critiques make me realize that this was not the best strategy. I wonder about the following approach:
* we may need two styles of notebooks: ""case studies"" and ""functionality demonstrations""
* ""case studies"" should be expected to use real-world data and dig into the sociotechnical context in as many different ways as we can think
* ""functionality demonstrations"" still need to be based on a credible real-world use-case, but they would be shorter and so necessarily more limited in what they can explore. Two ideas how to make this happen: 
  * motivation (e.g., the ML task, fairness considerations, features available) should be developed or at least reviewed by a domain practitioner
  * the example does not need to use real data--in fact it might be better if it uses obviously synthetic data, so as to avoid the issues of why the data was collected in the first place as well as the consent of all the parties involved
  * with the above two bullets (re. motivation and synthetic data), I'd be more comfortable to  provide a generic ""this is just a functionality demonstration, so ..."" disclaimer.

One of the next steps from my side is to work out the ""functionality demonstration"" version of the credit-card lending notebook. It's already semi-synthetic, and we have existing relationship with various financial services customers, so we could get them to review how realistic it is.

Also, of course, if you want to work out your own notebook from scratch and contribute to the repository, it would be great! But especially with case studies, this could be a lot of work--so I wonder whether we should set up more explicit authorship and citation format for such more extended contributions.","8","0.2981101241466563","Understanding Fairness Definition","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","628130972","issue_comment","https://github.com/Trusted-AI/AIF360/issues/413#issuecomment-628130972","@MiroDudik Sure!  I hear your distinctions between ""case studies"" and ""functionality demonstrations"" and I also hear the [discussion on documentation](https://github.com/fairlearn/fairlearn-proposals/pull/8/files#diff-21173c05e65dcf529aa6c8cbc26bf0e9R38) about whether to call things ""user guides"" or ""example notebooks.""  In your terminology here I'm trying to talk about ""case studies"" and in @romanlutz's terminology I'm trying to talk about ""example notebooks.""

@MiroDudik I hear that you plan to talk with some Azure customers in financial services and revise the ""functionality demonstration"" related to approving credit card applications, so will check that out when you do!  👍 

Within the scope of this issue, my impression at this point is that while folks on the team may think that ""fairness as a sociotechnical challenge"" is sort of an interesting aspiration, right now there's not alignment for collaborating on more substantive work in that direction.  I'll keep following along, and feel free to @ me if other ways to collaborate come up down the line.

Thanks for sharing your work in the open! 😄 ","8","0.3593424705065927","Understanding Fairness Definition","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","628170424","issue_comment","https://github.com/Trusted-AI/AIF360/issues/413#issuecomment-628170424","Wait hold on. I haven't read all details on both of the issues you opened, @kevinrobinson, but I'm worried there's been a misunderstanding here given your comment ""there's not alignment for collaborating on more substantive work in this direction."" I'd be happy to hop on a call to discuss this. (I suspect that would be faster/less prone to misunderstanding than comments.)","20","0.6550061181890161","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","628685506","issue_comment","https://github.com/Trusted-AI/AIF360/issues/413#issuecomment-628685506","@hannawallach @MiroDudik Sure, apologies if I am misunderstanding and thanks for helping to clarify.  I'm happy to chat and will ping on gitter.","20","0.7083732057416269","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","635627721","issue_comment","https://github.com/Trusted-AI/AIF360/issues/413#issuecomment-635627721","fwiw it's a different technical angle but Deontological Ethics By Monotonicity Shape Constraints ([Wang and Gupta 2020](https://arxiv.org/pdf/2001.11990.pdf)) has an interesting discussion on both the law school admissions context (https://github.com/fairlearn/fairlearn/issues/419) and the credit card application context (https://github.com/fairlearn/fairlearn/issues/418), where it makes some small attempts to connect the technical choices in the paper to the particular sociotechnical context for those deployment contexts.","8","0.9242165684145208","Understanding Fairness Definition","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","635629273","issue_comment","https://github.com/Trusted-AI/AIF360/issues/413#issuecomment-635629273","Oh interesting! I hadn't seen this paper. I will check it out....

On Thu, May 28, 2020 at 5:51 PM Kevin Robinson <notifications@github.com> wrote:
>
> fwiw it's a different technical angle but Deontological Ethics By Monotonicity Shape Constraints (Wang and Gupta 2020) has an interesting discussion on both the law school admissions context (#419) and the credit card application context (#418), where it makes some small attempts to connect the technical choices in the paper to the particular sociotechnical context for those deployment contexts.
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub, or unsubscribe.



-- 
hanna wallach
http://dirichlet.net/
","8","0.7234402852049916","Understanding Fairness Definition","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","646236707","issue_comment","https://github.com/Trusted-AI/AIF360/issues/413#issuecomment-646236707","fyi, I think this paper is super interesting: [Extending the Machine Learning Abstraction Boundary:A Complex Systems Approach to Incorporate Societal Context (Martin et al. 2020)](https://arxiv.org/pdf/2006.09663.pdf):
> Machine learning (ML) fairness research tends to focus primarily on mathematically-based interventions on often opaque algorithms or models and/or their immediate inputs and outputs. Such oversimplified mathematical models abstract away the underlying societal context where ML models are conceived, developed, and ultimately deployed...  we outline three new tools to improve the comprehension, identification and representation of societal context.

And here's a key diagram, with a systems dynamics diagram modeling a lending scenario:
<img width=""841"" alt=""Screen Shot 2020-06-18 at 1 02 31 PM"" src=""https://user-images.githubusercontent.com/1056957/85051163-26dc3380-b165-11ea-869c-fc628bf9c1cf.png"">

which is of course quite complex :)

My two cents is that it's cool to see ML folks integrating these older lines of work, and it's awesome to see more research on new methods for approaching sociotechnical fairness.  At the same time, the approach of adding more complex formal models to address the ""framing trap"" is a bit of a ""formalism trap"" :)  This approach probably makes it even more challenging for non-technical folks to design, critique or evaluate the system (eg, causal graphical models or systems dynamics models are very challenging for people to create, revise or use).  If the evaluation point included qualitative feedback from non-technical stakeholders, or from people impacted by the ML system, it might focus on really different things.  I find the contrast with plain language and storytelling methods like [Brown et al. (2020)](https://www.andrew.cmu.edu/user/achoulde/files/accountability_final_balanced.pdf) or [Young et al. (2019)](https://techpolicylab.uw.edu/wp-content/uploads/2019/03/TowardInclusiveTechPolicyDesign.pdf) helpful (and I also appreciate how that framing admits the possibilities that the issue may be more about decisions of ML practitioners rather than from deficits in the community members).

To connect this to `fairlearn`, I see this as the kind of thing that would **not** be in a notebook or part of the project, precisely because of the ""formalism trap.""  As an example, I don't think it would have helped improve the conversations today if we had been drawing proper systems dynamics models, as compared to just talking :)  I do think there's an opportunity to add a paragraph about this in an ""alternative approaches"" page, where this tradeoff is highlight in terms of abstraction traps, in a way that's short and accessible for a developer audience who might not recognize this as a ""formalism trap.""","25","0.5417611868129606","Research","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","646476242","issue_comment","https://github.com/Trusted-AI/AIF360/issues/413#issuecomment-646476242","Very cool! Of course, I got attracted by the diagram and tried to figure out what's going on there first. My conclusion was that it's really hard to understand, so I very much agree with your analysis :-)

Overall, I think we could focus a lot more on including stakeholders. I wonder what forms this could take. Invite people to hear how they'd feel about a proposed AI system? For me this is mostly about ""how do we engage with non-technical stakeholders in a way that makes this meaningful and understandable?"". So far everyone involved has at least some technical background. Should the goal be to be to get non-technical up to speed to an extent where they can point out gaps in the proposal (not technical details, but assumptions such as ""what feels like fairness?"", forgotten cases or scenarios that occur in real life, lack of centering on a good user experience overall)? 

Perhaps this isn't something that needs to be addressed right away, but it's something I keep thinking about.

I like your suggestion about including a paragraph about your conclusions. I'm struggling to imagine where it would fit well, and how it could be written in a way that's easily understandable. If you already have an idea I'm curious to see/hear what you have in mind, though!

Thanks for sharing.","25","0.376033633451101","Research","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","646604506","issue_comment","https://github.com/Trusted-AI/AIF360/issues/413#issuecomment-646604506","Thanks for sharing, Kevin! I didn't see this paper before. I was actually thinking about the connection between system dynamics and algorithmic fairness as a potential research direction, it's nice to see there's already some work there. I don't think a system dynamics approach would fall into the formalism trap necessarily; IMO the process of getting all the relevant information is much more valuable than the diagram itself. I think systems thinking can be particularly valuable in identifying undesirable reinforcing feedback loops à la weapons of math destruction. I agree that drawing ""proper diagrams"" should not be part of fairlearn though!

I don't think we'll be able to cover *all* relevant aspects of an example in a reasonably concise notebook. So I would probably try to focus on ensuring that the described scenario is reasonably realistic and whether our assumptions regarding the impact of different scenarios are correct. We should make sure that the description of the case, potential harms, and any related trade-offs are clear enough for non-technical folks to have an opinion on them. I think this would be a good goal regardless of whether we're actually able to involve stakeholders.","25","0.6256241578822223","Research","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","646703267","issue_comment","https://github.com/Trusted-AI/AIF360/issues/413#issuecomment-646703267","Yeah, this is awesome @hildeweerts @romanlutz! 👍 

@hildeweerts I really agree that this is an awesome line of research, and I'm super glad to see people exploring it!  I really like the Brown et al. (2020) and Young et al. (2019) work as other examples of other ways to engage in those kinds of conversations with a broader range of people.

@romanlutz @hildeweerts Your comments here have super helpful language for framing this work, since I recognize there's a bit of drift in this issue :)  So I think what might be good is to close this issue, and move forward in a more targeted way in the relevant individual issues (eg, https://github.com/fairlearn/fairlearn/pull/492).

This conversation also helped me see that @hildeweerts's comment on:

> We should make sure that the description of the case, potential harms, and any related trade-offs are clear enough for non-technical folks to have an opinion on them

and @romanlutz' question about ""where should this live?"" are a bit related!  I think the other thing I've been implicitly trying to do here is say... *what if Fairlearn was a place developers new to fairness came to get help in learning and doing the work of approaching fairness as a sociotechnical challenge?*  This is in the spirit that @hannawallach mentioned yesterday and @MiroDudik described in https://github.com/fairlearn/fairlearn/issues/311#issuecomment-592716247.

So to try to help move this forward, but also respect our current bandwidth, I'm going to close this issue out so we can focus on notebook examples.  I'll separately prototype a website that is ""for developers, promoting the idea that fairness as a sociotechnical challenge.""  This is similar to the idea in the [microrubric for critiques](https://github.com/fairlearn/fairlearn/pull/490).  That way we reduce the chatter a bit in the short term, and when the timing is right, we can use that prototype website as a concrete thing we can react to and critique and decide what might be good to merge into fairlearn.  👍 ","25","0.567591021439142","Research","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","623541578","issue_comment","https://github.com/Trusted-AI/AIF360/issues/406#issuecomment-623541578","Thank you for reaching out. We are still in a fairly early stage of moving from being a research project to a more general tool, and we have not finalised how we want to fit into the larger ecosystem.

One big source of concern for us is how the issue of fairness is presented in our package (documentation and any other materials) - we do not want to enable so-called 'fairwashing' of ML algorithms. I don't mean to imply anything about your work by that; I just want to make a major consideration for us clear.","13","0.3911107098793296","Artifact generation and benchmarking","Deployment"
"https://github.com/fairlearn/fairlearn","623597208","issue_comment","https://github.com/Trusted-AI/AIF360/issues/406#issuecomment-623597208","Thanks for getting in touch! Some of the questions about what we want to do with Fairlearn long-term were answered in #311.

Re. industry vs. academia, I'd say that we are looking to enable practitioners (in tech, but also other sectors) to effectively do fairness assessment and mitigation. The field of fair ML is dynamically evolving, so we expect to interact with academic research. Not just the academic research on new algorithms, but also academic research on how practitioners use these algorithms. Some of the people on the Fairlearn team have been involved in that style of research:

* [Improving Fairness in Machine Learning Systems: What Do Industry Practitioners Need?](https://arxiv.org/pdf/1812.05239.pdf)
* [Co-Designing Checklists to Understand Organizational Challenges and Opportunities around Fairness in AI](http://www.jennwv.com/papers/checklists.pdf).
* And see also: https://github.com/fairlearn/fairlearn-proposals/issues/2 .

Re. scikit-learn-contrib, we don't have an immediate plan, but it seems worth discussing!","25","0.4773943474098834","Research","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","623611178","issue_comment","https://github.com/Trusted-AI/AIF360/issues/406#issuecomment-623611178","+1 on previous responses.
@koaning if you're up for it we'd love to discuss this in this week's Fairlearn developer call. It's this Thursday at 8am PST / 11am ET / 3pm UTC / 5pm CEST. Please send me your email address(es) directly on Gitter (just pinged you) and I'll forward the invite.

+ @adrinjalali who may have thoughts on this as well","20","0.5842077922077925","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","623650648","issue_comment","https://github.com/Trusted-AI/AIF360/issues/406#issuecomment-623650648","I'd be delighted to see the two teams work together. From the users' perspective, I think it makes sense to have them together, and I believe fairlearn has enough focus on industry which would satisfy @koaning and @mbrouns concerns. I also think having more eyes and ideas on the same package would make it stronger and create a much nicer and more resilient codebase and workflow. It would also move fairlearn towards having a ""developer community"" which I think would be very beneficial and healthy for the project as well.

On the scikit-learn-contrib, I'd be more than happy to have at least one fairness library there, and I think it's about time. But I wouldn't say it's a necessity. I'm just waiting for these fairness packages to be mature enough so that we can confidently mention them on the sklearn side, either as a ""related project"" or have them as a project under scikit-learn-contrib.","20","0.3330304923059451","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","623674426","issue_comment","https://github.com/Trusted-AI/AIF360/issues/406#issuecomment-623674426","Thanks for the responses all. 

@romanlutz I won't be able to attend thursday because I'll be speaking virtually at a meetup. My collaborator @MBrouns will be able to. Will send both our information via gitter. ","20","0.5083823622445753","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","632189839","issue_comment","https://github.com/Trusted-AI/AIF360/issues/406#issuecomment-632189839","I'll close this issue for now. We'll start making seperate issues for different components and will slowly start moving them in this direction. ","0","0.5718688091569446","Dataset usage","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","622372597","issue_comment","https://github.com/Trusted-AI/AIF360/issues/395#issuecomment-622372597","I've noticed something similar in the metrics too - in those cases, `sklearn` writes a warning message, and returns zero.

Checking and substituting a trivial predictor seems reasonable. I think we shoulda lso report this inconsistency to `sklearn` because they might want to add a specific exception.","30","0.4609894367633262","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","644436208","issue_comment","https://github.com/Trusted-AI/AIF360/issues/380#issuecomment-644436208","#442 handles this
```
for t in range(len(self._hs)):
    if self.weights_[t] == 0:
        pred[t] = np.zeros(len(X))
    else:
        pred[t] = self._hs[t](X)
```","18","0.7171395483497397","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","622372949","issue_comment","https://github.com/Trusted-AI/AIF360/issues/371#issuecomment-622372949","The new pipeline is in place, and tested against Test-PyPI. Once we have a successful PROD release, I'll clean out the old pipeline and close this issue.","32","0.5896250642013351","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","624713348","issue_comment","https://github.com/Trusted-AI/AIF360/issues/370#issuecomment-624713348","I believe this is a duplicate of https://github.com/fairlearn/fairlearn/issues/300.  https://github.com/fairlearn/fairlearn/pull/283 may be addressing this, I'm not sure.","32","0.3822410147991543","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","624715538","issue_comment","https://github.com/Trusted-AI/AIF360/issues/370#issuecomment-624715538","Agree, this looks like a duplicate of #300 . Since that has prettier pictures, I'll close this","24","0.4348729227761486","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","624782102","issue_comment","https://github.com/Trusted-AI/AIF360/issues/370#issuecomment-624782102","oops actually I think https://github.com/fairlearn/fairlearn/pull/405 did this.","32","0.5504179728317662","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","605421706","issue_comment","https://github.com/Trusted-AI/AIF360/issues/353#issuecomment-605421706","moving to sphinx generated notebooks and having sphinx run in the CI would fix this though.","32","0.3551437946342406","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","606136316","issue_comment","https://github.com/Trusted-AI/AIF360/issues/353#issuecomment-606136316","I don't see how. We currently run tests against the master version of Fairlearn as opposed to the last release. How you get your notebook seems like an orthogonal question.","32","0.3742031263645096","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","607438859","issue_comment","https://github.com/Trusted-AI/AIF360/issues/353#issuecomment-607438859","Also, as I mentioned in the PR, I'm not hugely keen on splitting builds between GitHub Actions and ADO. Using Actions to auto-add PR reviewers makes a lot of sense. Splitting our builds (and releases) feels like a headache generator. I'm not so fussed as to which we use, but I think we should use one.","20","0.5171601731601733","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","644435070","issue_comment","https://github.com/Trusted-AI/AIF360/issues/345#issuecomment-644435070","The requirements to get this to work were all completed. A few people are working on trying this out. In case it proves to be useful we can add it to Fairlearn, but we certainly removed all blockers for it so I'm closing the issue.","24","0.3899521531100478","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","602000411","issue_comment","https://github.com/Trusted-AI/AIF360/issues/339#issuecomment-602000411","@MiroDudik should we restrict to just 0/1? We definitely want to consider multi-class classification in the not so distant future, so I don't think it makes sense.","26","0.5303030303030304","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","602618576","issue_comment","https://github.com/Trusted-AI/AIF360/issues/339#issuecomment-602618576","I've been wanting to mention that there's an issue with handling the outputs in the current codebase. As examples, here's how we handle the output in the `GradientBoostingClsasifier`:

https://github.com/scikit-learn/scikit-learn/blob/dcfb3df9a3df5aa2a608248316d537cd6b3643ee/sklearn/ensemble/_gb.py#L1095-L1105

(for some reason github's not rendering them right, so here's the code)

``` python
    def _validate_y(self, y, sample_weight):
        check_classification_targets(y)
        self.classes_, y = np.unique(y, return_inverse=True)
        n_trim_classes = np.count_nonzero(np.bincount(y, sample_weight))
        if n_trim_classes < 2:
            raise ValueError(""y contains %d class after sample_weight ""
                             ""trimmed classes with zero weights, while a ""
                             ""minimum of 2 classes are required.""
                             % n_trim_classes)
        self.n_classes_ = len(self.classes_)
        return y
```

and in `HistGradientBoostingClassifier`:

https://github.com/scikit-learn/scikit-learn/blob/dcfb3df9a3df5aa2a608248316d537cd6b3643ee/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py#L1129-L1142

(for some reason github's not rendering them right, so here's the code)

``` python
    def _encode_y(self, y):
        # encode classes into 0 ... n_classes - 1 and sets attributes classes_
        # and n_trees_per_iteration_
        check_classification_targets(y)

        label_encoder = LabelEncoder()
        encoded_y = label_encoder.fit_transform(y)
        self.classes_ = label_encoder.classes_
        n_classes = self.classes_.shape[0]
        # only 1 tree for binary classification. For multiclass classification,
        # we build 1 tree per class.
        self.n_trees_per_iteration_ = 1 if n_classes <= 2 else n_classes
        encoded_y = encoded_y.astype(Y_DTYPE, copy=False)
        return encoded_y
```

I prefer the second solution (which is a much more recent code).","26","0.7766203982134121","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","604187228","issue_comment","https://github.com/Trusted-AI/AIF360/issues/339#issuecomment-604187228","If this is addressed after the completion of #332 then we need to adjust the parts where we expect labels 0/1.","14","0.1835288367546432","Documentation","Development"
"https://github.com/fairlearn/fairlearn","604347729","issue_comment","https://github.com/Trusted-AI/AIF360/issues/339#issuecomment-604347729","generally, you just need to limit the case to a binary classification, not necessarily a `0/1` output, and where not clear, you can accept a `pos_label` to set the positive output.","23","0.4165243246662871","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","604585593","issue_comment","https://github.com/Trusted-AI/AIF360/issues/339#issuecomment-604585593","I agree. At least, that applies unless we have logic that relies on 0/1 like in PR #332 
The other alternative is to store the mapping from original labels to 0...m
I'm very much thinking of the longer term here because we won't always want to restrict Exponentiated Gradient to binary classification.","26","0.3838226990400903","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","607439260","issue_comment","https://github.com/Trusted-AI/AIF360/issues/339#issuecomment-607439260","Should this be closed? @moprescu  signed off on the fix.","24","0.5321345321345321","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","607991307","issue_comment","https://github.com/Trusted-AI/AIF360/issues/339#issuecomment-607991307","Which fix?","32","0.194961664841183","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","608645097","issue_comment","https://github.com/Trusted-AI/AIF360/issues/339#issuecomment-608645097","Our reductions for fair classification strongly rely on y being 0/1. Going to multi-class would require non-trivial algorithmic extensions. 

Handling arbitrarily encoded **binary** classification should be doable, but it is more involved than meets the eye, because:
* Fairness criteria / fairness metrics rely on a specific `pos_label` (which is right now a constant=1)
* Our reductions create signed weights and they rely on the fact that the class encoding is the same between the meta-estimator (our wrapper) and base estimator.

For now, I suggest that we enforce/validate that y is valued 0/1 and treat this as a feature request rather than a bug. (After we fix the validation of y!)
","26","0.4070880160231446","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","609906626","issue_comment","https://github.com/Trusted-AI/AIF360/issues/339#issuecomment-609906626","Everything you're suggesting @MiroDudik is reasonable and can be put in a new issue as a feature-request. However, this is most certainly a bug if you check my example in the very first post as it changed the labels.","20","0.3224082934609251","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","622374521","issue_comment","https://github.com/Trusted-AI/AIF360/issues/339#issuecomment-622374521","If I've understood correctly now, wouldn't requiring y to be 0/1 'fix' the bug (admittedly by rejecting otherwise valid input)?","23","0.4864433811802232","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","623120372","issue_comment","https://github.com/Trusted-AI/AIF360/issues/339#issuecomment-623120372","Yes, but is that a reasonable restriction?","26","0.4239811912225706","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","623349121","issue_comment","https://github.com/Trusted-AI/AIF360/issues/339#issuecomment-623349121","Is the fix I suggest not what you'd like to see @riedgar-ms ?","32","0.3546878177750662","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","623419054","issue_comment","https://github.com/Trusted-AI/AIF360/issues/339#issuecomment-623419054","I'm sure that a full solution would look something like that. It's more a case of how much of this we treat as a bug and how much we treat as a feature request. And I should be careful that I don't generate more debate about this than the energy required for a full fix ;-)","15","0.4593687742476669","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","624690891","issue_comment","https://github.com/Trusted-AI/AIF360/issues/339#issuecomment-624690891","How about this:

* let's raise an error if `y` has other values than 0/1; we could possibly mention that when transforming `y`, the user should pay attention to how the fairness constraints treat different values of `y`

* if `y` has only a single value and that value is either 0 or 1, we should proceed, but we should give a warning saying that we assume that the underlying `y` has binary encoding 0/1

Long-term, we should handle this via `pos_label` for binary labels. I expect that we will resolve this when we add multi-class classification and it will involve `_encode_y`.","26","0.5517014630938682","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","644434325","issue_comment","https://github.com/Trusted-AI/AIF360/issues/339#issuecomment-644434325","@riedgar-ms 's PR addressed this for the short-term which addresses my originally raised concern. The long-term solution will be relevant with multi-class classification so we can think about it then.","20","0.6232049092174354","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","785652365","issue_comment","https://github.com/Trusted-AI/AIF360/issues/339#issuecomment-785652365","Is there any update on multi-class classification with fairlearn?","26","0.6499898311978849","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","786441976","issue_comment","https://github.com/Trusted-AI/AIF360/issues/339#issuecomment-786441976","@kalikhademi I don't have updates since there's no ongoing work. If you'd like to submit a feature request with some context on what you're looking for please open a new issue. Thanks! ","11","0.612044006948466","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","614948318","issue_comment","https://github.com/Trusted-AI/AIF360/issues/331#issuecomment-614948318","#364 addressed this","11","0.1507849580138736","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","596530633","issue_comment","https://github.com/Trusted-AI/AIF360/issues/322#issuecomment-596530633","Tagging @rihorn2. We should try to resolve this issue within `CONTRIBUTING.md`.","20","0.4515484515484516","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","597078407","issue_comment","https://github.com/Trusted-AI/AIF360/issues/322#issuecomment-597078407","@kevinrobinson  Thank you for calling attention to this, I've been working on improving the end-to-end testing in our sibling project, interpretML. The blocking issue had been what you encountered, building the jupyter extension package from the typescript required publishing a version of the dashboard to npm (additionally, the dependence on jupyter extensions prevented pip install -e from working, and it was incompatible with databricks notebooks). The work in interpretML has been to move to a local Flask backend that serves the built static javascript files, which allows for locally testing changes in notebook using pip install local in interactive mode (-e). The plan is to move this architecture into fairlearn when all the issues are worked out (there is an issue with databricks that I'm working on currently). 

As for the mock-data, the idea is we can add new data to validate common use cases quickly without needing to set up the notebook and pip install. I'd love to have better test coverage than this, but in the meantime it provides some valuable validation that common scenarios work. Please feel free to add other scenarios, and we can improve the test-page to use precomputed metric values for a given dataset to replace the random numbers (this works well with the work done by @riedgar-ms for calculating metrics for upload).","24","0.3228200371057514","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","644432884","issue_comment","https://github.com/Trusted-AI/AIF360/issues/322#issuecomment-644432884","As a small improvement to the current process the recent updates I made to `scripts/build_widget.py` automate most of the UX dev process away. https://github.com/fairlearn/fairlearn/commit/a404a058918d233f65e084252d5da2e07ff2dbd0

With this it's basically
1. make changes to UX code within `visualizations`
2. run `build_widget.py` with `--use-local-changes` option
3. `pip install .` 
4. validate that it works as intended

Clearly that's no substitute for tests and a simpler process","13","0.6335959784235646","Artifact generation and benchmarking","Deployment"
"https://github.com/fairlearn/fairlearn","644742704","issue_comment","https://github.com/Trusted-AI/AIF360/issues/322#issuecomment-644742704","@romanlutz ah yeah, I'm aware of that script and the improvements to it, but I think we are talking about slightly different things :)  To try to clarify, let me break the issue into two components, and then connect this to why I brought this up back then, and why I don't think this is worth spending on time on right now.

### a. build_widget workflow
`build_widget` is a production step, it's not optimized for development experience.  On my laptop this takes ~90 seconds, which is slow enough that it impacts the local development workflow.

### b. test app workflow
 There's a `test` folder, which doesn't have tests but a separate JS app that can be used for local development.

Using that approach avoids any Python, but is still configured as a production build so takes almost a minute.  I'm using:
```
% time python scripts/build_widget.py --use-local-changes --yarn-path=/Users/krobinson/.yarn/bin/yarn
(25 seconds)
% cd /visualization/test/test-page
% time yarn add fairlearn-dashboard@""../../FairnessDashboard""
(15 seconds)
% yarn start
(9 seconds)
```

There's also some JS code that mocks the API to Python and provides a mix of static fixture data and randomly generated values.  These randomly generated values is what I'm really trying to get at, and why this is about *end-to-end product development*, and not just about ""how do I run the build.""

### why i brought this up
When I first started looking at this project, I pulled down the repo and started getting it running, and then was looking around the test app.  I was confused because the visualizations weren't semantically meaningful, and then I discovered that the metrics were being randomly generated.  I understand why this tradeoff makes sense, and how it's particularly helpful for iterating on the visual design elements in the UI.

But it also means you can't use the test app to evaluate UX changes, even for a local demo.  So I wrote a script to grab a dataset, compute the metrics and write them out to `__mock_data` and then updated `App.jsx` in the test app to read those.

As I was doing that, I realized that this was too involved for a prototype, and opened this issue :)

---

Anyway, I'm going to close this issue since it's no longer a priority for me, and from what @rihorn2 said above, it sounds like there might be other internal MS considerations like merging with interpretml and maybe other interop required on the Azure side.  It's hard to evaluate any of the engineering tradeoffs here without any context on the product roadmap, but maybe these notes are helpful for the future.  Thanks! 👍 ","13","0.491439200793976","Artifact generation and benchmarking","Deployment"
"https://github.com/fairlearn/fairlearn","595273819","issue_comment","https://github.com/Trusted-AI/AIF360/issues/317#issuecomment-595273819","To clarify: `eps` is used for:
1. defining the constraints, e.g., | E[h(x) | A=a] - E[h(x)] | <= `eps`
2. the extent to which they can be violated at convergence

See the current documentation:
> `eps`: Allowed fairness constraint violation; the solution `best_classifier` is
> guaranteed to have the classification error within `2*best_gap` of the best error
> under constraint `eps`; the constraint violation is at most `2*(eps+best_gap)`

This is sensible for demographic parity difference and equalized odds difference, but we should separate out this two uses for more generic constraint types.","10","0.4855341738553416","Model development","Development"
"https://github.com/fairlearn/fairlearn","644430767","issue_comment","https://github.com/Trusted-AI/AIF360/issues/317#issuecomment-644430767","#424 addressed the issue (and #443 is adding much needed documentation). Closing the issue accordingly!","24","0.6127206127206128","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","769084601","issue_comment","https://github.com/Trusted-AI/AIF360/issues/316#issuecomment-769084601","There are more issues with the testing of `ExponentiatedGradient` - basically, we have smoke tests, but we do not test enough. For example, if some portion of the algorithm were to be tweaked, we do not have tests (or benchmarking) to assess whether the tweak has improved the performance (where performance encompasses accuracy and correctness, as well as speed).","13","0.4017700558750359","Artifact generation and benchmarking","Deployment"
"https://github.com/fairlearn/fairlearn","594804130","issue_comment","https://github.com/Trusted-AI/AIF360/issues/314#issuecomment-594804130","We did quite a bit of work to make it somewhat consistent in the current format. @riedgar-ms spent many hours getting this right. When you say ""understood"" what do you actually mean? Do tools do further processing using docs?","20","0.4919656614571868","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","594808856","issue_comment","https://github.com/Trusted-AI/AIF360/issues/314#issuecomment-594808856","I believe I did try the `numpydoc` format for the ReST strings for a while. It's basically a different set of markup tags to the existing ones we have.

Changing would be nontrivial (we have to edit all of our existing documentation to conform to the new ReST variant, and possibly persuade ReadTheDocs to process it). As @romanlutz asks, what are the specific benefits from this?","15","0.450574970970521","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","603242798","issue_comment","https://github.com/Trusted-AI/AIF360/issues/314#issuecomment-603242798","It's just that the part of the ecosystem I know follows that format and we're all used to it, and people write tools to parse the format and extract information from it. We even have tests in our test suit which make sure all the parameters of a function or a method are documented in the docstring and we can check for the presence of the default values etc. I'm not sure how you could do that for the one we have here right now.","13","0.3282016705996929","Artifact generation and benchmarking","Deployment"
"https://github.com/fairlearn/fairlearn","603247463","issue_comment","https://github.com/Trusted-AI/AIF360/issues/314#issuecomment-603247463","We'd have to investigate, but since this is just the default `autodoc` format (as I understand it), I'd guess that there are similar tools for it. Those are definitely significant benefits which you list, but I think we should make sure that we can't get them with the existing docstrings before we commit to rewriting all of them.","30","0.243682499737863","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","612079629","issue_comment","https://github.com/Trusted-AI/AIF360/issues/314#issuecomment-612079629","Bump.... this should be part of the wider discussion we're having about documentation. Tagging @MiroDudik and @mesameki for that reason.","20","0.3953366518208014","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","613149548","issue_comment","https://github.com/Trusted-AI/AIF360/issues/314#issuecomment-613149548","In general, such changes are best addressed as soon as possible, since the amount of work for switching only increases over time. I don't know whether we see enough of a need to allocate time for this (unless there are volunteers, of course). @vingu  FYI","20","0.3144031110132804","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","613343591","issue_comment","https://github.com/Trusted-AI/AIF360/issues/314#issuecomment-613343591","I'm really not sure if `autodoc` gives you the options I mention, but for a better understanding of what I mean, you can check the [`sklearn/tests/test_docstring_parameters.py`](https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tests/test_docstring_parameters.py) file as an example.","28","0.3404674548172306","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","633857595","issue_comment","https://github.com/Trusted-AI/AIF360/issues/314#issuecomment-633857595","In addition to the previous points, in my opinion, numpy docstrings are also a much more human-readable format than the one that is currently used.","6","0.331789229878402","API expansion","Development"
"https://github.com/fairlearn/fairlearn","634369867","issue_comment","https://github.com/Trusted-AI/AIF360/issues/314#issuecomment-634369867","I agree with the human-readability comment. As far as I can tell--there are quite a few benefits in switching to `numpydoc` (much of the sklearn ecosystem and beyond use it and it is human readable). The main downside is that our documentation follows a different format and so we'd need to port it. Are there other downsides?","30","0.6855825334086205","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","634531994","issue_comment","https://github.com/Trusted-AI/AIF360/issues/314#issuecomment-634531994","It can be a gradual process. We can say whenever a PR touches a docstring, it should port it to numpydoc with whatever standard we choose.","30","0.4798707536459697","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","634645556","issue_comment","https://github.com/Trusted-AI/AIF360/issues/314#issuecomment-634645556","Sphinx doesn't require us to declare upfront which style docstring we're using? My biggest concern has been that we would have to convert everything at once, or have broken documentation.","14","0.5274781715459682","Documentation","Development"
"https://github.com/fairlearn/fairlearn","634690689","issue_comment","https://github.com/Trusted-AI/AIF360/issues/314#issuecomment-634690689","You can see how it's rendered already on the PR @riedgar-ms : https://250-133444044-gh.circle-artifacts.com/0/docs/_build/html/api_reference/fairlearn.datasets.html","32","0.5235826001955034","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","635751940","issue_comment","https://github.com/Trusted-AI/AIF360/issues/314#issuecomment-635751940","I love the idea of gradually porting the documentation. It seems that sphinx doesn't mind, so I think that we should just do it. @riedgar-ms ?","30","0.5069484655471918","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","635867996","issue_comment","https://github.com/Trusted-AI/AIF360/issues/314#issuecomment-635867996","Being able to convert gradually was my main concern; the benefits @adrinjalali described were very real, but I wasn't enamoured of the prospect of having to convert everything at once.","30","0.6118819776714514","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","635959366","issue_comment","https://github.com/Trusted-AI/AIF360/issues/314#issuecomment-635959366","I take it as a yes. So let's do it--can somebody put out a PR to augment our contributor guide to describe our docstring standard and the interim solution?","14","0.4564531513684054","Documentation","Development"
"https://github.com/fairlearn/fairlearn","592538634","issue_comment","https://github.com/Trusted-AI/AIF360/issues/312#issuecomment-592538634","Thanks for the report, and reading `check_array()`
https://scikit-learn.org/stable/modules/generated/sklearn.utils.check_array.html?highlight=check_array
I agree that it looks like that method converts the input to a Numpy array, which is going to kill any special columns.","17","0.5798993394149102","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","592540629","issue_comment","https://github.com/Trusted-AI/AIF360/issues/312#issuecomment-592540629","Yes, in scikit-learn, the [`ColumnTransformer`](https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/compose/_column_transformer.py) is the one supporting pandas dataframes to some extent. It may be helpful here.","17","0.4252074889017566","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","594791089","issue_comment","https://github.com/Trusted-AI/AIF360/issues/312#issuecomment-594791089","We also need to add test cases with certain kinds of popular models. I know tempeh makes this possible in an easy way. Will look into this as soon as I can.","13","0.401852924145918","Artifact generation and benchmarking","Deployment"
"https://github.com/fairlearn/fairlearn","595336054","issue_comment","https://github.com/Trusted-AI/AIF360/issues/312#issuecomment-595336054","I think that the right approach is to leave validation of `X` and `y` to the base estimator which we are wrapping--so the underlying base estimator should always see `X` and `y` exactly as they are provided to `fit`.

Our wrappers however need to access `y` and `sensitive_features`, so we will need to do some validation on those, and possibly also derive their canonical versions.","23","0.5857020913358941","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","595336875","issue_comment","https://github.com/Trusted-AI/AIF360/issues/312#issuecomment-595336875","@adrinjalali  is that compatible with scikit-learn? I remember you saying that we need to run the scikit-learn validation. In Miro's approach that would happen through the underlying base estimator, but only if it's a scikit-learn estimator. For other estimators it really depends on what validation they're doing. Thoughts?","32","0.4824157063593682","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","596407196","issue_comment","https://github.com/Trusted-AI/AIF360/issues/312#issuecomment-596407196","Yeah AFAIK pipeline doesn't do it for instance. It just passes the data alone.","32","0.5186751233262861","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","596721806","issue_comment","https://github.com/Trusted-AI/AIF360/issues/312#issuecomment-596721806","I adjusted the PR. Now the validation method still runs the scikit-learn validation, but we don't use the reformatted X. Instead, we use the original/input X to pass to the estimator, as suggested by @MiroDudik ","23","0.5619662174985768","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","592401451","issue_comment","https://github.com/Trusted-AI/AIF360/issues/311#issuecomment-592401451","As somebody who's not a member of either of the teams and has contributed to both of them, I believe there's value in having multiple teams working on the topic.

I do worry about the community aspect of AIF360 though. IBM seems very protective of their branding and ownership when it comes to the governance of the package, and that may hurt them in the long run (at least it has discouraged me to spend more time on it than I do).

If there were no company politics involved, in an ideal world, I'd be delighted to see a unified effort on a single package with an open governance model. But unfortunately we don't live in that world.

And that's quite aside from the technical differences and that they implement different methods. As the FAT-ML community works on the topic and advancements are made, it'd be nice to have the methods available to a broader community for us to see the feedback, what works, and what doesn't in practice. The sheer number of methods itself would require more workforce as either of these groups (aif360 and fairlearn) have available.","13","0.2805342976480813","Artifact generation and benchmarking","Deployment"
"https://github.com/fairlearn/fairlearn","592716247","issue_comment","https://github.com/Trusted-AI/AIF360/issues/311#issuecomment-592716247","Excellent question. I'm one of the folks that started the project. As @adrinjalali mentioned, our hope is to be truly open and collaborative. We are currently figuring out the right model of governance, so that we can make this happen. Feel free to share on this issue if you know of governance models that you think would work well (as we're starting small, but generate decent community interest).

On a high level, we firmly view fairness as a socio-technical challenge, so we want to provide techniques that enable humans to navigate fairness trade-offs in a manner that makes most sense for their context. Because of this, our package includes not only algorithm and metrics, but also the fairness assessment dashboard--and we hope that the development of UX for fairness assessment / unfairness mitigation will be our key area of contribution.
 
On the unfairness mitigation side, our initial scope is a bit different than AIF360: we are currently only looking at group fairness and have included only three mitigation techniques. However, our techniques are not tied to classification and/or binary sensitive features.

Ultimately we'd love to host at least these types of content:
* UX / visualizations for fairness assessment 
* algorithms for unfairness mitigation
* domain-specific use cases / guides for when to use and when not to use various metrics / algorithms / visualizations etc. ","20","0.1994317610838564","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","592729724","issue_comment","https://github.com/Trusted-AI/AIF360/issues/311#issuecomment-592729724","I second this! Somehow developers miss this point. 

> domain-specific use cases / guides for when to use and when not to use various metrics / algorithms / visualizations etc.","25","0.5961051436555758","Research","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","594792822","issue_comment","https://github.com/Trusted-AI/AIF360/issues/311#issuecomment-594792822","I'll close this for now since the question was answered. Feel free to reopen if there is more to discuss.","20","0.5419803126809497","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","591031747","issue_comment","https://github.com/Trusted-AI/AIF360/issues/308#issuecomment-591031747","Thanks for explaining the cloning behavior!

My only issue with your proposal is the name `warm_start`. It's not just that it somewhat mismatches how it's used elsewhere in `sklearn`, it also does not quite match what I'd expect.

I am fine with two options:

* *less preferred*: Always fit the estimator when calling fit on `ThresholdOptimizer` instance. This would drop our old functionality for now, but we can add it back in future.

* *more preferred*: Keep the ability to provide a fitted base estimator, but let's not use `warm_start=True` as a flag, but use some other keyword. I'm happy to use something like `prefitted_estimator=True` or just `prefit=True`. This would be analogous to running `CalibratedClassifierCV` with `cv='prefit'`.

I can think of two reasons why use a prefitted estimator:

* Scale: The base estimator may be fitted on a much larger data set than the threshold optimizer, so we don't want to spend a lot of time on fitting the base estimator over and over if we are only exploring different fairness criteria in the threshold optimizer (such exploration would have to be manual rather than inside `GridSearchCV`). Also, statistically speaking, to get a high-quality threshold optimizer requires much less data than getting the initial high-accuracy predictor.

* Availability: We may only have access to the base estimator, but not to the data it was fitted on, e.g., for data privacy reasons.
","23","0.8983635075059598","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","591389775","issue_comment","https://github.com/Trusted-AI/AIF360/issues/308#issuecomment-591389775",">  * _more preferred_: Keep the ability to provide a fitted base estimator, but let's not use `warm_start=True` as a flag, but use some other keyword. I'm happy to use something like `prefitted_estimator=True` or just `prefit=True`. This would be analogous to running `CalibratedClassifierCV` with `cv='prefit'`.

Yes, we could do that. We could even keep the `prefit` name in the interest of keeping the names short. However, we need to document that it's only a preference, and even with `prefit=True`, the estimator will do a `fit` on the child estimator.

>  * Scale: The base estimator may be fitted on a much larger data set than the threshold optimizer, so we don't want to spend a lot of time on fitting the base estimator over and over if we are only exploring different fairness criteria in the threshold optimizer (such exploration would have to be manual rather than inside `GridSearchCV`). Also, statistically speaking, to get a high-quality threshold optimizer requires much less data than getting the initial high-accuracy predictor.
> 
>  * Availability: We may only have access to the base estimator, but not to the data it was fitted on, e.g., for data privacy reasons.

I'm not opposed to having the `prefit` option, but both of these can be achieved w/o the `prefit` option, using a `FrozenEstimator`. You can see the discussion in https://github.com/scikit-learn/scikit-learn/issues/13288 and my implementation of it here: https://gist.github.com/adrinjalali/de9ac56c61f3931b38b24e577f54d083","23","0.8511208741692015","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","592024188","issue_comment","https://github.com/Trusted-AI/AIF360/issues/308#issuecomment-592024188","Great! Let's implement the `prefit` option then and put in a disclaimer similar to:
https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFromModel.html","30","0.712339533665182","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","587514778","issue_comment","https://github.com/Trusted-AI/AIF360/issues/300#issuecomment-587514778","Thanks for catching this--you are right that in those two instances we are using ""accuracy"" in a generic sense rather than the more specific ""accuracy rate"". We are currently revising the wizard, so there will be two steps after the selection of sensitive features:
* Accuracy metric (which we should perhaps rename to ""Error metric""); in this step the user selects any standard metric of model's predictive performance
* Disparity metric (which we might also rename ""Fairness metric"", because we have a few that are not disparities); which would quantify the disparities
The single-model view (Step 3) will no longer have the heading that you currently see (so ""Disparity in accuracy"" will vanish there).

So--I think that we should just change the second step from ""Accuracy metric"" to ""Error metric"". Thoughts?
","12","0.4600010491528092","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","589287337","issue_comment","https://github.com/Trusted-AI/AIF360/issues/300#issuecomment-589287337","@mesameki @rihorn2 ","20","0.4555903866248695","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","590432170","issue_comment","https://github.com/Trusted-AI/AIF360/issues/300#issuecomment-590432170","@MiroDudik Sure!  Yeah, this seems like a normal thing to crop up as products evolve over time :)  I think the revisions you discuss proposed here seem like a good steps to clarify, and if I read right https://github.com/fairlearn/fairlearn/pull/283 aims to address these kinds of revisions to the wizard. 👍

Given the scope of fairlearn is pretty broad, it does seem like the framing that it chooses to present to users about how to think through fairness questions is probably one of its core competencies, even beyond specific audits or visualizations.   So to broaden the discussion a bit, I opened up https://github.com/fairlearn/fairlearn/issues/307 separately, and asked some questions about how you're thinking about evolving fairlearn in relation to the various needs described in your awesome Holstein et al. (2019) paper.","25","0.6741218797522348","Research","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","624782389","issue_comment","https://github.com/Trusted-AI/AIF360/issues/300#issuecomment-624782389","I think https://github.com/fairlearn/fairlearn/pull/405 made the minimal copy change here?","6","0.3481324876673712","API expansion","Development"
"https://github.com/fairlearn/fairlearn","634320385","issue_comment","https://github.com/Trusted-AI/AIF360/issues/300#issuecomment-634320385","That's correct! I think that addresses the issue, so I'll close it. Feel free to reopen if there's more.","24","0.7883101477407527","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","595325367","issue_comment","https://github.com/Trusted-AI/AIF360/issues/298#issuecomment-595325367","This is completed with PR #305 ","20","0.3291536050156742","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","585453400","issue_comment","https://github.com/Trusted-AI/AIF360/issues/293#issuecomment-585453400","Could not reproduce.
```
4962 passed, 598 deselected, 1 warnings in 699.35 seconds
```
Closing this issue. If we get more information about the environment we can potentially reopen it, but at least coverage==5.0.3 by itself isn't the problem.","24","0.4742094064127962","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","585360434","issue_comment","https://github.com/Trusted-AI/AIF360/issues/290#issuecomment-585360434","Great question! We raised this a while back and never really got a good answer. Judging from other projects like https://github.com/dotnet/orleans which are not necessarily in the *microsoft* org, but started off at Microsoft, we don't need it since they don't have it.

@riedgar-ms @vingu @slbird @mesameki any idea?","20","0.906718589141041","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","585361309","issue_comment","https://github.com/Trusted-AI/AIF360/issues/290#issuecomment-585361309","Might not be necessary, but having a separate channel for security bugs (not that I see how we'd have any) could be wise. One would want controlled disclosure on those.","13","0.3084346651225634","Artifact generation and benchmarking","Deployment"
"https://github.com/fairlearn/fairlearn","585753020","issue_comment","https://github.com/Trusted-AI/AIF360/issues/290#issuecomment-585753020","we could have a private mailing list for instance, for those cases.","7","0.3306693306693307","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","585753196","issue_comment","https://github.com/Trusted-AI/AIF360/issues/290#issuecomment-585753196","But looking at the other projects in the ecosystem, I'm not sure if you really have to have that.","13","0.4285133211307709","Artifact generation and benchmarking","Deployment"
"https://github.com/fairlearn/fairlearn","595701567","issue_comment","https://github.com/Trusted-AI/AIF360/issues/290#issuecomment-595701567","Probably related, the license says:

```
    THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
    IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
    FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.
```","13","0.776480400333611","Artifact generation and benchmarking","Deployment"
"https://github.com/fairlearn/fairlearn","595788183","issue_comment","https://github.com/Trusted-AI/AIF360/issues/290#issuecomment-595788183","I'm still working with the relevant parties in Microsoft to update it. For now, I hope it doesn't block people from contributing to the repo. ","20","0.4031843862352336","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","597607865","issue_comment","https://github.com/Trusted-AI/AIF360/issues/290#issuecomment-597607865","After consulting with the OSS advisers in Microsoft, we will keep the current security policy for now. As the fairlearn community continues to develop, we will evaluate the policy again. ","13","0.6528740317977987","Artifact generation and benchmarking","Deployment"
"https://github.com/fairlearn/fairlearn","597614896","issue_comment","https://github.com/Trusted-AI/AIF360/issues/290#issuecomment-597614896","Thanks for following up on this. Could you please maybe expand on why this is needed, for the rest of us who are not at Microsoft to understand the background?","20","0.2799154334038054","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","597640827","issue_comment","https://github.com/Trusted-AI/AIF360/issues/290#issuecomment-597640827","The main feedback is that fairlearn hasn't established as an independent foundation, so we don't have another entity owning it specifically besides Microsoft. ","20","0.6003667245705463","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","581971516","issue_comment","https://github.com/Trusted-AI/AIF360/issues/285#issuecomment-581971516","yeah that's one approach which some projects take, and it works, especially when the project is kinda massive and it's hard for people to follow all conversations. For the time being, I'm not sure if it changes much in this repo though. That said, I don't see that as a bad process anyway.","25","0.4847436300723498","Research","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","580371324","issue_comment","https://github.com/Trusted-AI/AIF360/issues/278#issuecomment-580371324","It's a matrix of test cases with a lot of redundant code, which is why it is the way it is. Nobody would want to run the entire matrix on their machines because it would take hours. If you have a good suggestion I'm happy to add a step to publish the results somewhere (automatically after every run). A simple solution would be to put them in a publicly accessible blob, but that doesn't sound like it's easy to consume either. 

One thing to make this easier for people who don't have an azureml workspace is to provide an option to just generate the scripts and then not run them in azureml. That would allow running them locally then.","13","0.3486684377632346","Artifact generation and benchmarking","Deployment"
"https://github.com/fairlearn/fairlearn","581431763","issue_comment","https://github.com/Trusted-AI/AIF360/issues/278#issuecomment-581431763","Since it depends on azure-ml sdk thingy, WDYT of moving it to its own repo, and instead having a few benchmarks on the main repo?

Generating the scripts is also a good start.","13","0.5615530303030305","Artifact generation and benchmarking","Deployment"
"https://github.com/fairlearn/fairlearn","581964990","issue_comment","https://github.com/Trusted-AI/AIF360/issues/278#issuecomment-581964990","I'm not opposed to that at all. If you have any other kinds of benchmarks you want to run without azureml, feel free to add a separate directory for that. We can also rename the existing one from perf to perf-azureml until it's moved out in case you want to use ""perf"" for yours. ","13","0.4765659872042853","Artifact generation and benchmarking","Deployment"
"https://github.com/fairlearn/fairlearn","591386726","issue_comment","https://github.com/Trusted-AI/AIF360/issues/278#issuecomment-591386726","Just a note that if we move this forward, I'd be able to work on the performance issues using `numba` ;)","20","0.5273374312520219","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","644605574","issue_comment","https://github.com/Trusted-AI/AIF360/issues/278#issuecomment-644605574","Closing this one since the perf tests are moved away from this repo. We can have specific issues on benchmarks if needed in the future.","13","0.4059034145489473","Artifact generation and benchmarking","Deployment"
"https://github.com/fairlearn/fairlearn","644899317","issue_comment","https://github.com/Trusted-AI/AIF360/issues/273#issuecomment-644899317","when I was trying to access the dashboard, the user interactive dashboard did not show up, I only see [out]:<fairlearn.widget._fairlearn_dashboard.FairlearnDashboard at 0x7f0206c42cc0>","24","0.5522810522810522","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","645443949","issue_comment","https://github.com/Trusted-AI/AIF360/issues/273#issuecomment-645443949","@yuanshi4 you opened #484 for that purpose, right? Can we keep the discussion there and follow the steps we outlined? Thanks!","25","0.7491642402674431","Research","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","736888202","issue_comment","https://github.com/Trusted-AI/AIF360/issues/273#issuecomment-736888202","closing this since the dashboard is moving out of Fairlearn.","24","0.6745983323164533","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","728795469","issue_comment","https://github.com/Trusted-AI/AIF360/issues/270#issuecomment-728795469","This won't get addressed in this repo. Stay tuned for the `raiwidgets` release at the end of November!","24","0.8222350897510134","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","830715537","issue_comment","https://github.com/Trusted-AI/AIF360/issues/269#issuecomment-830715537","The both the dashboard and `_create_group_metric_set()` are going to be moved out of the Fairlearn repo. Closing this.","24","0.5383675464320625","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","578255823","issue_comment","https://github.com/Trusted-AI/AIF360/issues/266#issuecomment-578255823","https://github.com/Azure/MachineLearningNotebooks/issues/746 this fix seems to be the way to go...","30","0.2975444096133752","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","598503506","issue_comment","https://github.com/Trusted-AI/AIF360/issues/266#issuecomment-598503506","Closing this since it was caused by a pip upgrade. If we ever encounter such issues again we can simply pin pip.","21","0.5221378045585535","Installation and shell commands","Deployment"
"https://github.com/fairlearn/fairlearn","598683264","issue_comment","https://github.com/Trusted-AI/AIF360/issues/266#issuecomment-598683264","More like fixing the issue with the upgraded pip rather than pinning pip? pinning pip sounds like a very drastic measure to me :P","21","0.6890749601275917","Installation and shell commands","Deployment"
"https://github.com/fairlearn/fairlearn","600985844","issue_comment","https://github.com/Trusted-AI/AIF360/issues/266#issuecomment-600985844","Kind of depends on whether pip breaks something intentionally (as in ""this is the way we do things now"") or unintentionally (as in ""oops, broke this, but version n+1 will fix it again""). In this case we assumed it's the latter, although I don't recall whether we ever figured out if that's true.","32","0.5481807696340568","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","578239800","issue_comment","https://github.com/Trusted-AI/AIF360/issues/265#issuecomment-578239800","According to Miro this may be because it's at the edge of the grid where one of the classes is ignored, and that may lead to imbalance resulting in multiple solutions with the same least squares loss. So that means it's actually undetermined. I'm putting in some tolerance to accept both to unblock myself, but we'll need to see whether the hypothesis can be verified.","28","0.3976097757486235","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","578501443","issue_comment","https://github.com/Trusted-AI/AIF360/issues/265#issuecomment-578501443","That looks pretty similar to the transient a week or so ago. Same item, same rough tolerance required. Which is rather large (on the order of 10%). Even if this is a case of two equally good solutions, it seems odd that we've only seen it recently. AFAICT, the `LinearRegression` solver is supposed to be fully deterministic (it wraps SciPy, which in turn wraps LAPACK), so even if there are two solutions which are equally good it should still always chose the same one of them.","28","0.4294835980525181","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","578515022","issue_comment","https://github.com/Trusted-AI/AIF360/issues/265#issuecomment-578515022","Don't increase tolerance. The tolerance should still be at essentially the
numerical level.

If you want to unblock then modify the test so that it ignores the two
extremes (where one group gets the weight zero and the other group weight
one) and only do comparison of the inner elements of the array.

On Sun, Jan 26, 2020 at 8:22 AM Richard Edgar <notifications@github.com>
wrote:

> That looks pretty similar to the transient a week or so ago. Same item,
> same rough tolerance required. Which is rather large (on the order of 10%).
> Even if this is a case of two equally good solutions, it seems odd that
> we've only seen it recently. AFAICT, the LinearRegression solver is
> supposed to be fully deterministic (it wraps SciPy, which in turn wraps
> LAPACK), so even if there are two solutions which are equally good it
> should still always chose the same one of them.
>
> —
> You are receiving this because you are subscribed to this thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/fairlearn/fairlearn/issues/265?email_source=notifications&email_token=AE443RDMQPFXHWOCXKQ6LM3Q7WFCJA5CNFSM4KLI5IZ2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEJ5TWQY#issuecomment-578501443>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AE443RCFJRS2DK43ZD7A7Q3Q7WFCJANCNFSM4KLI5IZQ>
> .
>
","7","0.3715330986401602","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","578743390","issue_comment","https://github.com/Trusted-AI/AIF360/issues/265#issuecomment-578743390","@romanlutz has already modified the test to make the first element a special case. If the last element starts giving us trouble, we can do the same for the last. This issue is mentioned in the explanatory comment.","24","0.370595305877143","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","578757400","issue_comment","https://github.com/Trusted-AI/AIF360/issues/265#issuecomment-578757400","This still leaves the question about what causes the selected solution to change.... however, if there are multiple equally good solutions, then I suppose the answer is 'literally anything in the numeric stack' which would be a wild goose chase (since potential important differences would include the exact patches on each VM).

Perhaps the real issue is a need for more sophisticated testing, rather than what I created here - a golden value test which just makes sure that the algorithm keeps giving the same answers.","11","0.1983917675043955","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","578775056","issue_comment","https://github.com/Trusted-AI/AIF360/issues/265#issuecomment-578775056","Agreed about improving tests. I don't think that a comparison to the golden value is a bad idea, we should just make sure that our estimator(s) (on top of which we implement the reductions) have a unique minimum. This is easily achieved by adding a bit of regularization.

So my suggestion for a systematic solution:
* use estimators from `test/unit/reductions/exponentiated_gradient/simple_learners.py`
* modify those estimators, so they can have a bit of regularization

I prefer this to using estimators from sklearn.linear_model--these allow regularization, so we could solve the uniqueness problem there too--but I'd prefer to avoid sklearn dependence in tests like these (we could even try to avoid numpy, but I'm not that paranoid).","28","0.2230814880753681","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","587129615","issue_comment","https://github.com/Trusted-AI/AIF360/issues/265#issuecomment-587129615","Just to add to this: the ""fix windows notebook"" change that I just merged does not solve the overall problem, it just adds the special case handling back. Somehow it worked for a little while without it (probably because the affected platforms aren't part of the PR gate). The overall resolution remains to be carried out.","32","0.3721603654308904","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","623518240","issue_comment","https://github.com/Trusted-AI/AIF360/issues/265#issuecomment-623518240","Since we're in the process of dropping Python 3.5, should we close this as a 'won't fix' ?","32","0.1775898520084566","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","623788691","issue_comment","https://github.com/Trusted-AI/AIF360/issues/265#issuecomment-623788691","To my knowledge this occurs in py3.6 as well","4","0.2316453121822249","Installation and shell commands","Deployment"
"https://github.com/fairlearn/fairlearn","623789182","issue_comment","https://github.com/Trusted-AI/AIF360/issues/265#issuecomment-623789182","Do you have a repro? Our nightly builds are both green right now.","32","0.4918414918414919","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","624263732","issue_comment","https://github.com/Trusted-AI/AIF360/issues/265#issuecomment-624263732","Of course they are, because we put a special case in the test :-)

```
# TODO: investigate where the different outcomes for the first grid point are from, likely
    # due to some ignored data points at the edge resulting in another solution with the same
    # least squares loss (i.e. both solutions acceptable).
    # Reflects https://github.com/fairlearn/fairlearn/issues/265
    assert logging_all_close([[3.2, 11.2]], [all_predict[0]]) or \
        logging_all_close([[3.03010885, 11.2]], [all_predict[0]])

    assert logging_all_close([[-3.47346939, 10.64897959],
                              [-2.68, 10.12],
                              [-1.91764706, 9.61176471],
                              [-1.18461538,  9.12307692],
                              [-0.47924528,  8.65283019],
                              [0.2, 0.7]],
                             all_predict[1:])
```","32","0.3328814708125053","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","744635418","issue_comment","https://github.com/Trusted-AI/AIF360/issues/265#issuecomment-744635418","https://github.com/fairlearn/fairlearn/blob/b07209058322d36e0962e25f82cd720ada650cde/test/unit/reductions/grid_search/test_grid_search_regression.py#L82","28","0.6380549682875265","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","942640534","issue_comment","https://github.com/Trusted-AI/AIF360/issues/265#issuecomment-942640534","When deprecating Python 3.6 in #966 we should consider removing this special test case assertion. Hopefully, it works deterministically on all Python versions that are more recent than 3.6","32","0.6528740317977987","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","966572697","issue_comment","https://github.com/Trusted-AI/AIF360/issues/265#issuecomment-966572697","As a reminder, although Python 3.6 is deprecated, the ill-defined optimization solution is still there, so I suggest we leave this issue open.","6","0.3213693127237793","API expansion","Development"
"https://github.com/fairlearn/fairlearn","966586680","issue_comment","https://github.com/Trusted-AI/AIF360/issues/265#issuecomment-966586680","Plus it happens in 3.7 as I found out recently!","1","0.1711076280041798","Fix warnings","Maintenance"
"https://github.com/fairlearn/fairlearn","578345203","issue_comment","https://github.com/Trusted-AI/AIF360/issues/264#issuecomment-578345203","Yes! The following tweak should work:
* `load_data` (line 36) should be receiving the `event` argument similar to the one created in `EqualizedOdds`
* however, after line 42 (that is after the data gets loaded into `self.X` and `self.tags`), you should remove (from both `X` and `tags`) all the rows with Y=0, but do not reindex `X` and `tags`; also store somewhere the indices of the removed rows
* now the `signed_weights` method is only returning the weights over examples with Y=1, but we need to generated signed weights for all the original examples (the examples with Y=0 should receive the signed weights of zero); so after line 113, you need to reinsert zero weights corresponding to the removed examples","9","0.2844813032835316","Feature engineering methodology","Design"
"https://github.com/fairlearn/fairlearn","578428600","issue_comment","https://github.com/Trusted-AI/AIF360/issues/264#issuecomment-578428600","Thanks! Will try this and share results.","25","0.2659352142110763","Research","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","614948683","issue_comment","https://github.com/Trusted-AI/AIF360/issues/264#issuecomment-614948683","#362 addressed this","13","0.1949616648411828","Artifact generation and benchmarking","Deployment"
"https://github.com/fairlearn/fairlearn","728792732","issue_comment","https://github.com/Trusted-AI/AIF360/issues/263#issuecomment-728792732","This will be addressed in the dashboard's new repo with the `raiwidgets` package (to be released at the end of November)","24","0.8339123721289329","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","607442907","issue_comment","https://github.com/Trusted-AI/AIF360/issues/258#issuecomment-607442907","Do you mean to remove Test PyPI from our release pipeline, and always just push RC versions to PyPI?

The problem that the `.dev[n]` suffix is solving (and I believe it [can be combined with RC](https://www.python.org/dev/peps/pep-0440/#public-version-identifiers)) is that PyPI doesn't separate 'upload' from 'publish'. I always want to push exactly the same changeset to Test PyPI and PyPI, and that merging of upload and publish means that we can 'burn' versions in Test PyPI. So automatically adding the `dev[n]` suffix to Test PyPI sidesteps the issue.","32","0.6422662204870796","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","607492328","issue_comment","https://github.com/Trusted-AI/AIF360/issues/258#issuecomment-607492328","Could you maybe elaborate @riedgar-ms ? I'm not sure if I understand.","20","0.5724275724275724","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","607531805","issue_comment","https://github.com/Trusted-AI/AIF360/issues/258#issuecomment-607531805","Basic idea is that we have a release pipeline which takes a changeset, runs tests, releases to Test-PyPI and runs tests on that, and then finally releases to PyPI itself.

Suppose we don't have the `.dev[n]` mechanism, and we're trying to release v1.0.0. This gets to Test-PyPI and we have a problem. So we fix it. But now, we can't release v1.0.0 to Test-PyPI again... we have to go with v1.0.1. And that's the version which will ultimately go to PyPI itself. In a nutshell, the version released to PyPI depends on the number of issues we found at the Test-PyPI stage.

So by dynamically adding `.dev[n]` to the Test-PyPI versions (where `n` is specified at the time the release is created) we can bypass this.

Does that make sense?","32","0.7539424860853433","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","608077232","issue_comment","https://github.com/Trusted-AI/AIF360/issues/258#issuecomment-608077232","I think @adrinjalali is trying to say that you can release v1.0.0-rc0. Doesn't work? Release v1.0.0-rc1, etc. until one finally works as expected. Then release v1.0.0
That's at least how most projects I've seen handle it.","32","0.6282103546677537","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","608382183","issue_comment","https://github.com/Trusted-AI/AIF360/issues/258#issuecomment-608382183","> Basic idea is that we have a release pipeline which takes a changeset, runs tests, releases to Test-PyPI and runs tests on that, and then finally releases to PyPI itself.

I'm not sure why you need to upload to `test.pypi` to run the tests. The artifacts can be created and tested in the same pipeline w/o having to upload them anywhere. Another solution is if you really need a staging area, you can do what the scipy ecosystem does, which is to upload them somewhere else first. We used to have an account on rackspace, and have moved now to anaconda.org. You can check this org to see how it looks and see the files we're uploading there: https://anaconda.org/scipy-wheels-nightly . We also upload our nightly builds on that repo and give instructions on how to install the nightly from there on our website.

Regarding the `dev[n]`, you can do `rc[n]` instead, which is what we do as well. In worst case scenario, if there's a very minor issue in your release and you'd like to fix it, you can release a `.post[n]` version. We had to do it in our latest release since we forgot to exclude some `.c` files from the wheel.","32","0.6128883085795115","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","608397972","issue_comment","https://github.com/Trusted-AI/AIF360/issues/258#issuecomment-608397972","Now you mention it, the AzureML SDK packages do have another internal archive for their pre-release packages, so perhaps the actual upload to Test-PyPI isn't required. Burning versions there is what prompted the `dev[n]` workaround. The key point (as you note) is to test the actual package, well apart from the repo.

Side note: I did have Test-PyPI have `rc[n]` for a while, but decided to change to `dev[n]` since that's ""less significant"" and can actually be combined with `rc[n]`.

I do want to avoid something that happened to one of our dependencies (specifically, _nested_ dependencies, so hunting it down wasn't fun) a few months ago, where in the morning they release v4.0.0 .... and by the end of the day, they were up to v4.0.4 and finally managed to get it right.

If we change our release pipeline to do the install-from-local (or perhaps from a storage account) instead, I'd be happy to drop Test-PyPI. Perhaps that would be something to investigate in GitHub Actions @romanlutz ?","32","0.4721778375802429","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","609954561","issue_comment","https://github.com/Trusted-AI/AIF360/issues/258#issuecomment-609954561","I'm always for GitHub Actions, as you know. However, that means every step of that pipeline would have to be moved over, and since the validation is part of that, all of that, too. That validation includes all our tests AFAIK, so it basically means moving over everything.","32","0.8909801136363635","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","632064764","issue_comment","https://github.com/Trusted-AI/AIF360/issues/258#issuecomment-632064764","Closing this in favour of:
https://github.com/fairlearn/fairlearn-proposals/pull/11
There have also been changes to the release pipelines, removing the `.dev[n]` suffix.","32","0.5770121598147073","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","577251814","issue_comment","https://github.com/Trusted-AI/AIF360/issues/256#issuecomment-577251814","I think we should aim to follow the conventions in
https://scikit-learn.org/stable/developers/develop.html
","30","0.3508158508158507","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","577310547","issue_comment","https://github.com/Trusted-AI/AIF360/issues/256#issuecomment-577310547","in summary:

- `attr` is a public attribute, and is set via the `__init__` and `set_props`
- `_attr` is private, and is used internally by methods of the object, and are set usually in `fit`
- `attr_` is public, and is set in `fit`, and not in `__init__`","17","0.4466813463353255","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","644428501","issue_comment","https://github.com/Trusted-AI/AIF360/issues/256#issuecomment-644428501","#381 is fixing this for postprocessing, after which we should be conforming. Issue #222 will then track the remaining pieces required to get check_estimator to pass. ","32","0.4574960127591706","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","577307435","issue_comment","https://github.com/Trusted-AI/AIF360/issues/254#issuecomment-577307435","could you maybe post an example of what you want to see changed?

This may also be helpful as a guideline: https://scikit-learn.org/dev/developers/contributing.html#guidelines-for-writing-documentation","14","0.4716949716949715","Documentation","Development"
"https://github.com/fairlearn/fairlearn","623520287","issue_comment","https://github.com/Trusted-AI/AIF360/issues/254#issuecomment-623520287","As an example, at the time of writing:
https://fairlearn.github.io/api_reference/fairlearn.postprocessing.html
In the documentation for the `X` argument of the `fit()` method, there is a link to the pandas DataFrame documentation, but not the numpy ndarray doc.","23","0.3257158743397275","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","647803645","issue_comment","https://github.com/Trusted-AI/AIF360/issues/254#issuecomment-647803645","I grouped this under doc infrastructure since sphinx should throw warnings for this kind of problem. I think @riedgar-ms enabled warnings for similar things recently. Perhaps we can iterate on this. If anyone feels like going through the remaining issues that would be awesome to clean up. This is definitely related to #314 as well since the change in format may affect some of these things.","11","0.1609696969696969","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","876070688","issue_comment","https://github.com/Trusted-AI/AIF360/issues/254#issuecomment-876070688","This issue is too broad. I've fixed dozens of these little problems all over the code base (most recently #861), but if we want people to contribute in fixing these we need to create little and actionable issues. For example, whenever you stumble upon something that doesn't render properly open an issue and mention exactly where you encountered it (with screenshot and link) and what you expected (and perhaps how one might go about fixing it).

I know there are still plenty of doc build warnings that I haven't gotten to but there's also another related issue. I'm closing this one since it won't gain traction as is. ","24","0.4764043039905108","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","577236049","issue_comment","https://github.com/Trusted-AI/AIF360/issues/253#issuecomment-577236049","I'll take this to familiarize myself with the codebase :)","20","0.6007728289607486","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","577235654","issue_comment","https://github.com/Trusted-AI/AIF360/issues/252#issuecomment-577235654","We had very long discussions regarding the plotting API, and this is what we came up with, and will expand on: https://scikit-learn.org/stable/auto_examples/plot_roc_curve_visualization_api.html","2","0.3943062587130382","Performance measurement","Validation"
"https://github.com/fairlearn/fairlearn","584871664","issue_comment","https://github.com/Trusted-AI/AIF360/issues/252#issuecomment-584871664","Thanks @adrinjalali ! Ours would be based on the mitigation method and the data it trained on, so we wouldn't actually pass X, y. My PR hopefully makes it clear how to use it. Any comments/feedback appreciated!","23","0.2611343458801085","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","598503887","issue_comment","https://github.com/Trusted-AI/AIF360/issues/252#issuecomment-598503887","This was obviously completed a long time ago. Closing the issue.","20","0.7062720225510923","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","575262006","issue_comment","https://github.com/Trusted-AI/AIF360/issues/248#issuecomment-575262006","I see there are also tests there, why are the notebooks there then?","16","0.1711076280041798","Testing","Maintenance"
"https://github.com/fairlearn/fairlearn","575302906","issue_comment","https://github.com/Trusted-AI/AIF360/issues/248#issuecomment-575302906","The notebooks are there to actually show functionality to users, for example when you demo fairlearn.","15","0.1896436896436895","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","575324877","issue_comment","https://github.com/Trusted-AI/AIF360/issues/248#issuecomment-575324877","And we have tests which run the notebooks as unit tests via papermill too.","13","0.745356793743891","Artifact generation and benchmarking","Deployment"
"https://github.com/fairlearn/fairlearn","575591131","issue_comment","https://github.com/Trusted-AI/AIF360/issues/248#issuecomment-575591131","Alternatively you could have scripts from which notebooks are generated in the documentation platform.

There are different ways of doing it, in sklearn we use sphinx and sphinx-gallery to do that, and we provide `.py`, the notebook, and a link to binder for each user manual/example, here's an [example](https://scikit-learn.org/stable/auto_examples/release_highlights/plot_release_highlights_0_22_0.html#sphx-glr-auto-examples-release-highlights-plot-release-highlights-0-22-0-py)

Numpy is taking a different approach AFAIK, but usually we try to avoid having notebooks in the repo, since they're also hard to manage with version control systems (getting a meaning full `git diff` and `git blame` on them is pretty hard.)","32","0.2928334439283344","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","575633904","issue_comment","https://github.com/Trusted-AI/AIF360/issues/248#issuecomment-575633904","We should look in to that. We had noticed that Notebooks don't play particularly nicely with git ;-)","21","0.1946441155743481","Installation and shell commands","Deployment"
"https://github.com/fairlearn/fairlearn","575303557","issue_comment","https://github.com/Trusted-AI/AIF360/issues/247#issuecomment-575303557","Perf tests currently require extra steps, so I'd recommend running
```
python -m pytest --ignore=test/perf
```
or scope further down to what you actually want to run. That's why we tell users to run tests under `test/unit` for validation.","32","0.5204686617730097","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","575358558","issue_comment","https://github.com/Trusted-AI/AIF360/issues/247#issuecomment-575358558","The current PR https://github.com/fairlearn/fairlearn/pull/249 should also help with this by adding the Makefile that will by default exclude perf tests. I still want to address this in a comprehensive manner if you have suggestions.","13","0.5849307268931488","Artifact generation and benchmarking","Deployment"
"https://github.com/fairlearn/fairlearn","575568160","issue_comment","https://github.com/Trusted-AI/AIF360/issues/247#issuecomment-575568160","that's fair, but the contributing guide and the README should probably suggest the same thing then.","14","0.5383675464320626","Documentation","Development"
"https://github.com/fairlearn/fairlearn","575668512","issue_comment","https://github.com/Trusted-AI/AIF360/issues/247#issuecomment-575668512","Great point. I'll revise it.","25","0.4515484515484516","Research","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","598379304","issue_comment","https://github.com/Trusted-AI/AIF360/issues/247#issuecomment-598379304","Since these tests are now in another repository I'll close this.","24","0.5321345321345321","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","575304001","issue_comment","https://github.com/Trusted-AI/AIF360/issues/245#issuecomment-575304001","Correct, they're not. We do that explicitly in the pipelines that run PR checks. Is there a better way of skipping them without having to write
```
python -m pytest --ignore=test/perf
```
?","32","0.7926348688680528","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","575586258","issue_comment","https://github.com/Trusted-AI/AIF360/issues/245#issuecomment-575586258","That makes sense, but I think that exception is still giving an incorrect message.","6","0.228752642706131","API expansion","Development"
"https://github.com/fairlearn/fairlearn","575668919","issue_comment","https://github.com/Trusted-AI/AIF360/issues/245#issuecomment-575668919","Which exception? I definitely want to fix bad messages :-)","32","0.3792963188936346","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","575671166","issue_comment","https://github.com/Trusted-AI/AIF360/issues/245#issuecomment-575671166","The one which says: `fairlearn performance tests require azureml-sdk to be installed.`","29","0.1946441155743481","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","575671631","issue_comment","https://github.com/Trusted-AI/AIF360/issues/245#issuecomment-575671631","That is the truth AFAIK ","21","0.2027168234064786","Installation and shell commands","Deployment"
"https://github.com/fairlearn/fairlearn","575672489","issue_comment","https://github.com/Trusted-AI/AIF360/issues/245#issuecomment-575672489","The issue is I saw the message, installed `azureml-sdk`, re-ran the tests, and got the same message. The issue was that I didn't have `tempeh` installed.","29","0.6237212893263848","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","577219733","issue_comment","https://github.com/Trusted-AI/AIF360/issues/245#issuecomment-577219733","That's part of requirements.txt, though. Wouldn't it be expected to not be able to run things if you don't install the requirements?

The way the perf tests are set up right now is that you wouldn't run them locally anyway since that will take ages. So you really need some sort of cluster that takes care of it. We've chosen AzureML since it's the easiest option for us, but in theory you could plug in whatever you like. `tempeh` supports `azureml-sdk` right now, but you can feel free to add whatever other platforms you prefer. I am pretty much the only person behind `tempeh` right now, so anything useful you want to add is more than welcome. It's quite a bit hacky compared to `fairlearn`, though, because it's not the main priority.","15","0.2579453067257944","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","577309266","issue_comment","https://github.com/Trusted-AI/AIF360/issues/245#issuecomment-577309266","It'd be nice to have a little section for each part, saying what's needed for them, instead of referring people to the requirements.txt. As I mentioned, I ended up running all the tests with only a few of those packages listed in requirements.txt

For instance, you could say, 

```
Tests have extra dependencies, you can install them via:

    pip install tempeh etc etc
```","21","0.3421587004061231","Installation and shell commands","Deployment"
"https://github.com/fairlearn/fairlearn","577491484","issue_comment","https://github.com/Trusted-AI/AIF360/issues/245#issuecomment-577491484","By section you mean in the CONTRIBUTING guide, README, or in the test directory in a separate README? I think this is a great suggestion!","14","0.5057416267942585","Documentation","Development"
"https://github.com/fairlearn/fairlearn","577763955","issue_comment","https://github.com/Trusted-AI/AIF360/issues/245#issuecomment-577763955","I mean probably contributing.md, since people who are going to contribute are the people who are going to test the library.","13","0.5087976539589447","Artifact generation and benchmarking","Deployment"
"https://github.com/fairlearn/fairlearn","598502599","issue_comment","https://github.com/Trusted-AI/AIF360/issues/245#issuecomment-598502599","I guess we could rename this now since the perf test dependencies aren't the issue anymore, but rather test & notebook dependencies in general. The biggest factors IMO are tempeh which pulls in a bunch of things, and packages we use for smoke testing the compatibility light lightgbm.","13","0.5910776491740646","Artifact generation and benchmarking","Deployment"
"https://github.com/fairlearn/fairlearn","644427372","issue_comment","https://github.com/Trusted-AI/AIF360/issues/245#issuecomment-644427372","I keep coming back to this without a good resolution :-(

If I add instructions into the contributor guide about specific dependencies then I bet they'll be outdated pretty quickly.

`requirements.txt` at least specifies what the dependencies are for, so a selective install is possible.

I'm hoping to get rid of `tempeh` and `shap` as dependencies in there as soon as the datasets module exists and I can add a few datasets there.","4","0.2613091904673014","Installation and shell commands","Deployment"
"https://github.com/fairlearn/fairlearn","644608980","issue_comment","https://github.com/Trusted-AI/AIF360/issues/245#issuecomment-644608980","You may wanna look at this one: https://github.com/scikit-learn/scikit-learn/pull/17573

It's probably an overkill here though :D","5","0.5023388244864757","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","876068818","issue_comment","https://github.com/Trusted-AI/AIF360/issues/245#issuecomment-876068818","I think it's a good suggestion. It is quite a bit of engineering for this, but perhaps somebody wants to pick it up one day. 🙂 ","20","0.7749266862170091","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","876082971","issue_comment","https://github.com/Trusted-AI/AIF360/issues/245#issuecomment-876082971","Closing this since #645 includes this work.","24","0.1394984326018809","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","575309920","issue_comment","https://github.com/Trusted-AI/AIF360/issues/244#issuecomment-575309920","Anyone can add to this, but we try to focus on things that users request. Group fairness was what we started with, but if you want to add other kinds we'd love to have that as well!

The mitigation methods we have so far are for group fairness only.

We don't take a stand on what kind of fairness criteria is the right one. In fact, we quite explicitly say that fairness is a sociotechnical challenge, and every situation will require close examination by domain experts to figure out what makes sense. Group fairness is just one lens of assessing/mitigating unfairness, and often more tractable than individual fairness. That doesn't mean we want to discourage people from using other kinds. We explicitly encourage contributions of all kinds and I personally guarantee that I'll put in whatever time is necessary to get other techniques onboarded if anyone reaches out (at least unless we get flooded with such requests which I don't anticipate happening anytime soon). If you have ideas on what you'd like to add feel free to write a PR or set up some time to talk :-)","6","0.3412274000509295","API expansion","Development"
"https://github.com/fairlearn/fairlearn","575588323","issue_comment","https://github.com/Trusted-AI/AIF360/issues/244#issuecomment-575588323","That's awesome. I guess you can put that somehow in your ""terminology"" doc, since I got the question while reading that one.","25","0.6578237894963871","Research","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","575588536","issue_comment","https://github.com/Trusted-AI/AIF360/issues/244#issuecomment-575588536","I if you want I could also try and send a PR with your explanation modifying the terminology doc.","32","0.4496578690127077","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","575669770","issue_comment","https://github.com/Trusted-AI/AIF360/issues/244#issuecomment-575669770","Absolutely. I'm going to put out a PR with an update to markdown files (possibly README, but maybe also others) that shows clearly that we encourage participation and possible projects to get started with. This is something that needs to  be clarified along with that. Feel free to make some edits and send the PR :-)","20","0.5960307298335467","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","644425932","issue_comment","https://github.com/Trusted-AI/AIF360/issues/244#issuecomment-644425932","#462 explains this a little bit (mentioning other types of fairness). @riedgar-ms FYI in case you want to elaborate on this more in your PR(?)

In general I think we're doing a better job of writing about this now than we did at the point the question came up.","15","0.6621073116485959","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","644810355","issue_comment","https://github.com/Trusted-AI/AIF360/issues/244#issuecomment-644810355","I've tweaked the answer in the PR about this. Would welcome comments on the FAQ....","20","0.5383675464320626","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","808112746","issue_comment","https://github.com/Trusted-AI/AIF360/issues/244#issuecomment-808112746","Is this issue still relevant? @adrinjalali @romanlutz @riedgar-ms ","20","0.6933066933066933","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","808364538","issue_comment","https://github.com/Trusted-AI/AIF360/issues/244#issuecomment-808364538","I understand this as a question that turned into the realization that we should add/improve documentation including:
- what types of fairness are there (group vs. individual vs. ?)
- what do we have already and what can people add?

The existing documentation caused some confusion so we need to improve that.","25","0.3745474383772257","Research","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","575305208","issue_comment","https://github.com/Trusted-AI/AIF360/issues/243#issuecomment-575305208","Your observation is correct. This is somewhat related to https://github.com/fairlearn/fairlearn/issues/65, since this needs to be crystal clear to users, but we've discussed providing several options a few times. There should be an option to set the seed. That may break fairness guarantees, though, so we need to make sure to document this properly.","25","0.4896826462557628","Research","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","575587218","issue_comment","https://github.com/Trusted-AI/AIF360/issues/243#issuecomment-575587218","I'm still not sure how setting a seed would break the fairness guarantee here, since the output would still depend on how many `predict`s the user would have called before it gets to this particular input. Also, you can let the users set the seed, but be clear in the docs that in production they shouldn't (if that's the case).

But even then, I'm not sure how the user is supposed to handle the lack of reproducibility of the results.","23","0.5230924823013822","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","575635421","issue_comment","https://github.com/Trusted-AI/AIF360/issues/243#issuecomment-575635421","The lack of reproducibility is a concern of ours. It's a major motivation for us to get customer feedback on the exponentiated gradient algorithm in the Real World(tm). As @romanlutz says, we have been talking about the best way to expose the PRNG to the user to get reproducible results under 'lab' conditions at least.","11","0.2913358941527954","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","575671359","issue_comment","https://github.com/Trusted-AI/AIF360/issues/243#issuecomment-575671359","What I meant by breaking fairness guarantees is that if you provide the same seed all the time you've effectively derandomized the mitigated predictor. Some of these methods assume that randomization is allowed and don't achieve their objective if that's violated.

What you are saying is that you set the seed once in the beginning only, which is different from what I meant, and shouldn't break the guarantees.

I'll have to think about a good way of handling this and will propose a solution sometime next week (hopefully).","23","0.4862501668669071","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","575672106","issue_comment","https://github.com/Trusted-AI/AIF360/issues/243#issuecomment-575672106","> What you are saying is that you set the seed once in the beginning only, which is different from what I meant, and shouldn't break the guarantees.

The scope of the rng at least in sklearn is the model's lifetime, and not just a method call. If you want to pass sklearn's common tests, you kinda have to have it as a constructor parameter.","23","0.5185736249566038","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","575672933","issue_comment","https://github.com/Trusted-AI/AIF360/issues/243#issuecomment-575672933","I expect it would be. The RNG for predict would live for as long as the object. Each new incoming predict request (and if it's an array, each column of the array) needs a new random number, and you don't want to be reseeding it.","23","0.4333611342785654","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","576270314","issue_comment","https://github.com/Trusted-AI/AIF360/issues/243#issuecomment-576270314","I also found this one in our docs https://scikit-learn.org/stable/developers/develop.html#random-numbers","14","0.231645312182225","Documentation","Development"
"https://github.com/fairlearn/fairlearn","576436077","issue_comment","https://github.com/Trusted-AI/AIF360/issues/243#issuecomment-576436077","My original idea was to augment predict, so that we could call, e.g.,:
expgrad.predict(X, random_state=42)

This call format could co-exist with the sklearn convention. The idea is that if the predict method receives the random_state argument, it uses the RandomState object based on that argument rather than the one from expgrad.random_state_. In that case, expgrad.random_state_ does not advance.
","23","0.694231846182565","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","577220664","issue_comment","https://github.com/Trusted-AI/AIF360/issues/243#issuecomment-577220664","@MiroDudik I'm not particularly opposed to that, but I can't think of a scenario where it's useful. Can you elaborate?","15","0.6892177589852009","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","577681532","issue_comment","https://github.com/Trusted-AI/AIF360/issues/243#issuecomment-577681532","One simple scenario is the following:
- you are running a jupyter notebook
- you fit a mitigated estimator in one jupyter cell, it takes a while, but completes without any issues
- in the next jupyter cell, you call predict on the fitted estimator, but there is a bug in printing some kind of a statistic on the predictions
- you fix the bug and re-run the cell, but if the predictions are different, you cannot be quite sure whether you fixed the bug (which was somewhere in calculating the summary of predictions)

More generally--whenever there is any kind of a pipeline or additional processing / analysis done after you fit an estimator, I think that you would may want to replicate the behavior on the level of ""predict"". Debugging is one use case. Comparison of two strategies for postprocessing predictions might be another use case. (You want to compare postprocessing of the same predictions.)","23","0.3831050681487364","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","577767781","issue_comment","https://github.com/Trusted-AI/AIF360/issues/243#issuecomment-577767781","That's interesting. I'd say that's a case where a [advanced] user could capture the state of the rng and reset it before calling the predict again.

IMHO passing the rng or a seed to predict is pretty dangerous and may end up with people using it in a way they really shouldn't.

In the scenario you mention, you can still go back and re-run the whole pipeline and get the same result, if you could set the rng at the estimator level, and if you want to re-run a predict, you can set the seed from outside the estimator to force the re-run of the same path.

In contrast to your proposed API, if users set the rng at the estimator level, they will get two different outputs if they call `predict` on the same input, and still they have the reproducibility guarantees.

Also, note that not exposing the rng through the `__init__` doesn't mean it's really random. It means if the user needs to reproduce their results, they should set the global `numpy` seed or whichever rng you use internally in the estimator.","23","0.6725715712627978","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","577830198","issue_comment","https://github.com/Trusted-AI/AIF360/issues/243#issuecomment-577830198","Good point re. the ability to just explicitly reset the random seed before the call to predict. And I can also see how people might use the predict-level seed the wrong way.

How about this. Let's go with the `sklearn` convention, i.e., the `random_state` passed in `__init__`. If we ever need the reproducibility on the `predict` level, we can just explicitly reset the random seed on the rng being used. And if this comes up as a repeated issue, we can reconsider introducing `random_seed` override on the `predict` level.","23","0.795945806184714","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","577831146","issue_comment","https://github.com/Trusted-AI/AIF360/issues/243#issuecomment-577831146","So long as we're very sure that in `__init__` we take a reference to the PRNG, and not a copy.","1","0.3481324876673713","Fix warnings","Maintenance"
"https://github.com/fairlearn/fairlearn","579225725","issue_comment","https://github.com/Trusted-AI/AIF360/issues/243#issuecomment-579225725","in `sklearn` we never touch any of the given parameters in `__init__` anyway.

Regarding taking the reference, that's how it's done in [`check_random_state`](https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/utils/validation.py#L773) if the users passes a numpy `RandomState`.

The discussion around this may also interest you: https://github.com/scikit-learn/enhancement_proposals/pull/24","32","0.3198070672119115","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","808116352","issue_comment","https://github.com/Trusted-AI/AIF360/issues/243#issuecomment-808116352","Is this still relevant? If so, it would be great if someone can either list the steps we need to take to fix this issue (or maybe open a fresh issue). @MiroDudik @adrinjalali @romanlutz @riedgar-ms ","20","0.5153907496012758","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","808118430","issue_comment","https://github.com/Trusted-AI/AIF360/issues/243#issuecomment-808118430","Just a note that you could also do `@fairlearn/fairlearn-maintainers` instead of mentioning people one by one if you want to ping all maintainers ;) Typing at-sign-fairlearn would give you the autocomplete for the team's name.","20","0.7139188495120697","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","808123067","issue_comment","https://github.com/Trusted-AI/AIF360/issues/243#issuecomment-808123067","Oh, I didn't know that. Thanks!

(I actually tried tagging people who engaged in the discussion for each issue, which accidentally mostly coincided with most of the current maintainers :P ). ","32","0.2617255356108859","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","581400509","issue_comment","https://github.com/Trusted-AI/AIF360/issues/242#issuecomment-581400509","As much as possible this has been replaced for the moment. @adrinjalali and others may add to the Makefile as they see fit.","13","0.3140239605355884","Artifact generation and benchmarking","Deployment"
"https://github.com/fairlearn/fairlearn","587132194","issue_comment","https://github.com/Trusted-AI/AIF360/issues/238#issuecomment-587132194","hello @rihorn2!  Is the idea here that this is design work to make a new variant of `<WizardReport />` that would show different disparities at once, to better support directly comparing subgroup performance across each of them?  I'm imagining something like this, using a binary classifier as an example.  It's basically just collapsing the existing UI into a single layout:

![image](https://user-images.githubusercontent.com/1056957/74679343-99bfa880-518b-11ea-9cae-80b6c55ed10b.png)

Or is the idea more that a single visualization would allow comparing model performance for sensitive attributes across multiple measures of performance (eg, accuracy, precision, recall)?  Thanks! 👍 ","12","0.314673708034704","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","587514535","issue_comment","https://github.com/Trusted-AI/AIF360/issues/238#issuecomment-587514535","Hi Kevin, thank you for contributing to the repo! The idea with this item is to enable switching the metric here without going to the  'edit config' button and walking through the wizard again, just presenting the option in the metrics page of the wizard in a convenient dropdown. I believe Chinmay is including this in a PR he is working on.
Thanks,
Brandon","24","0.3453506493506494","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","590410314","issue_comment","https://github.com/Trusted-AI/AIF360/issues/238#issuecomment-590410314","@rihorn2 Ah thanks!  👍 I guess I missed this notification and also didn't notice https://github.com/fairlearn/fairlearn/pull/283 earlier.  Yeah, I see the kind of UX you are describing there:

<img width=""1029"" alt=""Screen Shot 2020-02-24 at 11 08 19 AM"" src=""https://user-images.githubusercontent.com/1056957/75169301-48b53480-56f6-11ea-89b3-a189619ace87.png"">

<img width=""1022"" alt=""Screen Shot 2020-02-24 at 11 08 39 AM"" src=""https://user-images.githubusercontent.com/1056957/75169252-363afb00-56f6-11ea-823a-2a399852a4f1.png"">

I'm mostly just being curious here, but am interested in learning more about the user need here if you or @chisingh could share more.  In other words, why are folks doing that kind of navigation?  I was assuming it was sort of building up context on multiple different perspectives on fairness, and so this question seemed super interesting for thinking about different ways to support that need directly.  But I am mostly just making guesses and so curious about what's actually come up for folks using fairlearn :)","25","0.5944645844952593","Research","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","728793908","issue_comment","https://github.com/Trusted-AI/AIF360/issues/238#issuecomment-728793908","This won't get addressed in this repo. Stay tuned for the `raiwidgets` release at the end of November!","24","0.8455896545068523","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","676445846","issue_comment","https://github.com/Trusted-AI/AIF360/issues/237#issuecomment-676445846","Hello,
I'm interested in this feature for Metrics.

If we take `group_summary` for example,

would the change be as simple as calling `np.unique(s_f, axis=0)` instead of `np.unique(s_f)`?
https://github.com/fairlearn/fairlearn/blob/master/fairlearn/metrics/_metrics_engine.py#L66
","15","0.8033917153183205","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","728795218","issue_comment","https://github.com/Trusted-AI/AIF360/issues/237#issuecomment-728795218","Sorry for missing your message, @Dref360 ! As you probably observed this went along with a complete redesign of the metrics API.

I'm closing this issue now since the remaining portion (dashboard integration) will not happen in this repo, but in the `raiwidgets` package's repo which will be released at the end of November.","24","0.526437050921587","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","876067033","issue_comment","https://github.com/Trusted-AI/AIF360/issues/236#issuecomment-876067033","Plotting cumulative distribution functions by group could be useful for regression scenarios. We've actually done this in a [notebook](https://github.com/fairlearn/fairlearn/blob/main/notebooks/Mitigating%20Disparities%20in%20Ranking%20from%20Binary%20Data.ipynb) (search for `plot_and_compare_cdfs`).

This [post](https://stackoverflow.com/questions/25577352/plotting-cdf-of-a-pandas-series-in-python) has some interesting responses that may be simpler since it would just involve using the prediction `pandas.Series` and its plotting functionality.

I imagine a function for this could live in the `metrics` module (or `experimental` if people want to be careful). Input parameters would include `y` and `sensitive_features`.","15","0.4394316610084247","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","838476761","issue_comment","https://github.com/Trusted-AI/AIF360/issues/235#issuecomment-838476761","@hildeweerts I think your comment on #796 is relevant here too (guidance would be needed around how to understand the results)","20","0.3165933528836756","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","839813385","issue_comment","https://github.com/Trusted-AI/AIF360/issues/235#issuecomment-839813385","There are a few potential implementations of Error bars/ Confidence bounds. My question is, how do we present them in a way that is clear and actionable. An example of one potential implementation (using the MetricFrame, though you could imagine this being woven into the plots as well) is below. 


**Wilson bounds** 
(@LeJit and Duncan)

```
def wilson(p, n, z = z_score):
    denominator = 1 + z**2/n
    centre_adjusted_probability = p + z*z / (2*n)
    adjusted_standard_deviation = np.sqrt((p*(1 - p) + z*z / (4*n)))/np.sqrt(n)
    lower_bound = (centre_adjusted_probability - z*adjusted_standard_deviation) / denominator
    upper_bound = (centre_adjusted_probability + z*adjusted_standard_deviation) / denominator
    return (lower_bound, upper_bound)
```
   
Using the above, you could then define functions for each bound, for a specific metric, and pass that into the metric frame. 

```
The output of the Metric Frame using mock data with race and gender as sensitive features, and a z-score of 1.96 (p-value of 0.05) 
```
![Bounds (snipped)](https://user-images.githubusercontent.com/69976597/118151066-3994d200-b3e1-11eb-95a4-5cccf36e8ccd.PNG)

The issue with the above is that MetricFrame will run into issues with equality and difference operators


","3","0.544949852887486","Bias detection metrics validation","Validation"
"https://github.com/fairlearn/fairlearn","841208815","issue_comment","https://github.com/Trusted-AI/AIF360/issues/235#issuecomment-841208815","Thank you for reanimating this discussion @michaelamoako !

As I said in the community call, I think that the statistical issues are too fraught for us to try offering error bounds intrinsically within `MetricFrame`. However, we absolutely should have examples for users to follow, similar to the one you give above. And as @MiroDudik mentioned on the call, we can adjust the `MetricFrame` so that when there are non-comparable columns (such as the confidence bounds, or a confusion matrix), a  `NaN` is returned, rather than throwing an exception.","15","0.3294642604987432","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","853020166","issue_comment","https://github.com/Trusted-AI/AIF360/issues/235#issuecomment-853020166","Talking to @alexquach about working on this.","15","0.5761643278421804","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","853026695","issue_comment","https://github.com/Trusted-AI/AIF360/issues/235#issuecomment-853026695","It would be good if three different formats for the errors:
 
- `+/- number`
- `+ number`, `- number`
- `lower bound, upper bound` (as @michaelamoako has above)
","3","0.6784063394232883","Bias detection metrics validation","Validation"
"https://github.com/fairlearn/fairlearn","876416879","issue_comment","https://github.com/Trusted-AI/AIF360/issues/235#issuecomment-876416879","I'm working on this.","20","0.2027168234064786","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","574248289","issue_comment","https://github.com/Trusted-AI/AIF360/issues/234#issuecomment-574248289","@Bee-zest thank you for raising this issue!
Can you please add 
- your fairlearn version
- whether you've made changes to fairlearn or not
- the way you're calling it
- any other context that may be useful for us to help you debug this.

Thanks!","24","0.3316078938854738","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","578953352","issue_comment","https://github.com/Trusted-AI/AIF360/issues/234#issuecomment-578953352","@Bee-zest Is this repeatable on your end? Try to replace `'simplex'` with `'revised simplex'` on lines 119 and 129 of `fairlearn/reductions/_exponentiated_gradient/_lagrangian.py`. Both of the LPs are bounded and feasible, so it should also be okay to slightly increase the tolerance.","10","0.7197440240918503","Model development","Development"
"https://github.com/fairlearn/fairlearn","580318729","issue_comment","https://github.com/Trusted-AI/AIF360/issues/234#issuecomment-580318729","I've managed to reproduce this under specific circumstances, e.g. when taking our ranking/lawschool notebook and binning by gender & race I hit this with expgrad. I'll try revised simplex as soon as I get time.","8","0.3515256369927649","Understanding Fairness Definition","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","582247053","issue_comment","https://github.com/Trusted-AI/AIF360/issues/234#issuecomment-582247053","I found that upgrading to `scipy==1.4.1` solved this in my case. I had `scipy==1.3`. Not sure though if that is the reason. Worth a try.","32","0.4368847712796758","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","582477473","issue_comment","https://github.com/Trusted-AI/AIF360/issues/234#issuecomment-582477473","I'm on `scipy==1.3.1` and saw this issue as well. Will try and see whether this issue repros for me with newer versions. If so, we can bump the requirements up.","32","0.3577051655343469","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","590990508","issue_comment","https://github.com/Trusted-AI/AIF360/issues/234#issuecomment-590990508","so far I increase the tolerance value. I would like to see if it can be sloved by upgrading ```scipy==1.4.1```","10","0.4845665961945032","Model development","Development"
"https://github.com/fairlearn/fairlearn","644424663","issue_comment","https://github.com/Trusted-AI/AIF360/issues/234#issuecomment-644424663","I can confirm my previous repro doesn't work anymore, so I'll close this issue. Python 3.5 support was removed recently and `scipy<1.4.1` is a thing of the past in this repo. Please reopen if you see this issue again. Thanks!","24","0.6150125104253543","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","644416778","issue_comment","https://github.com/Trusted-AI/AIF360/issues/232#issuecomment-644416778","This has partially been addressed through #241 #364 #424 and #443 but could use a little more work to make it intuitive what these functions actually do.","15","0.5119880119880119","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","753052617","issue_comment","https://github.com/Trusted-AI/AIF360/issues/231#issuecomment-753052617","Closing this since we have tests for pytorch and tensorflow due to #637 #586 ","28","0.4163495419309372","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","581396085","issue_comment","https://github.com/Trusted-AI/AIF360/issues/227#issuecomment-581396085","This was completed with https://github.com/fairlearn/fairlearn/commit/090664a31b22eda35cd36e4266fb3f121c409da9 and the subsequent release of fairlearn v0.4.2 last week. We decided not to create a fairlearn-core package, but instead just separated the matplotlib dependency into and extension of the fairlearn package. The blocker for this milestone was that a yarn build was required which meant some generated files were missing for conda-forge. This is not the case anymore now since we're checking those files in.","24","0.3004939260445869","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","581407041","issue_comment","https://github.com/Trusted-AI/AIF360/issues/225#issuecomment-581407041","This was completed with PR #228 ","24","0.1711076280041797","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","570566891","issue_comment","https://github.com/Trusted-AI/AIF360/issues/220#issuecomment-570566891","@riedgar-ms did this actually! Can you confirm @riedgar-ms ?","24","0.6007728289607486","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","570575193","issue_comment","https://github.com/Trusted-AI/AIF360/issues/220#issuecomment-570575193","yeah it's there.","17","0.1507849580138736","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","566215004","issue_comment","https://github.com/Trusted-AI/AIF360/issues/207#issuecomment-566215004","@adrinjalali might help us with this.","20","0.239138371668492","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","570548726","issue_comment","https://github.com/Trusted-AI/AIF360/issues/207#issuecomment-570548726","its development can be tracked under https://github.com/conda-forge/staged-recipes/pull/10518","20","0.3054708155379297","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","581395109","issue_comment","https://github.com/Trusted-AI/AIF360/issues/207#issuecomment-581395109","This is completed by @adrinjalali !","20","0.2975444096133751","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","570167201","issue_comment","https://github.com/Trusted-AI/AIF360/issues/200#issuecomment-570167201","you could alternatively use `.rst`, which is the same across platforms AFAIK.","11","0.3508158508158507","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","570596056","issue_comment","https://github.com/Trusted-AI/AIF360/issues/200#issuecomment-570596056","Have done this for the current `ReadMe.md` file.","21","0.3792963188936344","Installation and shell commands","Deployment"
"https://github.com/fairlearn/fairlearn","570595873","issue_comment","https://github.com/Trusted-AI/AIF360/issues/199#issuecomment-570595873","Better idea: record the output of `pip freeze` along with the build. Pinning everything will create its own issues.","21","0.3551437946342405","Installation and shell commands","Deployment"
"https://github.com/fairlearn/fairlearn","570596237","issue_comment","https://github.com/Trusted-AI/AIF360/issues/195#issuecomment-570596237","This has been fixed","24","0.1507849580138736","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","561857634","issue_comment","https://github.com/Trusted-AI/AIF360/issues/95#issuecomment-561857634","This currently throws a TypeError in the first fit() method of the Binary Classification on COMPAS dataset notebook. Running v 0.3 of fairlearn on DSVM","0","0.4919656614571868","Dataset usage","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","568814963","issue_comment","https://github.com/Trusted-AI/AIF360/issues/95#issuecomment-568814963","I just ran this and I don't see a type error. Maybe you're running with an older version of tempeh? Try getting the latest fairlearn (0.4.0) and tempeh (0.1.11). @paramrsingh  let me know if you keep seeing the issue, and ideally also the version of tempeh. Thanks!","29","0.6610952575078136","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","570169359","issue_comment","https://github.com/Trusted-AI/AIF360/issues/95#issuecomment-570169359","Not sure if it's relevant here, but in sklearn this is what we do to [check the classification output](https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/svm/_base.py#L523):

``` python
        y_ = column_or_1d(y, warn=True)
        check_classification_targets(y)
        cls, y = np.unique(y_, return_inverse=True)
```","26","0.5185736249566036","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","572738811","issue_comment","https://github.com/Trusted-AI/AIF360/issues/95#issuecomment-572738811","It's mostly a question of error handling since there's no particularly good reason to support this. We should check for this and raise an Exception to let the user know that sensitive feature labels shouldn't be float (or if this is something we want then enable that to work).","26","0.3170114506231417","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","876057997","issue_comment","https://github.com/Trusted-AI/AIF360/issues/95#issuecomment-876057997","To recap: we should add code to check whether the targets (`y`) are valid. We can use scikit-learn's `type_of_target`: https://github.com/scikit-learn/scikit-learn/blob/f2a6e109f7be6f0d554e44cb4cb48b41081dc259/sklearn/utils/multiclass.py#L201 
For mitigation techniques only binary classification and regression are currently supported. If `type_of_target` returns anything else we should raise an error.
This contribution should also have a set of tests to cover the various cases (binary and multi-class classification, regression, multi-label, etc.)","28","0.4297156587905485","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","544687652","issue_comment","https://github.com/Trusted-AI/AIF360/issues/90#issuecomment-544687652","I am assuming that you are talking about ensuring that demographic parity holds if you threshold at 0.65. You can definitely modify the algorithm from the paper to do this. However, note that our package currently does not cover the fair regression for demographic parity, only the fair regression algorithm that seek to achieve bounded group loss.","6","0.5632554112554116","API expansion","Development"
"https://github.com/fairlearn/fairlearn","544695749","issue_comment","https://github.com/Trusted-AI/AIF360/issues/90#issuecomment-544695749","> I am assuming that you are talking about ensuring that demographic parity holds if you threshold at 0.65. 


Yes. In fairlearn paper, the loss is  `err(Q) + DP`, and the `err(Q)` is the misclassification error.  If I want to optimize 0.65 cutoff under DP constraints, is it ok to change the fairness constraints part `DP` to support the 0.65 cutoff instead of the 0.5? ","6","0.5171601731601734","API expansion","Development"
"https://github.com/fairlearn/fairlearn","544738129","issue_comment","https://github.com/Trusted-AI/AIF360/issues/90#issuecomment-544738129","is err(Q) misclassification at 0.5 or at 0.65?","6","0.5820271682340651","API expansion","Development"
"https://github.com/fairlearn/fairlearn","544752974","issue_comment","https://github.com/Trusted-AI/AIF360/issues/90#issuecomment-544752974","> is err(Q) misclassification at 0.5 or at 0.65?

It's at 0.5. I don't think 0.65 cutoff makes any sense for misclassification error. ","6","0.7883101477407524","API expansion","Development"
"https://github.com/fairlearn/fairlearn","644413349","issue_comment","https://github.com/Trusted-AI/AIF360/issues/90#issuecomment-644413349","Closing this since it doesn't relate to current implementations and has gone stale since. If this is still an area of interest please reopen. Thanks!","24","0.6003667245705462","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","540189564","issue_comment","https://github.com/Trusted-AI/AIF360/issues/80#issuecomment-540189564","Fixed in #81.","24","0.1507849580138736","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","540183899","issue_comment","https://github.com/Trusted-AI/AIF360/issues/78#issuecomment-540183899","How did you install the `FairnessDashboard` ? You should be doing something like:

pip install 'azureml-contrib-explain-model<=0.1.50' --extra-index-url 'https://azuremlsdktestpypi.azureedge.net/azureml-core-run-submit-tests/5132569'

The `FairnessDashboard` is very much in preview right now (everything is subject to change), hence the unusual installation method.","21","0.3395478219696971","Installation and shell commands","Deployment"
"https://github.com/fairlearn/fairlearn","540201863","issue_comment","https://github.com/Trusted-AI/AIF360/issues/78#issuecomment-540201863","I tried to install the package as you suggested: azureml-contrib-explain-model<=0.1.50 But  The system cannot find the file specified.

When i show the azureml-contrib-explain-model, i have installed the oldest available one:

![image](https://user-images.githubusercontent.com/45678322/66520785-d4b42000-eab7-11e9-8ad2-4995baa1a510.png)
","21","0.7083732057416269","Installation and shell commands","Deployment"
"https://github.com/fairlearn/fairlearn","540544409","issue_comment","https://github.com/Trusted-AI/AIF360/issues/78#issuecomment-540544409","Yes; I was told that the dashboard is internal-only for now.

You can comment out the lines about the dashboard in the notebook, and the rest should work.","24","0.5012401596031486","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","540729069","issue_comment","https://github.com/Trusted-AI/AIF360/issues/78#issuecomment-540729069","Ok, thanks! ","0","0.0303030303030302","Dataset usage","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","548090645","issue_comment","https://github.com/Trusted-AI/AIF360/issues/78#issuecomment-548090645","Is there an expected timeline for when the DashBoard will be available to the public?","24","0.7134532134532134","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","548648357","issue_comment","https://github.com/Trusted-AI/AIF360/issues/65#issuecomment-548648357","API documentation exists, but it doesn't reflect randomization well yet.","14","0.4504580690627202","Documentation","Development"
"https://github.com/fairlearn/fairlearn","576977087","issue_comment","https://github.com/Trusted-AI/AIF360/issues/65#issuecomment-576977087","For ExponentiatedGradient, what is the preferred way of getting predicted **probabilities**?
Can we use `_best_classifier`?

The notebook example [link](https://github.com/fairlearn/fairlearn/blob/master/notebooks/Mitigating%20Disparities%20in%20Ranking%20from%20Binary%20Data.ipynb) expects `predict` to return scores for `LogisticRegression`:

`scores_expgrad_X = pd.Series(expgrad_X.predict(X_test), name=""scores_expgrad_X"")`

However, the result seems to be the predicted class.","10","0.3988597313339583","Model development","Development"
"https://github.com/fairlearn/fairlearn","577243820","issue_comment","https://github.com/Trusted-AI/AIF360/issues/65#issuecomment-577243820","Right. For probabilities we currently provide `_pmf_predict` from https://github.com/fairlearn/fairlearn/blob/master/fairlearn/reductions/_exponentiated_gradient/exponentiated_gradient.py but as you can tell from the underscore there's no guarantee it'll stay that way.","10","0.4720070661774697","Model development","Development"
"https://github.com/fairlearn/fairlearn","579967910","issue_comment","https://github.com/Trusted-AI/AIF360/issues/65#issuecomment-579967910","@romanlutz I looked at `_pmf_predict` in Exponentiated Gradient. I have a question on how the predictions from the returned classifiers are combined to get predicted probabilities.

**Current**
In `_mean_pred`[function](https://github.com/fairlearn/fairlearn/blob/master/fairlearn/reductions/_exponentiated_gradient/exponentiated_gradient.py#L17), the approach taken is to  call `predict` and then weight the result. This does not pass on the 'full uncertainty information' from the classifiers. As `predict` only returns 0 or 1 instead of probabilities. Encountered this in an example where the `_classifiers` had only one classifier. Then, the result from `_pmf_predict` had only 0s and 1s which is what `predict` returns. This is an issue.

**Alternative**
Instead, one can call `predict_proba` for each classifier and then weight. In expectation, the results should be same i.e. expectation of the two variables `weight*Bernoulli(prob)` and `Bernoulli(weight*prob)` is `weight*prob`.

Is this a better way for getting predicted probabilities? Trying it on the [notebook example](https://github.com/fairlearn/fairlearn/blob/master/notebooks/Mitigating%20Disparities%20in%20Ranking%20from%20Binary%20Data.ipynb) seem to give better AUC and disparity values. Did not do a thorough test.

```
def weighted_score(dataset, algorithm):
    pred_all = np.zeros((dataset.shape[0],len(algorithm._classifiers)))
    for i,clf in enumerate(algorithm._classifiers):
        pred_all[:,i] = clf.predict_proba(dataset)[:,1]

    score = pred_all.dot(algorithm._expgrad_result._weights)

    return score

scores_expgrad_X = pd.Series(weighted_score(X_test, expgrad_X), name=""scores_expgrad_X"")
```","10","0.55163004452104","Model development","Development"
"https://github.com/fairlearn/fairlearn","644412569","issue_comment","https://github.com/Trusted-AI/AIF360/issues/65#issuecomment-644412569","@MiroDudik what do you think of @harvineet 's idea above? I totally missed the comment... apologies.","20","0.4490456163054027","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","548648239","issue_comment","https://github.com/Trusted-AI/AIF360/issues/64#issuecomment-548648239","seems like all occurrences are gone","14","0.3274917853231109","Documentation","Development"
"https://github.com/fairlearn/fairlearn","542428600","issue_comment","https://github.com/Trusted-AI/AIF360/issues/62#issuecomment-542428600","Actually both BGL and demographic parity are group-fairness criteria. The terminology has been updated to reflect that. Currently we don't support any individual fairness, so we don't go into it in terminology.","6","0.43912216333741","API expansion","Development"
"https://github.com/fairlearn/fairlearn","594803613","issue_comment","https://github.com/Trusted-AI/AIF360/issues/61#issuecomment-594803613","Subtask - #316 ","5","0.4158451989777296","Troubleshooting","Maintenance"
"https://github.com/fairlearn/fairlearn","623654574","issue_comment","https://github.com/Trusted-AI/AIF360/issues/61#issuecomment-623654574","@riedgar-ms I've committed a PR for this, but won't get to more for the next few weeks. If you want to wait for this with a release, your release may not happen for a long time...","32","0.7938219972118277","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","644411440","issue_comment","https://github.com/Trusted-AI/AIF360/issues/61#issuecomment-644411440","This is represented through #316 so closing this issue","6","0.1711076280041797","API expansion","Development"
"https://github.com/fairlearn/fairlearn","536095813","issue_comment","https://github.com/Trusted-AI/AIF360/issues/56#issuecomment-536095813","It's a reasonable approach, but I'm not sure whether this will work with the current implementation without any code modifications (it hasn't been tested). Depending on the pattern of missingness, the statistical guarantees for the disparity on the test data will be affected though.","6","0.3997219905476783","API expansion","Development"
"https://github.com/fairlearn/fairlearn","535077713","issue_comment","https://github.com/Trusted-AI/AIF360/issues/38#issuecomment-535077713","Fixed in #39 ","24","0.1507849580138736","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","529632109","issue_comment","https://github.com/Trusted-AI/AIF360/issues/23#issuecomment-529632109","I think there are two ways of incorporating sample weights:

1. Sample weights should be only applied to the error metric, but not the fairness metric. The use case: such sample weights allow for varying misclassification costs on a per-example basis. However, the fairness metrics (such as demographic parity) are still standard.

2. Sample weights apply to both the error metric and the fairness metric. Here, a natural use case is covariate shift. For example, the data might be collected iid according to some data collection distribution, but we would like to minimize the error and disparity under some target distribution. The weights would then correspond to the ratios between the target and the data collection probabilities.

@Bee-zest: Which of the two scenarios did you have in mind? I think both might make sense in practice, and long term it would be nice enable both. ","6","0.2434734308461396","API expansion","Development"
"https://github.com/fairlearn/fairlearn","529666027","issue_comment","https://github.com/Trusted-AI/AIF360/issues/23#issuecomment-529666027","Good points. I think the 2nd one is my case now. I did some experiments by adding one line after the classred.py on line 149. `np.multiply(signed_weights, self.custom_weights)`.But it seems it doesn't work.  Is there any suggestion to correctly implement the custom weights? Thanks","28","0.2425234340127956","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","531283398","issue_comment","https://github.com/Trusted-AI/AIF360/issues/23#issuecomment-531283398","I think that the simplest stop-gap solution is to take the weighted data and resample it using cost-proportionate-sampling and then just run the algorithm on the resampled data.

More precisely, if the weight of an example is W, you can resample it with the probability cW where c is chosen for the whole data set, so that the probability is never larger than 1. This may substantially shrink your data size. If you're okay increasing your data size, you can also resample the data using something that looks more like ""randomized rounding"". You can pick a scaling constant s, and then first replicate the example floor(sW) times, and then include one more copy of the example with the probability prob=sW-floor(sW).

To do things properly, you would need to make modifications not just in classred.expgrad, but also in moments.DP and moment.MisclassificationError","23","0.6727359122227264","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","531314496","issue_comment","https://github.com/Trusted-AI/AIF360/issues/23#issuecomment-531314496","Resampling makes sense.  I have two questions.
1. For the 1st approach  
> resample it with the probability 

Here you mean resample with or without replacement? 

2.

> To do things properly, you would need to make modifications not just in classred.expgrad, but also in moments.DP and moment.MisclassificationError

 Couldn't I just preprocess (applying the resampling method) the dataX, dataA, dataY before fitting them into the expgrad? Or the resampling should be done in every call of the learner? 


","23","0.5899913051879045","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","539160964","issue_comment","https://github.com/Trusted-AI/AIF360/issues/23#issuecomment-539160964","Re 1: For strategy with cW, it's a rejection sampling, so you go one example at a time and either accept or reject (so there will be no repetitions). For the second strategy, you also go on example at a time--there you replicate each example floor(sW) times and then randomly add one more copy with probability sW-floor(sW)--so here you might have some repetitions (up to ceil(sW) for each example).

Re 2: yes--if you do resampling, you don't need to do anything else--but the problem is that you are throwing away a lot of data. As an alternative to resampling, you could just make sure that moments.DP and moments.MisclassificationError natively work with the weights.","23","0.4729870936767488","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","570170599","issue_comment","https://github.com/Trusted-AI/AIF360/issues/23#issuecomment-570170599","A somewhat related discussion which might help: https://github.com/scikit-learn/scikit-learn/issues/15657","23","0.3300793166564979","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","517680884","issue_comment","https://github.com/Trusted-AI/AIF360/issues/5#issuecomment-517680884","@MatthijsdeJ does the best classifier return probabilities? I'm struggling to get the probabilities; right now, I can only get it to return binary predictions.","23","0.7373205741626795","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","522025370","issue_comment","https://github.com/Trusted-AI/AIF360/issues/5#issuecomment-522025370","The `result.best_classifier` is in general a **randomized** classifier. When it returns a number between 0 and 1, it should be interpreted as the probability with which the decision maker should assign class 1, so the sampling that @MatthijsdeJ describes is the correct thing to do.

The number that it returns should **not be interpreted** as the probability that the example belongs to class 1. That's why thresholding at 0.5 is not justified. The actual reason for randomization is to enable the full range of accuracy / fairness tradeoffs. Randomization can also help mitigate unfairness without affecting accuracy: for example by randomizing between two classifiers that have the same accuracy, but differ in which of the subpopulation they put at an advantage it is possible to improve fairness without affecting accuracy.

In practice, it can be desirable to use deterministic classifiers. In that case, you can look at  `result.classifiers` to check whether any of them works. These are the classifiers that are used to compose the output of `best_classifier` according to weights in `result.weights`.","23","0.5688475878115101","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","522973704","issue_comment","https://github.com/Trusted-AI/AIF360/issues/5#issuecomment-522973704","I've marked this as a feature request to add a classifier to the result that automatically provides the binary output by randomly taking the result of one of the selected classifiers based on their weights.

We'll likely get to this within the next few weeks.","23","0.4847436300723498","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","551208669","issue_comment","https://github.com/Trusted-AI/AIF360/issues/5#issuecomment-551208669","Our `predict` method in `ExponentiatedGradient` should be doing this now. @MiroDudik can you sanity check that I'm not confusing myself?","23","0.331789229878402","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","602841336","issue_comment","https://github.com/Trusted-AI/AIF360/issues/5#issuecomment-602841336","A lot of this code has changed since the issue was created. I think #65 tackles the remaining problem more holistically by asking to provide explanations for what to expect from the prediction-related methods.","20","0.3897948090773202","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","481294629","issue_comment","https://github.com/Trusted-AI/AIF360/issues/4#issuecomment-481294629","From the docs:

> The function expgrad in the module fairlearn.classred implements the reduction of fair classification to weighted binary classification. **Any learner that supports _weighted binary classification_ can be provided** as input for this reduction

Seems like your classifier's fit function needs to have room for weights. 


","28","0.4920173160173161","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","481297760","issue_comment","https://github.com/Trusted-AI/AIF360/issues/4#issuecomment-481297760","That is indeed correct. See the documentation of expgrad:

    Required input arguments:
      ...
      learner -- a learner implementing methods fit(X,Y,W) and predict(X),
                 where X is the DataFrame of covariates, and Y and W
                 are the Series containing the labels and weights,
                 respectively; labels Y and predictions returned by
                 predict(X) are in {0,1}

But, it's a good idea to provide a more informative error message. Also,
maybe at some point in future, it might be nice to have wrapper that would
convert a standard (non-weighted) learner into a weighted learner.

On Tue, Apr 9, 2019 at 11:13 AM Kanishk Vashisht <notifications@github.com>
wrote:

> From the docs:
>
> The function expgrad in the module fairlearn.classred implements the
> reduction of fair classification to weighted binary classification. *Any
> learner that supports weighted binary classification can be provided* as
> input for this reduction
>
> Seems like your classifier's fit function needs to have room for weights.
>
> —
> You are receiving this because you are subscribed to this thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/Microsoft/fairlearn/issues/4#issuecomment-481294629>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/ATnNxCuK1KlBd5AQxUHmUPqUcOaiZiIkks5vfK4VgaJpZM4bx7Cq>
> .
>
","28","0.3444674455867046","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","521760283","issue_comment","https://github.com/Trusted-AI/AIF360/issues/4#issuecomment-521760283","I marked it as a feature-request. I cannot provide an estimate at this point, but will update as soon as this gets scheduled.","32","0.4751428879542758","Dependency and Release","Deployment"
"https://github.com/fairlearn/fairlearn","695921719","issue_comment","https://github.com/Trusted-AI/AIF360/issues/4#issuecomment-695921719","@Pooja1301 is looking into this now for `ExponentiatedGradient`, and then same resampling should be applicable to `GridSearch` subsequently. Stay tuned :-)","28","0.3201119475004826","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","698955245","issue_comment","https://github.com/Trusted-AI/AIF360/issues/4#issuecomment-698955245","I have started looking into it.","20","0.1949616648411828","Opinion","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","744755141","issue_comment","https://github.com/Trusted-AI/AIF360/issues/4#issuecomment-744755141","@violazhong @riedgar-ms and I discussed this earlier today. Capturing some of our notes.

The most important aspect of this work is passing in the resampling method. @riedgar-ms pointed out that this should probably be in the constructor or the `fit` method. Since we're passing the `estimator` in the constructor that seems to be the most suitable place IMO. Once we have that on the object we'd have to replace this line https://github.com/fairlearn/fairlearn/blob/d1581b4576fa5c4dbb0fb99bf5548c74ba05b0ac/fairlearn/reductions/_exponentiated_gradient/_lagrangian.py#L178
with something like this (pardon the pseudocode at times):
```
if estimator has fit function and fit function has sample_weight arg:
     estimator.fit(self.X, redY, **{self.sample_weight_name: redW})
elif user has provided resampling_method:
     resampled_X, resampled_Y = resampling_method(X, redY, redW)
     estimator.fit(resampled_X, resampled_Y)
else:
    raise exception and describe that we need sample_weight arg or resampling_method
```

I see a few different ways to do the resampling. I'm sure there are way more complicated ones as well.

### deterministic and keeping the same size
- scale weights so that the sum of all weights is equal to the number of rows in X
- add floor(scaled_weight) samples of each row to a new resampled dataset, e.g. if rescaled_weight=2.7 -> add 2
- order the residuals, so for our 2.7 that would be 0.7
- fill the remaining slots up starting from the largest residuals

Example:
```
dataset = [s1,s2,s3,s4,s5]
weights = [1, 2, 3, 4, 5]
scaled_weights = [1/3, 2/3, 1, 4/3, 5/3]
add one of samples s3, s4, and s5, then order residuals:
[1/3, 2/3, 0, 1/3, 2/3]
We have two slots left so those would go to samples s2 and s5 with the final resampled dataset:
[s3,s4,s5,s2,s5] (ordering doesn't matter)
```

### deterministic but not keeping the same size
- find smallest weight
- scale all weights up so that min = 1
- take `floor(weight)` and replicate each sample that many times

Example:
```
dataset = [s1,s2,s3,s4,s5]
weights = [0.1, 0.29, 1.3, 3.41, 9.54]
scaled_weights = [1, 2.9, 13, 34.1, 95.4]
final data set has s1 x 1, s2 x 2, s3 x 13, s4 x 34, s5 x 95
```
Of course, using `floor` is quite arbitrary here. This could just as well be cutting off at <0.5.

---

@Pooja1301  if you're still interested in picking up a part of this task, or a completely different one, please let me know and we can discuss. There are a ton of nice tasks coming up with plots that I believe will be fairly easy to get started on, so that should make it a good experience as a new contributor. Please let me know :-)","23","0.5049010575966395","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","757035577","issue_comment","https://github.com/Trusted-AI/AIF360/issues/4#issuecomment-757035577","[Edited. I think that my original sampling suggestion wasn't actually more sample efficient than the deterministic approach.]

I'm sorry--I'm a bit late to the discussion.

The deterministic strategy above would not be my first choice, because it's less sample efficient than the following randomized strategy guaranteed to always generate N data points:
* Rescale the weights so that they sum to N; let w_i be the rescaled weights.
* From each original data point i, deterministically generate floor(w_i) examples.
* Let w’_i = w_i-floor(w_i). Let N’ = sum_i w’_i (this is the number of examples that we still need to generate).
* Represent each example i by a line segment of length w'_i and put them all next to each other on a numerical axis, with the first line segment beginning at zero and the last line segment ending at N'.
* Draw N' numbers independently uniformly from [0,N'] and generate examples corresponding to where these numbers land on the numerical axis.

By the way, by sample-efficient I mean: how large N do you need to guarantee that a resampling wrapper (applied to a sample-weights-unaware learning algorithm) returns a model that's epsilon-close to the model that optimizes the weighted objective.

What are the benefits of a deterministic strategy over randomized? One that I can think of is reproducibility, but that can be handled by a random seed.

Re. API for the wrapper, I was thinking about something like:
```python
class Resampler(BaseEstimator):
    def __init__(self, estimator, random_state=None, n=None): 
    ...
    
    def fit(self, X, y, sample_weights=None, **kwargs):
    ...
```
If `n=None`, it just means that the data set of the same size as provided will be generated. We could also describe the data set size by specifying a scaling factor relative to the provided data (and default to 1).","23","0.4331432878667596","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","758127242","issue_comment","https://github.com/Trusted-AI/AIF360/issues/4#issuecomment-758127242","All of this sounds reasonable and is fairly close to @violazhong 's current implementation. @violazhong please let us know if any of this doesn't make sense. Happy to elaborate!","15","0.5097219165015775","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","759598999","issue_comment","https://github.com/Trusted-AI/AIF360/issues/4#issuecomment-759598999","Just a note that the solution would need to be a meta-estimator wrapping another estimator to pass the child estimator the modified samples. The scikit-learn API doesn't allow resampling as is, and the meta-estimator is the workaround.","23","0.5477342229635805","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","771144936","issue_comment","https://github.com/Trusted-AI/AIF360/issues/4#issuecomment-771144936","Re. meta-estimator... good point!","23","0.4285133211307709","Bias mitigation methodology","Design"
"https://github.com/fairlearn/fairlearn","790185707","issue_comment","https://github.com/Trusted-AI/AIF360/issues/4#issuecomment-790185707","@MiroDudik, could you provide more references (articles, papers, etc.) for the last two bullet points in your last comment? ""epsilon-close"" sounds like an important thing to make sure the resampling implementation does not mess up our reduction algorithm. 

@riedgar-ms  @romanlutz in case you are also interested... ","15","0.4708593335780417","Metrics operation","Validation"
"https://github.com/fairlearn/fairlearn","522042278","issue_comment","https://github.com/Trusted-AI/AIF360/issues/3#issuecomment-522042278","If the question is about interpreting the output of `best_classifier`, please see #5 .

However, I am assuming that your question is about optimizing a loss other than **misclassification error**, for example, when training a logistic regression model to optimize likelihood.

Our method currently only handles misclassification error. If you’re interested in optimizing more general objectives under fairness constraints, one of these papers might be helpful:

* [Alabi et al. 2018](http://proceedings.mlr.press/v75/alabi18a/alabi18a.pdf)
* [Narasimhan 2018](http://proceedings.mlr.press/v84/narasimhan18a/narasimhan18a.pdf)
* [Cotter et al. 2019](http://proceedings.mlr.press/v97/cotter19b/cotter19b.pdf)
* [Agarwal et al. 2019](http://proceedings.mlr.press/v97/agarwal19d/agarwal19d.pdf)
","6","0.5016731734523142","API expansion","Development"
"https://github.com/fairlearn/fairlearn","522186284","issue_comment","https://github.com/Trusted-AI/AIF360/issues/3#issuecomment-522186284","Thanks. Would you publish the code in fair regression paper? ","6","0.4314019314019314","API expansion","Development"
"https://github.com/fairlearn/fairlearn","522594902","issue_comment","https://github.com/Trusted-AI/AIF360/issues/3#issuecomment-522594902","There are multiple algorithms in that paper. Which one are you referring to?","6","0.5321345321345321","API expansion","Development"
"https://github.com/fairlearn/fairlearn","522603241","issue_comment","https://github.com/Trusted-AI/AIF360/issues/3#issuecomment-522603241","Actually, that's a different issue. Can you open a new one so that we can track this properly?","24","0.2316453121822249","UI","Requirement Analysis"
"https://github.com/fairlearn/fairlearn","425201343","issue_comment","https://github.com/Trusted-AI/AIF360/issues/1#issuecomment-425201343","Response received: 

Yes. The least squares classifier is used only as an example (to make test_fairlearn.py deterministic and free of dependencies), but in fact it is only a toy example.

You should be able to directly use any sklearn-style learner as long as it provides ""fit"" (with weights) and ""predict"" methods.

For example, our paper was using the learners
sklearn.linear_model.LogisticRegression(C=0.01)
sklearn.ensemble.GradientBoostingClassifier(learning_rate=0.25,n_estimators=150,random_state=101)

You can just replace LeastSquaresLearner() in test_fairlearn.py with any of these or your own. 

Miro","23","0.3793084552578223","Bias mitigation methodology","Design"
"https://github.com/algofairness/BlackBoxAuditing","491944339","issue_comment","https://github.com/Trusted-AI/AIF360/issues/10#issuecomment-491944339","Hi, thank you for asking the question. We're currently chasing a NeurIPS deadline and won't be able to get to it in the next 2 weeks. Adding better documentation is in our summer plans, though. So Stay Tuned!","20","0.6162594467679213","Opinion","Requirement Analysis"
"https://github.com/algofairness/BlackBoxAuditing","482830208","issue_comment","https://github.com/Trusted-AI/AIF360/issues/9#issuecomment-482830208","I'm uncertain about (the existence of) any other effects, but one effect is that, when kdd is true, the protected column is also changed by the repair. More specifically, with a repair degree of 1, every field in the protected column will be set to a value of 0. If not true, the protected column remains intact.","16","0.3374336144954702","Testing","Maintenance"
"https://github.com/algofairness/BlackBoxAuditing","513717049","issue_comment","https://github.com/Trusted-AI/AIF360/issues/9#issuecomment-513717049","Hi,

I think the following lines from CategoricRepairer.py say that it is actually the other way around:

""
    if self.kdd:
      cols_to_repair = [x for x in col_ids if col_type_dict[x] == ""Y""] 
    else:
      cols_to_repair = [x for x in col_ids if col_type_dict[x] in ""YX""]
""

i.e. if kdd is False the protected columns (indicated by 'X') are also repaired.

I'd be interested to hear if you have noticed any other effects!","17","0.510479241756775","Troubleshooting","Maintenance"
"https://github.com/algofairness/BlackBoxAuditing","368678262","issue_comment","https://github.com/Trusted-AI/AIF360/issues/8#issuecomment-368678262","Hi!

Thanks for letting us know about this. I am having some trouble reproducing your issue with any version of Orange3. Would you mind sending the code that caused the error?

Thank you!","21","0.2850801003667246","Installation and shell commands","Deployment"
"https://github.com/algofairness/BlackBoxAuditing","370270069","issue_comment","https://github.com/Trusted-AI/AIF360/issues/8#issuecomment-370270069","Hi,

Just installing the latest version of Orange3, and importing BlackBoxAuditing results in this error. What version of Orange3 do you have installed?
","21","0.7938219972118276","Installation and shell commands","Deployment"
"https://github.com/algofairness/BlackBoxAuditing","294368696","issue_comment","https://github.com/Trusted-AI/AIF360/issues/5#issuecomment-294368696","License added.","20","0.5504179728317661","Opinion","Requirement Analysis"
"https://github.com/algofairness/BlackBoxAuditing","294369785","issue_comment","https://github.com/Trusted-AI/AIF360/issues/5#issuecomment-294369785","Thanks!","0","0.0303030303030302","Dataset usage","Requirement Analysis"
"https://github.com/algofairness/BlackBoxAuditing","266836236","issue_comment","https://github.com/Trusted-AI/AIF360/issues/3#issuecomment-266836236","repair.py was an incomplete stand alone script to drive the repair procedure. That's why it's missing things. I think main.py has the full and correct code, (modulo the issues you found) but it will need some editing to extract the repair parts. ","13","0.3867776733360189","Artifact generation and benchmarking","Deployment"
"https://github.com/algofairness/BlackBoxAuditing","266812641","issue_comment","https://github.com/Trusted-AI/AIF360/issues/2#issuecomment-266812641","Okay, I missed that the Setup instructions in the readme specifies that one need to change the WEKA_PATH variable so it is not something you aren't aware of already. Though it is still a bad way to go about things because it hinders the workflow for anyone other than the original developer. The changed variable always show up in the upstaged changes in git, you don't want to commit this change but don't want to remove it either.","32","0.4687043488032364","Dependency and Release","Deployment"
"https://github.com/algofairness/BlackBoxAuditing","266754015","issue_comment","https://github.com/Trusted-AI/AIF360/issues/1#issuecomment-266754015","Also, the only version shown on the [website](https://www.tensorflow.org/versions/r0.12/get_started/index.html) are `r0.12`, `r0.11` and `r0.10` but the version specified in the readme is `0.6.0`","24","0.2426686217008797","UI","Requirement Analysis"
"https://github.com/algofairness/BlackBoxAuditing","266761932","issue_comment","https://github.com/Trusted-AI/AIF360/issues/1#issuecomment-266761932","updated, but not tested.","13","0.2027168234064786","Artifact generation and benchmarking","Deployment"
"https://github.com/dssg/aequitas","966996339","issue_comment","https://github.com/Trusted-AI/AIF360/issues/112#issuecomment-966996339","Have you tried converting all records from int/float to string?","17","0.5321345321345321","Troubleshooting","Maintenance"
"https://github.com/dssg/aequitas","919336680","issue_comment","https://github.com/Trusted-AI/AIF360/issues/111#issuecomment-919336680","Sure. Are you just looking for a way to invoke that script from the `pip install …` version of the package then?

Like…?

```sh
$ pip install aequitas
…

$ aequitas-serve
```

(If so, I imagine that wouldn't be too difficult to add….)

---

As for the HOST variable you can always:

```sh
$ HOST=localhost serve.py
```

But sure this could also be a proper command argument :smile: ","4","0.628777682613484","Installation and shell commands","Deployment"
"https://github.com/dssg/aequitas","919346794","issue_comment","https://github.com/Trusted-AI/AIF360/issues/111#issuecomment-919346794","Yes exactly ! And maybe add a default value for the host set to localhost 😃 ","0","0.1824283099450885","Dataset usage","Requirement Analysis"
"https://github.com/dssg/aequitas","919350035","issue_comment","https://github.com/Trusted-AI/AIF360/issues/111#issuecomment-919350035","Sure. Sounds like an enhancement to me.","15","0.5504179728317662","Metrics operation","Validation"
"https://github.com/dssg/aequitas","919360797","issue_comment","https://github.com/Trusted-AI/AIF360/issues/111#issuecomment-919360797","## Technical discussion

1. A CLI command could be added to the Python distribution, no- or little-different from the existing `aequitas-report`.
   * The existing `serve.py` could be included in the distribution via the setup.py `scripts` parameter [[docs]](https://python-packaging.readthedocs.io/en/latest/command-line-scripts.html).
   * …And/or a Python package function could be added to the `aequitas_webapp` package for CLI command invocation (via the `console_scripts` parameter) and optionally via straight Python.
   * …And/or the `aequitas_webapp` package could be given a `__main__.py` such that it is invokable by `python -m aequitas_webapp` (without downloading the source). **Dealer's choice?**
2. The `serve.py` script and/or derivative can read the environment and/or an argument. There's `argparse`. There's `sys.argv`. _Etc._
    ```python
    host = sys.argv[1] if len(sys.argv) > 1 else os.getenv('HOST', default='localhost')
    ```","4","0.661982419616008","Installation and shell commands","Deployment"
"https://github.com/dssg/aequitas","942154887","issue_comment","https://github.com/Trusted-AI/AIF360/issues/111#issuecomment-942154887","> Hello the community !
> 
> I'm using aequitas for the first time today and i was facing with some issues for launching the audit web-app locally.
> 
> To make the command `python -m serve` work, I had to clone the repository and add the following shell environment variable in my terminal `HOST=localhost` because it used in the server.py script.
> 
> I don't know if it was the best way to do it but finally it worked. So i was wondering if it would be possible to update the documentation or to simplify the process for the audit web-app
> 
> Thanks !

hey please, can u help me I am also facing the same problem you can contact me at yajushpratapsingh@gmail.com or my Whatsapp no, please I really need to solve this asap .","4","0.4145054842907602","Installation and shell commands","Deployment"
"https://github.com/dssg/aequitas","942412814","issue_comment","https://github.com/Trusted-AI/AIF360/issues/111#issuecomment-942412814","> ```shell
>  pip install aequitas
> ``` where we have to add these things can u explain it in details,I am too facing problem in hosting Aequitas locally, please help me in using it 

","21","0.515390749601276","Installation and shell commands","Deployment"
"https://github.com/dssg/aequitas","784429270","issue_comment","https://github.com/Trusted-AI/AIF360/issues/104#issuecomment-784429270","Closed by #105","24","0.1507849580138736","UI","Requirement Analysis"
"https://github.com/dssg/aequitas","800603705","issue_comment","https://github.com/Trusted-AI/AIF360/issues/86#issuecomment-800603705","I also have the same problem. 
my data is as follows:
score 0/1
label value 0/1
sex object
race object
age_category object","17","0.727671451355662","Troubleshooting","Maintenance"
"https://github.com/dssg/aequitas","802723024","issue_comment","https://github.com/Trusted-AI/AIF360/issues/86#issuecomment-802723024","Hi

Can you provide a minimal working example?

The following code runs fine for me on the latest aequitas:

```py
import random
import numpy as np
import pandas as pd

n_samples = 1000

df = pd.DataFrame({
    'label_value': (np.random.random((n_samples,)) > 0.95).astype(int),
    'score': (np.random.random((n_samples,)) > 0.90).astype(int),
    'gender': np.array(['M' if random.random() > 0.5 else 'F' for _ in range(n_samples)]),
    'race': np.array(['Caucasian' if random.random() > 0.2 else 'Non-Caucasian' for _ in range(n_samples)]),
    'age_category': np.array([int(random.random() * 4) for _ in range(n_samples)]).astype(str),
})
df.dtypes

from aequitas.group import Group
from aequitas.bias import Bias

attr_cols = list(set(df.columns) - {
    'entity_id', 'score', 'label_value', 'as_of_date'
})

# Initialize aequitas objects
g = Group()
b = Bias()

# Get confusion matrix and metrics for each individual group and attribute
confusion_matrix_metrics, _ = g.get_crosstabs(
    df, attr_cols=attr_cols,
)


bdf = b.get_disparity_predefined_groups(
    confusion_matrix_metrics, original_df=df, 
    ref_groups_dict={
        'race': 'Caucasian',
        'gender': 'M',
        'age_category': '1',
    }, 
    alpha=0.05, check_significance=True, 
    mask_significance=False,
)
bdf.style
```","17","0.7162731795567617","Troubleshooting","Maintenance"
"https://github.com/dssg/aequitas","654268020","issue_comment","https://github.com/Trusted-AI/AIF360/issues/84#issuecomment-654268020","I also looked here:
* https://dssg.github.io/aequitas/_modules/src/aequitas/group.html#Group.get_crosstabs
to find out how to set the score threshold

And here:
* https://developers.google.com/machine-learning/crash-course/classification/true-false-positive-negative
to make sure my definitions for tp, fp, tn, fn are correct","30","0.7433075146079631","Troubleshooting","Maintenance"
"https://github.com/dssg/aequitas","655967502","issue_comment","https://github.com/Trusted-AI/AIF360/issues/84#issuecomment-655967502","The numbers I calculated for predicted positive and predicted negative are also switched in Aequitas:

```
f_pp = df[((df['sex'] == 'Female') & (df['score'] >= t))] # 1248, is pn in Aequitas
f_pn = df[((df['sex'] == 'Female') & (df['score'] < t))] # 147, is pp in Aequitas
```

So when I say ""all those with a score < t are positive"", ""all those with a score >= t are negative"", I get the same results as Aequitas. Same for the other values. So Aequitas basically thinks that when the score is smaller than a threshold it counts as positive and when it is bigger than a threshold it counts as negative. I find this a bit counterintuitive, at least in my use case. Is there any setting or so to change this interpretation?","30","0.696445175595451","Troubleshooting","Maintenance"
"https://github.com/dssg/aequitas","658778875","issue_comment","https://github.com/Trusted-AI/AIF360/issues/84#issuecomment-658778875","- I assume `label_value == 0` is the negative class (""API's classification was not correct""), `label_value == 1` is the positive class (""API's classification was correct"")
- Therefore I thought that score means the probability predicted for the classification being correct (probability for positive class)
- However, the score in Aequitas is actually a score for the classification being incorrect (probability for negative class)

So the solution was to recalculate my score to `1 - score` and set the treshold to `1 - original_treshold`. Any score smaller than the treshold means ""predicted positive"", anything bigger than the treshold means ""predicted negative"".

Now I am getting the correct values for tp etc. from Aequitas.","30","0.4704446084833343","Troubleshooting","Maintenance"
"https://github.com/dssg/aequitas","659962067","issue_comment","https://github.com/Trusted-AI/AIF360/issues/84#issuecomment-659962067","And the part of the code that shows how the treshold is interpreted: 
https://github.com/dssg/aequitas/blob/bb17ce4f305e8a33af3ae18a2a9000555f2e684e/src/aequitas/bias.py#L729
https://github.com/dssg/aequitas/blob/bb17ce4f305e8a33af3ae18a2a9000555f2e684e/src/aequitas/bias.py#L730","3","0.4053030303030305","Bias detection metrics validation","Validation"
"https://github.com/dssg/aequitas","644735179","issue_comment","https://github.com/Trusted-AI/AIF360/issues/78#issuecomment-644735179","Wonderful, glad this was so quickly resolved! We will look into the `pip` versioning issue, thank you for flagging.","20","0.2722385141739981","Opinion","Requirement Analysis"
"https://github.com/dssg/aequitas","652095070","issue_comment","https://github.com/Trusted-AI/AIF360/issues/77#issuecomment-652095070","For example:

```
FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead

    divide = lambda x, y: x / y if y != 0 else pd.np.nan
```","26","0.382516571969697","Bias mitigation methodology","Design"
"https://github.com/dssg/aequitas","678859369","issue_comment","https://github.com/Trusted-AI/AIF360/issues/77#issuecomment-678859369","Should I update the aequita's version in `triage`?","24","0.4039048200122027","UI","Requirement Analysis"
"https://github.com/dssg/aequitas","680000359","issue_comment","https://github.com/Trusted-AI/AIF360/issues/77#issuecomment-680000359","Yes.

On Mon, Aug 24, 2020, 02:31 Adolfo De Unánue <notifications@github.com>
wrote:

> Should I update the aequita's version in triage?
>
> —
> You are receiving this because you modified the open/close state.
> Reply to this email directly, view it on GitHub
> <https://github.com/dssg/aequitas/issues/77#issuecomment-678859369>, or
> unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AALYNBGG2ED4AVNWLYGVJMTSCG7G7ANCNFSM4NZXCYHQ>
> .
>
","7","0.6900765408889483","Opinion","Requirement Analysis"
"https://github.com/dssg/aequitas","488746621","issue_comment","https://github.com/Trusted-AI/AIF360/issues/66#issuecomment-488746621","I don't see the full trace, but I'm guessing this is because aequitas relies on pandas to create the table, and pandas imputes column types?

I would guess the easiest way to ensure these are proper is to pass `dtype={…}`, _e.g._:

```python
data_frame.pg_copy_to(
    'aequitas_group',
    engine,
    dtype={
        'col0': sqlalchemy.Boolean,
        …
    }
)
```","17","0.5590649350649352","Troubleshooting","Maintenance"
"https://github.com/dssg/aequitas","850940215","issue_comment","https://github.com/Trusted-AI/AIF360/issues/66#issuecomment-850940215","Alternatively, you could remove columns that have null values by doing something like 

`data_frame.drop(data_frame.columns[[#fill with col numbers that need to be removed]], axis=1, inplace=True)
`

Another way to remove null values is to use Pandas dataframe dropna( ) function

`new_data_frame = data_frame.dropna()`

doing this will return a new dataframe with the rows/cols with Null/NaN values removed.
More info on pandas dropna() function can be found [here](https://www.journaldev.com/33492/pandas-dropna-drop-null-na-values-from-dataframe)","5","0.5324979381169016","Troubleshooting","Maintenance"
"https://github.com/dssg/aequitas","483698937","issue_comment","https://github.com/Trusted-AI/AIF360/issues/62#issuecomment-483698937","Makes sense to me.

Perhaps and/or: look for a database URI in the process environment, and only fall back to checking a config file, (either the existing config or if also desired a separate database-specific config file). 

The process environment allows for the most flexibility – (you can even still use a secrets file of some kind if you like, and either manually or automatically load it into your environment) – and that interface can help encourage safe-keeping of secrets, (whereas expecting files on disk containing secrets encourages the reverse). 

We ended up implementing this in the triage CLI, for reference. ","4","0.4208933543154583","Installation and shell commands","Deployment"
"https://github.com/dssg/aequitas","483832644","issue_comment","https://github.com/Trusted-AI/AIF360/issues/62#issuecomment-483832644","For LCH reference: https://github.com/dssg/triage/blob/master/src/triage/cli.py","1","0.5269473256050441","Fix warnings","Maintenance"
"https://github.com/dssg/aequitas","510552148","issue_comment","https://github.com/Trusted-AI/AIF360/issues/62#issuecomment-510552148","Let's just put a sample db config file in the repo","4","0.4348729227761489","Installation and shell commands","Deployment"
"https://github.com/dssg/aequitas","481503442","issue_comment","https://github.com/Trusted-AI/AIF360/issues/61#issuecomment-481503442","Thanks Claire, will begin looking into these items!

On Tue, Apr 9, 2019, 08:59 Claire Herdeman <notifications@github.com> wrote:

> I'm working through using Aequitas for a bias audit and am noticing a few
> issues in the documentation:
>
>    -
>
>    The Python API preprocessing input statement is incorrect, it should
>    be from Aequitas.preprocessing import preprocess_input_df, i.e.
>    preprocess_input_df should not be callable
>    -
>
>    The documentation says that attribute columns can be categorical or
>    continuous, this is confusing in terms of when cleaning functions are
>    called in the pre-processing step. For instance I had made age categories
>    using the cut function that returned a categorical datatype, which caused
>    the discretize cleaning function to be called which then threw an error. It
>    should be clear that categorical columns must be converted to strings.
>    -
>
>    Import statements are missing for the Plot and Bias modules; the docs
>    should include from aequitas.bias import Bias and from
>    aequitas.plotting import Plot in those example calls.
>    -
>
>    The call for the visualization of the disparity treemaps doesn't work
>    as is, it should be j = p.plot_disparity_all(.... rather than j =
>    aqp.plot_disparity_all(... to match the earlier use of Plot().
>
> —
> You are receiving this because you are subscribed to this thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/dssg/aequitas/issues/61>, or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AVahKnQX9B8wGtfqUnEkLMceCdNKMHY7ks5vfJzDgaJpZM4ckktN>
> .
>
","17","0.5241113940870336","Troubleshooting","Maintenance"
"https://github.com/dssg/aequitas","483818982","issue_comment","https://github.com/Trusted-AI/AIF360/issues/61#issuecomment-483818982","Updates added to doc_clarity_61 branch: https://github.com/dssg/aequitas/tree/doc_clarity_61, looking into 2nd issue opened","32","0.4018529241459179","Dependency and Release","Deployment"
"https://github.com/dssg/aequitas","466109762","issue_comment","https://github.com/Trusted-AI/AIF360/issues/58#issuecomment-466109762","I'm not sure if the problem is the _assemble_ref_groups or the fact that the plotting all disparities method does not have a model_id parameter
","17","0.4309009376273949","Troubleshooting","Maintenance"
"https://github.com/dssg/aequitas","466598410","issue_comment","https://github.com/Trusted-AI/AIF360/issues/58#issuecomment-466598410","Are you using the multimodel branch?

Thank you,

Loren Hinkson
loren.hinkson@gmail.com
310-703-8500

On Thu, Feb 21, 2019, 12:23 Pedro Saleiro <notifications@github.com> wrote:

> I'm not sure if the problem is the _assemble_ref_groups or the fact that
> the plotting all disparities method does not have a model_id parameter
>
> —
> You are receiving this because you were assigned.
> Reply to this email directly, view it on GitHub
> <https://github.com/dssg/aequitas/issues/58#issuecomment-466109762>, or mute
> the thread
> <https://github.com/notifications/unsubscribe-auth/AVahKglj9F89_E3IPLI8ohqYnFD0__Odks5vPuQEgaJpZM4bICFc>
> .
>
","7","0.4114773080290322","Opinion","Requirement Analysis"
"https://github.com/dssg/aequitas","464140934","issue_comment","https://github.com/Trusted-AI/AIF360/issues/51#issuecomment-464140934","To implement: need to filer FP t-test on predicted positives only and FN t-test on predicted negatives. for label and score we want everything","19","0.3569099062372605","Metrics operation","Validation"
"https://github.com/dssg/aequitas","490622497","issue_comment","https://github.com/Trusted-AI/AIF360/issues/49#issuecomment-490622497","All done (by @saleiro :wink:) – https://pypi.org/project/aequitas/","5","0.5515558267236121","Troubleshooting","Maintenance"
"https://github.com/dssg/aequitas","424738622","issue_comment","https://github.com/Trusted-AI/AIF360/issues/48#issuecomment-424738622","The sample datasets can be used for implementing this.","0","0.2562538133007932","Dataset usage","Requirement Analysis"
"https://github.com/dssg/aequitas","425798308","issue_comment","https://github.com/Trusted-AI/AIF360/issues/48#issuecomment-425798308","I'm unclear on what the output should be.

For example, Dwork et al. focus on using the Lipshitz condition as a constraint on the optimization function.  We want to take the predictions from an unconstrained algorithm and determine how far from the LIpshitz condition it is.

As a first step, would we want two matrices one with distance measured in the feature space and a second with distance measured in the outcome space (from which we could determine a score such as what fraction of the pairs of points fail the conditions)?

Similarly, would the input be the whole feature space for a prediction? This seems necessary, but undesirable, since we would require all the data.","6","0.6256938594786067","API expansion","Development"
"https://github.com/dssg/aequitas","428796907","issue_comment","https://github.com/Trusted-AI/AIF360/issues/48#issuecomment-428796907","@saleiro I've outlined how I think this should work in the issue ticket. Does that outline of functions seem reasonable? I'll start with the clustering?","20","0.6311802232854865","Opinion","Requirement Analysis"
"https://github.com/dssg/aequitas","429373592","issue_comment","https://github.com/Trusted-AI/AIF360/issues/48#issuecomment-429373592","@anisfeld  I suggest that you create a README file within the individual module and outline exactly what you are going to implement. Let's abstract if the features are passed in the df or not. It's up to the final user to decide what she wants to use as representation. They can even pass different dfs based on different representations, train/test splits over time etc...  ","6","0.2854576694782881","API expansion","Development"
"https://github.com/dssg/aequitas","376698429","issue_comment","https://github.com/Trusted-AI/AIF360/issues/10#issuecomment-376698429","it runs for 1 model at a time. it runs for 1 or many parameters depending if threshold is None or loaded from the configs.yaml","16","0.3636363636363634","Testing","Maintenance"
"https://github.com/dssg/aequitas","490215679","issue_comment","https://github.com/Trusted-AI/AIF360/issues/10#issuecomment-490215679","Get crosstabs does not catch multiple model_ids, grouping all records by the model_id passed or default model_id (1). 

To decide: should get crosstabs support multi-model crosstabs, or should this be in a different function called multimodel_crosstabs? Could pass none and then runs for loop on all unique, or could pass list and runs on for loop on subset.","17","0.6854312354312356","Troubleshooting","Maintenance"
"https://github.com/dssg/aequitas","376698721","issue_comment","https://github.com/Trusted-AI/AIF360/issues/2#issuecomment-376698721","it does so depending if thresholds are passed or not. if not it looks for '1's and defines the rank_abs as the count of ones in the score column.","17","0.5327045672866995","Troubleshooting","Maintenance"
"https://github.com/tensorflow/fairness-indicators","951359450","issue_comment","https://github.com/Trusted-AI/AIF360/issues/266#issuecomment-951359450","Closing due to a lack of information describing the issue.","11","0.6039464411557435","Troubleshooting","Maintenance"
"https://github.com/tensorflow/fairness-indicators","817974804","issue_comment","https://github.com/Trusted-AI/AIF360/issues/197#issuecomment-817974804","Looks like this has been resolved.","16","0.1949616648411828","Testing","Maintenance"
"https://github.com/tensorflow/fairness-indicators","696298372","issue_comment","https://github.com/Trusted-AI/AIF360/issues/143#issuecomment-696298372","Thanks for creating the issue.

The Fairness Indicators widget not rendering in Jupyter is a known issue that we are currently working on and tracking to fix.","24","0.4164701772770935","UI","Requirement Analysis"
"https://github.com/tensorflow/fairness-indicators","822835054","issue_comment","https://github.com/Trusted-AI/AIF360/issues/143#issuecomment-822835054","This should be fixed with FI v0.28.0.","6","0.2659352142110764","API expansion","Development"
"https://github.com/tensorflow/fairness-indicators","640884606","issue_comment","https://github.com/Trusted-AI/AIF360/issues/71#issuecomment-640884606","Hi, thank you for your feedback.

This issue should be caused by incompatible version between the tfdv and tfx-bsl. This should be already fixed. Would you mind rerunning the colab to see if the issue has gone?

Thanks!","21","0.4092503987240829","Installation and shell commands","Deployment"
"https://github.com/tensorflow/fairness-indicators","647586465","issue_comment","https://github.com/Trusted-AI/AIF360/issues/71#issuecomment-647586465","I'm also having this issue running a [different notebook](https://github.com/tensorflow/tfx/blob/master/tfx/examples/chicago_taxi_pipeline/taxi_pipeline_interactive.ipynb) on a docker image, but both my tfdv and tfx-bsl versions are 0.22.0. Is there a specific version that they should be?","21","0.5615530303030305","Installation and shell commands","Deployment"
"https://github.com/tensorflow/fairness-indicators","647648404","issue_comment","https://github.com/Trusted-AI/AIF360/issues/71#issuecomment-647648404","Hi kshivvy,

Based [this table](https://github.com/tensorflow/tfx#compatible-versions), the version 0.22.0 of tfdv and tfx-bsl are compatible with each other. I just did a test on same notebook you shared above and it works for me.

Did you install the tfdv and tfx-bsl packages inside the notebook? If so, you might need to restart the notebook to re-import the installed packages.

","21","0.5736879100281164","Installation and shell commands","Deployment"
"https://github.com/tensorflow/fairness-indicators","610041549","issue_comment","https://github.com/Trusted-AI/AIF360/issues/51#issuecomment-610041549","Hi - sorry you're running into an error. I ran the latest version of the fairness indicator widget in jupyter on my local environment and was able to render it successfully.

The [FairnessIndicatorViewer](https://github.com/tensorflow/model-analysis/blob/master/tensorflow_model_analysis/addons/fairness/notebook/jupyter/widget.py#L23) class should not be empty. Perhaps you're using an outdated or incomplete installation. Could you share a link or screenshot of your source code?","29","0.5255411255411255","Troubleshooting","Maintenance"
"https://github.com/tensorflow/fairness-indicators","610043034","issue_comment","https://github.com/Trusted-AI/AIF360/issues/51#issuecomment-610043034","To make sure your installation is up to date, please run the following:

    pip install tensorflow
    pip install tensorflow_data_validation
    pip install tensorflow-model-analysis
    pip install jupyter

    jupyter nbextension enable --py widgetsnbextension --sys-prefix
    jupyter nbextension install --py --symlink tensorflow_model_analysis --sys-prefix
    jupyter nbextension enable --py tensorflow_model_analysis --sys-prefix","29","0.8704925092359648","Troubleshooting","Maintenance"
"https://github.com/tensorflow/fairness-indicators","617645802","issue_comment","https://github.com/Trusted-AI/AIF360/issues/51#issuecomment-617645802","Hey Shuklak!

Thanks for the feedback. I have tried changing the jupyter widget extension command into the following and it works. It turns out to be the problem of permission. 

jupyter nbextension enable --py widgetsnbextension
jupyter nbextension install --py --user tensorflow_model_analysis
jupyter nbextension enable --py tensorflow_model_analysis --user 

","29","0.8386285972492868","Troubleshooting","Maintenance"
"https://github.com/tensorflow/fairness-indicators","647770773","issue_comment","https://github.com/Trusted-AI/AIF360/issues/51#issuecomment-647770773","Hi, I had same problem. I was able to enable the widgetsnbextension, but the install/enabling of tensorflow_model_analysis led to Error message: 

Traceback (most recent call last):
  File ""/usr/local/bin/jupyter-nbextension"", line 11, in <module>
    sys.exit(main())
  File ""/usr/local/lib/python2.7/dist-packages/jupyter_core/application.py"", line 270, in launch_instance
    return super(JupyterApp, cls).launch_instance(argv=argv, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/traitlets/config/application.py"", line 664, in launch_instance
    app.start()
  File ""/usr/local/lib/python2.7/dist-packages/notebook/nbextensions.py"", line 988, in start
    super(NBExtensionApp, self).start()
  File ""/usr/local/lib/python2.7/dist-packages/jupyter_core/application.py"", line 259, in start
    self.subapp.start()
  File ""/usr/local/lib/python2.7/dist-packages/notebook/nbextensions.py"", line 716, in start
    self.install_extensions()
  File ""/usr/local/lib/python2.7/dist-packages/notebook/nbextensions.py"", line 695, in install_extensions
    **kwargs
  File ""/usr/local/lib/python2.7/dist-packages/notebook/nbextensions.py"", line 211, in install_nbextension_python
    m, nbexts = _get_nbextension_metadata(module)
  File ""/usr/local/lib/python2.7/dist-packages/notebook/nbextensions.py"", line 1122, in _get_nbextension_metadata
    m = import_item(module)
  File ""/usr/local/lib/python2.7/dist-packages/traitlets/utils/importstring.py"", line 42, in import_item
    return __import__(parts[0])
ImportError: No module named tensorflow_model_analysis","16","0.4982162343658188","Testing","Maintenance"
"https://github.com/adebayoj/fairml","634816393","issue_comment","https://github.com/Trusted-AI/AIF360/issues/13#issuecomment-634816393","Hi @Dola47, thanks for raising this issue. I'll double check your example. I am currently in the midst of a deadline right now, so I'll get to this in about a week. 

Overall, this library is going to undergo an overhaul over the summer, so keep an eye out. 

Thanks! ","20","0.704782196969697","Opinion","Requirement Analysis"
"https://github.com/adebayoj/fairml","511440646","issue_comment","https://github.com/Trusted-AI/AIF360/issues/11#issuecomment-511440646","Hi, can you tell me what you are trying to do with plot_generic_dependence_dictionary? That will let me clarify how to help. 

thanks","9","0.7233262861169837","Feature engineering methodology","Design"
"https://github.com/adebayoj/fairml","511464871","issue_comment","https://github.com/Trusted-AI/AIF360/issues/11#issuecomment-511464871","Hi Julius, I was trying to replicate your code demo.
","9","0.4531218222493391","Feature engineering methodology","Design"
"https://github.com/adebayoj/fairml","539166066","issue_comment","https://github.com/Trusted-AI/AIF360/issues/11#issuecomment-539166066","Hi,

I got the same error when y try to reproduce what you do in your demo code. ","29","0.2808623144193614","Troubleshooting","Maintenance"
"https://github.com/adebayoj/fairml","539166265","issue_comment","https://github.com/Trusted-AI/AIF360/issues/11#issuecomment-539166265","![image](https://user-images.githubusercontent.com/45678322/66342166-c7136480-e916-11e9-8cf4-e135271f81a8.png)
","18","0.3716684921504196","Troubleshooting","Maintenance"
"https://github.com/adebayoj/fairml","539184861","issue_comment","https://github.com/Trusted-AI/AIF360/issues/11#issuecomment-539184861","Checking the files inside the repo, i realised that the issue is due to an error in the following line:

Is: 
from fairml import plot_dependencies

Instead of:
from fairml import plot_generic_dependence_dictionary

Thanks!","9","0.4635328439676267","Feature engineering methodology","Design"
"https://github.com/adebayoj/fairml","493287100","issue_comment","https://github.com/Trusted-AI/AIF360/issues/10#issuecomment-493287100","Hi Sethchandler, thank you for reaching out. Apologies for the late reply. Yes, you can use the data.","20","0.4504580690627203","Opinion","Requirement Analysis"
"https://github.com/adebayoj/fairml","404121148","issue_comment","https://github.com/Trusted-AI/AIF360/issues/8#issuecomment-404121148","Also, in line 17 of perturbation_strategies.py, random_sample[random_sample] should be replaced by random_sample[column_number]","9","0.4960925039872408","Feature engineering methodology","Design"
"https://github.com/adebayoj/fairml","403989137","issue_comment","https://github.com/Trusted-AI/AIF360/issues/7#issuecomment-403989137","Hi Kazemia,
Thanks for checking out the work. Here we only have implementations for 1 actually. We didn't include the implementations for 2-4 because these are all open source packages that can be used. The key new contribution of our work is the IOFP algorithm, so we assumed (probably wrongly) that people can find the implementations of the other methods publicly. For example, 3-4 come from the sklearn package.  For 2 (mRMR) there is this: https://github.com/fbrundu/pymrmr. 

We will be revamping this repo in about a month with significant updates that will include 2-4 as interfaces as well for replication.  Our updates will clear up this confusion. Happy to answer more questions. 
","23","0.2411269398345325","Bias mitigation methodology","Design"
"https://github.com/adebayoj/fairml","404106305","issue_comment","https://github.com/Trusted-AI/AIF360/issues/7#issuecomment-404106305","Thank you very much adebayoj. That made sense :)","26","0.231645312182225","Bias mitigation methodology","Design"
"https://github.com/adebayoj/fairml","286227668","issue_comment","https://github.com/Trusted-AI/AIF360/issues/4#issuecomment-286227668","Hi @achimkoh, you are right. I'll push an update shortly to handle the issues that you mentioned. I'll also update the readme to reflect the changes. Thanks for the issues, please free feel to update us on any other issues you have. 

thanks!","24","0.7886227247929377","UI","Requirement Analysis"
"https://github.com/adebayoj/fairml","298078160","issue_comment","https://github.com/Trusted-AI/AIF360/issues/4#issuecomment-298078160","Hi thanks for the reply. I had a few minutes and made a pull request so you can have a look!","20","0.3992952783650458","Opinion","Requirement Analysis"
"https://github.com/adebayoj/fairml","304508840","issue_comment","https://github.com/Trusted-AI/AIF360/issues/4#issuecomment-304508840","This would be great to have as an IPython notebook. IMO the more examples the better. If I find time I can do one.","25","0.5087976539589445","Research","Requirement Analysis"
"https://github.com/adebayoj/fairml","286227195","issue_comment","https://github.com/Trusted-AI/AIF360/issues/3#issuecomment-286227195","@afiodorov  thanks for taking a look at the code and for your feedback. 

1) you are right. with the constant-zero and median strategies, the loop is redundant. I plan on separating the direct perturbation code from overall method to make it so that the run is independent. I am testing a local branch atm that handles this. Will push a fix up later this wk. 

2) The purpose of direction perturbation. This is also a good question, and you are right, it is not explained in the thesis. We are working on suitable documentation to fully explain the overall issue. 

For now here is the justification for including direction perturbation: if you have a function f(x_1, x_2, x_2). What, fairml does on a high level is to give you the dependence of f on each of the x_i. Now the dependence is calculated as direct influence + indirect influence.  For direct influence, we generate a data transformation using any of the different direct perturbation strategies and then look at the impact of the black-box function on that perturbation. For the indirect influence, we use orthogonal transformation to generate those transformations. 

Certainly, we could just use orthogonal transformation on all variables including, but wanted to give people flexibility to pick whatever function that they are interested in using for this task. Hope this helps explain the use of the direct-perturbation strategy requirement. 
","9","0.6177114876618778","Feature engineering methodology","Design"
"https://github.com/adebayoj/fairml","285436740","issue_comment","https://github.com/Trusted-AI/AIF360/issues/2#issuecomment-285436740","@yinleon thanks for the note. doing a quick check. ","20","0.1694971694971694","Opinion","Requirement Analysis"
"https://github.com/adebayoj/fairml","285437548","issue_comment","https://github.com/Trusted-AI/AIF360/issues/2#issuecomment-285437548","@adebayoj LGTM 💯 ","26","0.5504179728317662","Bias mitigation methodology","Design"
"https://github.com/cosmicBboy/themis-ml","494489764","issue_comment","https://github.com/Trusted-AI/AIF360/issues/41#issuecomment-494489764","@srayagarwal can you post a minimally reproducible example?","29","0.7335997335997337","Troubleshooting","Maintenance"
"https://github.com/cosmicBboy/themis-ml","645335253","issue_comment","https://github.com/Trusted-AI/AIF360/issues/41#issuecomment-645335253","For example running the tutorial, as it is, gives this error when the fit function is called!
https://github.com/cosmicBboy/themis-ml/blob/master/examples/tutorial_fat*_2018.ipynb","23","0.3319360754519255","Bias mitigation methodology","Design"
"https://github.com/cosmicBboy/themis-ml","823161456","issue_comment","https://github.com/Trusted-AI/AIF360/issues/41#issuecomment-823161456","Hi i'm facing the same issue, did you guys solve it?","21","0.2903762903762904","Installation and shell commands","Deployment"
"https://github.com/cosmicBboy/themis-ml","326412412","issue_comment","https://github.com/Trusted-AI/AIF360/issues/9#issuecomment-326412412","For residuals on binary input variables, we have to use either deviance of pearson residuals.
See these resources for implementation details:

- https://web.as.uky.edu/statistics/users/pbreheny/760/S11/notes/4-12.pdf
- https://www.stat.ubc.ca/~rollin/teach/536w10/lec16.pdf
- http://data.princeton.edu/wws509/notes/c3s8.html","9","0.5313468716391474","Feature engineering methodology","Design"
"https://github.com/cosmicBboy/themis-ml","325856219","issue_comment","https://github.com/Trusted-AI/AIF360/issues/3#issuecomment-325856219","fixed by https://github.com/cosmicBboy/themis-ml/commit/e1990e67d62b844c6836e58377b1c59211d396e1 ","30","0.4871995820271684","Troubleshooting","Maintenance"
"https://github.com/cosmicBboy/themis-ml","325524151","issue_comment","https://github.com/Trusted-AI/AIF360/issues/1#issuecomment-325524151","fixed by https://github.com/cosmicBboy/themis-ml/commit/e9ad17a18a04c7a36eecf549e507a7611e481b21","11","0.2659352142110763","Troubleshooting","Maintenance"
"https://github.com/predictive-analytics-lab/EthicML","860569760","issue_comment","https://github.com/Trusted-AI/AIF360/issues/435#issuecomment-860569760","What's your thinking? git large file storage, or maybe DVC? Or download the files the first time that they are used?","21","0.6704304188380624","Installation and shell commands","Deployment"
"https://github.com/predictive-analytics-lab/EthicML","860572509","issue_comment","https://github.com/Trusted-AI/AIF360/issues/435#issuecomment-860572509","Ah, you misunderstand. We just have to tell poetry to exclude `ethicml/data/csvs/raw`. Those files are not actually needed. There is a processed version of `health.csv` in `ethicml/data/csvs` which is all that's needed.","13","0.3513556618819776","Artifact generation and benchmarking","Deployment"
"https://github.com/predictive-analytics-lab/EthicML","860573644","issue_comment","https://github.com/Trusted-AI/AIF360/issues/435#issuecomment-860573644","I assume it's as easy as
```
exclude = [""ethicml/data/csvs/raw/*""]
```

(source: https://python-poetry.org/docs/pyproject/#include-and-exclude)","15","0.3757331378299122","Metrics operation","Validation"
"https://github.com/predictive-analytics-lab/EthicML","860801421","issue_comment","https://github.com/Trusted-AI/AIF360/issues/435#issuecomment-860801421","It's back down to under 38MB:

![image](https://user-images.githubusercontent.com/7741417/121922572-015f1700-cd32-11eb-8483-e1a68ffc4005.png)

🎉 ","24","0.3792963188936344","UI","Requirement Analysis"
"https://github.com/predictive-analytics-lab/EthicML","840611352","issue_comment","https://github.com/Trusted-AI/AIF360/issues/431#issuecomment-840611352","Also, computing the basic metrics FPR, TPR, FNR, TNR etc... results in error as the labels are calculated implicitly based on the y_true (actual labels). This should be asked explicitly from the user, as it is possible for a subset to leave out some class label which results in sci-kit learn's error.

```
def confusion_matrix(
    prediction: Prediction, actual: DataTuple, pos_cls: int
) -> Tuple[int, int, int, int]:
    """"""Apply sci-kit learn's confusion matrix.""""""
    actual_y: np.ndarray = actual.y.to_numpy(dtype=np.int32)
    labels: np.ndarray = np.unique(actual_y)
    if labels.size == 1:
        labels = np.array([0, 1], dtype=np.int32)
    conf_matr: np.ndarray = conf_mtx(y_true=actual_y, y_pred=prediction.hard, labels=labels)
```
","19","0.7929458986293183","Metrics operation","Validation"
"https://github.com/predictive-analytics-lab/EthicML","841833962","issue_comment","https://github.com/Trusted-AI/AIF360/issues/431#issuecomment-841833962","Hi @Mahima-ai - thanks for raising an issue. These models should definitely get fixed if they're not right! Sorry if this has caused you problems, we just haven't got any datasets in EthicML where we do multi-class classification so we haven't really thought about it very much. That said, it is something I want us to do right - FairML work in general is far too focussed on binary classification problems. Do you have a dataset that we should add? (I understand that this is a long shot - datasets are not so open)

You a good point about the confusion matrix based metrics. One option is for the user to specify the label values, but as this is a property of the dataset, it would be nice if this information could be taken from there directly.

If you'd like to open a PR, you're more than welcome. Alternatively I will try to address this. Looking through the tests, we don't have enough that use the `non_binary_toy` dataset.

Sorry again for causing you problems, and thanks by the way, for using EthicML","0","0.3493931663574521","Dataset usage","Requirement Analysis"
"https://github.com/predictive-analytics-lab/EthicML","1010705666","issue_comment","https://github.com/Trusted-AI/AIF360/issues/431#issuecomment-1010705666","Hi @olliethomas,

I looked into this further and found that the `metric_per_sensitive_attribute` method creates a subset of the dataset. This subset has fewer rows than the actual dataset. Here, the number of unique labels differ, it gets reduced. This results in the `LabelOutOfBounds` Error or `ValueError`. The method definition is at https://github.com/predictive-analytics-lab/EthicML/blob/main/ethicml/metrics/per_sensitive_attribute.py#L23
```
def metric_per_sensitive_attribute(
    prediction: Prediction, actual: DataTuple, metric: Metric, use_sens_name: bool = True
) -> Dict[str, float]:
```
I am attaching a notebook (in the zip folder) on iris dataset to reproduce the same scenario for your perusal. 
[Multiclass Classification Support.zip](https://github.com/predictive-analytics-lab/EthicML/files/7852371/Multiclass.Classification.Support.zip)

","0","0.3586318953611182","Dataset usage","Requirement Analysis"
"https://github.com/predictive-analytics-lab/EthicML","1011004347","issue_comment","https://github.com/Trusted-AI/AIF360/issues/431#issuecomment-1011004347","Thanks for the notebook - I've tweaked it slightly and moved it to colab. It's available [here](https://colab.research.google.com/drive/1d-86IzPIALfg4lPlapqKym-Xi5dvPKGb?usp=sharing)

The error that you are seeing was due to the possible target values being inferred from the dataset. 

There are actually two solutions to this.

The first, I've added in #489 - which is the option to define the labels for a metric. so `em.TPR(pos_class=1)` becomes `em.TPR(pos_class=1, labels=[0,1,2])`. This new parameter has the default value of `None`, and when `None` is passed, the default behaviour of inferring the labels remains.

The second solution is probably closer to what most users want to do. The problem is the `s` definition from the block 
```
true_data=em.DataTuple(x=pd.DataFrame(data['sepal length (cm)']),
                       s=pd.DataFrame(data['sepal length (cm)']),
                       y=pd.DataFrame(data['target']))
```
What is happening here is that the protected attribute, `s`, is being set to a float where the number of unique `s`-values is, in this case, 35. The number of unique `y`-values is 3.
Given that the dataset is 150 samples, the chance of all targets being present in every unique s-group is low.

In the case of the notebook - if you set the sensitive attribute to some binary value, then everything works without having to define the labels.
```
true_data=em.DataTuple(x=pd.DataFrame(data['sepal length (cm)']),
                       s=pd.DataFrame(np.random.randint(0,2,size=(len(data), 1)), columns=[""s""]),
                       y=pd.DataFrame(data['target']))
```

Hopefully this solve the issue for now - we'll make a new release soon. If I've misunderstood, or this doesn't solve the problem, please let me know. Thanks again.","26","0.271658730405735","Bias mitigation methodology","Design"
"https://github.com/predictive-analytics-lab/EthicML","798669116","issue_comment","https://github.com/Trusted-AI/AIF360/issues/423#issuecomment-798669116","I know we've spoken about this before.

Conversation from 24th November 2020

> oliver  15:40
maybe 0/0 should be 0 :man-shrugging:
thomas_k  15:40
I think it’s more informative to report this as nan

and I know we'd discussed it before then too.... Maybe. we should just make it 0","24","0.2911401005571409","UI","Requirement Analysis"
"https://github.com/predictive-analytics-lab/EthicML","798775114","issue_comment","https://github.com/Trusted-AI/AIF360/issues/423#issuecomment-798775114","hm, I see... maybe my past self was right.","11","0.2027168234064787","Troubleshooting","Maintenance"
"https://github.com/predictive-analytics-lab/EthicML","738857332","issue_comment","https://github.com/Trusted-AI/AIF360/issues/403#issuecomment-738857332","there's actually a few variations on a theme here. There's

http://chalearnlap.cvc.uab.es/dataset/36/description/

and also, http://chalearnlap.cvc.uab.es/dataset/24/description/# 
","4","0.5522810522810521","Installation and shell commands","Deployment"
"https://github.com/predictive-analytics-lab/EthicML","738857479","issue_comment","https://github.com/Trusted-AI/AIF360/issues/403#issuecomment-738857479","I'm sure that there might be more here too","24","0.1507849580138736","UI","Requirement Analysis"
"https://github.com/predictive-analytics-lab/EthicML","681819145","issue_comment","https://github.com/Trusted-AI/AIF360/issues/380#issuecomment-681819145","Would be good to do #347 at the same time / just before","20","0.2659352142110764","Opinion","Requirement Analysis"
"https://github.com/predictive-analytics-lab/EthicML","1012107152","issue_comment","https://github.com/Trusted-AI/AIF360/issues/371#issuecomment-1012107152","done in #465 ","0","0.0303030303030302","Dataset usage","Requirement Analysis"
"https://github.com/predictive-analytics-lab/EthicML","674087353","issue_comment","https://github.com/Trusted-AI/AIF360/issues/354#issuecomment-674087353","Works on master.","32","0.3291536050156741","Dependency and Release","Deployment"
"https://github.com/predictive-analytics-lab/EthicML","674013604","issue_comment","https://github.com/Trusted-AI/AIF360/issues/353#issuecomment-674013604","Developing this further, the mapping could be specified like this:

```python
class SensSpec(NamedTuple):
    """"""Specification for a sensitive attribute.""""""

    multiplier: int
    columns: List[str]


def adult_sex_race() -> Dataset:
    s_mapping = {
        ""race"": SensSpec(multiplier=1, columns=[
            ""race_Amer-Indian-Eskimo"",
            ""race_Asian-Pac-Islander"",
            ""race_Black"",
            ""race_Other"",
            ""race_White"",
        ]),
        ""sex"": SensSpec(multiplier=5, columns=[""Sex_Male""]),
    }
    return Dataset(
        name=""Adult, race-sex"",
        # ...
        s_mapping=s_mapping,
    )
```

Then, `Dataset.load()` would do something like this:

```python
def load(self):
    # ...
    s = pd.DataFrame(np.zeros())
    for _, spec in self.s_mapping.items():
        values = undo_one_hot(all_data[spec.columns])
        s += spec.multiplier * values
    # ...
```

The `undo_one_hot()` is needed in order to combine all relevant columns into one number which we can then multiply with the multiplier.

The inverse mapping is a bit trickier.

```python
def undo_s_mapping(s: pd.Series, s_mapping: Dict[str, SensSpec]):
    final_df = {}
    for sens, spec in s_mapping.items():
        value = (s % (spec.multiplier + 1)) // spec.multiplier
        restored = one_hot(value)
        restored.columns = spec.columns
        final_df[sens] = restored  # for the multi-level column index

    return pd.concat(final_df, axis=1)
```","31","0.4575139146567717","Testing","Maintenance"
"https://github.com/predictive-analytics-lab/EthicML","644818956","issue_comment","https://github.com/Trusted-AI/AIF360/issues/344#issuecomment-644818956","One problem with this is that the dictionary is useful for logging to W&B. (I'm saying this as the one who left the suggestion in the code.)","15","0.5403860670764584","Metrics operation","Validation"
"https://github.com/predictive-analytics-lab/EthicML","676126223","issue_comment","https://github.com/Trusted-AI/AIF360/issues/338#issuecomment-676126223","This was done in #356, right?","0","0.0303030303030302","Dataset usage","Requirement Analysis"
"https://github.com/predictive-analytics-lab/EthicML","676127978","issue_comment","https://github.com/Trusted-AI/AIF360/issues/338#issuecomment-676127978","yes it was!","0","0.0303030303030302","Dataset usage","Requirement Analysis"
"https://github.com/predictive-analytics-lab/EthicML","598738504","issue_comment","https://github.com/Trusted-AI/AIF360/issues/302#issuecomment-598738504","https://github.com/yerevann/mimic3-benchmarks","9","0.3716684921504203","Feature engineering methodology","Design"
"https://github.com/predictive-analytics-lab/EthicML","598738627","issue_comment","https://github.com/Trusted-AI/AIF360/issues/302#issuecomment-598738627","https://mimic.physionet.org/gettingstarted/access/","5","0.3274917853231103","Troubleshooting","Maintenance"
"https://github.com/predictive-analytics-lab/EthicML","598738173","issue_comment","https://github.com/Trusted-AI/AIF360/issues/301#issuecomment-598738173","https://www.kaggle.com/wordsforthewise/lending-club","13","0.3274917853231109","Artifact generation and benchmarking","Deployment"
"https://github.com/predictive-analytics-lab/EthicML","599113603","issue_comment","https://github.com/Trusted-AI/AIF360/issues/291#issuecomment-599113603","Has this been added?","20","0.1066082511865644","Opinion","Requirement Analysis"
"https://github.com/predictive-analytics-lab/EthicML","599124904","issue_comment","https://github.com/Trusted-AI/AIF360/issues/291#issuecomment-599124904","I think Myles considers this to be the implementation: #293 . But it's kind of only one part.","30","0.552783650458069","Troubleshooting","Maintenance"
"https://github.com/predictive-analytics-lab/EthicML","589439483","issue_comment","https://github.com/Trusted-AI/AIF360/issues/290#issuecomment-589439483","Some things could even be moved to the base module directly. For example `evaluate_models` and `load_data`. Then you could write code like this:

```python
import ethicml as eml

results = eml.evaluate_models(
    datasets=[eml.data.Adult()],
    inprocess_models=[eml.algo.intra.Kamiran()],
    preprocess_models=[eml.algo.pre.Upsampler()],
    metrics=[eml.metrics.Accuracy()],
    per_sens_attribute=[eml.metrics.ProbPos()],
)
eml.plot.plot_results(results, ""Accuracy"", ""prob_pos_Male_0/Male_1"")
```","28","0.3258158508158509","Bias mitigation methodology","Design"
"https://github.com/predictive-analytics-lab/EthicML","680835167","issue_comment","https://github.com/Trusted-AI/AIF360/issues/290#issuecomment-680835167","closed by #367 ","24","0.1507849580138736","UI","Requirement Analysis"
"https://github.com/predictive-analytics-lab/EthicML","676135185","issue_comment","https://github.com/Trusted-AI/AIF360/issues/254#issuecomment-676135185","black has been added as a pre-commit hook, which myles has installed.","21","0.4348729227761487","Installation and shell commands","Deployment"
"https://github.com/predictive-analytics-lab/EthicML","574357797","issue_comment","https://github.com/Trusted-AI/AIF360/issues/229#issuecomment-574357797","See #246 for an implementation of this proposal.","15","0.3607628004179729","Metrics operation","Validation"
"https://github.com/predictive-analytics-lab/EthicML","549338676","issue_comment","https://github.com/Trusted-AI/AIF360/issues/225#issuecomment-549338676","It might be as simple as doing

```python
from fairlearn.reductions import ExponentiatedGradients
```

instead.","10","0.296969696969697","Model development","Development"
"https://github.com/predictive-analytics-lab/EthicML","566599274","issue_comment","https://github.com/Trusted-AI/AIF360/issues/225#issuecomment-566599274","@thomkeh @olliethomas  we have converted fairlearn into a package that provides a variety of unfairness  mitigation techniques as opposed to just the ExponentiatedGradient method. It's actually similar to ethicml in terms of objective. If you're interested in collaborating on a single effort we'd be more than happy to discuss.","6","0.3573555202088177","API expansion","Development"
"https://github.com/predictive-analytics-lab/EthicML","677622037","issue_comment","https://github.com/Trusted-AI/AIF360/issues/223#issuecomment-677622037","Although they don't give a reference, I think that they mean this dataset https://archive.ics.uci.edu/ml/datasets/Diabetes+130-US+hospitals+for+years+1999-2008#","0","0.532134532134532","Dataset usage","Requirement Analysis"
"https://github.com/predictive-analytics-lab/EthicML","533891277","issue_comment","https://github.com/Trusted-AI/AIF360/issues/170#issuecomment-533891277","I solved what I wanted to solve in #201 ","24","0.3607628004179731","UI","Requirement Analysis"
"https://github.com/predictive-analytics-lab/EthicML","676132760","issue_comment","https://github.com/Trusted-AI/AIF360/issues/129#issuecomment-676132760","The idea was that this would be a pre-cursor for multiple S. However, we didn't go down this route and went as proposed in #353 which was implemented in #350 and #358 ","15","0.7305718475073317","Metrics operation","Validation"
"https://github.com/predictive-analytics-lab/EthicML","508765019","issue_comment","https://github.com/Trusted-AI/AIF360/issues/125#issuecomment-508765019","We could also maybe shorten `ethicml.algorithms.preprocess` to `ethicml.algos.pre` and so on.","19","0.5023388244864757","Metrics operation","Validation"
"https://github.com/predictive-analytics-lab/EthicML","514251878","issue_comment","https://github.com/Trusted-AI/AIF360/issues/125#issuecomment-514251878","Done in #128 ","0","0.0303030303030302","Dataset usage","Requirement Analysis"
"https://github.com/predictive-analytics-lab/EthicML","519477884","issue_comment","https://github.com/Trusted-AI/AIF360/issues/116#issuecomment-519477884","It turns out that you cannot store dataframes with hierarchical columns in a feather file. So this idea seems not to be feasible.","17","0.4693039909178235","Troubleshooting","Maintenance"
"https://github.com/predictive-analytics-lab/EthicML","598977944","issue_comment","https://github.com/Trusted-AI/AIF360/issues/116#issuecomment-598977944","Done in #305 and #307 ","0","0.0303030303030302","Dataset usage","Requirement Analysis"
"https://github.com/predictive-analytics-lab/EthicML","483710423","issue_comment","https://github.com/Trusted-AI/AIF360/issues/67#issuecomment-483710423","Addressed in PR #72 ","20","0.3923719958202718","Opinion","Requirement Analysis"
"https://github.com/predictive-analytics-lab/EthicML","465693202","issue_comment","https://github.com/Trusted-AI/AIF360/issues/16#issuecomment-465693202","Then the type of data will be `data: NamedTuple[str, pd.DataFrame]` ?","17","0.2975444096133753","Troubleshooting","Maintenance"
"https://github.com/predictive-analytics-lab/EthicML","466376408","issue_comment","https://github.com/Trusted-AI/AIF360/issues/16#issuecomment-466376408","Done in #22 ","0","0.0303030303030302","Dataset usage","Requirement Analysis"
"https://github.com/predictive-analytics-lab/EthicML","464089748","issue_comment","https://github.com/Trusted-AI/AIF360/issues/9#issuecomment-464089748","More info on Parquet:

> Parquet is designed to faithfully serialize and de-serialize `DataFrame` s, supporting all of the pandas dtypes, including extension dtypes such as datetime with tz.

> Several caveats.

> * Duplicate column names and non-string columns names are not supported.
> * Index level names, if specified, must be strings.
> * Categorical dtypes can be serialized to parquet, but will de-serialize as `object` dtype.
> * Non supported types include `Period` and actual Python object types. These will raise a helpful error message on an attempt at serialization.

http://pandas.pydata.org/pandas-docs/stable/user_guide/io.html#io-parquet","17","0.795658016769822","Troubleshooting","Maintenance"
"https://github.com/predictive-analytics-lab/EthicML","464095337","issue_comment","https://github.com/Trusted-AI/AIF360/issues/9#issuecomment-464095337","I think this looks like a good idea. If we want to be able to have many jobs over the HPC then this will become necessary so best to get it in early","25","0.552783650458069","Research","Requirement Analysis"
"https://github.com/predictive-analytics-lab/EthicML","466385352","issue_comment","https://github.com/Trusted-AI/AIF360/issues/9#issuecomment-466385352","this has been started in #17 . closing this.","11","0.1711076280041797","Troubleshooting","Maintenance"
"https://github.com/Nathanlauga/transparentai","626331935","issue_comment","https://github.com/Trusted-AI/AIF360/issues/4#issuecomment-626331935","depreciated function","5","0.4555903866248697","Troubleshooting","Maintenance"
"https://github.com/Nathanlauga/transparentai","626331904","issue_comment","https://github.com/Trusted-AI/AIF360/issues/3#issuecomment-626331904","depreciated function","5","0.4555903866248696","Troubleshooting","Maintenance"
"https://github.com/Nathanlauga/transparentai","626331877","issue_comment","https://github.com/Trusted-AI/AIF360/issues/2#issuecomment-626331877","depreciated function","5","0.5504179728317662","Troubleshooting","Maintenance"
"https://github.com/Nathanlauga/transparentai","626331849","issue_comment","https://github.com/Trusted-AI/AIF360/issues/1#issuecomment-626331849","depreciated function","5","0.4871995820271684","Troubleshooting","Maintenance"
